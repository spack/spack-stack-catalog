spack:
  view: false

  concretizer:
    reuse: false
    unify: false

  compilers:
  - compiler:
      spec: gcc@11.2.0
      paths:
        cc: gcc
        cxx: g++
        f77: gfortran
        fc: gfortran
      flags: {}
      operating_system: sles15
      target: any
      modules:
      - PrgEnv-gnu
      - gcc/11.2.0
      - craype-x86-milan
      - libfabric

  packages:
    all:
      compiler: [gcc@11.2.0]
      providers:
        blas: [cray-libsci]
        mpi: [cray-mpich]
      target: [zen3]
      variants: +mpi
    binutils:
      variants: +ld +gold +headers +libiberty ~nls
    elfutils:
      variants: +bzip2 ~nls +xz
    hdf5:
      variants: +fortran +hl +shared
    libunwind:
      variants: +pic +xz
    ncurses:
      variants: +termlib
    openblas:
      variants: threads=openmp
    python:
      version: [3.8.13]
    trilinos:
      variants: +amesos +amesos2 +anasazi +aztec +belos +boost +epetra +epetraext
        +ifpack +ifpack2 +intrepid +intrepid2 +isorropia +kokkos +ml +minitensor +muelu
        +nox +piro +phalanx +rol +rythmos +sacado +stk +shards +shylu +stokhos +stratimikos
        +teko +tempus +tpetra +trilinoscouplings +zoltan +zoltan2 +superlu-dist gotype=long_long
    xz:
      variants: +pic

    # EXTERNALS
    cray-libsci:
      buildable: false
      externals:
      - spec: cray-libsci@21.08.1.2
        modules:
        - cray-libsci/21.08.1.2
    cray-mpich:
      # cray-mpich externals need to define prefix
      buildable: false
      externals:
      - spec: cray-mpich@8.1.17 %gcc
        prefix: /opt/cray/pe/mpich/8.1.17/ofi/gnu/9.1
        modules:
        - cray-mpich/8.1.17
        - cudatoolkit/11.5
    libfabric:
      buildable: false
      variants: fabrics=sockets,tcp,udp,rxm
      externals:
      - spec: libfabric@1.15.0.0
        prefix: /opt/cray/libfabric/1.15.0.0
        modules:
        - libfabric
    openssl:
      version: [1.1.0i]
      buildable: false
      externals:
      - spec: openssl@1.1.0
        prefix: /usr
    openssh:
      version: [7.9p1]
      buildable: false
      externals:
      - spec: openssh@7.9p1
        prefix: /usr
    slurm:
      buildable: false
      version: [20-11-8-1]
      externals:
      - spec: slurm@20-11-8-1
        prefix: /usr

    # SITE VARIANT/VERSION PREFERENCES
    mesa:
      variants: +osmesa~glx

  modules:
    prefix_inspections:
      ./lib:
      - LD_LIBRARY_PATH
      ./lib64:
      - LD_LIBRARY_PATH
    default:
      'enable:': [lmod]
      lmod:
        defaults:
        - cray-mpich@8.1.17
        core_compilers: [gcc@11.2.0]
        blacklist_implicits: true
        verbose: true
        hash_length: 0
        whitelist: [cray-mpich, cmake^ncurses@6.3]
        hierarchy: [mpi]
        projections: {}
        core_specs: []
        all:
          autoload: direct
          environment:
            set:
              ${PACKAGE}_ROOT: ${PREFIX}
          suffixes:
            +cuda cuda_arch=70: cuda70
            +cuda cuda_arch=80: cuda80
            +openmp: openmp
        cabana:
          suffixes:
            ^kokkos +cuda cuda_arch=70: cuda70
            ^kokkos +cuda cuda_arch=80: cuda80
        tau:
          suffixes:
            +cuda: cuda
        hpctoolkit:
          suffixes:
            +cuda: cuda
        bricks:
          suffixes:
            +cuda: cuda
        flux-core:
          suffixes:
            +cuda: cuda
        papi:
          suffixes:
            +cuda: cuda
        mpich:
          suffixes:
            ^hwloc+cuda: hwloc-cuda
            ^ncurses@6.3: ncurses6.3
            ^ncurses@6.2: ncurses6.2
        py-warpx:
          suffixes:
            ^warpx dims=2: dims2
            ^warpx dims=3: dims3
            ^warpx dims=rz: dimsRZ

  specs:
  # CPU
  - adios@1.13.1
  - adios2@2.8.3
  - aml@0.2.0
  - amrex@22.08
  - arborx@1.2
  - archer@2.0.0
  - argobots@1.1
  - ascent@0.8.0
  - axom@0.6.1
  - bolt@2.0
  - butterflypack@2.1.1
  - cabana@0.5.0
  - caliper@2.8.0
  - chai@2022.03.0 ~benchmarks ~tests
  - charliecloud@0.27
  - conduit@0.8.3
  - darshan-runtime@3.4.0
  - darshan-util@3.4.0
  - datatransferkit@3.1-rc3
  - dyninst@12.2.0
  - exaworks@0.1.0
  - faodel@1.2108.1
  - flecsi@1.4.2
  - flit@2.1.0
  - flux-core@0.40.0
  - fortrilinos@2.0.0
  - gasnet@2022.3.0
  - ginkgo@1.4.0
  - globalarrays@5.8
  - gmp@6.2.1
  - gotcha@1.0.3
  - gptune@3.0.0
  - hdf5-vol-async@1.2
  - hdf5@1.12.2 +fortran +hl +shared
  - heffte@2.2.0 +fftw
  - hpctoolkit@2022.05.15
  - hpx@1.8.1 networking=mpi
  - hypre@2.25.0
  - kokkos-kernels@3.6.00 +openmp
  - kokkos@3.6.01 +openmp
  - lammps@20220623
  - legion@21.03.0
  - libnrm@0.1.0
  - libquo@1.3.1
  - libunwind@1.6.2
  - mercury@2.1.0
  - metall@0.21
  - mfem@4.4.0
  - mpark-variant@1.4.0
  - mpifileutils@0.11.1 ~xattr
  - nccmp@1.9.0.1
  - nco@5.0.1
  - netlib-scalapack@2.2.0
  - nrm@0.1.0
  - nvhpc@22.7
  - omega-h@9.34.1
  - openmpi@4.1.4
  - openpmd-api@0.14.5
  - papi@6.0.0.1
  - papyrus@1.0.2
  - parallel-netcdf@1.12.2
  - parsec@3.0.2012 ~cuda
  - pdt@3.25.1
  - petsc@3.17.4
  - phist@1.9.5
  - plumed@2.8.0
  - precice@2.5.0
  - pumi@2.2.7
  - py-cinemasci@1.7.0
  - py-jupyterhub@1.4.1
  - py-libensemble@0.9.2
  - py-petsc4py@3.17.4
  - py-warpx@22.08 ^warpx dims=2
  - py-warpx@22.08 ^warpx dims=3
  - py-warpx@22.08 ^warpx dims=rz
  - qthreads@1.16 scheduler=distrib
  - raja@2022.03.0
  - scr@3.0
  - slate@2022.06.00 ~cuda
  - slepc@3.17.2
  - stc@0.9.0
  - strumpack@6.3.1 ~slate
  - sundials@6.2.0
  - superlu-dist@7.2.0
  - superlu@5.3.0
  - swig@4.0.2
  - swig@4.0.2-fortran
  - sz@2.1.12.2
  - tasmanian@7.7
  - tau@2.31.1 +mpi +python
  - turbine@1.3.0
  - trilinos@13.0.1 +amesos +amesos2 +anasazi +aztec +belos +boost +epetra +epetraext
    +ifpack +ifpack2 +intrepid +intrepid2 +isorropia +kokkos +ml +minitensor +muelu
    +nox +piro +phalanx +rol +rythmos +sacado +stk +shards +shylu +stokhos +stratimikos
    +teko +tempus +tpetra +trilinoscouplings +zoltan +zoltan2 +superlu-dist gotype=long_long
  - umap@2.1.0
  - umpire@2022.03.1
  - unifyfs@0.9.2
  - veloc@1.5
  - vtk-m@1.8.0
  - zfp@0.5.5

  # no cuda_arch
  - tau@2.31.1 +mpi +cuda
  - papi@6.0.0.1 +cuda
  - flux-core@0.40.0 +cuda
  - hpctoolkit@2022.05.15 +cuda

  # cuda_arch=80
  - adios2@2.8.3 +cuda cuda_arch=80
  - amrex@22.08 +cuda cuda_arch=80
  - arborx@1.2 +cuda cuda_arch=80 ^kokkos@3.6.01 +wrapper
  - ascent@0.8.0 +cuda cuda_arch=80
  - cabana@0.5.0 +cuda ^kokkos@3.6.01 +wrapper +cuda_lambda +cuda cuda_arch=80
  - caliper@2.8.0 +cuda cuda_arch=80
  - chai@2022.03.0 ~benchmarks ~tests +cuda cuda_arch=80 ^umpire ~shared
  - flecsi@2.1.0 +cuda cuda_arch=80
  - ginkgo@1.4.0 +cuda cuda_arch=80
  - heffte@2.2.0 +cuda cuda_arch=80
  - hpx@1.8.1 +cuda cuda_arch=80
  - hypre@2.25.0 +cuda cuda_arch=80
  - kokkos-kernels@3.6.00 +cuda cuda_arch=80 ^kokkos@3.6.00 +wrapper +cuda cuda_arch=80
  - kokkos@3.6.01 +wrapper +cuda cuda_arch=80
  - magma@2.6.2 +cuda cuda_arch=80
  - mfem@4.4.0 +cuda cuda_arch=80
  - petsc@3.17.4 +cuda cuda_arch=80
  - slate@2022.06.00 +cuda cuda_arch=80
  - slepc@3.17.2 +cuda cuda_arch=80
  - strumpack@6.3.1 ~slate +cuda cuda_arch=80
  - sundials@6.2.0 +cuda cuda_arch=80
  - superlu-dist@7.2.0 +cuda cuda_arch=80
  - tasmanian@7.7 +cuda cuda_arch=80
  - umpire@2022.03.1 ~shared +cuda cuda_arch=80
  - vtk-m@1.8.0 +cuda cuda_arch=80
  - zfp@0.5.5 +cuda cuda_arch=80

  # CPU FAILURES
  # - alquimia@1.0.9                    # pflotran:
  # - bricks@r0.1                       # bricks
  # - geopm@1.1.0                       # geopm
  # - h5bench@1.2                       # h5bench
  # - loki@0.1.7                        # loki
  # - paraview@5.10.1 +qt ^mesa@21.3.8  # llvm@12.0.1
  # - plasma@21.8.29                    # plasma
  # - pruners-ninja@1.0.1               # pruners-ninja
  # - rempi@1.1.0                       # rempi
  # - upcxx@2022.3.0                    # upcxx
  # - variorum@0.4.1                    # variorum
  # - wannier90@3.1.0                   # wannier90
  # ----
  # bricks: Error downloading object: docs/media/fast-MPI-ghostzone.png (7f174b6): Smudge error: Error downloading docs/media/fast-MPI-ghostzone.png
  # font-util: configure.ac:11: installing './compile'
  # geopm: configure: error: Failed to determine MPI Fortran build flags use --with-mpi-bin or --with-mpicxx or --disable-mpi
  # h5bench: collect2: error: ld returned 1 exit status
  # llvm@12.0.1: FAILED: openmp/libomptarget/libomptarget.so.12
  # loki: loki/SmallObj.h:462:57: error: ISO C++17 does not allow dynamic exception specifications
  # loki: https://github.com/spack/spack/issues/32122
  # pflotran: Error: Rank mismatch between actual argument at (1) and actual argument at (2) (scalar and rank-1)
  # plasma: Could NOT find CBLAS (missing: CBLAS_INCLUDE_DIRS CBLAS_LIBRARIES), Could NOT find Accelerate (missing: Accelerate_INCLUDE_DIRS Accelerate_LIBRARIES)
  # pruners-ninja: test/ninja_test_util.c:34: multiple definition of `a';
  # pruners-ninja: https://github.com/spack/spack/issues/32112
  # rempi: rempi_message_manager.h:53:3: error: 'string' does not name a type; did you mean 'stdin'?
  # rempi: https://github.com/spack/spack/issues/32123
  # upcxx: configure error: Requested PMI version cray could not be found
  # variorum: /usr/bin/ld: Intel/CMakeFiles/variorum_intel.dir/msr_core.c.o:(.bss+0x0): multiple definition of `g_platform'; CMakeFiles/variorum.dir/config_architecture.c.o:(.bss+0x0): first defined here
  # variorum: https://github.com/spack/spack/issues/32110
  # wannier90: Error: Type mismatch between actual argument at (1) and actual argument at (2) (COMPLEX(8)/INTEGER(4)).

  # GPU FAILURES
  # - bricks@r0.1 +cuda                   # bricks
  # - dealii@9.4.0 +cuda                  # dealii
  # - parsec@3.0.2012 +cuda cuda_arch=80  # parsec
  # - raja@2022.03.0 +cuda cuda_arch=80   # raja
  # - trilinos@13.4.0 +cuda cuda_arch=80  # trilinos
  # -----
  # bricks: Error downloading object: docs/media/fast-MPI-ghostzone.png (7f174b6): Smudge error: Error downloading docs/media/fast-MPI-ghostzone.png
  # dealii: Could NOT find CUDA: CMake Error at cmake/configure/configure_10_cuda.cmake:200
  # parsec: parsec/mca/device/cuda/transfer.c:168: multiple definition of `parsec_CUDA_d2h_max_flows'
  # raja: RAJA/policy/tensor/arch/avx2/avx2_int32.hpp(317): error: "RAJA::expt::Register<int32_t, RAJA::expt::avx2_register> &(const int32_t &)" contains a vector, which is not supported in device code
  # trilinos: The C++ compiler "/opt/cray/pe/mpich/8.1.17/ofi/gnu/9.1/bin/mpicxx" is not able to compile a simple test program.