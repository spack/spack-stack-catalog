spack:
  view: false

  concretizer:
    reuse: false
    unify: false

  compilers:
  - compiler:
      spec: gcc@11.2.0
      paths:
        cc: gcc
        cxx: g++
        f77: gfortran
        fc: gfortran
      flags: {}
      operating_system: sles15
      target: any
      modules:
      - PrgEnv-gnu
      - gcc/11.2.0
      - craype-x86-milan
      - libfabric

  packages:
    all:
      compiler: [gcc@11.2.0]
      providers:
        blas: [cray-libsci]
        mpi: [mvapich2]
      target: [zen3]
      variants: +mpi
    binutils:
      variants: +ld +gold +headers +libiberty ~nls
    elfutils:
      variants: +bzip2 ~nls +xz
    hdf5:
      variants: +fortran +hl +shared
    libunwind:
      variants: +pic +xz
    ncurses:
      variants: +termlib
    openblas:
      variants: threads=openmp
    python:
      version: [3.8.13]
    trilinos:
      variants: +amesos +amesos2 +anasazi +aztec +belos +boost +epetra +epetraext
        +ifpack +ifpack2 +intrepid +intrepid2 +isorropia +kokkos +ml +minitensor +muelu
        +nox +piro +phalanx +rol +rythmos +sacado +stk +shards +shylu +stokhos +stratimikos
        +teko +tempus +tpetra +trilinoscouplings +zoltan +zoltan2 +superlu-dist gotype=long_long
    xz:
      variants: +pic

    # CRAY EXTERNALS
    mvapich2:
      externals:
      - spec: mvapich2@3.0a
        prefix: /global/cfs/cdirs/m3896/shared/opt/mvapich2/mvapich2-3.0a/install
        environment:
          prepend_path:
            MODULEPATH: /global/cfs/cdirs/m3896/shared/modulefiles
        modules: [mvapich2/3.0a]
    cray-libsci:
      buildable: false
      externals:
      - spec: cray-libsci@21.08.1.2
        modules:
        - cray-libsci/21.08.1.2
    libfabric:
      buildable: false
      variants: fabrics=sockets,tcp,udp,rxm
      externals:
      - spec: libfabric@1.15.0.0
        prefix: /opt/cray/libfabric/1.15.0.0
        modules:
        - libfabric
    openssl:
      version: [1.1.0i]
      buildable: false
      externals:
      - spec: openssl@1.1.0
        prefix: /usr
    openssh:
      version: [7.9p1]
      buildable: false
      externals:
      - spec: openssh@7.9p1
        prefix: /usr
    slurm:
      buildable: false
      version: [20-11-8-1]
      externals:
      - spec: slurm@20-11-8-1
        prefix: /usr

    # SITE VARIANT/VERSION PREFERENCES
    mesa:
      variants: +osmesa~glx

  modules:
    prefix_inspections:
      ./lib:
      - LD_LIBRARY_PATH
      ./lib64:
      - LD_LIBRARY_PATH
    default:
      'enable:': [lmod]
      lmod:
        defaults:
        - mvapich2@3.0a
        core_compilers: [gcc@11.2.0]
        blacklist_implicits: true
        verbose: true
        hash_length: 0
        whitelist: [mvapich2, cmake^ncurses@6.3]
        hierarchy: [mpi]
        projections: {}
        core_specs: []
        all:
          autoload: direct
          environment:
            set:
              ${PACKAGE}_ROOT: ${PREFIX}
          suffixes:
            +cuda cuda_arch=70: cuda70
            +cuda cuda_arch=80: cuda80
            +openmp: openmp
        cabana:
          suffixes:
            ^kokkos +cuda cuda_arch=70: cuda70
            ^kokkos +cuda cuda_arch=80: cuda80
        tau:
          suffixes:
            +cuda: cuda
        hpctoolkit:
          suffixes:
            +cuda: cuda
        bricks:
          suffixes:
            +cuda: cuda
        flux-core:
          suffixes:
            +cuda: cuda
        papi:
          suffixes:
            +cuda: cuda
        mpich:
          suffixes:
            ^hwloc+cuda: hwloc-cuda
            ^ncurses@6.3: ncurses6.3
            ^ncurses@6.2: ncurses6.2
        py-warpx:
          suffixes:
            ^warpx dims=2: dims2
            ^warpx dims=3: dims3
            ^warpx dims=rz: dimsRZ

  specs:
  # CPU FAILURES
  - alquimia@1.0.9                    # pflotran:
  - ascent@0.8.0                      # ascent
  - bricks@r0.1                       # bricks
  - flecsi@1.4.2                      # pfunit
  - geopm@1.1.0                       # py-numpy
  - gptune@3.0.0                      # py-numpy
  - h5bench@1.2                       # h5bench
  - libnrm@0.1.0                      # mpich
  - loki@0.1.7                        # loki
  - mpifileutils@0.11.1 ~xattr        # libcircle
  - nrm@0.1.0                         # py-numpy
  - parallel-netcdf@1.12.2            # parallel-netcdf
  - paraview@5.10.1 +qt ^mesa@21.3.8  # llvm@12.0.1
  - petsc@3.17.4                      # petsc
  - phist@1.9.5                       # phist
  - plasma@21.8.29                    # plasma
  - precice@2.5.0                     # petsc
  - pruners-ninja@1.0.1               # pruners-ninja
  - py-cinemasci@1.7.0                # py-numpy
  - py-libensemble@0.9.2              # py-numpy
  - py-petsc4py@3.17.4                # py-numpy
  - py-warpx@22.08 ^warpx dims=2      # py-numpy
  - py-warpx@22.08 ^warpx dims=3      # py-numpy
  - py-warpx@22.08 ^warpx dims=rz     # py-numpy
  - rempi@1.1.0                       # rempi
  - slepc@3.17.2                      # petsc
  - unifyfs@0.9.2                     # unifyfs
  - upcxx@2022.3.0                    # upcxx
  - variorum@0.4.1                    # variorum
  - wannier90@3.1.0                   # wannier90
  # ----
  # ascent: Could NOT find MPI (missing: MPI_C_FOUND) (found version "3.1")
  # bricks: Error downloading object: docs/media/fast-MPI-ghostzone.png (7f174b6): Smudge error: Error downloading docs/media/fast-MPI-ghostzone.png
  # font-util: configure.ac:11: installing './compile'
  # h5bench: collect2: error: ld returned 1 exit status
  # libcircle: libtool: link: warning: library `/global/cfs/cdirs/m3896/shared/opt/mvapich2/mvapich2-3.0a/install/lib/libmpi.la' was moved.; /usr/bin/grep: /usr/lib64/libxml2.la: No such file or directory
  # llvm@12.0.1: FAILED: openmp/libomptarget/libomptarget.so.12
  # loki: loki/SmallObj.h:462:57: error: ISO C++17 does not allow dynamic exception specifications
  # mpich: /usr/bin/ld: cannot find -l-L/opt/cray/pe/gcc/11.2.0/snos/lib/gcc/x86_64-suse-linux/11.2.0
  # parallel-netcdf: /usr/bin/grep: /gpfs/alpine/csc439/world-shared/mvapich2/mvapich2-3.0a/install/lib/libmpi.la: No such file or directory
  # petsc: ??
  # pflotran: Error: Rank mismatch between actual argument at (1) and actual argument at (2) (scalar and rank-1)
  # pfunit: Fatal Error: Cannot read module file '/global/cfs/cdirs/m3896/shared/opt/mvapich2/mvapich2-3.0a/install/include/mpi.mod' opened at (1), because it was created by a different version of GNU Fortran
  # phist: Fatal Error: Cannot read module file '/global/cfs/cdirs/m3896/shared/opt/mvapich2/mvapich2-3.0a/install/include/mpi.mod' opened at (1), because it was created by a different version of GNU Fortran
  # plasma: Could NOT find CBLAS (missing: CBLAS_INCLUDE_DIRS CBLAS_LIBRARIES), Could NOT find Accelerate (missing: Accelerate_INCLUDE_DIRS Accelerate_LIBRARIES)
  # pruners-ninja: test/ninja_test_util.c:34: multiple definition of `a';
  # py-numpy@1.23.2: ...
  # rempi: rempi_message_manager.h:53:3: error: 'string' does not name a type; did you mean 'stdin'?
  # unifyfs: libtool: link: warning: library `/global/cfs/cdirs/m3896/shared/opt/mvapich2/mvapich2-3.0a/install/lib/libmpi.la' was moved. ; /usr/bin/grep: /usr/lib64/libxml2.la: No such file or directory
  # upcxx: configure error: Requested PMI version cray could not be found
  # variorum: /usr/bin/ld: Intel/CMakeFiles/variorum_intel.dir/msr_core.c.o:(.bss+0x0): multiple definition of `g_platform'; CMakeFiles/variorum.dir/config_architecture.c.o:(.bss+0x0): first defined here
  # wannier90: Error: Type mismatch between actual argument at (1) and actual argument at (2) (COMPLEX(8)/INTEGER(4)).

  # GPU FAILURES
  - ascent@0.8.0 +cuda cuda_arch=80     # ascent
  - bricks@r0.1 +cuda                   # bricks
  - dealii@9.4.0 +cuda                  # dealii
  - ginkgo@1.4.0 +cuda cuda_arch=80     # ginkgo
  - parsec@3.0.2012 +cuda cuda_arch=80  # parsec
  - petsc@3.17.4 +cuda cuda_arch=80     # petsc
  - raja@2022.03.0 +cuda cuda_arch=80   # raja
  - slepc@3.17.2 +cuda cuda_arch=80     # petsc
  - trilinos@13.4.0 +cuda cuda_arch=80  # trilinos
  # -----
  # ascent: Could NOT find MPI (missing: MPI_C_FOUND) (found version "3.1")
  # bricks: Error downloading object: docs/media/fast-MPI-ghostzone.png (7f174b6): Smudge error: Error downloading docs/media/fast-MPI-ghostzone.png
  # dealii: Could NOT find CUDA: CMake Error at cmake/configure/configure_10_cuda.cmake:200
  # ginkgo: what():  /tmp/lpeyrala/spack-stage/spack-stage-ginkgo-1.4.0-iebunhbjqtdwoffjhiecqwmigunx6cna/spack-src/cuda/base/cublas_bindings.hpp:117: gemm: CUBLAS_STATUS_INVALID_VALUE
  # parsec: parsec/mca/device/cuda/transfer.c:168: multiple definition of `parsec_CUDA_d2h_max_flows'
  # petsc: ??
  # raja: RAJA/policy/tensor/arch/avx2/avx2_int32.hpp(317): error: "RAJA::expt::Register<int32_t, RAJA::expt::avx2_register> &(const int32_t &)" contains a vector, which is not supported in device code
  # trilinos: The C++ compiler "/opt/cray/pe/mpich/8.1.17/ofi/gnu/9.1/bin/mpicxx" is not able to compile a simple test program.