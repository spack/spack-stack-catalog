spack:
  view: false

  concretizer:
    reuse: false
    unify: false

  compilers:
  - compiler:
      spec: gcc@11.2.0
      paths:
        cc: gcc
        cxx: g++
        f77: gfortran
        fc: gfortran
      flags: {}
      operating_system: sles15
      target: any
      modules:
      - PrgEnv-gnu
      - gcc/11.2.0
      - craype-x86-milan
      - libfabric

  packages:
    all:
      compiler: [gcc@11.2.0]
      providers:
        blas: [cray-libsci]
        mpi: [mvapich2]
      target: [zen3]
      variants: +mpi
    binutils:
      variants: +ld +gold +headers +libiberty ~nls
    elfutils:
      variants: +bzip2 ~nls +xz
    hdf5:
      variants: +fortran +hl +shared
    libunwind:
      variants: +pic +xz
    ncurses:
      variants: +termlib
    openblas:
      variants: threads=openmp
    python:
      version: [3.8.13]
    trilinos:
      variants: +amesos +amesos2 +anasazi +aztec +belos +boost +epetra +epetraext
        +ifpack +ifpack2 +intrepid +intrepid2 +isorropia +kokkos +ml +minitensor +muelu
        +nox +piro +phalanx +rol +rythmos +sacado +stk +shards +shylu +stokhos +stratimikos
        +teko +tempus +tpetra +trilinoscouplings +zoltan +zoltan2 +superlu-dist gotype=long_long
    xz:
      variants: +pic

    # CRAY EXTERNALS
    mvapich2:
      externals:
      - spec: mvapich2@3.0a
        prefix: /global/cfs/cdirs/m3896/shared/opt/mvapich2/mvapich2-3.0a/install
        environment:
          prepend_path:
            MODULEPATH: /global/cfs/cdirs/m3896/shared/modulefiles
        modules: [mvapich2/3.0a]
    cray-libsci:
      buildable: false
      externals:
      - spec: cray-libsci@21.08.1.2
        modules:
        - cray-libsci/21.08.1.2
    libfabric:
      buildable: false
      variants: fabrics=sockets,tcp,udp,rxm
      externals:
      - spec: libfabric@1.15.0.0
        prefix: /opt/cray/libfabric/1.15.0.0
        modules:
        - libfabric
    openssl:
      version: [1.1.0i]
      buildable: false
      externals:
      - spec: openssl@1.1.0
        prefix: /usr
    openssh:
      version: [7.9p1]
      buildable: false
      externals:
      - spec: openssh@7.9p1
        prefix: /usr
    slurm:
      buildable: false
      version: [20-11-8-1]
      externals:
      - spec: slurm@20-11-8-1
        prefix: /usr

    # SITE VARIANT/VERSION PREFERENCES
    mesa:
      variants: +osmesa~glx

  modules:
    prefix_inspections:
      ./lib:
      - LD_LIBRARY_PATH
      ./lib64:
      - LD_LIBRARY_PATH
    default:
      'enable:': [lmod]
      lmod:
        defaults:
        - mvapich2@3.0a
        core_compilers: [gcc@11.2.0]
        blacklist_implicits: true
        verbose: true
        hash_length: 0
        whitelist: [mvapich2, cmake^ncurses@6.3]
        hierarchy: [mpi]
        projections: {}
        core_specs: []
        all:
          autoload: direct
          environment:
            set:
              ${PACKAGE}_ROOT: ${PREFIX}
          suffixes:
            +cuda cuda_arch=70: cuda70
            +cuda cuda_arch=80: cuda80
            +openmp: openmp
        cabana:
          suffixes:
            ^kokkos +cuda cuda_arch=70: cuda70
            ^kokkos +cuda cuda_arch=80: cuda80
        tau:
          suffixes:
            +cuda: cuda
        hpctoolkit:
          suffixes:
            +cuda: cuda
        bricks:
          suffixes:
            +cuda: cuda
        flux-core:
          suffixes:
            +cuda: cuda
        papi:
          suffixes:
            +cuda: cuda
        mpich:
          suffixes:
            ^hwloc+cuda: hwloc-cuda
            ^ncurses@6.3: ncurses6.3
            ^ncurses@6.2: ncurses6.2
        py-warpx:
          suffixes:
            ^warpx dims=2: dims2
            ^warpx dims=3: dims3
            ^warpx dims=rz: dimsRZ

  specs:
  # CPU
  - adios@1.13.1
  - adios2@2.8.3
  - aml@0.2.0
  - amrex@22.08
  - arborx@1.2
  - archer@2.0.0
  - argobots@1.1
  - ascent@0.8.0
  - axom@0.6.1
  - bolt@2.0
  - butterflypack@2.1.1
  - cabana@0.5.0
  - caliper@2.8.0
  - chai@2022.03.0 ~benchmarks ~tests
  - charliecloud@0.27
  - conduit@0.8.3
  - darshan-runtime@3.4.0
  - darshan-util@3.4.0
  - datatransferkit@3.1-rc3
  - dyninst@12.2.0
  - exaworks@0.1.0 # 
  - faodel@1.2108.1
  - flit@2.1.0
  - flux-core@0.40.0
  - fortrilinos@2.0.0
  - gasnet@2022.3.0
  - ginkgo@1.4.0
  - globalarrays@5.8
  - gmp@6.2.1
  - gotcha@1.0.3
  - gptune@3.0.0
  - hdf5-vol-async@1.2
  - hdf5@1.12.2 +fortran +hl +shared
  - heffte@2.2.0 +fftw
  - hpctoolkit@2022.05.15
  - hpx@1.8.1 networking=mpi
  - hypre@2.25.0
  - kokkos-kernels@3.6.00 +openmp
  - kokkos@3.6.01 +openmp
  - lammps@20220623
  - legion@21.03.0
  - libnrm@0.1.0
  - libquo@1.3.1
  - libunwind@1.6.2
  - mercury@2.1.0
  - metall@0.21
  - mfem@4.4.0
  - mpark-variant@1.4.0
  - nccmp@1.9.0.1
  - nco@5.0.1
  - netlib-scalapack@2.2.0
  - nrm@0.1.0
  - nvhpc@22.7
  - omega-h@9.34.1
  - openmpi@4.1.4
  - openpmd-api@0.14.5
  - papi@6.0.0.1
  - papyrus@1.0.2
  - parsec@3.0.2012 ~cuda
  - pdt@3.25.1
  - petsc@3.17.4
  - plumed@2.8.0
  - precice@2.5.0
  - pumi@2.2.7
  - py-cinemasci@1.7.0
  - py-jupyterhub@1.4.1
  - py-libensemble@0.9.2
  - py-petsc4py@3.17.4
  - py-warpx@22.08 ^warpx dims=2
  - py-warpx@22.08 ^warpx dims=3
  - py-warpx@22.08 ^warpx dims=rz
  - qthreads@1.16 scheduler=distrib
  - raja@2022.03.0
  - scr@3.0
  - slate@2022.06.00 ~cuda
  - slepc@3.17.2
  - stc@0.9.0
  - strumpack@6.3.1 ~slate
  - sundials@6.2.0
  - superlu-dist@7.2.0
  - superlu@5.3.0
  - swig@4.0.2
  - swig@4.0.2-fortran
  - sz@2.1.12.2
  - tasmanian@7.7
  - tau@2.31.1 +mpi +python
  - trilinos@13.0.1 +amesos +amesos2 +anasazi +aztec +belos +boost +epetra +epetraext
    +ifpack +ifpack2 +intrepid +intrepid2 +isorropia +kokkos +ml +minitensor +muelu
    +nox +piro +phalanx +rol +rythmos +sacado +stk +shards +shylu +stokhos +stratimikos
    +teko +tempus +tpetra +trilinoscouplings +zoltan +zoltan2 +superlu-dist gotype=long_long
  - turbine@1.3.0
  - umap@2.1.0
  - umpire@2022.03.1
  - veloc@1.5
  - vtk-m@1.8.0
  - zfp@0.5.5

  # no cuda_arch
  - tau@2.31.1 +mpi +cuda
  - papi@6.0.0.1 +cuda
  - flux-core@0.40.0 +cuda
  - hpctoolkit@2022.05.15 +cuda

  # cuda_arch=80
  - adios2@2.8.3 +cuda cuda_arch=80
  - amrex@22.08 +cuda cuda_arch=80
  - arborx@1.2 +cuda cuda_arch=80 ^kokkos@3.6.01 +wrapper
  - ascent@0.8.0 +cuda cuda_arch=80
  - cabana@0.5.0 +cuda ^kokkos@3.6.01 +wrapper +cuda_lambda +cuda cuda_arch=80
  - caliper@2.8.0 +cuda cuda_arch=80
  - chai@2022.03.0 ~benchmarks ~tests +cuda cuda_arch=80 ^umpire ~shared
  - flecsi@2.1.0 +cuda cuda_arch=80
  - heffte@2.2.0 +cuda cuda_arch=80
  - hpx@1.8.1 +cuda cuda_arch=80
  - hypre@2.25.0 +cuda cuda_arch=80
  - kokkos-kernels@3.6.00 +cuda cuda_arch=80 ^kokkos@3.6.00 +wrapper +cuda cuda_arch=80
  - kokkos@3.6.01 +wrapper +cuda cuda_arch=80
  - lammps@20220623 +cuda cuda_arch=80
  - legion@21.03.0 +cuda
  - magma@2.6.2 +cuda cuda_arch=80
  - mfem@4.4.0 +cuda cuda_arch=80
  - petsc@3.17.4 +cuda cuda_arch=80
  - raja@2022.03.0 +cuda cuda_arch=80
  - slate@2022.06.00 +cuda cuda_arch=80
  - slepc@3.17.2 +cuda cuda_arch=80
  - strumpack@6.3.1 ~slate +cuda cuda_arch=80
  - sundials@6.2.0 +cuda cuda_arch=80
  - superlu-dist@7.2.0 +cuda cuda_arch=80
  - tasmanian@7.7 +cuda cuda_arch=80
  - umpire@2022.03.1 ~shared +cuda cuda_arch=80
  - vtk-m@1.8.0 +cuda cuda_arch=80
  - zfp@0.5.5 +cuda cuda_arch=80

  # CPU FAILURES
  # - alquimia@1.0.9                    # pflotran:
  # - bricks@r0.1                       # bricks
  # - flecsi@1.4.2                      # pfunit
  # - geopm@1.1.0                       # py-numpy
  # - h5bench@1.2                       # h5bench
  # - loki@0.1.7                        # loki
  # - mpifileutils@0.11.1 ~xattr        # libcircle
  # - parallel-netcdf@1.12.2            # parallel-netcdf
  # - paraview@5.10.1 +qt ^mesa@21.3.8  # llvm@12.0.1
  # - phist@1.9.5                       # phist
  # - plasma@21.8.29                    # plasma
  # - pruners-ninja@1.0.1               # pruners-ninja
  # - rempi@1.1.0                       # rempi
  # - unifyfs@0.9.2                     # unifyfs
  # - upcxx@2022.3.0                    # upcxx
  # - variorum@0.4.1                    # variorum
  # - wannier90@3.1.0                   # wannier90
  # ----
  # bricks: Error downloading object: docs/media/fast-MPI-ghostzone.png (7f174b6): Smudge error: Error downloading docs/media/fast-MPI-ghostzone.png
  # font-util: configure.ac:11: installing './compile'
  # h5bench: collect2: error: ld returned 1 exit status
  # libcircle: libtool: link: warning: library `/global/cfs/cdirs/m3896/shared/opt/mvapich2/mvapich2-3.0a/install/lib/libmpi.la' was moved.; /usr/bin/grep: /usr/lib64/libxml2.la: No such file or directory
  # llvm@12.0.1: FAILED: openmp/libomptarget/libomptarget.so.12
  # loki: loki/SmallObj.h:462:57: error: ISO C++17 does not allow dynamic exception specifications
  # parallel-netcdf: /usr/bin/grep: /gpfs/alpine/csc439/world-shared/mvapich2/mvapich2-3.0a/install/lib/libmpi.la: No such file or directory
  # pflotran: Error: Rank mismatch between actual argument at (1) and actual argument at (2) (scalar and rank-1)
  # pfunit: Fatal Error: Cannot read module file '/global/cfs/cdirs/m3896/shared/opt/mvapich2/mvapich2-3.0a/install/include/mpi.mod' opened at (1), because it was created by a different version of GNU Fortran
  # phist: Fatal Error: Cannot read module file '/global/cfs/cdirs/m3896/shared/opt/mvapich2/mvapich2-3.0a/install/include/mpi.mod' opened at (1), because it was created by a different version of GNU Fortran
  # plasma: Could NOT find CBLAS (missing: CBLAS_INCLUDE_DIRS CBLAS_LIBRARIES), Could NOT find Accelerate (missing: Accelerate_INCLUDE_DIRS Accelerate_LIBRARIES)
  # pruners-ninja: test/ninja_test_util.c:34: multiple definition of `a';
  # rempi: rempi_message_manager.h:53:3: error: 'string' does not name a type; did you mean 'stdin'?
  # unifyfs: libtool: link: warning: library `/global/cfs/cdirs/m3896/shared/opt/mvapich2/mvapich2-3.0a/install/lib/libmpi.la' was moved. ; /usr/bin/grep: /usr/lib64/libxml2.la: No such file or directory
  # upcxx: configure error: Requested PMI version cray could not be found
  # variorum: /usr/bin/ld: Intel/CMakeFiles/variorum_intel.dir/msr_core.c.o:(.bss+0x0): multiple definition of `g_platform'; CMakeFiles/variorum.dir/config_architecture.c.o:(.bss+0x0): first defined here
  # wannier90: Error: Type mismatch between actual argument at (1) and actual argument at (2) (COMPLEX(8)/INTEGER(4)).

  # GPU FAILURES
  # - axom@0.6.1 +cuda cuda_arch=80       # axom
  # - bricks@r0.1 +cuda                   # bricks
  # - dealii@9.4.0 +cuda                  # dealii
  # - ginkgo@1.4.0 +cuda cuda_arch=80     # ginkgo
  # - parsec@3.0.2012 +cuda cuda_arch=80  # parsec
  # - trilinos@13.4.0 +cuda cuda_arch=80  # trilinos
  # - upcxx@2022.3.0 +cuda cudar_arch=80  # upcxx
  # -----
  # axom: /usr/bin/ld: ../../../core/CMakeFiles/core.dir/utilities/FileUtilities.cpp.o: in function `__sti____cudaRegisterAll()': /tmp/tmpxft_00027a8c_00000000-6_FileUtilities.cudafe1.stub.c:14: undefined reference to `__cudaRegisterLinkedBinary_9689a2e4_17_FileUtilities_cpp_14a850bb'
  # bricks: Error downloading object: docs/media/fast-MPI-ghostzone.png (7f174b6): Smudge error: Error downloading docs/media/fast-MPI-ghostzone.png
  # dealii: Could NOT find CUDA: CMake Error at cmake/configure/configure_10_cuda.cmake:200
  # ginkgo: what():  /tmp/lpeyrala/spack-stage/spack-stage-ginkgo-1.4.0-iebunhbjqtdwoffjhiecqwmigunx6cna/spack-src/cuda/base/cublas_bindings.hpp:117: gemm: CUBLAS_STATUS_INVALID_VALUE
  # parsec: parsec/mca/device/cuda/transfer.c:168: multiple definition of `parsec_CUDA_d2h_max_flows'
  # trilinos: The C++ compiler "/opt/cray/pe/mpich/8.1.17/ofi/gnu/9.1/bin/mpicxx" is not able to compile a simple test program.
  # upcxx: configure error: User requested --enable-mpi but I don't know how to build mpi programs for your system