AMReX-Codes/pyamrex:
  data_format: 2
  description: '[Experimental] AMReX Python Bindings'
  filenames:
  - spack.yaml
  full_name: AMReX-Codes/pyamrex
  latest_release: null
  readme: '<h1><a id="user-content-pyamrex" class="anchor" aria-hidden="true" href="#pyamrex"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>pyAMReX</h1>

    <p><a href="https://www.python.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/acd285afab5d7ddd4942e5215ade53e84551c9d7d635642ba92c19fde7d4345b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d507974686f6e332d79656c6c6f77677265656e"
    alt="Python3" title="Python3 API" data-canonical-src="https://img.shields.io/badge/language-Python3-yellowgreen"
    style="max-width: 100%;"></a> <a target="_blank" rel="noopener noreferrer nofollow"
    href="https://camo.githubusercontent.com/b4bbc2488e2de908b64d4a3c99de80e3b0842cc33e938283b89433bf1193a072/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d7072652d2d616c7068612d79656c6c6f77677265656e"><img
    src="https://camo.githubusercontent.com/b4bbc2488e2de908b64d4a3c99de80e3b0842cc33e938283b89433bf1193a072/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d7072652d2d616c7068612d79656c6c6f77677265656e"
    alt="Python3 API: Pre-Alpha" title="Status: Pre-Alpha" data-canonical-src="https://img.shields.io/badge/phase-pre--alpha-yellowgreen"
    style="max-width: 100%;"></a>

    <a href="https://spdx.org/licenses/BSD-3-Clause-LBNL.html" rel="nofollow"><img
    src="https://camo.githubusercontent.com/c468c77da60663856e2be1cdd66db538d4bca1b2a3bdf34a76a7f3953e58fc26/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4253442d2d332d2d436c617573652d2d4c424e4c2d626c75652e737667"
    alt="License AMReX" data-canonical-src="https://img.shields.io/badge/license-BSD--3--Clause--LBNL-blue.svg"
    style="max-width: 100%;"></a><br>

    <a target="_blank" rel="noopener noreferrer" href="https://github.com/AMReX-Codes/pyamrex/workflows/linux/badge.svg?branch=development"><img
    src="https://github.com/AMReX-Codes/pyamrex/workflows/linux/badge.svg?branch=development"
    alt="linux" style="max-width: 100%;"></a>

    <a target="_blank" rel="noopener noreferrer" href="https://github.com/AMReX-Codes/pyamrex/workflows/macos/badge.svg?branch=development"><img
    src="https://github.com/AMReX-Codes/pyamrex/workflows/macos/badge.svg?branch=development"
    alt="macos" style="max-width: 100%;"></a>

    <a target="_blank" rel="noopener noreferrer" href="https://github.com/AMReX-Codes/pyamrex/workflows/windows/badge.svg?branch=development"><img
    src="https://github.com/AMReX-Codes/pyamrex/workflows/windows/badge.svg?branch=development"
    alt="windows" style="max-width: 100%;"></a></p>

    <p>pyAMReX is part of AMReX.</p>

    <p>Due to its <strong>highly experimental</strong> nature, we develop it currently
    in a separate respository.</p>

    <p>We will add further information here once first development versions are ready
    for testing.</p>

    <h2><a id="user-content-users" class="anchor" aria-hidden="true" href="#users"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Users</h2>

    <p><em>to do</em></p>

    <ul>

    <li>pip/pypa</li>

    <li>conda-forge</li>

    <li>spack</li>

    <li>brew</li>

    <li>...</li>

    </ul>

    <h3><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h3>

    <p><em>to do</em></p>

    <div class="highlight highlight-source-python"><pre><span class="pl-k">import</span>
    <span class="pl-s1">amrex</span>


    <span class="pl-s1">small_end</span> <span class="pl-c1">=</span> <span class="pl-s1">amrex</span>.<span
    class="pl-v">Int_Vect</span>()

    <span class="pl-s1">big_end</span> <span class="pl-c1">=</span> <span class="pl-s1">amrex</span>.<span
    class="pl-v">Int_Vect</span>(<span class="pl-c1">2</span>, <span class="pl-c1">3</span>,
    <span class="pl-c1">4</span>)


    <span class="pl-s1">b</span> <span class="pl-c1">=</span> <span class="pl-s1">amrex</span>.<span
    class="pl-v">Box</span>(<span class="pl-s1">small_end</span>, <span class="pl-s1">big_end</span>)

    <span class="pl-en">print</span>(<span class="pl-s1">b</span>)


    <span class="pl-c"># ...</span></pre></div>

    <h2><a id="user-content-developers" class="anchor" aria-hidden="true" href="#developers"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Developers</h2>

    <p>If you are new to CMake, <a href="https://hsf-training.github.io/hsf-training-cmake-webpage/"
    rel="nofollow">this short tutorial</a> from the HEP Software foundation is the
    perfect place to get started with it.</p>

    <p>If you just want to use CMake to build the project, jump into sections <em>1.
    Introduction</em>, <em>2. Building with CMake</em> and <em>9. Finding Packages</em>.</p>

    <h3><a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h3>

    <p>pyAMReX depends on the following popular third party software.</p>

    <ul>

    <li>a mature <a href="https://en.wikipedia.org/wiki/C%2B%2B17" rel="nofollow">C++17</a>
    compiler, e.g., GCC 8, Clang 7, NVCC 11.0, MSVC 19.15 or newer</li>

    <li><a href="https://cmake.org" rel="nofollow">CMake 3.20.0+</a></li>

    <li>

    <a href="https://amrex-codes.github.io" rel="nofollow">AMReX <em>development</em></a>:
    we automatically download and compile a copy of AMReX</li>

    <li>

    <a href="https://github.com/pybind/pybind11/">pybind11</a> 2.10.1+: we automatically
    download and compile a copy of pybind11 (<a href="https://github.com/pybind/pybind11/blob/master/LICENSE">new
    BSD</a>)

    <ul>

    <li>

    <a href="https://python.org" rel="nofollow">Python</a> 3.7+</li>

    <li>

    <a href="https://numpy.org" rel="nofollow">Numpy</a> 1.15+</li>

    </ul>

    </li>

    </ul>

    <p>Optional dependencies include:</p>

    <ul>

    <li>

    <a href="https://www.openmp.org" rel="nofollow">mpi4py</a> 2.1+: for multi-node
    and/or multi-GPU execution</li>

    <li>

    <a href="https://ccache.dev" rel="nofollow">CCache</a>: to speed up rebuilds (for
    CUDA support, needs 3.7.9+ and 4.2+ is recommended)</li>

    <li>further <a href="https://github.com/AMReX-Codes/amrex/">optional dependencies
    of AMReX</a>

    </li>

    <li>

    <a href="https://docs.pytest.org/en/stable/" rel="nofollow">pytest</a> 6.2+: for
    running unit tests</li>

    </ul>

    <p>Optional CUDA-capable dependencies for tests include:</p>

    <ul>

    <li>

    <a href="https://github.com/cupy/cupy#installation">cupy</a> 11.2+</li>

    <li>

    <a href="https://numba.readthedocs.io/en/stable/user/installing.html" rel="nofollow">numba</a>
    0.56+</li>

    <li>

    <a href="https://pytorch.org/get-started/locally/" rel="nofollow">torch</a> 1.12+</li>

    </ul>

    <h3><a id="user-content-install-dependencies" class="anchor" aria-hidden="true"
    href="#install-dependencies"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install
    Dependencies</h3>

    <p>macOS/Linux:</p>

    <div class="highlight highlight-source-shell"><pre>spack env activate -d <span
    class="pl-c1">.</span>

    <span class="pl-c"><span class="pl-c">#</span> optional:</span>

    <span class="pl-c"><span class="pl-c">#</span> spack add cuda</span>

    spack install</pre></div>

    <p>(in new terminals, re-activate the environment with <code>spack env activate
    -d .</code> again)</p>

    <p>or macOS/Linux:</p>

    <div class="highlight highlight-source-shell"><pre>brew update

    brew install ccache cmake libomp mpi4py numpy open-mpi python</pre></div>

    <p>Now, <code>cmake --version</code> should be at version 3.20.0 or newer.</p>

    <p>Or go:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    optional:                                    --user</span>

    python3 -m pip install -U pip setuptools wheel

    python3 -m pip install -U cmake</pre></div>

    <p>If you wish to run unit tests, then please install <code>pytest</code></p>

    <div class="highlight highlight-source-shell"><pre>python3 -m pip install -U pytest</pre></div>

    <h3><a id="user-content-configure-your-compiler" class="anchor" aria-hidden="true"
    href="#configure-your-compiler"><span aria-hidden="true" class="octicon octicon-link"></span></a>Configure
    your compiler</h3>

    <p>For example, using the Clang compiler:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-k">export</span>
    CC=<span class="pl-s"><span class="pl-pds">$(</span>which clang<span class="pl-pds">)</span></span>

    <span class="pl-k">export</span> CXX=<span class="pl-s"><span class="pl-pds">$(</span>which
    clang++<span class="pl-pds">)</span></span></pre></div>

    <p>If you also want to select a CUDA compiler:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-k">export</span>
    CUDACXX=<span class="pl-s"><span class="pl-pds">$(</span>which nvcc<span class="pl-pds">)</span></span>

    <span class="pl-k">export</span> CUDAHOSTCXX=<span class="pl-s"><span class="pl-pds">$(</span>which
    clang++<span class="pl-pds">)</span></span></pre></div>

    <h3><a id="user-content-build" class="anchor" aria-hidden="true" href="#build"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h3>

    <p>From the base of the pyAMReX source directory, execute:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    optional controls (example):</span>

    <span class="pl-c"><span class="pl-c">#</span>export AMREX_SPACEDIM=3</span>

    <span class="pl-c"><span class="pl-c">#</span>export AMREX_MPI=ON</span>

    <span class="pl-c"><span class="pl-c">#</span>export AMREX_OMP=ON</span>

    <span class="pl-c"><span class="pl-c">#</span>export AMREX_GPU_BACKEND=CUDA</span>

    <span class="pl-c"><span class="pl-c">#</span>export AMREX_SRC=$PWD/../amrex</span>

    <span class="pl-c"><span class="pl-c">#</span>export CMAKE_BUILD_PARALLEL_LEVEL=8</span>


    python3 -m pip install -U -r requirements.txt

    python3 -m pip install -v --force-reinstall --no-deps <span class="pl-c1">.</span></pre></div>

    <p>If you are iterating on builds, it will faster to rely on <code>ccache</code>
    and to let CMake call the <code>pip</code> install logic:</p>

    <div class="highlight highlight-source-shell"><pre>cmake -S <span class="pl-c1">.</span>
    -B build

    cmake --build build --target pip_install -j 8</pre></div>

    <h3><a id="user-content-test" class="anchor" aria-hidden="true" href="#test"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Test</h3>

    <p>After successful installation, you can run the unit tests (assuming <code>pytest</code>
    is

    installed). If <code>AMREX_MPI=ON</code>, then please prepend the following commands
    with <code>mpiexec -np &lt;NUM_PROCS&gt;</code></p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Run all tests</span>

    python3 -m pytest tests/


    <span class="pl-c"><span class="pl-c">#</span> Run tests from a single file</span>

    python3 -m pytest tests/test_intvect.py


    <span class="pl-c"><span class="pl-c">#</span> Run a single test (useful during
    debugging)</span>

    python3 -m pytest tests/test_intvect.py::test_iv_conversions


    <span class="pl-c"><span class="pl-c">#</span> Run all tests, do not capture "print"
    output and be verbose</span>

    python3 -m pytest -s -vvvv tests/</pre></div>

    <h3><a id="user-content-build-options" class="anchor" aria-hidden="true" href="#build-options"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build Options</h3>

    <p>If you are using the pip-driven install, selected <a href="https://amrex-codes.github.io/amrex/docs_html/BuildingAMReX.html#building-with-cmake"
    rel="nofollow">AMReX CMake options</a> can be controlled with environment variables:</p>

    <table>

    <thead>

    <tr>

    <th>Environment Variable</th>

    <th>Default &amp; Values</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><code>AMREX_OMP</code></td>

    <td>ON/<strong>OFF</strong>

    </td>

    <td>Enable OpenMP</td>

    </tr>

    <tr>

    <td><code>AMREX_GPU_BACKEND</code></td>

    <td>

    <strong>NONE</strong>/SYCL/CUDA/HIP</td>

    <td>On-node, accelerated GPU backend</td>

    </tr>

    <tr>

    <td><code>AMREX_MPI</code></td>

    <td>ON/<strong>OFF</strong>

    </td>

    <td>Enable MPI</td>

    </tr>

    <tr>

    <td><code>AMREX_PRECISION</code></td>

    <td>SINGLE/<strong>DOUBLE</strong>

    </td>

    <td>Precision of AMReX Real type</td>

    </tr>

    <tr>

    <td><code>AMREX_SPACEDIM</code></td>

    <td>1/2/<strong>3</strong>

    </td>

    <td>Dimension of AMReX</td>

    </tr>

    <tr>

    <td><code>AMREX_BUILD_SHARED_LIBS</code></td>

    <td>ON/<strong>OFF</strong>

    </td>

    <td>Build the core AMReX library as shared library</td>

    </tr>

    <tr>

    <td><code>AMREX_SRC</code></td>

    <td><em>None</em></td>

    <td>Absolute path to AMReX source directory (preferred if set)</td>

    </tr>

    <tr>

    <td><code>AMREX_REPO</code></td>

    <td><code>https://github.com/AMReX-Codes/amrex.git</code></td>

    <td>Repository URI to pull and build AMReX from</td>

    </tr>

    <tr>

    <td><code>AMREX_BRANCH</code></td>

    <td><code>development</code></td>

    <td>Repository branch for <code>AMREX_REPO</code>

    </td>

    </tr>

    <tr>

    <td><code>AMREX_INTERNAL</code></td>

    <td>

    <strong>ON</strong>/OFF</td>

    <td>Needs a pre-installed AMReX library if set to <code>OFF</code>

    </td>

    </tr>

    <tr>

    <td><code>PYBIND11_INTERNAL</code></td>

    <td>

    <strong>ON</strong>/OFF</td>

    <td>Needs a pre-installed pybind11 library if set to <code>OFF</code>

    </td>

    </tr>

    <tr>

    <td><code>CMAKE_BUILD_PARALLEL_LEVEL</code></td>

    <td>2</td>

    <td>Number of parallel build threads</td>

    </tr>

    <tr>

    <td><code>PYAMREX_LIBDIR</code></td>

    <td><em>None</em></td>

    <td>If set, search for pre-built a pyAMReX library</td>

    </tr>

    <tr>

    <td><code>PYINSTALLOPTIONS</code></td>

    <td><em>None</em></td>

    <td>Additional options for <code>pip install</code>, e.g., <code>-v --user</code>

    </td>

    </tr>

    </tbody>

    </table>

    <p>For example, one can also build against a local AMReX copy.

    Assuming AMReX'' source is located in <code>$HOME/src/amrex</code>, then <code>export
    AMREX_SRC=$HOME/src/amrex</code>.</p>

    <p>Or as a one-liner, assuming your AMReX source directory is located in <code>../amrex</code>:</p>

    <div class="highlight highlight-source-shell"><pre>AMREX_SRC=<span class="pl-smi">$PWD</span>/../amrex
    python3 -m pip install -v --force-reinstall <span class="pl-c1">.</span></pre></div>

    <p>Note that you need to use absolute paths for external source trees, because
    pip builds in a temporary directory.</p>

    <p>Or build against an AMReX feature branch of a colleague.

    Assuming your colleague pushed AMReX to <code>https://github.com/WeiqunZhang/amrex/</code>
    in a branch <code>new-feature</code> then</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c1">unset</span>
    AMREX_SRC  <span class="pl-c"><span class="pl-c">#</span> preferred if set</span>

    AMREX_REPO=https://github.com/WeiqunZhang/amrex.git AMREX_BRANCH=new-feature python3
    -m pip install -v --force-reinstall <span class="pl-c1">.</span></pre></div>

    <p>You can speed up the install further if you pre-install AMReX, e.g. with a
    package manager.

    Set <code>AMREX_INTERNAL=OFF</code> and add installation prefix of AMReX to the
    environment variable <a href="https://cmake.org/cmake/help/latest/envvar/CMAKE_PREFIX_PATH.html"
    rel="nofollow">CMAKE_PREFIX_PATH</a>.

    Please see the <a href="#Developers">short CMake tutorial that we linked above</a>
    if this sounds new to you.</p>

    <h2><a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Acknowledgements</h2>

    <p>This work was supported by the Laboratory Directed Research and Development
    Program of Lawrence Berkeley National Laboratory under U.S. Department of Energy
    Contract No. DE-AC02-05CH11231.</p>

    <h2><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>pyAMReX Copyright (c) 2021-2022, The Regents of the University of California,

    through Lawrence Berkeley National Laboratory (subject to receipt of any

    required approvals from the U.S. Dept. of Energy).  All rights reserved.</p>

    <p>If you have questions about your rights to use or distribute this software,

    please contact Berkeley Lab''s Innovation &amp; Partnerships Office at

    <a href="mailto:IPO@lbl.gov">IPO@lbl.gov</a>.</p>

    <p>NOTICE.  This Software was developed under funding from the U.S. Department

    of Energy and the U.S. Government consequently retains certain rights. As

    such, the U.S. Government has been granted for itself and others acting on

    its behalf a paid-up, nonexclusive, irrevocable, worldwide license in the

    Software to reproduce, distribute copies to the public, prepare derivative

    works, and perform publicly and display publicly, and to permit other to do

    so.</p>

    <p>License for pyamrex can be found at <a href="LICENSE">LICENSE</a>.</p>

    '
  stargazers_count: 18
  subscribers_count: 15
  topics:
  - amrex
  - python
  updated_at: 1675839641.0
ArjunaCluster/spack:
  data_format: 2
  description: Spack Repos and Configuration Files
  filenames:
  - environments/slurm/spack.yaml
  full_name: ArjunaCluster/spack
  latest_release: null
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1637623732.0
C2SM/spack-c2sm:
  data_format: 2
  description: Repository for c2sm spack config and repo files
  filenames:
  - upstreams/daint/spack.yaml
  full_name: C2SM/spack-c2sm
  latest_release: v0.18.1.1
  readme: '<h1><a id="user-content-the-spack-extension-of-c2sm-and-mch" class="anchor"
    aria-hidden="true" href="#the-spack-extension-of-c2sm-and-mch"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>The spack extension of C2SM and MCH</h1>

    <p><a href="https://C2SM.github.io/spack-c2sm/v0.18.1.1" rel="nofollow"><img src="https://camo.githubusercontent.com/67d98f3f50b1ad629290b2fc5a38331fc19df5a656363fb14bdd892b371dcf0e/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f616e7369636f6c6f72746167732f62616467652f3f76657273696f6e3d6c6174657374"
    alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/ansicolortags/badge/?version=latest"
    style="max-width: 100%;"></a></p>

    <p>Spack is the package manager used by C2SM and MeteoSwiss to install and deploy
    software on supercomputers, local machines and the cloud.</p>

    <h2><a id="user-content-documentations" class="anchor" aria-hidden="true" href="#documentations"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentations</h2>

    <p><strong>Infos about c2sm-supported software and machines</strong></p>

    <ul>

    <li><a href="https://C2SM.github.io/spack-c2sm/latest" rel="nofollow">spack-c2sm
    latest</a></li>

    <li><a href="https://C2SM.github.io/spack-c2sm/v0.18.1.1" rel="nofollow">spack-c2sm
    v0.18.1.1</a></li>

    </ul>

    <p><strong>General infos about spack</strong></p>

    <ul>

    <li><a href="https://spack.readthedocs.io/en/v0.18.1/" rel="nofollow">Official
    spack v0.18.1</a></li>

    </ul>

    <h2><a id="user-content-workflow" class="anchor" aria-hidden="true" href="#workflow"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Workflow</h2>

    <p>With spack v0.18 we suggest local/individual spack instances and the use of
    spack environments.</p>

    <p>A user clones the spack repo</p>

    <div class="highlight highlight-source-shell"><pre>git clone --depth 1 --recurse-submodules
    --shallow-submodules -b v0.18.1.1 https://github.com/C2SM/spack-c2sm.git</pre></div>

    <p>gets spack in the command line</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c1">.</span>
    spack-c2sm/setup-env.sh</pre></div>

    <p>activates an environment</p>

    <div class="highlight highlight-source-shell"><pre>spack env activate -p <span
    class="pl-k">&lt;</span>path_to_env<span class="pl-k">&gt;</span></pre></div>

    <p>and starts exploring</p>

    <div class="highlight highlight-source-shell"><pre>spack info <span class="pl-k">&lt;</span>package<span
    class="pl-k">&gt;</span>

    spack spec <span class="pl-k">&lt;</span>spec<span class="pl-k">&gt;</span></pre></div>

    <p>and building</p>

    <div class="highlight highlight-source-shell"><pre>spack install <span class="pl-k">&lt;</span>spec<span
    class="pl-k">&gt;</span>

    spack dev-build <span class="pl-k">&lt;</span>spec<span class="pl-k">&gt;</span></pre></div>

    <p>a package.</p>

    <p>Updating spack-c2sm is in the hands of the user.</p>

    <div class="highlight highlight-source-shell"><pre>git pull

    git submodule update --recursive</pre></div>

    <p>After an update we advice to clean</p>

    <div class="highlight highlight-source-shell"><pre>spack uninstall -a

    spack clean -a

    rm -rf <span class="pl-k">~</span>/.spack</pre></div>

    <p>and rebuild.</p>

    <h2><a id="user-content-command-cheat-sheet" class="anchor" aria-hidden="true"
    href="#command-cheat-sheet"><span aria-hidden="true" class="octicon octicon-link"></span></a>Command
    cheat sheet</h2>

    <table>

    <thead>

    <tr>

    <th></th>

    <th>Command</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Clone</td>

    <td><code>git clone --depth 1 --recurse-submodules --shallow-submodules -b &lt;branch/tag&gt;
    https://github.com/C2SM/spack-c2sm.git</code></td>

    </tr>

    <tr>

    <td>Load</td>

    <td>

    <code>. spack-c2sm/setup-env.sh</code> autodetects machine <br>or<br><code>. spack-c2sm/setup-env.sh
    &lt;machine&gt;</code> forces machine<br>or<br><code>. spack-c2sm/setup-env.sh
    unknown</code> uses blank config<br><code>spack compiler find</code> <a href="https://spack.readthedocs.io/en/v0.18.1/command_index.html?highlight=spack%20load#spack-compiler-find"
    rel="nofollow">autodetects compilers</a><br><code>spack external find --all</code>
    <a href="https://spack.readthedocs.io/en/v0.18.1/command_index.html?highlight=spack%20load#spack-external-find"
    rel="nofollow">autodetects externally installed packages</a>

    </td>

    </tr>

    <tr>

    <td>Update</td>

    <td>

    <code>git pull</code><br><code>git submodule update --recursive</code>

    </td>

    </tr>

    <tr>

    <td>Clean</td>

    <td>

    <code>spack uninstall -a</code> <a href="https://spack.readthedocs.io/en/v0.18.1/command_index.html?highlight=spack%20load#spack-uninstall"
    rel="nofollow">uninstalls all packages</a><br><code>spack clean -a</code> <a href="https://spack.readthedocs.io/en/v0.18.1/command_index.html?highlight=spack%20load#spack-clean"
    rel="nofollow">cleans all misc caches</a><br><code>rm -rf ~/.spack</code> removes
    user scope data</td>

    </tr>

    </tbody>

    </table>

    <p><a href="https://spack.readthedocs.io/en/v0.18.1/basic_usage.html#specs-dependencies"
    rel="nofollow"><strong>Spec syntax</strong></a>: <code>&lt;package&gt;</code><a
    href="https://spack.readthedocs.io/en/v0.18.1/basic_usage.html#version-specifier"
    rel="nofollow"><code>@&lt;version&gt;</code></a><a href="https://spack.readthedocs.io/en/v0.18.1/basic_usage.html#compiler-specifier"
    rel="nofollow"><code>%&lt;compiler&gt;</code></a><a href="https://spack.readthedocs.io/en/v0.18.1/basic_usage.html#variants"
    rel="nofollow"><code>+&lt;variant&gt; ~&lt;variant&gt;</code></a><a href="https://spack.readthedocs.io/en/v0.18.1/basic_usage.html#specs-dependencies"
    rel="nofollow"><code>^&lt;sub-package&gt; +&lt;sub-package-variant&gt;</code></a><a
    href="https://spack.readthedocs.io/en/v0.18.1/basic_usage.html#compiler-flags"
    rel="nofollow"><code>&lt;compiler flags&gt;</code></a></p>

    <table>

    <thead>

    <tr>

    <th></th>

    <th>Command</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Find</td>

    <td>

    <code>spack find</code> lists all installed packages. <br><code>spack find &lt;spec&gt;</code>
    lists all installed packages that match the spec.</td>

    </tr>

    <tr>

    <td>Info</td>

    <td><code>spack info &lt;package&gt;</code></td>

    </tr>

    <tr>

    <td>Spec</td>

    <td>

    <code>spack spec &lt;spec&gt;</code> concretizes abstract spec (unspecfied variant
    = <strong>any</strong>)<br><em>Spack is not required to use the default of an
    unspecified variant. The default value is only a tiebreaker for the concretizer.</em>

    </td>

    </tr>

    <tr>

    <td>Install</td>

    <td><code>spack install &lt;spec&gt;</code></td>

    </tr>

    <tr>

    <td>Locate</td>

    <td>

    <code>spack location --install-dir &lt;spec&gt;</code> prints location of <strong>all</strong>
    installs that satisfy the spec</td>

    </tr>

    <tr>

    <td><a href="https://spack.readthedocs.io/en/v0.18.1/command_index.html?highlight=spack%20load#spack-load"
    rel="nofollow">Load env</a></td>

    <td>

    <code>spack load &lt;spec&gt;</code> loads run environment</td>

    </tr>

    <tr>

    <td><a href="https://spack.readthedocs.io/en/v0.18.1/environments.html" rel="nofollow">Activate
    env</a></td>

    <td><code>spack env activate -p &lt;env_name&gt;</code></td>

    </tr>

    <tr>

    <td><a href="https://spack.readthedocs.io/en/v0.18.1/environments.html" rel="nofollow">Deactivate
    env</a></td>

    <td><code>spack deactivate</code></td>

    </tr>

    </tbody>

    </table>

    '
  stargazers_count: 3
  subscribers_count: 18
  topics: []
  updated_at: 1676997668.0
CHIP-SPV/CHIP-SPV-Spack:
  data_format: 2
  description: Support for building CHIP-SPV via Spack
  filenames:
  - Environments/LevelZero/spack.yaml
  - Environments/Rothdt-POCL/spack.yaml
  full_name: CHIP-SPV/CHIP-SPV-Spack
  latest_release: null
  readme: '

    <h1><a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h1>

    <p><a href="https://github.com/CHIP-SPV/chip-spv">CHIP-SPV</a> is software that

    allows software written to use the <a href="https://https://github.com/ROCm-Developer-Tools/HIP"
    rel="nofollow">Heterogeneous-compute Interface for

    Portability (HIP)</a>

    interface and kernel language to target GPUs via the

    <a href="https://registry.khronos.org/spir" rel="nofollow">SPIR-V</a> intermediate
    language.

    CHIP-SPV can use either the Intel Level Zero runtime or an OpenCL

    runtime as a backend.</p>

    <p>This repository contains support for building CHIP-SPV and its

    dependencies via the <a href="https://github.com/spack/spack">Spack</a> package

    manager.</p>

    <p>Note: most development to date has been done with the Level Zero

    environment, and it is expected that substantial work is needed for

    the environment targeting the OpenCL backend to work.</p>

    <h1><a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h1>

    <ul>

    <li>An x86_64 system running a common Linux distribution.  OpenSLES 15 is

    the best tested to date.</li>

    <li>A working Spack installation.</li>

    <li>Clang 15.x installed and registered as a Spack compiler.  Installing

    this compiler via Spack (i.e., by installing a package like <code>llvm@15.0.7</code>,

    and then using <code>spack compiler add</code>) is the approach that is currently

    best tested.</li>

    </ul>

    <h1><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h1>

    <ol start="0">

    <li>Clone this repository to the target system.</li>

    </ol>

    <div class="highlight highlight-source-shell"><pre>$ git clone https://github.com/CHIP-SPV/CHIP-SPV-Spack</pre></div>

    <ol>

    <li>

    <p>Verify that the compiler version is correctly represented in the

    desired environment''s <code>spack.yaml</code> file.  For instance, if the Clang

    compiler to be used is version 15.0.7, ensure that the version

    numbers for the <code>compilers</code> definition, the <code>llvm</code> spec,
    and

    the <code>spirv-llvm-translator</code> spec are consistent.  In this instance,

    these should be <code>clang@15.0.7</code>, <code>llvm@15.0.7</code> and

    <code>spirv-llvm-translator@15</code>, respectively.</p>

    </li>

    <li>

    <p>Activate and concretize the environment.  E.g.,</p>

    </li>

    </ol>

    <div class="highlight highlight-source-shell"><pre>$ <span class="pl-c1">cd</span>
    CHIP-SPV-Spack/Environments/LevelZero

    $ spack activate <span class="pl-c1">.</span>

    $ spack concretize -f</pre></div>

    <ol start="3">

    <li>Build the environment.</li>

    </ol>

    <div class="highlight highlight-source-shell"><pre>$ spack install</pre></div>

    <p>If all goes well with the build, a <code>spack find</code> with the environment

    active should show <code>chip-spv</code> available.</p>

    <ol start="4">

    <li>Use the installed software.  The easiest way to use the installed

    software when compiling and running HIP code is to activate the

    environment and then build the software.  Because other packages like

    <code>cmake</code> and <code>boost</code> are not roots in the Spack environments,
    they

    are not automatically added to one''s environment when one activates

    the Spack environment.  To use these, one must load them before

    activating the Spack environment, or ensure that they are available

    using the <code>module</code> command.</li>

    </ol>

    <h1><a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>TODO</h1>

    <ul>

    <li>Ensure the OpenCL-based environment can use any OpenCL implementation.</li>

    <li>Clean up and verify the OpenCL-based environment using POCL.</li>

    <li>Incorporate HIP libraries like HipBLAS into the environments.</li>

    <li>Support using the software installed by the environment via <code>module</code>

    </li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1676574454.0
CivetWang/HPCHarryW:
  data_format: 2
  description: null
  filenames:
  - assignment/spack.yaml
  full_name: CivetWang/HPCHarryW
  latest_release: null
  readme: '<h1><a id="user-content-hpcharryw" class="anchor" aria-hidden="true" href="#hpcharryw"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HPCHarryW</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1653448958.0
E4S-Project/e4s:
  data_format: 2
  description: E4S for Spack
  filenames:
  - environments/22.11/cuda-x86_64/spack.yaml
  - environments/23.02/rocm-x86_64/spack.yaml
  - environments/23.02/cuda-ppc64le/spack.yaml
  - environments/22.11/cuda-ppc64le/spack.yaml
  - environments/22.11/cuda-aarch64/spack.yaml
  - environments/23.02/cuda-x86_64/spack.yaml
  - environments/22.11/rocm-x86_64/spack.yaml
  - environments/23.02/oneapi-x86_64/spack.yaml
  - environments/22.11/oneapi-x86_64/spack.yaml
  - environments/23.02/cuda-aarch64/spack.yaml
  full_name: E4S-Project/e4s
  latest_release: null
  readme: "<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/E4S-Project/e4s/blob/master/logos/E4S-dark-green.png\"\
    ><img src=\"https://github.com/E4S-Project/e4s/raw/master/logos/E4S-dark-green.png\"\
    \ width=\"200\" alt=\"E4S\" style=\"max-width: 100%;\"></a></p> \n<p><a target=\"\
    _blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/58e7ffdceb32cd7a8facd6b6cd3920a56c15e0e2ef1d3398158ef4ec0d6ec886/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f4534532d50726f6a6563742f653473\"\
    ><img src=\"https://camo.githubusercontent.com/58e7ffdceb32cd7a8facd6b6cd3920a56c15e0e2ef1d3398158ef4ec0d6ec886/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f4534532d50726f6a6563742f653473\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/E4S-Project/e4s\"\
    \ style=\"max-width: 100%;\"></a>\n<a target=\"_blank\" rel=\"noopener noreferrer\
    \ nofollow\" href=\"https://camo.githubusercontent.com/94cdab1cd9efc5521be1590b3d0b5dc5f707838e0d20015c14f23921ef2b7326/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6534732f62616467652f3f76657273696f6e3d6c6174657374\"\
    ><img src=\"https://camo.githubusercontent.com/94cdab1cd9efc5521be1590b3d0b5dc5f707838e0d20015c14f23921ef2b7326/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6534732f62616467652f3f76657273696f6e3d6c6174657374\"\
    \ alt=\"Documentation\" data-canonical-src=\"https://readthedocs.org/projects/e4s/badge/?version=latest\"\
    \ style=\"max-width: 100%;\"></a>\n<a target=\"_blank\" rel=\"noopener noreferrer\
    \ nofollow\" href=\"https://camo.githubusercontent.com/c2e45205070ba0928aece78cf95f8658ef1cf69f7c113dc56f7d05e29e68755e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f4534532d50726f6a6563742f6534732e737667\"\
    ><img src=\"https://camo.githubusercontent.com/c2e45205070ba0928aece78cf95f8658ef1cf69f7c113dc56f7d05e29e68755e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f4534532d50726f6a6563742f6534732e737667\"\
    \ alt=\"GitHub Issues\" data-canonical-src=\"https://img.shields.io/github/issues/E4S-Project/e4s.svg\"\
    \ style=\"max-width: 100%;\"></a>\n<a target=\"_blank\" rel=\"noopener noreferrer\
    \ nofollow\" href=\"https://camo.githubusercontent.com/011fb27187d5b878949948752f73e86dea6828febca879ee69a5a5d52ce651ca/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f4534532d50726f6a6563742f653473\"\
    ><img src=\"https://camo.githubusercontent.com/011fb27187d5b878949948752f73e86dea6828febca879ee69a5a5d52ce651ca/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f4534532d50726f6a6563742f653473\"\
    \ alt=\"GitHub pull requests\" data-canonical-src=\"https://img.shields.io/github/issues-pr/E4S-Project/e4s\"\
    \ style=\"max-width: 100%;\"></a></p>\n<h1><a id=\"user-content-e4s\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#e4s\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>E4S</h1>\n<p>The <a href=\"https://e4s-project.github.io/\"\
    \ rel=\"nofollow\">Extreme-scale Scientific Software Stack (E4S)</a> is a community\
    \ effort to provide open source\nsoftware packages for developing, deploying and\
    \ running scientific applications on high-performance\ncomputing (HPC) platforms.\
    \ E4S provides from-source builds and containers of a\n<a href=\"https://e4s-project.github.io/Resources/ProductInfo.html\"\
    \ rel=\"nofollow\">broad collection of HPC software packages</a>.</p>\n<p>E4S\
    \ is available to download in the following formats:</p>\n<ul>\n<li>\n<p>Containers:\
    \ Docker, Singularity, CharlieCloud, OVA</p>\n</li>\n<li>\n<p>Spack manifest (<code>spack.yaml</code>)\
    \ to install from source. These can be found in <a href=\"https://github.com/E4S-Project/e4s/tree/master/environments\"\
    >environments</a> directory.</p>\n</li>\n<li>\n<p><a href=\"http://aws.amazon.com/\"\
    \ rel=\"nofollow\">AWS EC2 image</a> with image name <code>ami-0db9d49091db1c25f</code>\
    \ in <strong>US-West-2 (Oregon)</strong></p>\n</li>\n<li>\n<p><a href=\"https://oaciss.uoregon.edu/e4s/inventory.html\"\
    \ rel=\"nofollow\">E4S Build Cache</a></p>\n</li>\n</ul>\n<p>Please see <a href=\"\
    https://github.com/E4S-Project/e4s/blob/master/E4S_Products.md\">E4S Product Dictionary</a>\
    \ for complete list of E4S products.</p>\n<h2><a id=\"user-content-useful-links\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#useful-links\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Useful Links</h2>\n<ul>\n<li>User\
    \ Documentation: <a href=\"https://e4s.readthedocs.io\" rel=\"nofollow\">https://e4s.readthedocs.io</a>\n\
    </li>\n<li>Main Page: <a href=\"https://e4s-project.github.io/\" rel=\"nofollow\"\
    >https://e4s-project.github.io/</a>\n</li>\n<li>E4S GitHub: <a href=\"https://github.com/E4S-Project/\"\
    >https://github.com/E4S-Project/</a>\n</li>\n<li>E4S Slack Channel: <a href=\"\
    https://e4s-project.slack.com\" rel=\"nofollow\">https://e4s-project.slack.com</a>\n\
    </li>\n<li>Slack Channel Invitation: <a href=\"https://communityinviter.com/apps/e4s-project/e4s\"\
    \ rel=\"nofollow\">https://communityinviter.com/apps/e4s-project/e4s</a>\n</li>\n\
    <li>E4S Dashboard: <a href=\"https://dashboard.e4s.io/\" rel=\"nofollow\">https://dashboard.e4s.io/</a>\n\
    </li>\n</ul>\n<h2><a id=\"user-content-related-projects\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#related-projects\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Related Projects</h2>\n<ul>\n<li>\n<p><a href=\"https://github.com/E4S-Project/E4S-Project.github.io\"\
    >E4S-Project/E4S-Project.github.io</a> - E4S Documentation repo that is hosted\
    \ on <a href=\"https://e4s-project.github.io/\" rel=\"nofollow\">https://e4s-project.github.io/</a></p>\n\
    </li>\n<li>\n<p><a href=\"https://github.com/E4S-Project/testsuite\">E4S-Project/testsuite</a>\
    \ - E4S Testsuite with collection of validation tests that can be run post-install.</p>\n\
    </li>\n<li>\n<p><a href=\"https://github.com/E4S-Project/e4s-cl\">E4S-Project/e4s-cl</a>\
    \ - E4S Container Launcher is a tool to easily run MPI applications in E4S containers.</p>\n\
    </li>\n<li>\n<p><a href=\"https://github.com/E4S-Project/e4s-ci-badges\">E4S-Project/e4s-ci-badges</a>\
    \ - Display CI badges for E4S products that are available from <a href=\"https://shields.io/\"\
    \ rel=\"nofollow\">shields.io</a></p>\n</li>\n</ul>\n<h2><a id=\"user-content-license\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#license\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>License</h2>\n<p>E4S is released\
    \ as MIT license for more details see <a href=\"https://github.com/E4S-Project/e4s/blob/master/LICENSE\"\
    >LICENSE</a> file</p>\n<h2><a id=\"user-content-contact\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#contact\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Contact</h2>\n<ul>\n<li>Mike Heroux (<a href=\"mailto:maherou@sandia.gov\"\
    >maherou@sandia.gov</a>)</li>\n<li>Sameer Shende (<a href=\"mailto:sameer@cs.uoregon.edu\"\
    >sameer@cs.uoregon.edu</a>)</li>\n</ul>\n"
  stargazers_count: 17
  subscribers_count: 10
  topics: []
  updated_at: 1678287231.0
ECP-WarpX/WarpX:
  data_format: 2
  description: WarpX is an advanced, time-based electromagnetic & electrostatic Particle-In-Cell
    code.
  filenames:
  - Tools/machines/desktop/spack-macos-openmp.yaml
  - Tools/machines/desktop/spack-ubuntu-openmp.yaml
  full_name: ECP-WarpX/WarpX
  latest_release: '23.03'
  readme: '<h1><a id="user-content-warpx" class="anchor" aria-hidden="true" href="#warpx"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>WarpX</h1>

    <p><a href="https://dev.azure.com/ECP-WarpX/WarpX/_build/latest?definitionId=1&amp;branchName=development"
    rel="nofollow"><img src="https://camo.githubusercontent.com/992695de99c1381c366d74cf333cc4b4a29d53878b4808d643fb673748a73c5b/68747470733a2f2f6465762e617a7572652e636f6d2f4543502d57617270582f57617270582f5f617069732f6275696c642f7374617475732f4543502d57617270582e57617270583f6272616e63684e616d653d646576656c6f706d656e74"
    alt="Code Status development" data-canonical-src="https://dev.azure.com/ECP-WarpX/WarpX/_apis/build/status/ECP-WarpX.WarpX?branchName=development"
    style="max-width: 100%;"></a>

    <a href="https://dev.azure.com/ECP-WarpX/WarpX/_build?definitionId=2" rel="nofollow"><img
    src="https://camo.githubusercontent.com/f95e83fed565d15a3d259197389c3fbb68e0e9d529df7da1f73702528739c16f/68747470733a2f2f6465762e617a7572652e636f6d2f4543502d57617270582f57617270582f5f617069732f6275696c642f7374617475732f4543502d57617270582e4e696768746c793f6272616e63684e616d653d6e696768746c79266c6162656c3d6e696768746c792532307061636b61676573"
    alt="Nightly Installation Tests" data-canonical-src="https://dev.azure.com/ECP-WarpX/WarpX/_apis/build/status/ECP-WarpX.Nightly?branchName=nightly&amp;label=nightly%20packages"
    style="max-width: 100%;"></a>

    <a href="https://warpx.readthedocs.io/en/latest/?badge=latest" rel="nofollow"><img
    src="https://camo.githubusercontent.com/2fe6e4a1201d33edf1bdd9968c6c0446da41d44fef1b7a1e532cddc4fbd4c2ae/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f77617270782f62616467652f3f76657273696f6e3d6c6174657374"
    alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/warpx/badge/?version=latest"
    style="max-width: 100%;"></a>

    <a href="https://spack.readthedocs.io/en/latest/package_list.html#warpx" rel="nofollow"><img
    src="https://camo.githubusercontent.com/86cd172f84e0a3b89161faaeb17eb247cdb10062ed0e65f9f291db3011697416/68747470733a2f2f696d672e736869656c64732e696f2f737061636b2f762f7761727078"
    alt="Spack Version" data-canonical-src="https://img.shields.io/spack/v/warpx"
    style="max-width: 100%;"></a>

    <a href="https://anaconda.org/conda-forge/warpx" rel="nofollow"><img src="https://camo.githubusercontent.com/4434c608221acef026a4af7cb491f491413ab135a781e3537087938592334617/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f766e2f636f6e64612d666f7267652f7761727078"
    alt="Conda Version" data-canonical-src="https://img.shields.io/conda/vn/conda-forge/warpx"
    style="max-width: 100%;"></a>

    <a href="https://gitter.im/ECP-WarpX/community?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge"
    rel="nofollow"><img src="https://camo.githubusercontent.com/c1799ddb014bc7c83ab051f3668c05f25049c81bbf1ae3c4e4dd6a61c68314aa/68747470733a2f2f6261646765732e6769747465722e696d2f4543502d57617270582f636f6d6d756e6974792e737667"
    alt="Gitter" data-canonical-src="https://badges.gitter.im/ECP-WarpX/community.svg"
    style="max-width: 100%;"></a><br>

    <a href="https://warpx.readthedocs.io/en/latest/install/users.html" rel="nofollow"><img
    src="https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565"
    alt="Supported Platforms" data-canonical-src="https://img.shields.io/badge/platforms-linux%20%7C%20osx%20%7C%20win-blue"
    style="max-width: 100%;"></a>

    <a href="https://github.com/ECP-WarpX/WarpX/compare/development"><img src="https://camo.githubusercontent.com/4cb34757a4ca0098f7c5b01c1d559e13991f5cb4e6add106761194c2967a7911/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d6974732d73696e63652f4543502d57617270582f57617270582f6c61746573742f646576656c6f706d656e742e737667"
    alt="GitHub commits since last release" data-canonical-src="https://img.shields.io/github/commits-since/ECP-WarpX/WarpX/latest/development.svg"
    style="max-width: 100%;"></a>

    <a href="https://www.exascaleproject.org/research/" rel="nofollow"><img src="https://camo.githubusercontent.com/fe7a996983f2a22d3a469de3af6e13b7062bca7f02ffad7974bb724b27c2b218/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f737570706f7274656425323062792d4543502d6f72616e6765"
    alt="Exascale Computing Project" data-canonical-src="https://img.shields.io/badge/supported%20by-ECP-orange"
    style="max-width: 100%;"></a>

    <a href="https://isocpp.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/5d59fff46d59a1783cc24942cb4eb374014513db99f991164bd051bcd94aa598/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d4325324225324231372d6f72616e67652e737667"
    alt="Language: C++17" data-canonical-src="https://img.shields.io/badge/language-C%2B%2B17-orange.svg"
    style="max-width: 100%;"></a>

    <a href="https://python.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/9cf7ec75b074af6953db1304db75950ab917ecd8a1aecb41f0d1191d10872298/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d507974686f6e2d6f72616e67652e737667"
    alt="Language: Python" data-canonical-src="https://img.shields.io/badge/language-Python-orange.svg"
    style="max-width: 100%;"></a><br>

    <a href="https://spdx.org/licenses/BSD-3-Clause-LBNL.html" rel="nofollow"><img
    src="https://camo.githubusercontent.com/c468c77da60663856e2be1cdd66db538d4bca1b2a3bdf34a76a7f3953e58fc26/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4253442d2d332d2d436c617573652d2d4c424e4c2d626c75652e737667"
    alt="License WarpX" data-canonical-src="https://img.shields.io/badge/license-BSD--3--Clause--LBNL-blue.svg"
    style="max-width: 100%;"></a>

    <a href="https://doi.org/10.5281/zenodo.4571577" rel="nofollow"><img src="https://camo.githubusercontent.com/a80e066c199b28e39d95c8a3cd8a7061bb4c190c717db8b58ff8cea2de0952be/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f444f4925323028736f75726365292d31302e353238312f7a656e6f646f2e343537313537372d626c75652e737667"
    alt="DOI (source)" data-canonical-src="https://img.shields.io/badge/DOI%20(source)-10.5281/zenodo.4571577-blue.svg"
    style="max-width: 100%;"></a>

    <a href="https://doi.org/10.1016/j.parco.2021.102833" rel="nofollow"><img src="https://camo.githubusercontent.com/1f6ca17eba9f0dbca214c58a50e39d5e4d2c5513476e963147c57c7b9f40f378/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f444f49253230287061706572292d31302e313031362f6a2e706172636f2e323032312e3130323833332d626c75652e737667"
    alt="DOI (paper)" data-canonical-src="https://img.shields.io/badge/DOI%20(paper)-10.1016/j.parco.2021.102833-blue.svg"
    style="max-width: 100%;"></a></p>

    <h2><a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h2>

    <p>WarpX is an advanced <strong>electromagnetic &amp; electrostatic Particle-In-Cell</strong>
    code.

    It supports many features including Perfectly-Matched Layers (PML), mesh refinement,
    and the boosted-frame technique.</p>

    <p>WarpX is a <em>highly-parallel and highly-optimized code</em>, which can run
    on GPUs and multi-core CPUs, and includes load balancing capabilities.

    WarpX scales to the world''s largest supercomputers and was awarded the <a href="https://www.exascaleproject.org/ecp-supported-collaborative-teams-win-the-2022-acm-gordon-bell-prize-and-special-prize/"
    rel="nofollow">2022 ACM Gordon Bell Prize</a>.</p>

    <h2><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h2>

    <p><a href="https://picmi-standard.github.io" rel="nofollow"><img src="https://camo.githubusercontent.com/343c1eefa7d19641daf3e00da21e54db3a6211fe5f692c3004f2836a185668d8/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d253232776f726b7325323077697468253232266d6573736167653d2532325049434d4925323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="PICMI" data-canonical-src="https://img.shields.io/static/v1?label=%22works%20with%22&amp;message=%22PICMI%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="https://www.openPMD.org" rel="nofollow"><img src="https://camo.githubusercontent.com/062e5330b80f6eca55b1df50d6d154214f5a2033b7a87344ef2a580fd7a616dc/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d253232776f726b7325323077697468253232266d6573736167653d2532326f70656e504d4425323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="openPMD" data-canonical-src="https://img.shields.io/static/v1?label=%22works%20with%22&amp;message=%22openPMD%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="https://yt-project.org" rel="nofollow"><img src="https://camo.githubusercontent.com/9e6cacd2df0d5a581d8afad30a57807b71f4b67c58e74faa4080dad7d81c6184/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d253232776f726b7325323077697468253232266d6573736167653d253232797425323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="yt-project" data-canonical-src="https://img.shields.io/static/v1?label=%22works%20with%22&amp;message=%22yt%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a></p>

    <p>In order to learn how to install and run the code, please see the online documentation:

    <a href="https://warpx.readthedocs.io" rel="nofollow">https://warpx.readthedocs.io</a></p>

    <p>To contact the developers, feel free to open an issue on this repo, or visit
    our Gitter room at <a href="https://gitter.im/ECP-WarpX/community" rel="nofollow">https://gitter.im/ECP-WarpX/community</a></p>

    <h2><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <p><a href="https://amrex-codes.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/7053679f4412132d376afadf481432a9d435336f8127e7c8650808bc66d019b2/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d25323272756e732532306f6e253232266d6573736167653d253232414d52655825323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="AMReX" data-canonical-src="https://img.shields.io/static/v1?label=%22runs%20on%22&amp;message=%22AMReX%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="https://picsar.net" rel="nofollow"><img src="https://camo.githubusercontent.com/793037a9842c5343f4942ce7475c7c7696e69b44621531f666492ac87b5e80b8/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d25323272756e732532306f6e253232266d6573736167653d25323250494353415225323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="PICSAR" data-canonical-src="https://img.shields.io/static/v1?label=%22runs%20on%22&amp;message=%22PICSAR%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="https://openpmd-api.readthedocs.io" rel="nofollow"><img src="https://camo.githubusercontent.com/b7108e47d5ad6b76b60f07a4e04173ba260c5eef9bb244680f65ff91d8a319f8/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d25323272756e732532306f6e253232266d6573736167653d2532326f70656e504d442d61706925323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="openPMD-api" data-canonical-src="https://img.shields.io/static/v1?label=%22runs%20on%22&amp;message=%22openPMD-api%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="https://csmd.ornl.gov/adios" rel="nofollow"><img src="https://camo.githubusercontent.com/d525e37817dc6dfc5f173eb31f4e9fd52947e668793967565910166b335ced93/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d25323272756e732532306f6e253232266d6573736167653d2532324144494f5325323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="ADIOS" data-canonical-src="https://img.shields.io/static/v1?label=%22runs%20on%22&amp;message=%22ADIOS%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="https://www.hdfgroup.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/005f778c667adb78e4302d47579b9bedc5ec0f59f88c13552f6b4bb399f93438/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d25323272756e732532306f6e253232266d6573736167653d2532324844463525323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="HDF5" data-canonical-src="https://img.shields.io/static/v1?label=%22runs%20on%22&amp;message=%22HDF5%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="http://www.ascent-dav.org" rel="nofollow"><img src="https://camo.githubusercontent.com/204f53a0d216a0a2fce9a367e3ba3a1957ac2285ed89026cb80321df6a125fc4/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d25323272756e732532306f6e253232266d6573736167653d253232417363656e7425323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="Ascent" data-canonical-src="https://img.shields.io/static/v1?label=%22runs%20on%22&amp;message=%22Ascent%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="https://sensei-insitu.org" rel="nofollow"><img src="https://camo.githubusercontent.com/6f870e29c1d57a4e4209ec97a00fbe4f73c8fd6fb589bf4c12f3feef9d3aaaeb/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d25323272756e732532306f6e253232266d6573736167653d25323253454e53454925323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="SENSEI" data-canonical-src="https://img.shields.io/static/v1?label=%22runs%20on%22&amp;message=%22SENSEI%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a></p>

    <p>Our workflow is described in <a href="CONTRIBUTING.rst">CONTRIBUTING.rst</a>.</p>

    <h2><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>WarpX Copyright (c) 2018-2023, The Regents of the University of California,

    through Lawrence Berkeley National Laboratory (subject to receipt of any

    required approvals from the U.S. Dept. of Energy).  All rights reserved.</p>

    <p>If you have questions about your rights to use or distribute this software,

    please contact Berkeley Lab''s Innovation &amp; Partnerships Office at

    <a href="mailto:IPO@lbl.gov">IPO@lbl.gov</a>.</p>

    <p>NOTICE.  This Software was developed under funding from the U.S. Department

    of Energy and the U.S. Government consequently retains certain rights. As

    such, the U.S. Government has been granted for itself and others acting on

    its behalf a paid-up, nonexclusive, irrevocable, worldwide license in the

    Software to reproduce, distribute copies to the public, prepare derivative

    works, and perform publicly and display publicly, and to permit other to do

    so.</p>

    <p>License for WarpX can be found at <a href="LICENSE.txt">LICENSE.txt</a>.</p>

    '
  stargazers_count: 175
  subscribers_count: 15
  topics:
  - laser
  - plasma
  - physics
  - gpu
  - simulation
  - particle-in-cell
  - pic
  - research
  updated_at: 1679432864.0
FTHPC/Correlation_Compressibility:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: FTHPC/Correlation_Compressibility
  latest_release: v0.1
  readme: "<h1><a id=\"user-content-compressibility-analysis-correlation_compressibility\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#compressibility-analysis-correlation_compressibility\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Compressibility\
    \ Analysis (Correlation_Compressibility)</h1>\n<h2><a id=\"user-content-statement-of-purpose\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#statement-of-purpose\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Statement of Purpose</h2>\n<p>This\
    \ repo contains scripts to perform compressibility analysis on several leading\
    \ lossy compressors.\nThe compressibility analysis relies on deriving statistics\
    \ on scientific data and explore their relationships to their compression ratios\
    \ from various lossy compressors (based on various compression scheme).\nThe extracted\
    \ relationships between compression ratios and statistical predictors are modeled\
    \ via regression models, which provide a statistical framework to predict compression\
    \ ratios for the different studied lossy compressors.</p>\n<p>This repo contains\
    \ an automatic framework of scripts that perform the compression of scientific\
    \ datasets from 8 compressors (SZ2, ZFP, MGARD, FPZIP, Digit Rounding and Bit\
    \ Grooming), the derivation of the statistical predictors of compression ratios\
    \ (SVD, standard deviation, quantized entropy), and scripts to perform the training\
    \ of the regression models (linear and spline regressions) as well as the validation\
    \ of the regression predictions.\nA runtime analysis is also performed and associated\
    \ codes are provided.</p>\n<h3><a id=\"user-content-main-code-structures\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#main-code-structures\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Main code structures</h3>\n<p>Compression\
    \ metrics, including compression ratios, and derivation of statistical predictors\
    \ (SVD, standard deviation, quantized entropy) codes are found in <code>compress_package</code>\
    \ and are run via <code>scripts/run.sh</code> as described in the section \"How\
    \ to compute statistical predictors and compression analysis on datasets\".\n\
    Linear and spline regressions training and validation (functions <code>cr_regression_linreg</code>\
    \ and <code>cr_regression_gam</code> from the script <code>replicate_figures/functions_paper.R</code>).\n\
    Codes for the different runtime analysis are found in the folder <code>runtime_analysis</code>\
    \ and are automated with the script <code>runtime.sh</code>, the study includes\
    \ compression time for SZ2, ZFP, MAGRD, FPZIP, data quantization, SVD, local (tiled)\
    \ variogram and local (tiled) variogram, and runtime for training and prediction\
    \ of the regressions.<br>\nFinally, the script <code>replicate_figures/graphs_paper_container.R</code>\
    \ replicates and saves all the figures from the paper ad as well as numbers from\
    \ the tables.</p>\n<p>For each dataset in the <code>dataset</code> folder, slicing\
    \ is performed for each variable field (e.g. density in Miranda), each slice is\
    \ stored in a class. The class is updated as compressions with the 8 compressors\
    \ is performed and updated as the statistical predictors are derived. Results\
    \ of each class are stored in a .csv file (example of csv files can be found at\
    \ <code>replicate_figures/generated_data/</code>).\nAll the datasets stored in\
    \ the <code>dataset</code> folder can be analyzed with the given set of codes,\
    \ one needs to source <code>scripts/config.json</code> with the appropriate dataset\
    \ name as described in the below section \"How to compute statistical predictors\
    \ and compression analysis on datasets\".\nThe regression analysis and its prediction\
    \ is then performed on R dataframes based on the aforementioned .csv files.</p>\n\
    <h2><a id=\"user-content-system-information\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#system-information\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>System Information</h2>\n<p>The hardware and software\
    \ versions used for the performance evaluations can be found in the table below.\
    \ These nodes come from Clemson University's Palmetto Cluster.</p>\n<p>These nodes\
    \ have:</p>\n<table>\n<thead>\n<tr>\n<th>component</th>\n<th>version</th>\n<th>component</th>\n\
    <th>version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>CPU</td>\n<td>Intel Xeon\
    \ 6148G (40 cores)</td>\n<td>sz2</td>\n<td>2.1.12.2</td>\n</tr>\n<tr>\n<td>GPU</td>\n\
    <td>2 Nvidia v100</td>\n<td>sz3</td>\n<td>3.1.3.1</td>\n</tr>\n<tr>\n<td>Memory</td>\n\
    <td>372GB</td>\n<td>zfp</td>\n<td>0.5.5</td>\n</tr>\n<tr>\n<td>Network</td>\n\
    <td>2 Mellanox MT27710 (HDR)</td>\n<td>mgard</td>\n<td>1.0.0</td>\n</tr>\n<tr>\n\
    <td>FileSystem</td>\n<td>BeeGFS 7.2.3 (24 targets)</td>\n<td>bit grooming</td>\n\
    <td>2.1.9</td>\n</tr>\n<tr>\n<td>Compiler</td>\n<td>GCC 8.4.1</td>\n<td>digit\
    \ rounding</td>\n<td>2.1.9</td>\n</tr>\n<tr>\n<td>OS</td>\n<td>CentOS 8.2.2004</td>\n\
    <td>R</td>\n<td>4.1.3</td>\n</tr>\n<tr>\n<td>MPI</td>\n<td>OpenMPI 4.0.5</td>\n\
    <td>Python</td>\n<td>3.9.12</td>\n</tr>\n<tr>\n<td>LibPressio</td>\n<td>0.83.4</td>\n\
    <td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h2><a id=\"user-content-first-time-setup\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#first-time-setup\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>First time setup</h2>\n<h3><a\
    \ id=\"user-content-container-installation-for-ease-of-setup\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#container-installation-for-ease-of-setup\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Container Installation\
    \ (for ease of setup)</h3>\n<p>We provide a container for <code>x86_64</code>\
    \ image for ease of installation.</p>\n<p>This container differs from our experimental\
    \ setup slightly. The production build used <code>-march=native -mtune=native</code>\
    \ for architecture optimized builds where as the container does not use these\
    \ flags to maximize compatibility across <code>x86_64</code> hardware.</p>\n<p>NOTE\
    \ this file is &gt;= 11 GB , download with caution.</p>\n<h3><a id=\"user-content-manual-installation\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#manual-installation\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Manual Installation</h3>\n<p>By\
    \ default, it is recommended to follow the install locations that are indicated\
    \ on the top of <code>scripts/run.sh</code>\nand the top of <code>config.json</code>.\
    \ These two files provide the configuration options to get the program running.</p>\n\
    <p>Spack should be installed in the following location: <code>$HOME/spack/</code></p>\n\
    <p>This Github repo should be cloned in the following location: <code>$HOME/Correlation_Compressibility/</code>\n\
    This location is also referenced as the <code>COMPRESS_HOME</code> environment\
    \ variable.</p>\n<p>A dataset folder called 'datasets' should be in the following\
    \ location: <code>$HOME/Correlation_Compressibility/datasets/</code>.</p>\n<p>Clone\
    \ the repo but make sure to install or load <code>git-lfs</code> first.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> install/module load git-lfs, needed to download example_data\
    \ for building the container</span>\nsudo dnf install git-lfs <span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span>Fedora/CentOS Stream 8</span>\nsudo apt-get install\
    \ git-lfs <span class=\"pl-c\"><span class=\"pl-c\">#</span> Ubuntu</span>\nspack\
    \ install git-lfs<span class=\"pl-k\">;</span> spack load git-lfs <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> using spack</span>\n\n<span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> clone this repository</span>\ngit clone https://github.com/FTHPC/Correlation_Compressibility\
    \ <span class=\"pl-smi\">$HOME</span>/Correlation_Compressibility\n<span class=\"\
    pl-c1\">cd</span> <span class=\"pl-smi\">$HOME</span>/Correlation_Compressibility</pre></div>\n\
    <p>If you forgot to install <code>git-lfs</code> before and have an empty file\
    \ in the  <code>datasets</code> folder, you should install <code>git-lfs</code>\n\
    and then run the following:</p>\n<pre><code>git lfs fetch\ngit lfs checkout\n\
    </code></pre>\n<p>Once Spack is installed, there is a <code>spack.yaml</code>\
    \ configuration file containing the Spack environment necessary to run the program.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c1\">cd</span>\
    \ <span class=\"pl-smi\">$HOME</span>\ngit clone --depth=1 https://github.com/spack/spack\n\
    git clone --depth=1 https://github.com/robertu94/spack_packages \n<span class=\"\
    pl-c1\">source</span> ./spack/share/spack/setup-env.sh \nspack compiler find\n\
    spack external find \nspack repo add --scope=site ./spack_packages \n<span class=\"\
    pl-c1\">cd</span> <span class=\"pl-smi\">$HOME</span>/Correlation_Compressibility\
    \ \nspack env activate <span class=\"pl-c1\">.</span>\nspack install\n<span class=\"\
    pl-k\">export</span> COMPRESS_HOME=<span class=\"pl-smi\">$HOME</span>/Correlation_Compressibility\
    \ </pre></div>\n<p>These commands will install the environment. The environment\
    \ only needs to be installed once.\nIf you are using an older &lt; gcc11, then\
    \ you will need to add the following to the <code>spack.yaml</code> file:</p>\n\
    <pre><code>^libstdcompat+boost\n</code></pre>\n<p>after <code>^mgard@robertu94+cuda</code>\
    \ but before the <code>,</code>.</p>\n<h3><a id=\"user-content-to-run-the-training-and-prediction-timing-analysis-demonstration\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#to-run-the-training-and-prediction-timing-analysis-demonstration\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To run the\
    \ training and prediction timing analysis demonstration</h3>\n<p>In order to run\
    \ the timing analysis, a dataset must be specified.\nThere are two datasets setup\
    \ within this demonstration.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sh runtime_analysis/runtime.sh -d [DATASET]</pre></div>\n<p>[DATASET] can\
    \ be either [NYX] or [SCALE]</p>\n<p>After running the above script, an *.RData\
    \ file(s) will be produced giving the approprirate timing information of\nthe\
    \ training and prediction for the regression models.</p>\n<p>Note: A quicker and\
    \ more efficient quantized entropy method is demonstrated in <code>qentropy.cc</code></p>\n\
    <h4><a id=\"user-content-the-following-below-runs-qentropycc\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#the-following-below-runs-qentropycc\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>The following below runs <code>qentropy.cc</code>\n\
    </h4>\n<div class=\"highlight highlight-source-shell\"><pre>g++ -std=c++2a -O3\
    \ qentropy.cc -o qentropy -march=native -mtune=native\n./qentropy</pre></div>\n\
    <p>Note: Please run the runtime analysis for both datasets before running the\
    \ following section.</p>\n<h3><a id=\"user-content-replication-of-figures-how-to-run-statistical-prediction-of-compression-ratios-and-the-prediction-validation\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#replication-of-figures-how-to-run-statistical-prediction-of-compression-ratios-and-the-prediction-validation\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Replication\
    \ of figures: how to run statistical prediction of compression ratios and the\
    \ prediction validation</h3>\n<p>The script <code>graphs_paper_container.R</code>\
    \  saves the graphs presented in the paper and provides associated validation\
    \ metrics (correlation and median absolute error percentage).</p>\n<p>The script\
    \ <code>graphs_paper_container.R</code> will source the scripts  <code>load_dataset_paper.R</code>\
    \ and <code>functions_paper.R</code> that respectively load the dataset of interest\
    \ and perform the regression analysis (training and prediction in cross-validation).\n\
    As a consequence the scripts  <code>load_dataset_paper.R</code> and <code>functions_paper.R</code>\
    \ do not need to be run by the user.</p>\n<p>The script <code>graphs_paper_container.R</code>\
    \  is run via the command:\n<code>bash sh replicate.sh</code></p>\n<p>From running\
    \ the script once, it will save all Figures 1, 3, 4 and 5 into .png files from\
    \ the paper as well as corresponding validation metrics.\nFigure 2 is not saved\
    \ as it provides a simple vizualization of slices of the datasets.\nSlices of\
    \ the datasets are generated in the Section \"How to compute statistical predictors\
    \ and compression metrics\" and can be stored, however we do not save them here\
    \ to save space in the container.\nNumbers for Tables 2, 3 and 5 are printed in\
    \ the R console.\nAll printed validation metrics are save into a file named <code>figure_replication.log</code>.\n\
    Figures and the log-file are saved in the same folder as the one where R script\
    \ is run and the filename structure is <code>figY_*.png</code> with Y is the figure\
    \ number reference in the paper and <code>*</code> provides additional informnation\
    \ about the data and the compressor.<br>\nNumbers for Table 4 are saved in the\
    \ last section in .txt files <code>statistic_benchmark_runtime_X.txt</code> with\
    \ X the studied dataset (NYX or SCALE).</p>\n<p>In order to limit the container\
    \ size to aid reproducibility, we only added a restricted number of scientific\
    \ datasets in the container and we rely on csv files from our production runs\
    \ (saved as described above in the Section \"How to compute statistical predictors\
    \ on datasets\").\nMore datasets are available on <a href=\"https://sdrbench.github.io\"\
    \ rel=\"nofollow\">SDRBench</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1675473427.0
HEPonHPC/hepnos_eventselection:
  data_format: 2
  description: null
  filenames:
  - docker/hepnos/spack.yaml
  full_name: HEPonHPC/hepnos_eventselection
  latest_release: null
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1658856345.0
LLNL/uberenv:
  data_format: 2
  description: Automates using spack to build and deploy software
  filenames:
  - .ci/test-project/spack_configs/toss_3_x86_64_ib/spack.yaml
  - .ci/test-project/spack_configs/linux_ubuntu_22/spack.yaml
  - .ci/test-project/spack_configs/darwin/spack.yaml
  full_name: LLNL/uberenv
  latest_release: v1.0.0
  readme: '<h1><a id="user-content-uberenv" class="anchor" aria-hidden="true" href="#uberenv"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>uberenv</h1>

    <p>Automates using a package manager to build and deploy software.</p>

    <p><a href="https://uberenv.readthedocs.io" rel="nofollow"><img src="https://camo.githubusercontent.com/02247bd3961daeb4d17d6d1d8f821df0c991efeb0c5f0411fed0a94bd9fa3ebe/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f75626572656e762f62616467652f3f76657273696f6e3d6c6174657374"
    alt="Read the Docs" data-canonical-src="https://readthedocs.org/projects/uberenv/badge/?version=latest"
    style="max-width: 100%;"></a></p>

    <p>Uberenv is a python script that helps automate building

    third-party dependencies for development and deployment.</p>

    <p>Uberenv uses Spack (<a href="https://www.spack.io/" rel="nofollow">https://www.spack.io/</a>)
    on Unix-based systems (e.g. Linux and macOS)

    and Vcpkg (<a href="https://github.com/microsoft/vcpkg">https://github.com/microsoft/vcpkg</a>)
    on Windows systems.</p>

    <p>Uberenv was released as part of the Conduit project (<a href="https://github.com/LLNL/conduit/">https://github.com/LLNL/conduit/</a>).

    It is included in-source in several projects, this repo is used to hold the latest
    reference version.</p>

    <p>For more details, see Uberenv''s documention:</p>

    <p><a href="https://uberenv.readthedocs.io" rel="nofollow">https://uberenv.readthedocs.io</a></p>

    <p>You can also find details about how it is used in Conduit''s documentation:</p>

    <p><a href="https://llnl-conduit.readthedocs.io/en/latest/building.html#building-conduit-and-third-party-dependencies"
    rel="nofollow">https://llnl-conduit.readthedocs.io/en/latest/building.html#building-conduit-and-third-party-dependencies</a></p>

    <p>Conduit''s source repo also serves as an example for uberenv and spack configuration
    files, etc:</p>

    <p><a href="https://github.com/LLNL/conduit/tree/master/scripts/uberenv">https://github.com/LLNL/conduit/tree/master/scripts/uberenv</a></p>

    '
  stargazers_count: 20
  subscribers_count: 9
  topics:
  - shell
  - build-tools
  updated_at: 1676463765.0
Lumi-supercomputer/lumi-spack-settings:
  data_format: 2
  description: Spack configuration files for LUMI
  filenames:
  - 22.08/0.19.0/spack.yaml
  full_name: Lumi-supercomputer/lumi-spack-settings
  latest_release: null
  readme: '<h1><a id="user-content-spack-configuration-files-for-lumi" class="anchor"
    aria-hidden="true" href="#spack-configuration-files-for-lumi"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Spack configuration files for LUMI</h1>

    <p>Repository containing configuration files for the Spack instances installed
    in <code>/appl/lumi/spack</code> on LUMI for public use. The files in this repository
    can be found in <code>/appl/lumi/spack/etc/</code> on LUMI. The folder hierarchy
    is determined by the Cray Programming Environment (CPE) version and Spack release
    version. For example, the directory</p>

    <pre><code>22.08/0.18.1/

    22.08/0.18.1-user/

    </code></pre>

    <p>contains the configuration files for Spack version 0.18.1 configured to use
    CPE 22.08. The first instance <code>0.18.1</code> is the upstream instance, which
    is maintained by the LUMI Support Team. The second instance <code>0.18.1-user</code>
    is a separate instance configured to install packages in a user-defined directory
    in e.g. <code>/project/</code>. It is chained to the upstream instance, so that
    already installed packages can be reused.</p>

    <p>If you are user of LUMI, and want to set up your own instance, you can copy
    the <code>compilers.yaml</code>and  <code>packages.yaml</code> files to your instance.
    The <code>config.yaml</code> needs to be modified if you want to use that one.</p>

    '
  stargazers_count: 0
  subscribers_count: 12
  topics: []
  updated_at: 1675956191.0
MeteoSwiss/fdb-fortran:
  data_format: 2
  description: Fortran Interface to ECMWF's FDB
  filenames:
  - docker/spack.yaml
  full_name: MeteoSwiss/fdb-fortran
  latest_release: null
  stargazers_count: 0
  subscribers_count: 4
  topics:
  - numericalweatherpredictions
  updated_at: 1678447894.0
MichaelBrim/unify-olcf-scripts:
  data_format: 2
  description: null
  filenames:
  - summit/spack-env/spack.yaml
  - crusher/spack-env/spack.yaml
  full_name: MichaelBrim/unify-olcf-scripts
  latest_release: null
  readme: '<h1><a id="user-content-unify-olcf-scripts" class="anchor" aria-hidden="true"
    href="#unify-olcf-scripts"><span aria-hidden="true" class="octicon octicon-link"></span></a>unify-olcf-scripts</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1649778838.0
NCAR/spack-casper:
  data_format: 2
  description: Spack production user software stack on the Casper system
  filenames:
  - spack.yaml
  full_name: NCAR/spack-casper
  latest_release: null
  readme: '<h1><a id="user-content-ncar-spack-deployment" class="anchor" aria-hidden="true"
    href="#ncar-spack-deployment"><span aria-hidden="true" class="octicon octicon-link"></span></a>NCAR
    Spack Deployment</h1>

    <p>This branch tracks the <strong>production</strong> deployment of Spack for
    the following configuration:</p>

    <table>

    <thead>

    <tr>

    <th></th>

    <th>casper</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Creation date</td>

    <td>Wed Mar 22 21:50:19 MDT 2023</td>

    </tr>

    <tr>

    <td>ncar-spack commit</td>

    <td>fc3df9ab78502b74368eb656923301f672d491f6</td>

    </tr>

    <tr>

    <td>Host version</td>

    <td>23.03</td>

    </tr>

    <tr>

    <td>Spack version</td>

    <td></td>

    </tr>

    <tr>

    <td>Deployment path</td>

    <td>/glade/u/apps/casper/23.03</td>

    </tr>

    <tr>

    <td>Environments path</td>

    <td>/glade/work/csgteam/spack-deployments/casper/23.03/envs</td>

    </tr>

    </tbody>

    </table>

    <p>This repository should <em>only</em> be updated via the <code>publish</code>
    script contained in the build environment. Any manual changes to this branch will
    cause headaches when you or another consultant attempt to publish new packages!</p>

    '
  stargazers_count: 0
  subscribers_count: 8
  topics: []
  updated_at: 1679548145.0
NCAR/spack-gust:
  data_format: 2
  description: Spack production user software stack on the Gust test system
  filenames:
  - spack.yaml
  full_name: NCAR/spack-gust
  latest_release: null
  readme: '<h1><a id="user-content-ncar-spack-deployment" class="anchor" aria-hidden="true"
    href="#ncar-spack-deployment"><span aria-hidden="true" class="octicon octicon-link"></span></a>NCAR
    Spack Deployment</h1>

    <p>This branch tracks the <strong>production</strong> deployment of Spack for
    the following configuration:</p>

    <table>

    <thead>

    <tr>

    <th></th>

    <th>gust</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Creation date</td>

    <td>Sat Dec 17 18:52:51 MST 2022</td>

    </tr>

    <tr>

    <td>ncar-spack commit</td>

    <td>9f557cbfe06b92815bc29b92e7cf4eff09917601</td>

    </tr>

    <tr>

    <td>Host version</td>

    <td>22.12</td>

    </tr>

    <tr>

    <td>Spack version</td>

    <td>165cd55980572fc3ccef5f322ad8ad8b3e5dd3db</td>

    </tr>

    <tr>

    <td>Deployment path</td>

    <td>/glade/u/apps/gust/22.12</td>

    </tr>

    <tr>

    <td>Environments path</td>

    <td>/glade/work/csgteam/spack-deployments/gust/22.12/envs</td>

    </tr>

    </tbody>

    </table>

    <p>This repository should <em>only</em> be updated via the <code>publish</code>
    script contained in the build environment. Any manual changes to this branch will
    cause headaches when you or another consultant attempt to publish new packages!</p>

    '
  stargazers_count: 3
  subscribers_count: 13
  topics: []
  updated_at: 1676353353.0
NERSC/spack-infrastructure:
  data_format: 2
  description: null
  filenames:
  - spack-configs/perlmutter-e4s-22.11/gcc/spack.yaml
  - spack-configs/perlmutter-e4s-22.05/prod/nvhpc/spack.yaml
  - spack-configs/perlmutter-e4s-22.11/nvhpc/spack.yaml
  - spack-configs/perlmutter-e4s-22.05/prod/cuda/spack.yaml
  - spack-configs/perlmutter-e4s-22.05/cuda/spack.yaml
  - spack-configs/perlmutter-user-spack/spack.yaml
  - spack-configs/perlmutter-e4s-22.05/prod/cce/spack.yaml
  - spack-configs/perlmutter-e4s-22.05/nvhpc/spack.yaml
  - spack-configs/perlmutter-e4s-22.11/cce/spack.yaml
  - spack-configs/perlmutter-e4s-22.05/prod/gcc/spack.yaml
  - spack-configs/perlmutter-e4s-22.11/prod/nvhpc/spack.yaml
  - spack-configs/perlmutter-e4s-22.11/cuda/spack.yaml
  - spack-configs/cori-e4s-22.02/spack.yaml
  - spack-configs/perlmutter-e4s-22.11/prod/cce/spack.yaml
  - spack-configs/cori-e4s-22.02/ci/spack.yaml
  - spack-configs/perlmutter-e4s-22.11/prod/cuda/spack.yaml
  - spack-configs/cori-e4s-22.02/ci/gerty/spack.yaml
  - spack-configs/perlmutter-e4s-21.11/prod/spack.yaml
  - spack-configs/perlmutter-e4s-22.11/prod/gcc/spack.yaml
  full_name: NERSC/spack-infrastructure
  latest_release: null
  readme: "<h1><a id=\"user-content-spack-infrastructure\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#spack-infrastructure\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Spack Infrastructure</h1>\n<p>The spack infrastructure\
    \ repository contains spack configuration in the form of <code>spack.yaml</code>\
    \ required to build spack stacks on Cori and Perlmutter system. We leverage gitlab\
    \ to automate software stack deployment which is configured using the <a href=\"\
    https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/.gitlab-ci.yml\"\
    \ rel=\"nofollow\">.gitlab-ci.yml</a> file. The documentation is available at\
    \ <a href=\"https://nersc-spack-infrastructure.rtfd.io\" rel=\"nofollow\">https://nersc-spack-infrastructure.rtfd.io</a></p>\n\
    <h2><a id=\"user-content-spack-configuration\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#spack-configuration\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Spack Configuration</h2>\n<p>The spack configuration\
    \ can be found in <a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/tree/main/spack-configs\"\
    \ rel=\"nofollow\">spack-configs</a> directory with subdirectory for each deployment.\n\
    Each pipeline can be run if one sets the variable <code>PIPELINE_NAME</code> to\
    \ a unique value in order to run a pipeline. You can check the <a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/.gitlab-ci.yml\"\
    \ rel=\"nofollow\">.gitlab-ci.yml</a> for the gitlab configuration. The pipeline\
    \ can be run via <a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/pipelines/new\"\
    \ rel=\"nofollow\">web interface</a>, if you chose this route, you must set <code>PIPELINE_NAME</code>\
    \ to the appropriate value.</p>\n<p>If you want to trigger pipeline via <a href=\"\
    https://software.nersc.gov/NERSC/spack-infrastructure/-/pipelines/new\" rel=\"\
    nofollow\">web-interface</a> you will need to define PIPELINE_NAME variable to\
    \ trigger the appropriate pipeline.</p>\n<h2><a id=\"user-content-running-ci-pipelines\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#running-ci-pipelines\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running CI Pipelines</h2>\n<p>This\
    \ project is configured with several <a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/pipeline_schedules\"\
    \ rel=\"nofollow\">scheduled pipelines</a> that will run at different times.</p>\n\
    <p>Currently, we have a shell runner installed on Perlmutter using <code>e4s</code>\
    \ account which is configured with following settings. You can find list of runners\
    \ and their runner status under <a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/settings/ci_cd\"\
    \ rel=\"nofollow\">Settings &gt; CI/CD &gt; Runners</a>. Please make sure you\
    \ login to the appropriate hostname when starting the gitlab runner.</p>\n<table>\n\
    <thead>\n<tr>\n<th>System</th>\n<th>Runner Name</th>\n<th>Hostname</th>\n</tr>\n\
    </thead>\n<tbody>\n<tr>\n<td>perlmutter</td>\n<td><code>perlmutter-e4s</code></td>\n\
    <td><code>login27</code></td>\n</tr>\n<tr>\n<td>cori</td>\n<td><code>cori-e4s</code></td>\n\
    <td><code>cori02</code></td>\n</tr>\n<tr>\n<td>muller</td>\n<td><code>muller-e4s</code></td>\n\
    <td><code>login02</code></td>\n</tr>\n<tr>\n<td>gerty</td>\n<td><code>gerty-e4s</code></td>\n\
    <td><code>gert01</code></td>\n</tr>\n</tbody>\n</table>\n<p>The runner configuration\
    \ files are located in <code>~/.gitlab-runner</code> for user <strong>e4s</strong>.</p>\n\
    <p>The production pipelines are triggered via web-interface which requires approval\
    \ from a project maintainer. Production pipelines should be run when we need to\
    \ do full redeployment of stack.</p>\n<h2><a id=\"user-content-current-challenges\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#current-challenges\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Current Challenges</h2>\n<p>There\
    \ are several challenges with building spack stack at NERSC which can be summarized\
    \ as follows</p>\n<ul>\n<li>\n<p><strong>System OS + Cray Programming Environment\
    \ (CPE) changes</strong>: A system upgrade such as change to <code>glibc</code>\
    \ or upgrades in CPE can lead to full software stack rebuild, especially if you\
    \ have external packages set for packages like <code>cray-mpich</code>, <code>cray-libsci</code>\
    \ which generally change between versions</p>\n</li>\n<li>\n<p><strong>Incompatibile\
    \ compilers</strong>: Some packages can't be built with certain compilers (<code>nvhpc</code>,\
    \ <code>aocc</code>) which could be due to several factors.</p>\n<ul>\n<li>An\
    \ application doesn't have support though it was be added in newer version but\
    \ you don't have it in your spack release used for deployment</li>\n<li>Lack of\
    \ support in spack package recipe or spack-core base including spack-cray detection.\
    \ This may require getting fix and cherry-pick commit or waiting for new version</li>\n\
    <li>Spack Cray detection is an important part in build errors including how one\
    \ specifies externals via <code>modules</code> vs <code>prefix</code> both could\
    \ be provided and it requires experimentation. An example of this is trying to\
    \ get <code>cray-mpich</code> external one could set something like this with\
    \ modules or prefix</li>\n</ul>\n<div class=\"highlight highlight-source-yaml\"\
    ><pre>  <span class=\"pl-ent\">cray-mpich</span>:\n    <span class=\"pl-ent\"\
    >buildable</span>: <span class=\"pl-c1\">false</span>\n    <span class=\"pl-ent\"\
    >externals</span>:\n    - <span class=\"pl-ent\">spec</span>: <span class=\"pl-s\"\
    >cray-mpich@8.1.11 %gcc@9.3.0</span>\n      <span class=\"pl-ent\">prefix</span>:\
    \ <span class=\"pl-s\">/opt/cray/pe/mpich/8.1.11/ofi/gnu/9.1</span>\n      <span\
    \ class=\"pl-ent\">modules</span>:\n      - <span class=\"pl-s\">cray-mpich/8.1.11</span>\n\
    \      - <span class=\"pl-s\">cudatoolkit/21.9_11.4</span></pre></div>\n<ul>\n\
    <li>\n<strong>Spack concretizer</strong> prevent one from chosing a build configration\
    \ for a spec. This requires a few troubleshooting step but usually boils down\
    \ to:\n<ul>\n<li>Read the spack package file <code>spack edit &lt;package&gt;</code>\
    \ for conflicts and try <code>spack spec</code> to see concretized spec.</li>\n\
    <li>Try different version, different compiler, different dependency. Some packages\
    \ have conflicting variant for instance one can't enable <code>+openmp</code>\
    \ and <code>+pthread</code> it is mutually exclusive.</li>\n</ul>\n</li>\n</ul>\n\
    </li>\n</ul>\n<p>There is a document <a href=\"https://docs.google.com/document/d/1jWrCcK8LgpNDMytXhLdBYpIusidkoowrZAH1zos7zIw/edit?usp=sharing\"\
    \ rel=\"nofollow\">Spack E4S Issues on Permlutter</a> outlining current issues\
    \ with spack. If you need access to document please contact <strong>Shahzeb Siddiqui</strong>.</p>\n\
    <h2><a id=\"user-content-contact\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #contact\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contact</h2>\n\
    <p>If you need elevated privledge or assistance with this project please contact\
    \ one of the maintainers:</p>\n<ul>\n<li><strong>Shahzeb Siddiqui (<a href=\"\
    mailto:shahzebsiddiqui@lbl.gov\">shahzebsiddiqui@lbl.gov</a>)</strong></li>\n\
    <li><strong>Erik Palmer (<a href=\"mailto:epalmer@lbl.gov\">epalmer@lbl.gov</a>)</strong></li>\n\
    <li><strong>Justin Cook (<a href=\"mailto:JSCook@lbl.gov\">JSCook@lbl.gov</a>)</strong></li>\n\
    <li>E4S Team: <strong>Sameer Shende (<a href=\"mailto:sameer@cs.uoregon.edu\"\
    >sameer@cs.uoregon.edu</a>)</strong>, <strong>Christopher Peyralans (<a href=\"\
    mailto:lpeyrala@uoregon.edu\">lpeyrala@uoregon.edu</a>)</strong>, <strong>Wyatt\
    \ Spear (<a href=\"mailto:wspear@cs.uoregon.edu\">wspear@cs.uoregon.edu</a>)</strong>,\
    \ <strong>Nicholas Chaimov (<a href=\"mailto:nchaimov@paratools.com\">nchaimov@paratools.com</a>)</strong>\n\
    </li>\n</ul>\n"
  stargazers_count: 8
  subscribers_count: 14
  topics: []
  updated_at: 1673545287.0
NOAA-EMC/GSI:
  data_format: 2
  description: Gridpoint Statistical Interpolation
  filenames:
  - ci/spack.yaml
  full_name: NOAA-EMC/GSI
  latest_release: gefs_v12.0.2
  stargazers_count: 43
  subscribers_count: 21
  topics: []
  updated_at: 1679364131.0
NOAA-EMC/NCEPLIBS-grib_util:
  data_format: 2
  description: This is a collection of NCEP GRIB related utilities.
  filenames:
  - ci/spack.yaml
  full_name: NOAA-EMC/NCEPLIBS-grib_util
  latest_release: v1.2.4
  readme: '<h1><a id="user-content-nceplibs-grib_util" class="anchor" aria-hidden="true"
    href="#nceplibs-grib_util"><span aria-hidden="true" class="octicon octicon-link"></span></a>NCEPLIBS-grib_util</h1>

    <p>This is a collection of NCEP GRIB related utilities. This is related

    to the <a href="https://github.com/NOAA-EMC/NCEPLIBS">NCEPLIBS</a> project.</p>

    <p>For complete documentation see

    <a href="https://noaa-emc.github.io/NCEPLIBS-grib_util/" rel="nofollow">https://noaa-emc.github.io/NCEPLIBS-grib_util/</a>.
    For the NCEP WMO GRIB2

    Documentation see

    <a href="https://www.nco.ncep.noaa.gov/pmb/docs/grib2/grib2_doc/" rel="nofollow">https://www.nco.ncep.noaa.gov/pmb/docs/grib2/grib2_doc/</a>.</p>

    <h2><a id="user-content-related-nceplibs-projects" class="anchor" aria-hidden="true"
    href="#related-nceplibs-projects"><span aria-hidden="true" class="octicon octicon-link"></span></a>Related
    NCEPLIBS Projects</h2>

    <table>

    <thead>

    <tr>

    <th>Repository</th>

    <th>Notes</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><a href="https://github.com/NOAA-EMC/NCEPLIBS-g2c">NCEPLIBS-g2c</a></td>

    <td>C implementation of the GRIB 2 functions</td>

    </tr>

    <tr>

    <td><a href="https://github.com/NOAA-EMC/NCEPLIBS-g2">NCEPLIBS-g2</a></td>

    <td>Fortran implementation of the GRIB 2 functions</td>

    </tr>

    <tr>

    <td><a href="https://github.com/NOAA-EMC/NCEPLIBS-g2tmpl">NCEPLIBS-g2tmpl</a></td>

    <td>Utilities for GRIB2 templates</td>

    </tr>

    </tbody>

    </table>

    <h2><a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Authors</h2>

    <table>

    <thead>

    <tr>

    <th>Utility</th>

    <th>Author(s)</th>

    <th>User(s)</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>cnvgrib</td>

    <td>Stephen Gilbert, Gordon, Boi Vuong</td>

    <td>???</td>

    </tr>

    <tr>

    <td>copygb</td>

    <td>Mark Iredell, Stephen Gilbert, Trojan, Boi Vuong</td>

    <td>UFS_UTILS</td>

    </tr>

    <tr>

    <td>copygb2</td>

    <td>Mark Iredell, Stephen Gilbert, Boi Vuong</td>

    <td>???</td>

    </tr>

    <tr>

    <td>degrib2</td>

    <td>Boi Vuong</td>

    <td>???</td>

    </tr>

    <tr>

    <td>grb2index</td>

    <td>Mark Iredell, Stephen Gilbert, Boi Vuong</td>

    <td>???</td>

    </tr>

    <tr>

    <td>grbindex</td>

    <td>Mark Iredell, Stephen Gilbert, Boi Vuong, W. Ebisuzaki</td>

    <td>FAA and AWIPS (CONUS grid id 211)</td>

    </tr>

    <tr>

    <td>tocgrib</td>

    <td>Stephen Gilbert, Boi Vuong, Farley, R. E. Jones</td>

    <td>RAP for FAA</td>

    </tr>

    <tr>

    <td>tocgrib2</td>

    <td>Stephen Gilbert, Boi Vuong</td>

    <td>(GFS, NAM, SMOKE, RAP, HRRR, NWPS, etc.) in production for AWIPS  and NDFD</td>

    </tr>

    <tr>

    <td>tocgrib2super</td>

    <td>Stephen Gilbert, Boi Vuong</td>

    <td>(GFS, NAM, SMOKE, RAP, HRRR, NWPS, etc.) in production for AWIPS  and NDFD</td>

    </tr>

    <tr>

    <td>wgrib</td>

    <td>W. Ebisuzaki</td>

    <td>FAA and AWIPS (CONUS grid id 211)</td>

    </tr>

    </tbody>

    </table>

    <p>Code Manager : Hang Lei, Edward Hartnett</p>

    <h2><a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>This package requires the following third party libraries:</p>

    <ul>

    <li><a href="http://www.ece.uvic.ca/~mdadams/jasper/" rel="nofollow">Jasper</a></li>

    <li><a href="http://www.libpng.org/pub/png/libpng.html" rel="nofollow">libpng</a></li>

    <li><a href="http://www.gzip.org/zlib/" rel="nofollow">libz</a></li>

    </ul>

    <p>This package requires the folling NCEPLIBS libraries:</p>

    <ul>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-sp">NCEPLIBS-sp</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-ip">NCEPLIBS-ip</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-bacio">NCEPLIBS-bacio</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-g2">NCEPLIBS-g2</a></li>

    <li>

    <a href="https://github.com/NOAA-EMC/NCEPLIBS-w3nco">NCEPLIBS-w3nco</a> (before
    version 1.3.0)</li>

    <li>

    <a href="https://github.com/NOAA-EMC/NCEPLIBS-w3emc">NCEPLIBS-w3emc</a> (starting
    version 1.3.0)</li>

    </ul>

    <h2><a id="user-content-installing" class="anchor" aria-hidden="true" href="#installing"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installing</h2>

    <pre><code>mkdir build

    cd build

    cmake -DCMAKE_INSTALL_PREFIX=/path/to/install -DCMAKE_PREFIX_PATH=/path/to/dependencies
    ..

    make -j4

    make install

    </code></pre>

    <h2><a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Disclaimer</h2>

    <p>The United States Department of Commerce (DOC) GitHub project code is

    provided on an "as is" basis and the user assumes responsibility for

    its use. DOC has relinquished control of the information and no longer

    has responsibility to protect the integrity, confidentiality, or

    availability of the information. Any claims against the Department of

    Commerce stemming from the use of its GitHub project will be governed

    by all applicable Federal law. Any reference to specific commercial

    products, processes, or services by service mark, trademark,

    manufacturer, or otherwise, does not constitute or imply their

    endorsement, recommendation or favoring by the Department of

    Commerce. The Department of Commerce seal and logo, or the seal and

    logo of a DOC bureau, shall not be used in any manner to imply

    endorsement of any commercial product or activity by DOC or the United

    States Government.</p>

    '
  stargazers_count: 6
  subscribers_count: 3
  topics: []
  updated_at: 1656698184.0
NOAA-EMC/UPP:
  data_format: 2
  description: null
  filenames:
  - ci/spack.yaml
  full_name: NOAA-EMC/UPP
  latest_release: upp-srw-v2.1.0
  readme: '<h1><a id="user-content-unified-post-processing-upp" class="anchor" aria-hidden="true"
    href="#unified-post-processing-upp"><span aria-hidden="true" class="octicon octicon-link"></span></a>Unified
    Post-Processing (UPP)</h1>

    <p>The Unified Post Processor (UPP) software package is a software

    package designed to generate useful products from raw model

    output.</p>

    <p>The UPP is currently used in operations with the Global Forecast

    System (GFS), GFS Ensemble Forecast System (GEFS), North American

    Mesoscale (NAM), Rapid Refresh (RAP), High Resolution Rapid Refresh

    (HRRR), Short Range Ensemble Forecast (SREF), and Hurricane WRF (HWRF)

    applications. It is also used in the Unified Forecasting System (UFS),

    including the Rapid Refresh Forecast System (RRFS), Hurricane Application

    Forecasting System (HAFS), and the Medium Range Weather (MRW) and Short

    Range Weather (SRW) Applications.</p>

    <p>The UPP provides the capability to compute a variety of diagnostic

    fields and interpolate to pressure levels or other vertical

    coordinates.</p>

    <p>UPP also incorporates the Joint Center for Satellite Data Assimilation

    (JCSDA) Community Radiative Transfer Model (CRTM) to compute model

    derived brightness temperature (TB) for various instruments and

    channels. This additional feature enables the generation of a number

    of simulated satellite products including GOES products.</p>

    <p>Output from the UPP is in National Weather Service (NWS) and World

    Meteorological Organization (WMO) GRIB2 format and can be used

    directly by visualization, plotting, or verification packages, or for

    further downstream post-processing, e.g. statistical post-processing

    techniques.</p>

    <p>Examples of UPP products include:</p>

    <ul>

    <li>T, Z, humidity, wind, cloud water, cloud ice, rain, and snow on pressure levels</li>

    <li>SLP, shelter level T, humidity, and wind fields</li>

    <li>Precipitation-related fields</li>

    <li>PBL-related fields</li>

    <li>Severe weather products (e.g. CAPE, Vorticity, Wind shear)</li>

    <li>Radiative/Surface fluxes</li>

    <li>Cloud related fields</li>

    <li>Aviation products</li>

    <li>Radar reflectivity products</li>

    <li>Satellite look-alike products</li>

    </ul>

    <h2><a id="user-content-user-support" class="anchor" aria-hidden="true" href="#user-support"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>User Support</h2>

    <p>Support for the UFS UPP is provided through <a href="https://github.com/NOAA-EMC/UPP/discussions">GitHub
    Discussions</a>.</p>

    <h2><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h2>

    <p>User Guide for latest public release: <a href="https://upp.readthedocs.io/en/latest/"
    rel="nofollow">https://upp.readthedocs.io/en/latest/</a>.</p>

    <p>Technical code-level documentation: <a href="https://noaa-emc.github.io/UPP/"
    rel="nofollow">https://noaa-emc.github.io/UPP/</a>.</p>

    <h2><a id="user-content-developer-information" class="anchor" aria-hidden="true"
    href="#developer-information"><span aria-hidden="true" class="octicon octicon-link"></span></a>Developer
    Information</h2>

    <p>Please see review the <a href="https://github.com/NOAA-EMC/UPP/wiki">wiki</a></p>

    <h2><a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Authors</h2>

    <p>NCEP/EMC Developers</p>

    <p>Code Managers: Wen Meng, Huiya Chuang, Kate Fossell</p>

    <h2><a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>The UPP requires certain NCEPLIB packages to be installed via

    the HPC-Stack project.</p>

    <ul>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-g2">NCEPLIBS-g2</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-g2tmpl">NCEPLIBS-g2tmpl</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-sp">NCEPLIBS-sp</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-ip">NCEPLIBS-ip</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-bacio">NCEPLIBS-bacio</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-w3emc">NCEPLIBS-w3emc</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-w3nco">NCEPLIBS-w3nco</a></li>

    <li><a href="https://github.com/noaa-emc/emc_crtm">CRTM</a></li>

    </ul>

    <p>Also required to build NCEPpost executable (cmake option

    BUILD_POSTEXEC):</p>

    <ul>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-sigio">NCEPLIBS-sigio</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-sfcio">NCEPLIBS-sfcio</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-nemsio">NCEPLIBS-nemsio</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-gfsio">NCEPLIBS-gfsio</a></li>

    </ul>

    <p>The <a href="https://github.com/NOAA-EMC/NCEPLIBS-wrf_io">NCEPLIBS-wrf_io</a>

    library is required to build with NCEPpost with WRF-IO library (cmake

    option BUILD_WITH_WRFIO).</p>

    <p>The following third-party libraries are required:</p>

    <ul>

    <li><a href="https://github.com/Unidata/netcdf-c">netcdf-c</a></li>

    <li><a href="https://github.com/Unidata/netcdf-fortran">netcdf-fortran</a></li>

    <li><a href="https://github.com/jasper-software/jasper">Jasper</a></li>

    <li><a href="http://www.libpng.org/pub/png/libpng.html" rel="nofollow">libpng</a></li>

    <li><a href="https://zlib.net/" rel="nofollow">libz</a></li>

    </ul>

    <h2><a id="user-content-building" class="anchor" aria-hidden="true" href="#building"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Building</h2>

    <p>Builds include:</p>

    <ul>

    <li>

    <p>Inline post (UPP library): Currently only supported for the GFS, RRFS,

    HAFS, and the UFS-MRW Application.</p>

    </li>

    <li>

    <p>Offline post (UPP executable): Supported for Regional applications

    including SRW, RRFS, HAFS, and standalone applications of UPP.</p>

    </li>

    </ul>

    <p>CMake is used to manage all builds of the UPP.

    The script <code>UPP/tests/compile_upp.sh</code> can be used to automatically

    build UPP on fully supported platforms where HPC-stack is supported.

    Details in this script can be used to build on new platforms.</p>

    <h2><a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Disclaimer</h2>

    <p>The United States Department of Commerce (DOC) GitHub project code is

    provided on an "as is" basis and the user assumes responsibility for

    its use. DOC has relinquished control of the information and no longer

    has responsibility to protect the integrity, confidentiality, or

    availability of the information. Any claims against the Department of

    Commerce stemming from the use of its GitHub project will be governed

    by all applicable Federal law. Any reference to specific commercial

    products, processes, or services by service mark, trademark,

    manufacturer, or otherwise, does not constitute or imply their

    endorsement, recommendation or favoring by the Department of

    Commerce. The Department of Commerce seal and logo, or the seal and

    logo of a DOC bureau, shall not be used in any manner to imply

    endorsement of any commercial product or activity by DOC or the United

    States Government.</p>

    '
  stargazers_count: 23
  subscribers_count: 15
  topics: []
  updated_at: 1677617073.0
NOAA-EMC/WW3:
  data_format: 2
  description: WAVEWATCH III
  filenames:
  - model/ci/spack_gnu.yaml
  - model/ci/spack_intel.yaml
  full_name: NOAA-EMC/WW3
  latest_release: 6.07.1
  readme: "<h1><a id=\"user-content-the-wavewatch-iii-framework\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#the-wavewatch-iii-framework\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>The WAVEWATCH III Framework</h1>\n\
    <p>WAVEWATCH III<sup>\xAE</sup>  is a community wave modeling framework that includes\
    \ the\nlatest scientific advancements in the field of wind-wave modeling and dynamics.</p>\n\
    <h2><a id=\"user-content-general-features\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#general-features\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>General Features</h2>\n<p>WAVEWATCH III<sup>\xAE</sup> solves the\
    \ random phase spectral action density\nbalance equation for wavenumber-direction\
    \ spectra. The model includes options\nfor shallow-water (surf zone) applications,\
    \ as well as wetting and drying of\ngrid points. Propagation of a wave spectrum\
    \ can be solved using regular\n(rectilinear or curvilinear) and unstructured (triangular)\
    \ grids. See\n<a href=\"https://github.com/NOAA-EMC/WW3/wiki/About-WW3\">About\
    \ WW3</a> for a\ndetailed description of WAVEWATCH III<sup>\xAE</sup> .</p>\n\
    <h2><a id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#installation\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<p>The WAVEWATCH III<sup>\xAE</sup>  framework\
    \ package has two parts that need to be combined so\nall runs smoothly: the GitHub\
    \ repo itself, and a binary data file bundle that\nneeds to be obtained from our\
    \ ftp site. Steps to successfully acquire and install\nthe framework are outlined\
    \ in our <a href=\"https://github.com/NOAA-EMC/WW3/wiki/Quick-Start\">Quick Start</a>\n\
    guide.</p>\n<h2><a id=\"user-content-disclaimer\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#disclaimer\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Disclaimer</h2>\n<p>The United States Department of Commerce (DOC)\
    \ GitHub project code is provided\non an 'as is' basis and the user assumes responsibility\
    \ for its use. DOC has\nrelinquished control of the information and no longer\
    \ has responsibility to\nprotect the integrity, confidentiality, or availability\
    \ of the information. Any\nclaims against the Department of Commerce stemming\
    \ from the use of its GitHub\nproject will be governed by all applicable Federal\
    \ law. Any reference to\nspecific commercial products, processes, or services\
    \ by service mark,\ntrademark, manufacturer, or otherwise, does not constitute\
    \ or imply their\nendorsement, recommendation or favoring by the Department of\
    \ Commerce. The\nDepartment of Commerce seal and logo, or the seal and logo of\
    \ a DOC bureau,\nshall not be used in any manner to imply endorsement of any commercial\
    \ product\nor activity by DOC or the United States Government.</p>\n"
  stargazers_count: 193
  subscribers_count: 47
  topics: []
  updated_at: 1679444380.0
NOAA-EMC/spack-stack:
  data_format: 2
  description: null
  filenames:
  - configs/templates/unified-dev/spack.yaml
  - configs/templates/ufs-weather-model-static/spack.yaml
  - configs/templates/skylab-dev/spack.yaml
  - configs/templates/ufs-srw-public-v2/spack.yaml
  - configs/templates/skylab-no-python-dev/spack.yaml
  - configs/templates/gfs-v16.2/spack.yaml
  full_name: NOAA-EMC/spack-stack
  latest_release: spack-stack-1.2.0
  readme: '<h1><a id="user-content-spack-stack" class="anchor" aria-hidden="true"
    href="#spack-stack"><span aria-hidden="true" class="octicon octicon-link"></span></a>spack-stack</h1>

    <p>Spack-stack enables the installation of software required

    for HPC system deployments of NOAA''s Unified Forecast System (UFS) and

    other weather and climate models, including components of the Joint

    Effort for Data assimilation Integration (JEDI).</p>

    <p>Spack-stack is a collaborative effort between:</p>

    <ul>

    <li><a href="https://www.emc.ncep.noaa.gov/emc_new.php" rel="nofollow">NOAA Environmental
    Modeling Center (EMC)</a></li>

    <li><a href="https://www.jcsda.org/" rel="nofollow">UCAR Joint Center for Satellite
    Data Assimilation (JCSDA)</a></li>

    <li>

    <a href="https://epic.noaa.gov/" rel="nofollow">Earth Prediction Innovation Center
    (EPIC)</a>.</li>

    </ul>

    <p>Spack-stack is a thin layer around a fork of the

    <a href="https://github.com/spack/spack">spack</a> repository. Spack is a

    community-supported, multi-platform, Python-based package manager

    originally developed by the Lawrence Livermore National Laboratory

    (LLNL). Spack is provided as a submodule to spack-stack so that a

    stable version can be referenced. For more information about spack see

    the <a href="https://computing.llnl.gov/projects/spack-hpc-package-manager" rel="nofollow">LLNL
    project page for

    spack</a>

    and the <a href="https://spack.readthedocs.io/en/latest/" rel="nofollow">spack

    documentation</a>.</p>

    <p>The stack can be installed on a range of platforms, from Linux and

    macOS laptops to HPC systems, and comes pre-configured for many

    systems. Users can install the necessary packages for a particular

    application and later add the missing packages for another application

    without having to rebuild the entire stack.</p>

    <p>spack-stack is mainly a collection of Spack configuration files, but

    provides a Spack extension to simplify the installation process:</p>

    <ul>

    <li>

    <p><code>spack stack create</code> is provided to copy common, site-specific,
    and

    application-specific configuration files into a coherent Spack

    environment and to create container recipes</p>

    </li>

    <li>

    <p><code>spack stack setup-meta-modules</code> creates compiler, MPI and Python

    meta-modules for a convenient setup of a user environment using

    modules (lua and tcl)</p>

    </li>

    </ul>

    <p>Documentation for installing and using spack-stack can be found here:

    <a href="https://spack-stack.readthedocs.io/en/latest/" rel="nofollow">https://spack-stack.readthedocs.io/en/latest/</a></p>

    <p>spack-stack is maintained by:</p>

    <ul>

    <li>

    <p><a href="https://www.github.com/AlexanderRichert-NOAA">Alex Richert</a>, <a
    href="https://www.github.com/Hang-Lei-NOAA">Hang

    Lei</a>, <a href="https://www.github.com/edwardhartnett">Ed

    Hartnett</a> NOAA-EMC</p>

    </li>

    <li>

    <p><a href="https://www.github.com/climbfuji">Dom Heinzeller</a>, JCSDA</p>

    </li>

    </ul>

    <p>For more information about the organization of the spack-stack

    project, see the <a href="project_charter.md">Project Charter</a>.</p>

    <h2><a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Disclaimer</h2>

    <p>The United States Department of Commerce (DOC) GitHub project code is

    provided on an "as is" basis and the user assumes responsibility for

    its use. DOC has relinquished control of the information and no longer

    has responsibility to protect the integrity, confidentiality, or

    availability of the information. Any claims against the Department of

    Commerce stemming from the use of its GitHub project will be governed

    by all applicable Federal law. Any reference to specific commercial

    products, processes, or services by service mark, trademark,

    manufacturer, or otherwise, does not constitute or imply their

    endorsement, recommendation or favoring by the Department of

    Commerce. The Department of Commerce seal and logo, or the seal and

    logo of a DOC bureau, shall not be used in any manner to imply

    endorsement of any commercial product or activity by DOC or the United

    States Government.</p>

    '
  stargazers_count: 15
  subscribers_count: 7
  topics: []
  updated_at: 1678872587.0
PDC-support/PDC-SoftwareStack:
  data_format: 2
  description: null
  filenames:
  - spack-settings/22.06/0.18.1/prod/spack.yaml
  full_name: PDC-support/PDC-SoftwareStack
  latest_release: null
  readme: '<h1><a id="user-content-pdc-software-stack" class="anchor" aria-hidden="true"
    href="#pdc-software-stack"><span aria-hidden="true" class="octicon octicon-link"></span></a>PDC
    Software Stack</h1>

    <p>Repository to store documentation, installation procedure, installation procedures
    and data for validation of installed software</p>

    <h2><a id="user-content-modules" class="anchor" aria-hidden="true" href="#modules"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Modules</h2>

    <p>Modules for easybuild and CrayPE are within the module folder.</p>

    <h2><a id="user-content-easybuild" class="anchor" aria-hidden="true" href="#easybuild"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>EasyBuild</h2>

    <p>EasyBuild easyconfigs should be stored in <em>easybuild/easyconfigs</em> folder</p>

    <h2><a id="user-content-spack" class="anchor" aria-hidden="true" href="#spack"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Spack</h2>

    <p>Spack installation procedures for software should be store in the <em>spack</em>
    folder</p>

    <h2><a id="user-content-manual-installations" class="anchor" aria-hidden="true"
    href="#manual-installations"><span aria-hidden="true" class="octicon octicon-link"></span></a>Manual
    installations</h2>

    <p>Procedures should be store in the <em>other</em> folder</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1666354279.0
ParaToolsInc/exago-crusher:
  data_format: 2
  description: Spack-based deployment of ROCm enabled ExaGO for OLCF Crusher
  filenames:
  - spack.yaml
  full_name: ParaToolsInc/exago-crusher
  latest_release: null
  readme: "<h1><a id=\"user-content-exago-on-olcf-crusher\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#exago-on-olcf-crusher\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>ExaGO on OLCF Crusher</h1>\n<p>ROCm-enabled ExaGO\
    \ on OLCF Crusher using Spack</p>\n<h2><a id=\"user-content-install-from-build-cache-example\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#install-from-build-cache-example\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Install\
    \ from Build Cache (Example)</h2>\n<ul>\n<li>View a demo video of these instructions\
    \ run at <a href=\"https://asciinema.org/a/508123\" rel=\"nofollow\">https://asciinema.org/a/508123</a>\n\
    </li>\n</ul>\n<pre><code>$crusher:~&gt; git clone https://github.com/ParaToolsInc/exago-crusher.git\n\
    $crusher:~&gt; cd exago-crusher\n\n$crusher:~/exago-crusher&gt; git clone https://github.com/spack/spack\n\
    $crusher:~/exago-crusher&gt; (cd spack &amp;&amp; git checkout dac31ef3c)\n\n\
    $crusher:~/exago-crusher&gt; export SPACK_DISABLE_LOCAL_CONFIG=1\n$crusher:~/exago-crusher&gt;\
    \ export SPACK_USER_CACHE_PATH=$(pwd)/_cache\n$crusher:~/exago-crusher&gt; . spack/share/spack/setup-env.sh\n\
    \n$crusher:~/exago-crusher&gt; spack mirror add paratools /gpfs/alpine/csc439/world-shared/E4S/ParaTools/exago\n\
    $crusher:~/exago-crusher&gt; spack buildcache keys -it\ngpg: key 4345F04B40005581:\
    \ public key \"University of Oregon - E4S\" imported\ngpg: Total number processed:\
    \ 1\ngpg:               imported: 1\ngpg: inserting ownertrust of 6\n\n$crusher:~/exago-crusher&gt;\
    \ time spack -e . concretize -f | tee concretize.log\n... output truncated for\
    \ brevity; see concretize.log in this repo for full output\nreal\t0m46.340s\n\
    user\t1m25.263s\nsys\t0m2.094s\n\n$crusher:~/exago-crusher&gt; time spack -e .\
    \ install --cache-only\n... output truncated for brevity\nreal\t7m40.626s\nuser\t\
    4m44.081s\nsys\t0m33.864s\n\n$crusher:~/exago-crusher&gt; spack find -lv exago\
    \ hiop ipopt coinhsl\n==&gt; 8 installed packages\n-- cray-sles15-zen3 / gcc@11.2.0\
    \ --------------------------------\nue74alo coinhsl@2015.06.23+blas\nmnelc7u exago@develop~cuda+hiop~ipo~ipopt+mpi+python+raja+rocm\
    \ amdgpu_target=gfx90a build_type=RelWithDebInfo\nanahf5s exago@develop~cuda+hiop~ipo~ipopt+mpi+python+raja+rocm\
    \ amdgpu_target=gfx90a build_type=RelWithDebInfo\n2qog6zw exago@develop~cuda+hiop~ipo+ipopt+mpi+python+raja+rocm\
    \ amdgpu_target=gfx90a build_type=RelWithDebInfo\ndr3jlyb exago@develop~cuda+hiop~ipo+ipopt+mpi+python+raja+rocm\
    \ amdgpu_target=gfx90a build_type=RelWithDebInfo\nwtqj2hu hiop@0.6.2~cuda~cusolver~deepchecking~ginkgo~ipo~jsrun~kron+mpi+raja+rocm~shared+sparse\
    \ amdgpu_target=gfx90a build_type=RelWithDebInfo\nrlw4qhu hiop@0.6.2~cuda~cusolver~deepchecking~ginkgo~ipo~jsrun+kron+mpi+raja+rocm~shared+sparse\
    \ amdgpu_target=gfx90a build_type=RelWithDebInfo\nufjh4v7 ipopt@3.14.5+coinhsl~debug+metis~mumps\n\
    </code></pre>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1657663349.0
PawseySC/hpc-container-training:
  data_format: 2
  description: 'Training material on using containers in an HPC setting. '
  filenames:
  - demos/spack_blast/spack.yaml
  full_name: PawseySC/hpc-container-training
  latest_release: null
  readme: '<h1><a id="user-content-readme" class="anchor" aria-hidden="true" href="#readme"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Readme</h1>

    '
  stargazers_count: 1
  subscribers_count: 5
  topics:
  - docker
  - singularity
  - hpc
  - pawsey
  - training-materials
  updated_at: 1650946836.0
PawseySC/pawsey-spack-config:
  data_format: 2
  description: Configuration files for Spack at Pawsey
  filenames:
  - systems/setonix/environments/env_s3_clients/spack.yaml
  - systems/setonix/environments/env_devel/spack.yaml
  - systems/setonix/environments/env_num_libs/spack.yaml
  - systems/setonix/environments/env_wrf/spack.yaml
  full_name: PawseySC/pawsey-spack-config
  latest_release: null
  readme: '<h1><a id="user-content-pawsey-spack-configuration" class="anchor" aria-hidden="true"
    href="#pawsey-spack-configuration"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pawsey
    Spack Configuration</h1>

    <p>Scripts and configuration files used by the Pawsey Supercomputing Research
    Centre to deploy Spack and to install the scientific software stack on its supercomputing
    systems.</p>

    <h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>Here is how to launch the software stack installation.</p>

    <ol>

    <li>Make sure the system you want to install the software stack on has a corresponding
    directory in <code>systems</code>. If not, you can start by creating a copy of
    an existing one.</li>

    <li>Edit the file <code>systems/&lt;system&gt;/settings.sh</code> as needed.</li>

    <li>Set and export the <code>INSTALL_PREFIX</code> variable to the full path of
    the filesystem location where you want the installation to be placed in. Note
    that it has to end with the same string as the one stored in the <code>DATE_DAG</code>
    variable, meaning that installations are versioned by installation date.</li>

    <li>Set and export the <code>INSTALL_GROUP</code> variable to the linux group
    that is going to own the installed files.</li>

    <li>Set and export the <code>SYSTEM</code> variable to the system you want to
    run the installation for, if it differs from the content of the <code>PAWSEY_CLUSTER</code>
    environment variable.</li>

    <li>Run the <code>scripts/install_software_stack.sh</code> script, preferably
    in a Slurm job or as a process detached from the login shell to prevent the installation
    from being aborted in case the SSH connection were to be interrupted unexpectedly.</li>

    </ol>

    <h3><a id="user-content-singularity" class="anchor" aria-hidden="true" href="#singularity"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity</h3>

    <p>You will need to ask the platforms team to apply root permissions to Singularity
    ss soon as it is installed. The script to run as root is found in the <code>bin</code>
    directory within the spack installation prefix.</p>

    <h3><a id="user-content-software-stack-modulefile" class="anchor" aria-hidden="true"
    href="#software-stack-modulefile"><span aria-hidden="true" class="octicon octicon-link"></span></a>Software
    stack modulefile</h3>

    <p>The platforms team will need to install the <code>$INSTALL_PREFIX/staff_modulefiles/pawseyenv/*lua</code>
    module such that it will be loaded before the Cray compilers. They will also need
    to update user account creation process, following the updated <code>$INSTALL_PREFIX/spack/bin/spack_create_user_moduletree.sh</code>.</p>

    <h2><a id="user-content-repository-structure" class="anchor" aria-hidden="true"
    href="#repository-structure"><span aria-hidden="true" class="octicon octicon-link"></span></a>Repository
    structure</h2>

    <p>The repository is composed of the directories:</p>

    <ul>

    <li>

    <code>fixes/</code>: patches implemented by Pawsey staff to be applied to Spack
    prior to production use. They are meant to improve usability of Spack for Pawsey-specific
    use cases.</li>

    <li>

    <code>repo/</code>: custom Spack package recipes for software not yet supported
    by Spack or that needed modification in the build process to work on Pawsey systems.</li>

    <li>

    <code>shpc_registry/</code>: custom Singularity-HPC (SHPC) recipes to deploy containers.</li>

    <li>

    <code>scripts/</code>: BASH scripts used to automate the deployment process.</li>

    <li>

    <code>systems/&lt;system&gt;</code>: a directory containing configuration files
    specific to a system. Scripts will use these files to customise the Spack deployment
    and installation of the software stack.</li>

    </ul>

    <p>The <code>scripts/install_software_stack.sh</code> is the top-level script
    that executes the installation from start to finish except licensed software,
    that need some manual work. Refer to this script also as documentation of the
    installation process.</p>

    <h2><a id="user-content-the-scripts-directory" class="anchor" aria-hidden="true"
    href="#the-scripts-directory"><span aria-hidden="true" class="octicon octicon-link"></span></a>The
    <code>scripts</code> directory</h2>

    <p>This project makes up a build system for the scientific software stack on Pawsey
    supercomputers. On a high level, there are two logical compontents to it:

    one to deploy Spack and SHPC (a software package to manage containers), and the
    other to use the tools mentioned before to install scientific software.</p>

    <p>The deployment of Spack and SHPC is implemented through the following executables
    BASH scripts within the <code>scripts</code> directory:</p>

    <ul>

    <li>

    <code>install_spack.sh</code> installs Spack on the system and creates the directory
    structure for the system-wide software stack installation.</li>

    <li>

    <code>install_python.sh</code> installs Python using Spack. To do so, and only
    in this case, Spack chooses <code>cray-python</code> as interpreter. Once Python
    is installed for different architectures and versions, <code>cray-python</code>
    won''t be used anymore.</li>

    <li>

    <code>install_shpc.sh</code> installs SHPC, a tool used to deploy containers.</li>

    </ul>

    <p>The software stack deployment is implemented in these scripts instead:</p>

    <ul>

    <li>

    <code>concretize_environments.sh</code> runs the concretization step for all Spack
    environments to be installed.</li>

    <li>

    <code>install_environments.sh</code> will install all Spack environments using
    Spack.</li>

    <li>

    <code>install_shpc_containers.sh</code> will pull Pawsey-supported containers
    and install them using SHPC.</li>

    <li>

    <code>post_installation_operations.sh</code> refreshes Lmod modulefiles for the
    installed software, applies permissions to licensed software, and other operations
    needed after the full stack deployment executed by Spack.</li>

    </ul>

    <h2><a id="user-content-the-systemssystem-directory" class="anchor" aria-hidden="true"
    href="#the-systemssystem-directory"><span aria-hidden="true" class="octicon octicon-link"></span></a>The
    <code>systems/&lt;system&gt;</code> directory</h2>

    <p>This is where system specific configurations are placed. In particular, the
    following items must always be present.</p>

    <ul>

    <li>

    <code>configs/</code> is a directory containing <code>yaml</code> configuraiton
    files for Spack. There are three types of configuration:

    <ul>

    <li>

    <code>site/</code>: Spack configuration files that are valid for all users, which
    will sit in <code>$spack/etc/spack</code>.</li>

    <li>

    <code>project/</code>: Spack configuration files that are valid for project-wide
    installations executed by any user using the dedicated script <code>spack_project.sh</code>.</li>

    <li>

    <code>spackuser/</code>: Spack configuration files for system-wide installs, performed
    by Pawsey staff, which will sit in <code>/home/spack/.spack/</code>, allowing
    the <code>spack</code> user to override system-wide settings.</li>

    </ul>

    </li>

    <li>

    <code>environments/</code>: Spack environments to be deployed.</li>

    <li>

    <code>templates/</code>: modulefile templates for Spack.</li>

    </ul>

    <h2><a id="user-content-notes" class="anchor" aria-hidden="true" href="#notes"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Notes</h2>

    <h3><a id="user-content-module-categories-in-use" class="anchor" aria-hidden="true"
    href="#module-categories-in-use"><span aria-hidden="true" class="octicon octicon-link"></span></a>Module
    categories in use</h3>

    <ul>

    <li>Spack (with compiler/arch tree)

    <ul>

    <li>

    <strong>NOTE</strong>: if updating list, still need to manually update <code>templates/modules/modulefile.lua</code>

    </li>

    <li><code>astro-applications</code></li>

    <li><code>bio-applications</code></li>

    <li><code>applications</code></li>

    <li><code>libraries</code></li>

    <li><code>programming-languages</code></li>

    <li><code>utilities</code></li>

    <li><code>visualisation</code></li>

    <li><code>python-packages</code></li>

    <li><code>benchmarking</code></li>

    <li><code>developer-tools</code></li>

    <li><code>dependencies</code></li>

    </ul>

    </li>

    <li>Pawsey custom builds (with compiler/arch tree)

    <ul>

    <li><code>custom/modules</code></li>

    </ul>

    </li>

    <li>Pawsey utilities (without compiler/arch tree: Spack, SHPC, utility scripts)

    <ul>

    <li><code>pawsey/modules</code></li>

    </ul>

    </li>

    <li>SHPC containers modules (without compiler/arch tree)

    <ul>

    <li><code>containers/modules</code></li>

    </ul>

    </li>

    </ul>

    <h3><a id="user-content-testing-modules" class="anchor" aria-hidden="true" href="#testing-modules"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Testing Modules</h3>

    <p>Current <code>modules.yaml</code> and the template <code>modulefile.lua</code>
    rely on additional features of Spack found in the feature/improved-lmod-modules
    (<a href="https://github.com/PawseySC/spack/tree/feature/improved-lmod-modules">https://github.com/PawseySC/spack/tree/feature/improved-lmod-modules</a>).<br>

    The update provides extra tokens that can be used when creating the module name
    and also extra keywords to the template.<br>

    These features have now been packaged in a patch, that is applied by <code>install_spack.sh</code>.</p>

    '
  stargazers_count: 0
  subscribers_count: 11
  topics: []
  updated_at: 1641801068.0
RMeli/my-spack:
  data_format: 2
  description: Spack environments
  filenames:
  - envs/alps/dlaf-mkl-cuda/spack.yaml
  - envs/local/dlaf-mkl/spack.yaml
  - envs/alps/cp2k-dlaf-cpu/spack.yaml
  full_name: RMeli/my-spack
  latest_release: null
  readme: '<h1><a id="user-content-my-spack" class="anchor" aria-hidden="true" href="#my-spack"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>My Spack</h1>

    <p>Spack-related stuff for @RMeli.</p>

    <h2><a id="user-content-package-repository" class="anchor" aria-hidden="true"
    href="#package-repository"><span aria-hidden="true" class="octicon octicon-link"></span></a>Package
    Repository</h2>

    <p><a href="https://spack.readthedocs.io/en/latest/repositories.html" rel="nofollow">Spack
    Package Repositories</a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1679671251.0
SCOREC/rhel7-spack-config:
  data_format: 2
  description: rhel7 spack configuration and scripts
  filenames:
  - v0.18.1/spack.yaml
  full_name: SCOREC/rhel7-spack-config
  latest_release: null
  readme: "<h1><a id=\"user-content-setup-on-scorec\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#setup-on-scorec\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>setup on SCOREC</h1>\n<pre><code>cd /opt/scorec/spack/rhel7-spack-config/\n\
    source setupSpack.sh\n</code></pre>\n<h1><a id=\"user-content-rhel7-spack-config\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#rhel7-spack-config\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>rhel7-spack-config</h1>\n<p>rhel7\
    \ spack configuration and scripts</p>\n<p>The <code>install.sh</code> script maintained\
    \ in this repo is for documentation purposes (e.g., in case we had to reinstall\
    \ the entire stack from scratch) and should not be executed as it will not use\
    \ all of our existing package installs.  More discussion of package installation\
    \ is below.</p>\n<h2><a id=\"user-content-useful-commands\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#useful-commands\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>useful commands</h2>\n<p>regenerate lmod module tree:</p>\n<pre><code>spack\
    \ module lmod refresh\n</code></pre>\n<h2><a id=\"user-content-installing-new-packages\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#installing-new-packages\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>installing new\
    \ packages</h2>\n<p>Our spack repo is tracking the master spack branch.  Spack\
    \ package updates could result in additional installation of packages with little\
    \ or no package source code changes.  These additional installs can be avoided\
    \ when installing new packages by first examining the output of the <code>spack\
    \ spec -I</code> command.  If a utility/infrastructure level package, such as\
    \ cmake or mpich, is marked with a <code>[+]</code> symbol in the leftmost column\
    \ then it means that the existing install will be used.  If spack does not default\
    \ to using the existing install you can append the hash of the package to the\
    \ spec command.</p>\n<p>For example, lets see what happens when we ask for a pumi\
    \ install using gcc 7.3.0</p>\n<pre><code>$ spack spec -I pumi@develop%gcc@7.3.0\n\
    Input spec\n--------------------------------\n -   pumi@develop%gcc@7.3.0\n\n\
    Concretized\n--------------------------------\n -   pumi@develop%gcc@7.3.0 build_type=RelWithDebInfo\
    \ ~fortran~shared simmodsuite=none ~zoltan arch=linux-rhel7-x86_64 \n[+]     \
    \ ^cmake@3.13.1%gcc@7.3.0~doc+ncurses+openssl+ownlibs~qt arch=linux-rhel7-x86_64\
    \ \n[+]          ^ncurses@6.1%gcc@7.3.0~symlinks~termlib arch=linux-rhel7-x86_64\
    \ \n[+]              ^pkgconf@1.5.4%gcc@7.3.0 arch=linux-rhel7-x86_64 \n[+]  \
    \        ^openssl@1.1.1%gcc@7.3.0+systemcerts arch=linux-rhel7-x86_64 \n[+]  \
    \            ^perl@5.16.3%gcc@7.3.0+cpanm patches=0eac10ed90aeb0459ad8851f88081d439a4e41978e586ec743069e8b059370ac\
    \ +shared+threads arch=linux-rhel7-x86_64 \n[+]              ^zlib@1.2.11%gcc@7.3.0+optimize+pic+shared\
    \ arch=linux-rhel7-x86_64 \n -       ^mpich@3.3%gcc@7.3.0 device=ch3 +hydra netmod=tcp\
    \ +pmi+romio~verbs arch=linux-rhel7-x86_64 \n[+]          ^findutils@4.6.0%gcc@7.3.0\
    \ patches=84b916c0bf8c51b7e7b28417692f0ad3e7030d1f3c248ba77c42ede5c1c5d11e,bd9e4e5cc280f9753ae14956c4e4aa17fe7a210f55dd6c84aa60b12d106d47a2\
    \ arch=linux-rhel7-x86_64 \n[+]              ^autoconf@system%gcc@7.3.0 arch=linux-rhel7-x86_64\
    \ \n[+]              ^automake@system%gcc@7.3.0 arch=linux-rhel7-x86_64 \n[+]\
    \              ^libtool@system%gcc@7.3.0 arch=linux-rhel7-x86_64 \n[+]       \
    \       ^m4@1.4.16%gcc@7.3.0 patches=c0a408fbffb7255fcc75e26bd8edab116fc81d216bfd18b473668b7739a4158e\
    \ +sigsegv arch=linux-rhel7-x86_64 \n[+]              ^texinfo@6.5%gcc@7.3.0 arch=linux-rhel7-x86_64\n\
    </code></pre>\n<p>Spack wants to install mpich 3.3, but we don't want to change\
    \ to the new mpich version yet.  So, we will get the hash of the existing mpich\
    \ 3.2.1 install:</p>\n<pre><code>$ spack find -ldv mpich%gcc@7.3.0\n==&gt; 1 installed\
    \ package\n-- linux-rhel7-x86_64 / gcc@7.3.0 -------------------------------\n\
    niuhmad    mpich@3.2.1 device=ch3 +hydra netmod=tcp +pmi+romio~verbs\n</code></pre>\n\
    <p>then append the hash <code>niuhmad</code> to the spec for pumi using the <code>^</code>\
    \ syntax to specify it as a dependency:</p>\n<pre><code>$ spack spec -I pumi@develop%gcc@7.3.0\
    \ ^/niuhmad\nInput spec\n--------------------------------\n -   pumi@develop%gcc@7.3.0\n\
    [+]      ^mpich@3.2.1%gcc@7.3.0 device=ch3 +hydra netmod=tcp +pmi+romio~verbs\
    \ arch=linux-rhel7-x86_64 \n\nConcretized\n--------------------------------\n\
    \ -   pumi@develop%gcc@7.3.0 build_type=RelWithDebInfo ~fortran~shared simmodsuite=none\
    \ ~zoltan arch=linux-rhel7-x86_64 \n[+]      ^cmake@3.13.1%gcc@7.3.0~doc+ncurses+openssl+ownlibs~qt\
    \ arch=linux-rhel7-x86_64 \n[+]          ^ncurses@6.1%gcc@7.3.0~symlinks~termlib\
    \ arch=linux-rhel7-x86_64 \n[+]              ^pkgconf@1.5.4%gcc@7.3.0 arch=linux-rhel7-x86_64\
    \ \n[+]          ^openssl@1.1.1%gcc@7.3.0+systemcerts arch=linux-rhel7-x86_64\
    \ \n[+]              ^perl@5.16.3%gcc@7.3.0+cpanm patches=0eac10ed90aeb0459ad8851f88081d439a4e41978e586ec743069e8b059370ac\
    \ +shared+threads arch=linux-rhel7-x86_64 \n[+]              ^zlib@1.2.11%gcc@7.3.0+optimize+pic+shared\
    \ arch=linux-rhel7-x86_64 \n[+]      ^mpich@3.2.1%gcc@7.3.0 device=ch3 +hydra\
    \ netmod=tcp +pmi+romio~verbs arch=linux-rhel7-x86_64 \n</code></pre>\n<p>And\
    \ see that in the Concretized spec it is now using the existing mpich 3.2.1 install.</p>\n\
    <h2><a id=\"user-content-contents\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #contents\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>contents</h2>\n\
    <p>compilers.yaml - compiler list\nconfig.yaml - global config\ninstall.sh - package\
    \ installation commands\nmodules.yaml - hierarchical layout for lua modules\n\
    packages.yaml - system installed packages\nREADME.md - this file\nsetupSpack.sh\
    \ - env needed for executing spack commands</p>\n"
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1643231013.0
SouthernMethodistUniversity/mp_testing:
  data_format: 2
  description: null
  filenames:
  - mp/testing/01_spack/spack_lammps.yaml
  - mp/testing/01_spack/spack_nvhpc.yaml
  - mp/testing/05_openmm/spack.yaml
  - mp/testing/08_pytorch/spack.yaml
  full_name: SouthernMethodistUniversity/mp_testing
  latest_release: null
  readme: '<h1><a id="user-content-notes" class="anchor" aria-hidden="true" href="#notes"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Notes</h1>

    <p><strong>All code in this repo is for testing. The code may not work and may
    change. Pull requests and issues welcome.</strong></p>

    <p>See <a href="quick_start_notes.md">Quick Start Notes</a> for a short overview
    of MP usage.</p>

    <h2><a id="user-content-usage-example" class="anchor" aria-hidden="true" href="#usage-example"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage Example</h2>

    <div class="highlight highlight-source-shell"><pre>git clone https://github.com/SouthernMethodistUniversity/mp_testing.git

    <span class="pl-c1">cd</span> mp_testing/demos/00_nemo

    ./submit_jobs.sh</pre></div>

    <h2><a id="user-content-applications" class="anchor" aria-hidden="true" href="#applications"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Applications</h2>

    <ul>

    <li>LAMMPS (NGC)</li>

    <li>AMBER</li>

    <li>NAMD (NGC)</li>

    <li>OpenMM</li>

    <li>Gaussian</li>

    <li>VASP</li>

    <li>CRYSTAL</li>

    <li>Q-Chem</li>

    <li>Quantum Espresso</li>

    </ul>

    <h2><a id="user-content-analysis-tools" class="anchor" aria-hidden="true" href="#analysis-tools"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Analysis Tools</h2>

    <ul>

    <li>Memory profiling</li>

    <li>Performance profiling</li>

    </ul>

    <h2><a id="user-content-librariesapis" class="anchor" aria-hidden="true" href="#librariesapis"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Libraries/APIs</h2>

    <ul>

    <li>Raja</li>

    <li>Magma</li>

    <li>heFFTe</li>

    <li>Pandas</li>

    <li>NumPy</li>

    <li>TensorFlow</li>

    <li>PyTorch</li>

    <li>DALI</li>

    </ul>

    <h2><a id="user-content-languages" class="anchor" aria-hidden="true" href="#languages"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Languages</h2>

    <ul>

    <li>C</li>

    <li>C++</li>

    <li>Python</li>

    <li>Some custom layer in C++/CUDA</li>

    <li>Fortran</li>

    <li>CUDA Fortran</li>

    <li>Julia</li>

    </ul>

    <h2><a id="user-content-molecular-dynamics" class="anchor" aria-hidden="true"
    href="#molecular-dynamics"><span aria-hidden="true" class="octicon octicon-link"></span></a>Molecular
    Dynamics</h2>

    <ul>

    <li>OpenMM</li>

    <li>AMBER</li>

    <li>Desmond</li>

    <li>GROMACS</li>

    <li>Mentioned MIC modes?</li>

    <li>NGC for Keras/TF and Pytorch</li>

    </ul>

    <h2><a id="user-content-issues" class="anchor" aria-hidden="true" href="#issues"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Issues</h2>

    <ul>

    <li>Can''t run enroot images directly via <code>enroot start hello_world.sqsh</code>.
    The OS

    needs squashfuse and fuse-overlayfs installed. I installed these on Easley and

    it works.</li>

    <li>Custom build and final images for containerized Spack environments fails due

    to apparently assuming that Spack already exists. See: <code>01_spack/spack_nvhpc.yaml</code>.</li>

    <li>Spack-blessed NVIDIA container fails to build due to public key error. See:
    <code>01_spack/spack_lammps.yaml</code>.</li>

    <li>

    <code>export ENROOT_MOUNT_HOME=1</code> to bind $HOME.</li>

    <li>Default flags and <code>target=zen2</code> gave LAMMPS run times of 4:44,
    while <code>target=zen2 cppflags=-O3</code>

    </li>

    <li>Running containers or non-hpc-x MPI produces warnings about <code>Unknown
    interface name</code> /

    <code>An invalid value was given for btl_tcp_if_include</code>. It appears not
    to see the Mellanox / IB correctly?</li>

    </ul>

    <h2><a id="user-content-maybe-useful" class="anchor" aria-hidden="true" href="#maybe-useful"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Maybe Useful</h2>

    <ul>

    <li><a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/gpu-applications-catalog.pdf"
    rel="nofollow">https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/gpu-applications-catalog.pdf</a></li>

    <li><a href="https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html"
    rel="nofollow">https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html</a></li>

    <li><a href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/index.html"
    rel="nofollow">https://docs.nvidia.com/deeplearning/dali/user-guide/docs/index.html</a></li>

    <li><a href="https://secure.cci.rpi.edu/wiki/" rel="nofollow">https://secure.cci.rpi.edu/wiki/</a></li>

    </ul>

    <h2><a id="user-content-things-we-need-to-plan-for" class="anchor" aria-hidden="true"
    href="#things-we-need-to-plan-for"><span aria-hidden="true" class="octicon octicon-link"></span></a>Things
    we need to plan for</h2>

    <ul>

    <li>How and when do we decide we''re updating Nvidia Drivers / Cuda. I think we
    need to be very clear about this if we''re not going to maintain the latest and
    greatest. (we''re currently on 11.4, but 11.7 and associated drivers are available)</li>

    </ul>

    '
  stargazers_count: 2
  subscribers_count: 2
  topics: []
  updated_at: 1674857302.0
UO-OACISS/e4s:
  data_format: 2
  description: E4S Spack environments and container recipes
  filenames:
  - docker-recipes/runner/rhel8-ppc64le/spack.yaml
  - docker-recipes/minimal/ubuntu20.04-ppc64le/spack.yaml
  - docker-recipes/archived/minimal/rhel8-x86_64/spack.yaml
  - docker-recipes/archived/minimal/ubuntu22.04-ppc64le/spack.yaml
  - docker-recipes/runner/rhel8-x86_64/spack.yaml
  - docker-recipes/runner/rhel8-aarch64/spack.yaml
  - docker-recipes/archived/minimal/ubuntu22.04-aarch64/spack.yaml
  - docker-recipes/runner/ubuntu22.04-aarch64/spack.yaml
  - docker-recipes/archived/minimal/rhel8-ppc64le/spack.yaml
  - docker-recipes/archived/minimal/ubuntu22.04-x86_64/spack.yaml
  - docker-recipes/runner/ubuntu20.04-x86_64-oneapi/spack.yaml
  - docker-recipes/runner/ubuntu20.04-ppc64le/spack.yaml
  - docker-recipes/runner/ubuntu20.04-x86_64/spack.yaml
  - docker-recipes/archived/minimal/rhel8-aarch64/spack.yaml
  - docker-recipes/minimal/ubuntu20.04-aarch64/spack.yaml
  - docker-recipes/runner/ubuntu20.04-aarch64/spack.yaml
  - docker-recipes/minimal/ubuntu20.04-x86_64/spack.yaml
  full_name: UO-OACISS/e4s
  latest_release: null
  readme: '<p>This is a collection of configurations for building ECP SDK

    containers with combinations of packages, including the full

    E4S set.</p>

    <p>These are the set of stacks that are targeted for the first release:</p>

    <p><a target="_blank" rel="noopener noreferrer" href="figures/SDKdefinition1.png"><img
    src="figures/SDKdefinition1.png" alt="SDK definitions" style="max-width: 100%;"></a></p>

    <p>The configuration files for each container platform will be specified under
    each directory.  For example, the Docker configurations are under the "docker"
    subdirectory.  Each subdirectory will have a README.md file to explain how to
    build the container image for each stack.</p>

    '
  stargazers_count: 20
  subscribers_count: 6
  topics: []
  updated_at: 1673374590.0
actions-marketplace-validations/haampie-spack_setup-spack:
  data_format: 2
  description: null
  filenames:
  - example-environment/spack.yaml
  full_name: actions-marketplace-validations/haampie-spack_setup-spack
  latest_release: null
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1669840356.0
adamqc/devcontainer:
  data_format: 2
  description: null
  filenames:
  - complex/spack.yaml
  - real/spack.yaml
  full_name: adamqc/devcontainer
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1646725990.0
ai2cm/fv3net:
  data_format: 2
  description: explore the FV3 data for parameterization
  filenames:
  - docker/ufs_utils/spack.yaml
  full_name: ai2cm/fv3net
  latest_release: n2f-3km-initial-submission
  readme: '<h1><a id="user-content-fv3net" class="anchor" aria-hidden="true" href="#fv3net"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>fv3net</h1>

    <p><a href="https://circleci.com/gh/ai2cm/fv3net/tree/master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a552f20b68ca052bd00beeb5ce611dd080f121e453b44f371d63405aeec20c78/68747470733a2f2f636972636c6563692e636f6d2f67682f616932636d2f6676336e65742f747265652f6d61737465722e7376673f7374796c653d737667"
    alt="CircleCI" data-canonical-src="https://circleci.com/gh/ai2cm/fv3net/tree/master.svg?style=svg"
    style="max-width: 100%;"></a></p>

    <p>Improving the GFDL FV3 model physics with machine learning. See the <a href="https://vulcanclimatemodeling.com/docs/fv3net/"
    rel="nofollow">documentation</a> for more information on using this suite of tools.</p>

    <p>Disclaimer: This is a work in progress.</p>

    '
  stargazers_count: 14
  subscribers_count: 8
  topics: []
  updated_at: 1675296401.0
alexpacheco/spackenv:
  data_format: 2
  description: 'Spack Environments '
  filenames:
  - cent8/envs/solhawk/spack.yaml
  - cent8/envs/avx512/lusoft/spack.yaml
  - cent8/envs/avx/lusoft/spack.yaml
  - cent8/envs/x86_64/spack.yaml
  - cent8/envs/avx2/lusoft/spack.yaml
  full_name: alexpacheco/spackenv
  latest_release: null
  readme: '<h1><a id="user-content-spack-environments" class="anchor" aria-hidden="true"
    href="#spack-environments"><span aria-hidden="true" class="octicon octicon-link"></span></a>SPACK
    Environments</h1>

    <p>This repo contains the environment definitions to deploy site-software on Lehigh
    University''s Research Computing resources via SPACK environments.</p>

    <h2><a id="user-content-software-deployment-for-centos-8x" class="anchor" aria-hidden="true"
    href="#software-deployment-for-centos-8x"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Software deployment for CentOS 8.x</h2>

    <p>Software is deployed using two Spack installations.</p>

    <ol>

    <li>For compilers and module environments</li>

    <li>Site software for general use</li>

    </ol>

    <h3><a id="user-content-compilers" class="anchor" aria-hidden="true" href="#compilers"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Compilers</h3>

    <p>This spack installation provides the gcc, nvhpc and cuda compilers, and lmod
    software for module management. In the future, this installation will also provide
    intel-oneapi compilers. For legacy reasons, intel@19.0.3 and intel@20.0.3 were
    installed in /share/Apps/intel with older intel compilers. This installation should
    not be used for deploying site software nor should the software provided be made
    available using the module environment.</p>

    <p>To reproduce installation</p>

    <pre><code>git clone https://github.com/alexpacheco/spackenv.git

    cd spackenv/compilers/envs/compilers

    spack env activate -d .

    spack concretize -f # optional

    spack install

    </code></pre>

    <p>The directory <code>etc/lmod</code> contains the LMOD configuration to switch
    between avx, avx2 and avx512 enabled <code>MODULEPATHS</code></p>

    <h3><a id="user-content-lu-software" class="anchor" aria-hidden="true" href="#lu-software"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>LU Software</h3>

    <p>This spack installation provides the deployed site-software on Sol and Hawk.</p>

    <p>To reproduce this installation, you need to first copy the site configuration
    files from <code>etc/spack</code> to your spack install tree. This assumes that
    SLURM and the compiler environment above is already installed. Edit the <code>packages.yaml</code>
    file to point to the location of slurm (/usr/local), rmda-core (/usr), gcc, intel,
    cuda, and nvhpc. The file <code>repo.yaml</code> is hardwired with  location of
    the lubio repository and should be changed to your location. The directory <code>templates</code>
    contains the template lua file for a few modules as defined in the <code>modules.yaml</code>
    file  and should be copied to the <code>etc</code> directory in your spack installation
    tree.</p>

    <p>On Sol, these files are available at <code>/share/Apps/lusoft/etc/spack</code>.</p>

    <h4><a id="user-content-available-environments" class="anchor" aria-hidden="true"
    href="#available-environments"><span aria-hidden="true" class="octicon octicon-link"></span></a>Available
    Environments</h4>

    <h5><a id="user-content-solhawk" class="anchor" aria-hidden="true" href="#solhawk"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>solhawk</h5>

    <p>This environment builds the entire software except the various python and r
    packages for ivybridge, haswell and skylake_avx512 architectures. This environment
    also builds the tcl environment modules that is not currently used. This should
    be build first and any new packages should be added to this environment.</p>

    <pre><code>cd spackenv/cent8/envs/solhawk

    spack env activate -d .

    spack concretize -f # optional

    spack install

    </code></pre>

    <h4><a id="user-content-avxavx2avx512" class="anchor" aria-hidden="true" href="#avxavx2avx512"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>avx/avx2/avx512</h4>

    <p>These environment builds the software stack except the various python and r
    packages for ivybridge/haswell/skylake_avx512 architectures. If software in the
    <code>solhawk</code> environment is already built, then these environments are
    only setting up the installation root for the LMOD module files <code>/share/Apps/lusoft/share/modules/lmod/{avx,avx2,avx512}</code>.
    The only reason these environments exist is due to SPACK''s inability to built
    a architecture based LMOD module tree similar to the TCL module tree.

    <em>Note</em>: If you change the path of the installation root, make sure that
    you change the corresponding path in <code>compilers/etc/SitePackage.lua</code>.</p>

    <pre><code>cd spackenv/cent8/envs/avx2/lusoft

    spack env activate -d .

    spack concretize -f # optional

    spack install

    </code></pre>

    <h4><a id="user-content-python-and-r-packages" class="anchor" aria-hidden="true"
    href="#python-and-r-packages"><span aria-hidden="true" class="octicon octicon-link"></span></a>Python
    and R packages</h4>

    <p>Rather than building module files for various python and r packages, a single
    module is created for a filesystem view of all python and r packages respectively.
    The path to the r filesystem is setup as <code>R_LIBS_SITE</code> so that any
    application such as <code>trinity</code> that requires many R packages only need
    to load the r module. If new packages added to the above environments require
    a dependent R package, then that dependency should be added to the rpoject environment
    and concretized. The python environment uses a <code>concretization: together</code>
    and may not provide the same python package as the above software environments.
    The filesystem views are hardwired as <code>/share/Apps/py_spack/3.8.6/{avx,avx2,avx512}</code>
    and <code>/share/Apps/r_spack/4.0.3/{avx,avx2,avx512}</code>.</p>

    <pre><code>cd spackenv/cent8/envs/avx/python

    spack env activate -d .

    spack concretize -f # optional

    spack install

    </code></pre>

    <pre><code>cd spackenv/cent8/envs/avx512/rproject

    spack env activate -d .

    spack concretize -f # optional

    spack install

    </code></pre>

    <h4><a id="user-content-x86_64" class="anchor" aria-hidden="true" href="#x86_64"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>x86_64</h4>

    <p>This environment builds unoptimized software such as anaconda python, gnu parallel,
    scree, tmux, etc for generic x86_64 processor.</p>

    <h2><a id="user-content-centos-7x-software" class="anchor" aria-hidden="true"
    href="#centos-7x-software"><span aria-hidden="true" class="octicon octicon-link"></span></a>CentOS
    7.x software</h2>

    <p>This just collects the various environments for building software before the
    CentOS 8.x upgrade.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1657632897.0
antoine-morvan/spack-offline-env:
  data_format: 2
  description: null
  filenames:
  - matrixtest_env/spack.yaml
  - complete_env/spack.yaml
  - compilers_env/spack.yaml
  - simple_env/spack.yaml
  full_name: antoine-morvan/spack-offline-env
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1644310043.0
ashermancinelli/vimconfig:
  data_format: 2
  description: null
  filenames:
  - spack/spack.yaml
  - spack/envs/triage/spack.yaml
  full_name: ashermancinelli/vimconfig
  latest_release: null
  readme: "<h1><a id=\"user-content-vimconfig\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#vimconfig\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>vimconfig</h1>\n<p>Lots and lots of different configurations for various\
    \ programs all wrapped up into one repo. Under heavy development so tread with\
    \ some caution :)</p>\n<h2><a id=\"user-content-how-to-use\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#how-to-use\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>How to use</h2>\n<p>The top directory has a\
    \ script to deal with installation - you should pretty much only interact with\
    \ the repo through that script.\nThe help message is quite descriptive:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>$ ./configure --h\n\n  Usage:\n\
    \n  -p <span class=\"pl-k\">&lt;</span>path<span class=\"pl-k\">&gt;</span>  \
    \         Sets install prefix. Default: /people/manc568/.local\n  -r <span class=\"\
    pl-k\">&lt;</span>path<span class=\"pl-k\">&gt;</span>           Path to RC file\
    \ <span class=\"pl-k\">for</span> given shell. Default: /qfs/people/manc568/.bashrc\n\
    \  -d                  Default installation. Installs ctags, vim, and bash\n \
    \ -s <span class=\"pl-k\">&lt;</span>pkg<span class=\"pl-k\">&gt;</span>     \
    \       Show installation script <span class=\"pl-k\">for</span> pacakge\n  -i\
    \                  One or more of the following list, separated by commas with\
    \ no spaces:\n\n       zsh\n       bash\n       ctags\n       vim\n       tmux\n\
    \       emacs\n       profiles\n       modules\n       rice\n       rice.sh\n\
    \       fresh</pre></div>\n<h2><a id=\"user-content-examples\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#examples\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Examples</h2>\n<p>For example, to just install\
    \ my vim configuration, you'd do:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ ./configure -i vim</pre></div>\n<p>Or to install configs for multiple\
    \ programs:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ ./configure\
    \ -i vim,ctags,tmux,emacs,bash</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1670527897.0
bfovet/config:
  data_format: 2
  description: My personal configuration files
  filenames:
  - spack-env/linux-ubuntu22.04-skylake/spack.yaml
  full_name: bfovet/config
  latest_release: null
  readme: '<h1><a id="user-content-config" class="anchor" aria-hidden="true" href="#config"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>config</h1>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1658465316.0
bsurc/BSU-software-configs:
  data_format: 2
  description: null
  filenames:
  - falcon/environments/applications/namd/_spack.yaml
  - falcon/environments/compilers/_spack.yaml
  - falcon/environments/applications/vasp/_spack.yaml
  - falcon/environments/libraries/netcdf/_spack.yaml
  - falcon/environments/applications/openfoam/_spack.yaml
  - borah/environments/applications/wrf/_spack.yaml
  - borah/environments/b4s/_spack.yaml
  - falcon/environments/applications/vacuumms/_spack.yaml
  - falcon/environments/Core/_spack.yaml
  - borah/environments/_netcdf+hdf5+fftw/_spack.yaml
  - falcon/environments/base/_spack.yaml
  - falcon/environments/applications/lammps/_spack.yaml
  - falcon/environments/libraries/hdf5/_spack.yaml
  - borah/environments/libraries/netcdf/_spack.yaml
  - falcon/environments/applications/ncview/_spack.yaml
  - borah/environments/libraries/hdf5/_spack.yaml
  - falcon/environments/applications/gromacs/_spack.yaml
  - falcon/environments/applications/wps/_spack.yaml
  - falcon/environments/applications/wrf/_spack.yaml
  - borah/environments/base/_spack.yaml
  - falcon/environments/applications/ncl/_spack.yaml
  - falcon/environments/applications/quantum-espresso/_spack.yaml
  - borah/environments/compilers/_spack.yaml
  full_name: bsurc/BSU-software-configs
  latest_release: null
  readme: '<h1><a id="user-content-spack-configurations-used-to-stand-up-stacks-at-boise-state-university"
    class="anchor" aria-hidden="true" href="#spack-configurations-used-to-stand-up-stacks-at-boise-state-university"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Spack configurations
    used to stand up stacks at Boise State University</h1>

    <p>(C) 2022 Frank Willmore, et. al. Boise State Univesity Reseach Computing

    <a href="mailto:frankwillmore@boisestate.edu">frankwillmore@boisestate.edu</a></p>

    <p>Note that the environment (spack.yaml) files as checked in are named _spack.yaml,
    since spack rewrites and reorders spack.yaml as it digests the environment. _spack.yaml
    can be regarded as the master, and copied to spack.yaml when processing an environment.
    There is a .gitignore under BOISESTATE set to ignore spack.yaml''s under this
    tree.</p>

    <p>Base configurations provided gcc and oneapi compilers, cuda built with these
    compilers, mpich, openmpi, and intel-oneapi-mpi MPI stacks built with these compilers,
    and modules that will load the correct cuda build and compiler when loading the
    MPI.</p>

    <p>Copies of modules are checked in as well, as these needed to be modified considerably
    from the original generated modules.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1676657373.0
buildtesters/buildtest-nersc:
  data_format: 2
  description: null
  filenames:
  - buildspecs/apps/e4s/22.02/spack.yaml
  - buildspecs/apps/e4s/22.05/spack.yaml
  full_name: buildtesters/buildtest-nersc
  latest_release: null
  readme: '<h1><a id="user-content-buildtest-nersc" class="anchor" aria-hidden="true"
    href="#buildtest-nersc"><span aria-hidden="true" class="octicon octicon-link"></span></a>buildtest-nersc</h1>

    <p>This repository contains tests for Cori and Perlmutter using the <a href="https://buildtest.readthedocs.io/en/devel/"
    rel="nofollow">buildtest</a> framework.</p>

    <h2><a id="user-content-useful-links" class="anchor" aria-hidden="true" href="#useful-links"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Useful Links</h2>

    <ul>

    <li>CDASH: <a href="https://my.cdash.org/index.php?project=buildtest-nersc" rel="nofollow">https://my.cdash.org/index.php?project=buildtest-nersc</a>

    </li>

    <li>Upstream Repo: <a href="https://software.nersc.gov/NERSC/buildtest-nersc"
    rel="nofollow">https://software.nersc.gov/NERSC/buildtest-nersc</a>

    </li>

    <li>Github Mirror Repo: <a href="https://github.com/buildtesters/buildtest-nersc">https://github.com/buildtesters/buildtest-nersc</a>

    </li>

    </ul>

    <h2><a id="user-content-buildtest-references" class="anchor" aria-hidden="true"
    href="#buildtest-references"><span aria-hidden="true" class="octicon octicon-link"></span></a>Buildtest
    References</h2>

    <ul>

    <li>Documentation: <a href="https://buildtest.readthedocs.io/en/devel/" rel="nofollow">https://buildtest.readthedocs.io/en/devel/</a>

    </li>

    <li>Schema Docs: <a href="https://buildtesters.github.io/buildtest/" rel="nofollow">https://buildtesters.github.io/buildtest/</a>

    </li>

    <li>Slack Channel: <a href="https://hpcbuildtest.slack.com" rel="nofollow">https://hpcbuildtest.slack.com</a>

    </li>

    <li>Getting Started: <a href="https://buildtest.readthedocs.io/en/devel/getting_started.html"
    rel="nofollow">https://buildtest.readthedocs.io/en/devel/getting_started.html</a>

    </li>

    <li>Writing Buildspecs: <a href="https://buildtest.readthedocs.io/en/devel/buildspec_tutorial.html"
    rel="nofollow">https://buildtest.readthedocs.io/en/devel/buildspec_tutorial.html</a>

    </li>

    <li>Contributing Guide: <a href="https://buildtest.readthedocs.io/en/devel/contributing.html"
    rel="nofollow">https://buildtest.readthedocs.io/en/devel/contributing.html</a>

    </li>

    </ul>

    <h2><a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Setup</h2>

    <p>To get started, please <a href="https://docs.nersc.gov/connect/" rel="nofollow">connect
    to NERSC system</a> and clone this repo and buildtest:</p>

    <pre><code>git clone https://github.com/buildtesters/buildtest

    git clone https://software.nersc.gov/NERSC/buildtest-nersc

    </code></pre>

    <p>Note if you don''t have access to Gitlab server you may clone the mirror on
    Github:</p>

    <pre><code>git clone https://github.com/buildtesters/buildtest-nersc

    </code></pre>

    <p>You will need python 3.7 or higher to <a href="https://buildtest.readthedocs.io/en/devel/installing_buildtest.html"
    rel="nofollow">install buildtest</a>, on Cori/Perlmutter this can be done by loading
    <strong>python</strong>

    module and create a conda environment as shown below.</p>

    <pre><code>module load python

    conda create -n buildtest

    conda activate buildtest

    </code></pre>

    <p>Now let''s install buildtest, assuming you have cloned buildtest in $HOME directory
    source the setup script. For csh users you need to source <strong>setup.csh</strong></p>

    <pre><code>source ~/buildtest/setup.sh


    # csh users

    source ~/buildtest/setup.csh

    </code></pre>

    <p>Next, navigate to <code>buildtest-nersc</code> directory and set environment
    <code>BUILDTEST_CONFIGFILE</code> to point to <a href="https://software.nersc.gov/NERSC/buildtest-nersc/-/blob/devel/config.yml"
    rel="nofollow">config.yml</a> which is the configuration file for NERSC system.</p>

    <pre><code>cd buildtest-nersc

    export BUILDTEST_CONFIGFILE=$(pwd)/config.yml

    </code></pre>

    <p>Make sure the configuration is valid, this can be done by running the following.
    buildtest will validate the configuration file with the JSON schema :</p>

    <pre><code>buildtest config validate

    </code></pre>

    <p>Please make sure you are using tip of <a href="https://github.com/buildtesters/buildtest/tree/devel">devel</a>
    branch of buildtest when writing tests. You should sync your local devel branch
    with upstream

    fork, for more details see <a href="https://buildtest.readthedocs.io/en/devel/contributing/code_contribution_guide.html"
    rel="nofollow">contributing guide</a>.</p>

    <p>First time around you should discover all buildspecs this can be done via <code>buildtest
    buildspec find</code>.  The command below will find

    and validate all buildspecs in the <strong>buildtest-nersc</strong> repo and load
    them in buildspec cache. Note that one needs to specify <code>--root</code> to
    specify location where

    all buildspecs are located, we have not configured <a href="https://buildtest.readthedocs.io/en/devel/configuring_buildtest/overview.html#buildspec-roots"
    rel="nofollow">buildspec_root</a> in the configuration file since we don''t have
    a central location where this repo will reside.</p>

    <pre><code>cd buildtest-nersc

    buildtest buildspec find --root buildspecs --rebuild -q

    </code></pre>

    <p>The buildspecs are loaded in buildspec cache file (JSON) that is used by <code>buildtest
    buildspec find</code> for querying cache. Subsequent runs will

    read from cache.  For more details see <a href="https://buildtest.readthedocs.io/en/devel/gettingstarted/buildspecs_interface.html"
    rel="nofollow">buildspec interface</a>.</p>

    <h2><a id="user-content-building-tests" class="anchor" aria-hidden="true" href="#building-tests"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Building Tests</h2>

    <p><strong>Note: All tests are written in YAML using .yml extension</strong></p>

    <p>To build tests use <code>buildtest build</code> command for example we build
    all tests in <code>system</code> directory as follows</p>

    <pre><code>buildtest build -b system/

    </code></pre>

    <p>You can specify multiple buildspecs either files or directory via <code>-b</code>
    option</p>

    <pre><code>buildtest build -b slurm/partition.yml -b slurmutils/

    </code></pre>

    <p>You can exclude a buildspec via <code>-x</code> option this behaves same way
    as <code>-b</code> option so you can specify

    a directory or filepath which could be absolute path, or relative path. This is
    useful when

    you want to run multiple tests grouped in directory but exclude a few.</p>

    <pre><code>buildtest build -b slurm -x slurm/sinfo.yml

    </code></pre>

    <p>buildtest can run tests via tags which can be useful when grouping tests, to
    see a list of available tags you

    can run: <code>buildtest buildspec find --tags</code></p>

    <p>For instance if you want to run all <code>lustre</code> tests you can run the
    following:</p>

    <pre><code>buildtest build --tags lustre

    </code></pre>

    <p>For more details on buildtest test please see the <a href="https://buildtest.readthedocs.io/en/devel/getting_started.html"
    rel="nofollow">buildtest tutorial</a></p>

    <h2><a id="user-content-tags-breakdown" class="anchor" aria-hidden="true" href="#tags-breakdown"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Tags Breakdown</h2>

    <p>When you write buildspecs, please make sure you attach one or more <code>tags</code>
    to the test that way your test will get picked up during one of the CI checks.
    Shown

    below is a summary of tag description</p>

    <ul>

    <li>

    <strong>daily</strong> - this tag is used for running daily system checks using
    gitlab CI. Tests should run relatively quick</li>

    <li>

    <strong>system</strong> - this tag is used for classifying all system tests that
    may include: system configuration, servers, network, cray tests. This tag should
    be used</li>

    <li>

    <strong>slurm</strong> - this tag is used for slurm test that includes slurm utility
    check, slurm controller, etc... This tag <strong>shouldn''t</strong> be used for
    job submission that is managed by <strong>jobs</strong> tag. The <code>slurm</code>
    tag tests should be short running test that use a Local Executor.</li>

    <li>

    <strong>jobs</strong> - this tag is used for testing slurm policies by submitting
    jobs to scheduler.</li>

    <li>

    <strong>compile</strong> - this tag is used for compilation of application (OpenMP,
    MPI, OpenACC, CUDA, upc, bupc, etc...)</li>

    <li>

    <strong>e4s</strong> - this tag is used for running tests for E4S stack via <code>spack
    test</code> or <a href="https://github.com/E4S-Project/testsuite">E4S Testsuite</a>.</li>

    <li>

    <strong>module</strong> - this tag is used for testing module system</li>

    <li>

    <strong>benchmark</strong> - this tag is used for benchmark tests. This can be
    application benchmarks, mini-benchmarks, kernels, etc...</li>

    </ul>

    <p>You can see breakdown of tags and buildspec summary with the following commands</p>

    <pre><code>buildtest buildspec summary

    buildtest buildspec find --group-by-tags

    </code></pre>

    <h2><a id="user-content-querying-tests" class="anchor" aria-hidden="true" href="#querying-tests"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Querying Tests</h2>

    <p>You can use <code>buildtest report</code> and <code>buildtest inspect</code>
    to query tests. The commands differ slightly and data is

    represented differently. The <code>buildtest report</code> command will show output
    in tabular form and only show some of the metadata,

    if you want to access the entire test record use <code>buildtest inspect</code>
    command which displays the content in JSON format.

    For more details on querying tests see <a href="https://buildtest.readthedocs.io/en/devel/gettingstarted/query_test_report.html"
    rel="nofollow">https://buildtest.readthedocs.io/en/devel/gettingstarted/query_test_report.html</a></p>

    <h2><a id="user-content-ci-setup" class="anchor" aria-hidden="true" href="#ci-setup"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>CI Setup</h2>

    <p>Tests are run on schedule basis with one schedule corresponding to one gitlab
    job in <a href="https://software.nersc.gov/NERSC/buildtest-nersc/-/blob/devel/.gitlab-ci.yml"
    rel="nofollow">.gitlab-ci.yml</a>. The scheduled pipelines are configured in

    <a href="https://software.nersc.gov/NERSC/buildtest-nersc/-/pipeline_schedules"
    rel="nofollow">https://software.nersc.gov/NERSC/buildtest-nersc/-/pipeline_schedules</a>.
    Each schedule has a variable <code>TESTNAME</code> defined to control which pipeline

    is run since we have multiple gitlab jobs. In the <code>.gitlab-ci.yml</code>
    we make use of conditional rules using <a href="https://docs.gitlab.com/ee/ci/yaml/#onlyexcept-basic"
    rel="nofollow">only</a>.</p>

    <p>The scheduled jobs are run at different intervals (1x/day, 1x/week, etc...)
    at different times of day to avoid overloading the system. The gitlab jobs

    will run jobs based on tags, alternately some tests may be defined by running
    all tests in a directory (<code>buildtest build -b apps</code>). If you want to
    add a new

    scheduled job, please define a <a href="https://software.nersc.gov/NERSC/buildtest-nersc/-/pipeline_schedules/new"
    rel="nofollow">new schedule</a> with an appropriate time. The

    <code>target branch</code> should be <code>devel</code> and define a unique variable
    used to distinguish scheduled jobs. Next, create a job in <code>.gitlab-ci.yml</code>
    that references the scheduled job and define variable <code>TESTNAME</code> in
    the scheduled pipeline.</p>

    <h2><a id="user-content-integrations" class="anchor" aria-hidden="true" href="#integrations"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Integrations</h2>

    <p>This project has integration with Slack to notify CI builds to <a href="https://hpcbuildtest.slack.com"
    rel="nofollow">buildtest Slack</a> at <strong>#buildtest-nersc</strong> workspace.
    The integrations can be

    found at <a href="https://software.nersc.gov/NERSC/buildtest-nersc/-/settings/integrations"
    rel="nofollow">https://software.nersc.gov/NERSC/buildtest-nersc/-/settings/integrations</a>.</p>

    <p>This project has setup a push mirror to <a href="https://github.com/buildtesters/buildtest-nersc">https://github.com/buildtesters/buildtest-nersc</a>
    which can be seen at <a href="https://software.nersc.gov/NERSC/buildtest-nersc/-/settings/repository"
    rel="nofollow">https://software.nersc.gov/NERSC/buildtest-nersc/-/settings/repository</a>

    under <strong>Mirroring Repositories</strong>. If the push mirror is not setup,
    please add the mirror.</p>

    <h2><a id="user-content-cdash" class="anchor" aria-hidden="true" href="#cdash"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>CDASH</h2>

    <p>buildtest will push test results to <a href="https://www.cdash.org/" rel="nofollow">CDASH</a>
    server

    at <a href="https://my.cdash.org/index.php?project=buildtest-nersc" rel="nofollow">https://my.cdash.org/index.php?project=buildtest-nersc</a>
    using <code>buildtest cdash upload</code> command.</p>

    <h2><a id="user-content-contributing-guide" class="anchor" aria-hidden="true"
    href="#contributing-guide"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contributing
    Guide</h2>

    <p>To contribute back you will want to make sure your buildspec is validated before
    you contribute back, this could be

    done by running test manually <code>buildtest build</code> or see if buildspec
    is valid via <code>buildtest buildspec find</code>. It

    would be good to run your test and make sure it is working as expected, you can
    view test detail using <code>buildtest inspect name &lt;testname&gt;</code> or
    <code>buildtest inspect query &lt;testname&gt;</code>. For more

    details on querying test please see <a href="https://buildtest.readthedocs.io/en/devel/gettingstarted/query_test_report.html"
    rel="nofollow">https://buildtest.readthedocs.io/en/devel/gettingstarted/query_test_report.html</a>.</p>

    <p>If you want to contribute your tests, please see <a href="https://software.nersc.gov/NERSC/buildtest-nersc/-/blob/devel/CONTRIBUTING.md"
    rel="nofollow">CONTRIBUTING.md</a></p>

    <h2><a id="user-content-submitting-an-issue" class="anchor" aria-hidden="true"
    href="#submitting-an-issue"><span aria-hidden="true" class="octicon octicon-link"></span></a>Submitting
    an Issue</h2>

    <p>Please submit all issues to <a href="https://github.com/buildtesters/buildtest-nersc/issues">https://github.com/buildtesters/buildtest-nersc/issues</a>.
    When creating an issue, please see the <a href="https://github.com/buildtesters/buildtest-nersc/labels">labels</a>

    and try to select one or more labels to categorize issue. Please use the following
    labels depending on the type of issue you are reporting</p>

    <ul>

    <li>

    <a href="https://github.com/buildtesters/buildtest-nersc/labels/bug">Bug</a>:
    When creating an issue related to a test bug</li>

    <li>

    <a href="https://github.com/buildtesters/buildtest-nersc/labels/new-test">new-test</a>:
    An issue for adding a new test</li>

    <li>

    <a href="https://github.com/buildtesters/buildtest-nersc/labels/E4S-Testsuite">E4S-Testsuite</a>:
    Issues related to <a href="https://github.com/E4S-Project/testsuite">E4S testsuite
    project</a>

    </li>

    <li>

    <a href="https://github.com/buildtesters/buildtest-nersc/labels/spack">spack</a>:
    Issues related to <code>spack test</code>

    </li>

    <li>

    <a href="https://github.com/buildtesters/buildtest-nersc/labels/documentation">documentation</a>:
    Issues with documentation such as README.md, CONTRIBUTING.md</li>

    <li>

    <a href="https://github.com/buildtesters/buildtest-nersc/labels/gitlab-ci">gitlab-ci</a>:
    Issues with Gitlab CI/CD</li>

    </ul>

    '
  stargazers_count: 6
  subscribers_count: 4
  topics:
  - buildtest
  updated_at: 1671402594.0
celeritas-project/celeritas:
  data_format: 2
  description: Celeritas is a new Monte Carlo transport code designed for high-performance
    simulation of high-energy physics detectors.
  filenames:
  - scripts/spack.yaml
  full_name: celeritas-project/celeritas
  latest_release: v0.2.1
  readme: "<h1><a id=\"user-content-celeritas\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#celeritas\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Celeritas</h1>\n<p>The Celeritas project implements HEP detector physics\
    \ on GPU accelerator\nhardware with the ultimate goal of supporting the massive\
    \ computational\nrequirements of the <a href=\"https://home.cern/science/accelerators/high-luminosity-lhc\"\
    \ rel=\"nofollow\">HL-LHC upgrade</a>.</p>\n<h1><a id=\"user-content-documentation\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#documentation\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h1>\n<p>Most of\
    \ the Celeritas documentation is readable through the codebase through a\ncombination\
    \ of <a href=\"doc/index.rst\">static RST documentation</a> and Doxygen-markup\n\
    comments in the source code itself. The full <a href=\"https://celeritas-project.github.io/celeritas/user/index.html\"\
    \ rel=\"nofollow\">Celeritas user\ndocumentation</a> (including selected code\
    \ documentation incorporated\nby Breathe) and the <a href=\"https://celeritas-project.github.io/celeritas/dev/index.html\"\
    \ rel=\"nofollow\">Celeritas code documentation</a> are mirrored on\nour GitHub\
    \ pages site. You can generate these yourself by\nsetting the <code>CELERITAS_BUILD_DOCS=ON</code>\
    \ configuration option and running <code>ninja doc</code> (user) or <code>ninja\
    \ doxygen</code> (developer). A continuously updated version of\nthe <a href=\"\
    https://celeritas.readthedocs.io/en/latest/\" rel=\"nofollow\">static Celeritas\
    \ user documentation</a> (without API documentation) is\nhosted on <code>readthedocs.io</code>.</p>\n\
    <h1><a id=\"user-content-installation-for-applications\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#installation-for-applications\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Installation for applications</h1>\n<p>The easiest\
    \ way to install Celeritas as a library/app is with Spack:</p>\n<ul>\n<li>Follow\
    \ the first two steps above to install <a href=\"https://spack.readthedocs.io/en/latest/getting_started.html\"\
    \ rel=\"nofollow\">Spack</a> and set up its CUDA usage.</li>\n<li>Install Celeritas\
    \ with <code>spack install celeritas</code>\n</li>\n<li>Use <code>spack load celeritas</code>\
    \ to add the installation to your <code>PATH</code>.</li>\n</ul>\n<p>Then see\
    \ the \"Downstream usage as a library\" section of the <a href=\"doc/installation.rst\"\
    >installation\ndocumentation</a> for how to use Celeritas in your application\
    \ or framework.</p>\n<h1><a id=\"user-content-installation-for-developers\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#installation-for-developers\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation for developers</h1>\n\
    <p>Since Celeritas is still under heavy development and is not yet full-featured\n\
    for downstream integration, you are likely installing it for development\npurposes.\
    \ The <a href=\"doc/installation.rst\">installation documentation</a> has a\n\
    complete description of the code's dependencies and installation process for\n\
    development.</p>\n<p>As an example, if you have the <a href=\"https://github.com/spack/spack\"\
    >Spack</a> package manager\ninstalled and want to do development on a CUDA system\
    \ with Volta-class graphics\ncards, execute the following steps:</p>\n<div class=\"\
    highlight highlight-text-shell-session\"><pre># <span class=\"pl-s1\">Set up CUDA\
    \ (optional)</span>\n$ <span class=\"pl-s1\">spack external find cuda</span>\n\
    $ <span class=\"pl-s1\">spack config add packages:all:variants:<span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>+cuda cuda_arch=70<span class=\"pl-pds\"\
    >\"</span></span></span>\n# <span class=\"pl-s1\">Install celeritas dependencies</span>\n\
    $ <span class=\"pl-s1\">spack env create celeritas scripts/spack.yaml</span>\n\
    $ <span class=\"pl-s1\">spack env activate celeritas</span>\n$ <span class=\"\
    pl-s1\">spack install</span>\n# <span class=\"pl-s1\">Configure, build, and <span\
    \ class=\"pl-c1\">test</span></span>\n$ <span class=\"pl-s1\">./build.sh base</span></pre></div>\n\
    <p>If you don't use Spack but have all the dependencies you want (Geant4,\ngoogletest,\
    \ VecGeom, etc.) in your <code>CMAKE_PREFIX_PATH</code>, you can configure and\n\
    build Celeritas as you would any other project:</p>\n<div class=\"highlight highlight-text-shell-session\"\
    ><pre>$ <span class=\"pl-s1\">mkdir build <span class=\"pl-k\">&amp;&amp;</span>\
    \ <span class=\"pl-c1\">cd</span> build</span>\n$ <span class=\"pl-s1\">cmake\
    \ ..</span>\n$ <span class=\"pl-s1\">make <span class=\"pl-k\">&amp;&amp;</span>\
    \ ctest</span></pre></div>\n<p>Celeritas guarantees full compatibility and correctness\
    \ only on the\ncombinations of compilers and dependencies tested under continuous\
    \ integration.\nCurrently supported compilers are GCC 11.2 + NVCC 11.8, and HIP-Clang\
    \ 15.0, but\nsince we compile with extra warning flags and avoid non-portable\
    \ code, most\nother compilers <em>should</em> work.\nCurrently Geant4 11.0 and\
    \ VecGeom 1.2 are the only versions that are guaranteed\nto work, but older versions\
    \ might be OK.\nThe full set of configurations is viewable on <a href=\"https://cloud.cees.ornl.gov/jenkins-ci/blue/organizations/jenkins/Celeritas/activity?branch=master\"\
    \ rel=\"nofollow\">the CI web site</a>.\nCompatibility fixes that do not cause\
    \ newer versions to fail are welcome.</p>\n<h1><a id=\"user-content-development\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#development\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Development</h1>\n<p>See the\
    \ <a href=\"CONTRIBUTING.rst\">contribution guide</a> for the contribution process,\n\
    <a href=\"doc/appendices/development.rst\">the development guidelines</a> for\
    \ further\ndetails on coding in Celeritas, and <a href=\"doc/appendices/administration.rst\"\
    >the administration guidelines</a> for community standards and roles.</p>\n<h1><a\
    \ id=\"user-content-citing-celeritas\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #citing-celeritas\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Citing Celeritas</h1>\n<p>If using Celeritas in your work, we ask\
    \ that you cite the code using its\n<a href=\"https://www.osti.gov/doecode/biblio/94866\"\
    \ rel=\"nofollow\">DOECode</a> registration:</p>\n<blockquote>\n<p>Johnson, Seth\
    \ R., Amanda Lund, Soon Yung Jun, Stefano Tognini, Guilherme Lima, Paul Romano,\
    \ Philippe Canal, Ben Morgan, and Tom Evans. \u201CCeleritas,\u201D July 2022.\
    \ <a href=\"https://doi.org/10.11578/dc.20221011.1\" rel=\"nofollow\">https://doi.org/10.11578/dc.20221011.1</a>.</p>\n\
    </blockquote>\n<p>Additional references for code implementation details, benchmark\
    \ problem\nresults, etc., can be found in our continually evolving <a href=\"\
    doc/_static/celeritas.bib\">citation\nfile</a>.</p>\n"
  stargazers_count: 30
  subscribers_count: 10
  topics:
  - hep
  - cuda
  - computational-physics
  - monte-carlo
  updated_at: 1676731995.0
charmoniumQ/wf-reg-test:
  data_format: 2
  description: null
  filenames:
  - spack/spack.yaml
  full_name: charmoniumQ/wf-reg-test
  latest_release: null
  readme: "<h1><a id=\"user-content-wf-reg-test\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#wf-reg-test\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>wf-reg-test</h1>\n<p>Software tends to break or \"collapse\" over\
    \ time, even if it is unchanged, due to non-obvious changes in the computational\
    \ environment.\nCollapse in computational experiments undermines long-term credibility\
    \ and hinders day-to-day operations.\nWe propose to create the first public dataset\
    \ of automatically executable scientific experiments.\nThis data could be used\
    \ to identify best practices, make continuous testing feasible, and repair broken\
    \ programs.\nThese techniques increase the replicability of computational experiments.</p>\n\
    <p>Conceptually, we intend to collect the following:</p>\n<div class=\"highlight\
    \ highlight-source-python\"><pre><span class=\"pl-k\">for</span> <span class=\"\
    pl-s1\">registry</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\"\
    >registries</span>:\n    <span class=\"pl-k\">for</span> <span class=\"pl-s1\"\
    >experiment</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">registry</span>:\n\
    \        <span class=\"pl-k\">for</span> <span class=\"pl-s1\">version</span>\
    \ <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">experiment</span>:\n \
    \           <span class=\"pl-k\">for</span> <span class=\"pl-s1\">i</span> <span\
    \ class=\"pl-c1\">in</span> <span class=\"pl-en\">range</span>(<span class=\"\
    pl-s1\">num_repetitions</span>):\n                <span class=\"pl-s1\">execution</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-en\">execute</span>(<span class=\"\
    pl-s1\">version</span>)\n                <span class=\"pl-s1\">data</span>.<span\
    \ class=\"pl-en\">append</span>((\n                    <span class=\"pl-s1\">execution</span>.<span\
    \ class=\"pl-s1\">date</span>,   <span class=\"pl-s1\">execution</span>.<span\
    \ class=\"pl-s1\">output</span>,\n                    <span class=\"pl-s1\">execution</span>.<span\
    \ class=\"pl-s1\">logs</span>,   <span class=\"pl-s1\">execuiton</span>.<span\
    \ class=\"pl-s1\">res_usage</span>,\n                    <span class=\"pl-s1\"\
    >version</span>.<span class=\"pl-s1\">date</span>,     <span class=\"pl-s1\">version</span>.<span\
    \ class=\"pl-s1\">code</span>,\n                    <span class=\"pl-s1\">experiment</span>.<span\
    \ class=\"pl-s1\">name</span>,  <span class=\"pl-s1\">registry</span>.<span class=\"\
    pl-s1\">name</span>,\n                ))</pre></div>\n<h1><a id=\"user-content-reproducing\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#reproducing\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Reproducing</h1>\n<p>See <a href=\"\
    REPRODUCING.md\"><code>REPRODUCING.md</code></a> for instructions on reproducing\
    \ these results.</p>\n<h1><a id=\"user-content-todo\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#todo\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>TODO</h1>\n<p>See <a href=\"TODO.md\"><code>TODO.md</code></a> for\
    \ instructions on reproducing these results.</p>\n<h1><a id=\"user-content-contributing\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#contributing\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contributing</h1>\n<p>See <a\
    \ href=\"CONTRIBUTING.md\"><code>CONTRIBUTING.md</code></a> for instructions on\
    \ setting up a development environment.</p>\n"
  stargazers_count: 1
  subscribers_count: 3
  topics: []
  updated_at: 1676689037.0
deephyper/deephyper-platform-configurations:
  data_format: 2
  description: This repository provides a set of configuration files and example scripts
    for running DeepHyper experiments on various platforms.
  filenames:
  - ANL/Polaris/spack.yaml
  - ANL/Swing/spack.yaml
  full_name: deephyper/deephyper-platform-configurations
  latest_release: null
  readme: '<h1><a id="user-content-deephyper-platform-configurations" class="anchor"
    aria-hidden="true" href="#deephyper-platform-configurations"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>DeepHyper Platform Configurations</h1>

    <p>This repository provides a set of configuration files and example scripts for
    running DeepHyper experiments on various platforms.</p>

    <p>The <code>generic</code> subdirectory contains a minimal DeepHyper environment
    example that can be used as a starting point for systems for which there is no
    existing recipe.</p>

    <h2><a id="user-content-using-spackyaml-files" class="anchor" aria-hidden="true"
    href="#using-spackyaml-files"><span aria-hidden="true" class="octicon octicon-link"></span></a>Using
    spack.yaml files</h2>

    <p>Each platform subdirectory in this repository provides a <code>spack.yaml</code>
    file.

    A <code>spack.yaml</code> file fully describes a Spack environment, including

    system-provided packages and compilers. It does so independently of any

    <code>compilers.yaml</code> or <code>packages.yaml</code> files installed in <code>~/.spack</code>,
    thereby

    preventing interference with user-specific spack configurations as much as

    possible.</p>

    <p>You may use <code>spack.yaml</code> files to create a

    <a href="https://spack.readthedocs.io/en/latest/environments.html" rel="nofollow">Spack
    environment</a>

    in which DeepHyper packages will be installed.</p>

    <p>If you don''t have Spack installed on your platform, clone it and set it up

    as follows.</p>

    <div class="highlight highlight-source-shell"><pre>git clone -c feature.manyFiles=true
    https://github.com/spack/spack.git

    <span class="pl-c1">.</span> spack/share/spack/setup-env.sh</pre></div>

    <p>Remember that the second line needs to be executed every time you open a new

    terminal; it may be helpful to create an alias in your bashrc file as a

    shortcut.</p>

    <p>You will then need to clone <code>deephyper-spack-packages</code>, which contains
    the DeepHyper packages.</p>

    <div class="highlight highlight-source-shell"><pre>git clone https://github.com/deephyper/deephyper-spack-packages.git

    spack repo add deephyper-spack-packages</pre></div>

    <p>Now clone the present repository and <code>cd</code> into the subdirectory
    relevant

    to your platform. For example:</p>

    <div class="highlight highlight-source-shell"><pre>git clone https://github.com/deephyper/deephyper-platform-configurations.git

    <span class="pl-c1">cd</span> deephyper-platform-configurations/ANL/Polaris</pre></div>

    <p>Edit the path to <code>deephyper-spack-packages</code> in the <code>repos</code>
    field of the <code>spack.yaml</code> file to

    match your installation.</p>

    <p>Then, execute the following command

    (changing <em>myenv</em> into an appropriate name for your environment).</p>

    <div class="highlight highlight-source-shell"><pre>spack env create myenv spack.yaml</pre></div>

    <p>Change to a directory outside of the <code>deephyper-platform-configurations</code>
    folders

    and activate the environment as follows.</p>

    <div class="highlight highlight-source-shell"><pre>spack env activate myenv</pre></div>

    <p>Once you have added the specs you need in your environment, install

    everything by executing the following command.</p>

    <div class="highlight highlight-source-shell"><pre>spack install</pre></div>

    <p>You may add more specs later on. For more information on how to manage

    Spack environments, please refer to the Spack documentation.</p>

    <h2><a id="user-content-acknowledgment" class="anchor" aria-hidden="true" href="#acknowledgment"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Acknowledgment</h2>

    <p>This repository was created by following the example of the <a href="https://github.com/mochi-hpc-experiments/platform-configurations">Mochi
    Project</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1675421226.0
dyokelson/soma_c:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: dyokelson/soma_c
  latest_release: null
  readme: '<p>Your project "soma" has been setup!

    Enjoy programming with Mochi!</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1675627136.0
dyokelson/soma_cpp:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: dyokelson/soma_cpp
  latest_release: null
  readme: '<p>Your project "soma" has been setup!

    Enjoy programming with Mochi!</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1675794086.0
eic/containers:
  data_format: 2
  description: Container building infrastructure (mirror of https://eicweb.phy.anl.gov/containers/eic_container)
  filenames:
  - spack.yaml
  full_name: eic/containers
  latest_release: null
  readme: '<h1><a id="user-content-eic-software-environment-container" class="anchor"
    aria-hidden="true" href="#eic-software-environment-container"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>EIC software environment container</h1>

    <p>For installation instructions of <code>eic-shell</code>, see <a href="https://github.com/eic/eic-shell">https://github.com/eic/eic-shell</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1679421635.0
epfl-radio-astro/ska-spack-env:
  data_format: 2
  description: SKA Spack environments related files
  filenames:
  - bipp-izar-gcc/spack.yaml
  - bipp-jed-gcc/spack.yaml
  - env-bipp-izar/spack.yaml
  full_name: epfl-radio-astro/ska-spack-env
  latest_release: null
  readme: '<h1><a id="user-content-ska-spack-env" class="anchor" aria-hidden="true"
    href="#ska-spack-env"><span aria-hidden="true" class="octicon octicon-link"></span></a>ska-spack-env</h1>

    <p>SKA Spack environments related files</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1674857920.0
esm-tools/esm_tools:
  data_format: 2
  description: Simple Infrastructure for Earth System Simulations
  filenames:
  - configs/spack_envs/albedo-spack.yaml
  full_name: esm-tools/esm_tools
  latest_release: v6.0.0
  stargazers_count: 20
  subscribers_count: 9
  topics: []
  updated_at: 1671059683.0
eugeneswalker/exago-crusher:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: eugeneswalker/exago-crusher
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1657654737.0
eugeneswalker/noaa:
  data_format: 2
  description: null
  filenames:
  - clang/spack.yaml
  - oneapi/failures/spack.yaml
  - gnu/spack.yaml
  - nvhpc/failures/spack.yaml
  - clang/failures/spack.yaml
  - gnu/failures/spack.yaml
  - oneapi/spack.yaml
  full_name: eugeneswalker/noaa
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1675202595.0
eugeneswalker/noaa-prototyping:
  data_format: 2
  description: null
  filenames:
  - gnu/spack.yaml
  - nvhpc/spack.yaml
  - oneapi/spack.yaml
  full_name: eugeneswalker/noaa-prototyping
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1666800656.0
eugeneswalker/qmcpack-ci-container:
  data_format: 2
  description: null
  filenames:
  - docker-images/spack.yaml
  - spack.yaml
  full_name: eugeneswalker/qmcpack-ci-container
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1678143972.0
floquet/builds:
  data_format: 2
  description: Notes and scripts for building applications on HPCs
  filenames:
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-26_00,09/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-26_00,09/yamls/share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-26_00,09/yamls/share/spack/gitlab/cloud_pipelines/stacks/tutorial/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-26_00,09/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-26_00,09/yamls/share/spack/gitlab/cloud_pipelines/stacks/build_systems/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-26_00,09/yamls/share/spack/gitlab/cloud_pipelines/stacks/radiuss/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-30_18,20/yamls/share/spack/gitlab/cloud_pipelines/stacks/build_systems/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-26_00,09/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-mac/spack.yaml
  full_name: floquet/builds
  latest_release: null
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1641760136.0
fnalacceleratormodeling/synergia2-containers:
  data_format: 2
  description: null
  filenames:
  - ubuntu-gcc/spack.yaml
  - ubuntu-clang/spack.yaml
  full_name: fnalacceleratormodeling/synergia2-containers
  latest_release: null
  readme: '<h1><a id="user-content-synergia2-containers" class="anchor" aria-hidden="true"
    href="#synergia2-containers"><span aria-hidden="true" class="octicon octicon-link"></span></a>synergia2-containers</h1>

    <p>This repository contains docker recipes for building containers that contain
    all dependencies for synergia2. These recipes are generated using <a href="https://spack.readthedocs.io/en/latest/environments.html"
    rel="nofollow">spack environments</a> via <a href="https://spack.readthedocs.io/en/latest/containers.html"
    rel="nofollow"><code>spack containerize</code></a>, with some minor modifications.
    GithubActions is used to build these containers for <code>x86_64_v2</code> ISA
    and these containers can be pulled from the github container registry. For instructions
    on how to pull a particular image, visit the page associated with it <a href="https://github.com/orgs/fnalacceleratormodeling/packages?repo_name=synergia2-containers">here</a>.</p>

    <p>These containers are used as test environments for testing synergia2 via GithubActions.</p>

    '
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1678463172.0
giordano/julia-on-fugaku:
  data_format: 2
  description: null
  filenames:
  - benchmarks/blas-axpy/spack-env/spack.yaml
  full_name: giordano/julia-on-fugaku
  latest_release: null
  readme: "<h1><a id=\"user-content-julia-on-fugaku-2022-07-23\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#julia-on-fugaku-2022-07-23\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Julia on Fugaku (2022-07-23)</h1>\n\
    <p><em>Note: many links refer to internal documentation which is accessible only\
    \ to Fugaku users.</em></p>\n<h2><a id=\"user-content-read-the-paper\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#read-the-paper\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Read the paper</h2>\n<p>Benchmarks\
    \ present in this repository have been published in the paper <a href=\"https://doi.org/10.1109/CLUSTER51413.2022.00072\"\
    \ rel=\"nofollow\">Productivity meets\nPerformance: Julia on A64FX</a>, presented\
    \ at\nthe 2022 IEEE International Conference on Cluster Computing (CLUSTER22),\
    \ as part of the\n<a href=\"https://arm-hpc-user-group.github.io/eahpc-2022/\"\
    \ rel=\"nofollow\">Embracing Arm for High Performance Computing\nWorkshop</a>\
    \ (pre-print available on arXiv:\n<a href=\"https://arxiv.org/abs/2207.12762\"\
    \ rel=\"nofollow\"><code>2207.12762</code></a>).  See the <a href=\"./CITATION.bib\"\
    ><code>CITATION.bib</code></a>\nfile for a BibTeX entry to cite the paper.</p>\n\
    <h2><a id=\"user-content-storage\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #storage\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Storage</h2>\n\
    <p>Before doing anything on Fugaku, be aware that there are <a href=\"https://www.fugaku.r-ccs.riken.jp/en/operation/20220408_01\"\
    \ rel=\"nofollow\">tight\nlimits</a> on the size of (20 GiB)\nand the number of\
    \ inodes in (200k) your home directory.  If you use many Julia Pkg\nartifacts,\
    \ it's very likely you'll hit these limits.  You'll notice that you hit the limit\n\
    because any disk I/O operation will result in a <code>Disk quota exceeded</code>\
    \ error like this:</p>\n<div class=\"highlight highlight-text-shell-session\"\
    ><pre><span class=\"pl-e\">[user@fn01sv03 ~]</span>$ <span class=\"pl-s1\">touch\
    \ foo</span>\n<span class=\"pl-c1\">touch: cannot touch 'foo': Disk quota exceeded</span></pre></div>\n\
    <p>You can check the quota of your home directory with <code>accountd</code> for\
    \ the size, and <code>accountd -i</code> for the number of inodes.</p>\n<h3><a\
    \ id=\"user-content-using-the-data-directory\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#using-the-data-directory\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Using the data directory</h3>\n<p>In order to\
    \ avoid clogging up the home directory you may want to move the Julia depot to\
    \ the\ndata directory:</p>\n<div class=\"highlight highlight-source-shell\"><pre>DATADIR=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>/data/&lt;YOUR GROUP&gt;/<span\
    \ class=\"pl-smi\">${USER}</span><span class=\"pl-pds\">\"</span></span>\n<span\
    \ class=\"pl-k\">export</span> JULIA_DEPOT_PATH=<span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span><span class=\"pl-smi\">${DATADIR}</span>/julia-depot<span class=\"\
    pl-pds\">\"</span></span></pre></div>\n<h2><a id=\"user-content-interactive-usage\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#interactive-usage\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Interactive usage</h2>\n<p>The\
    \ login nodes you access via <code>login.fugaku.r-ccs.riken.jp</code> (<a href=\"\
    https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/use_latest/AccessToTheSystem/LoggingInToTheFugakuComputerWithLocalAccount.html\"\
    \ rel=\"nofollow\">connection\ninstructions</a>)\nhave Cascade Lake CPUs, so they\
    \ aren't much useful if you want to run an aarch64 Julia.</p>\n<p>You can <a href=\"\
    https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/use_latest/JobExecution/Overview.html\"\
    \ rel=\"nofollow\">submit jobs to the\nqueue</a>\nto run Julia code on the A64FX\
    \ compute nodes, but this can be cumbersone if you need quick\nfeedback during\
    \ development or debugging.  You can also request an <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/use_latest/JobExecution/InteractiveJob.html\"\
    \ rel=\"nofollow\">interactive\nnode</a>,\nfor example with:</p>\n<pre><code>pjsub\
    \ --interact -L \"node=1\" -L \"rscgrp=int\" -L \"elapse=30:00\" --sparam \"wait-time=600\"\
    \ --mpi \"max-proc-per-node=4\"\n</code></pre>\n<h2><a id=\"user-content-available-software\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#available-software\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Available software</h2>\n<p>Fugaku\
    \ uses the <a href=\"https://spack.io/\" rel=\"nofollow\">Spack package manager</a>.\
    \  For more information about how\nto use it, see the <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/FugakuSpackGuide/\"\
    \ rel=\"nofollow\">Fugaku Spack User\nGuide</a>.</p>\n<p>Note that Spack is installed\
    \ in <code>/vol0004</code>, this means that if your home directory isn't\nmounted\
    \ on this volume you will have to <a href=\"https://www.fugaku.r-ccs.riken.jp/en/operation/20211130_02\"\
    \ rel=\"nofollow\">explicitly request the\npartition</a> in your submission\n\
    job scripts or commands, for example by adding <code>-x PJM_LLIO_GFSCACHE=/vol0004</code>\
    \ to the\n<code>pjsub</code> command, or the line</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>PJM\
    \ -x PJM_LLIO_GFSCACHE=/vol0004</span></pre></div>\n<p>in a job script.</p>\n\
    <h2><a id=\"user-content-using-julia-on-the-compute-nodes\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#using-julia-on-the-compute-nodes\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Using Julia on the compute nodes</h2>\n<p>There\
    \ is a Julia module built with Spack <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/UsingOSS/oss_e.html#packages-installed-on-the-compute-nodes\"\
    \ rel=\"nofollow\">available on the compute\nnodes</a>,\nbut as of this writing\
    \ (2022-07-23) the version of Julia provided is 1.6.3, so you may want\nto download\
    \ a more recent version from the <a href=\"https://julialang.org/downloads/\"\
    \ rel=\"nofollow\">official\nwebsite</a>.  Use the <code>aarch64</code> builds\
    \ for Glibc Linux,\npreferably <a href=\"https://julialang.org/downloads/#current_stable_release\"\
    \ rel=\"nofollow\">latest stable</a> or even\nthe <a href=\"https://julialang.org/downloads/nightlies/\"\
    \ rel=\"nofollow\">nightly build</a> if you feel confident.</p>\n<p>To enable\
    \ full vectorisation you may need to set the environment variable\n<code>JULIA_LLVM_ARGS=\"\
    -aarch64-sve-vector-bits-min=512\"</code>.  Example:\n<a href=\"https://github.com/JuliaLang/julia/issues/40308#issuecomment-901478623\"\
    >https://github.com/JuliaLang/julia/issues/40308#issuecomment-901478623</a>. \
    \ However, note that\nare a couple of severe bugs when using 512-bit vectors:</p>\n\
    <ul>\n<li>\n<a href=\"https://github.com/JuliaLang/julia/issues/44401\">https://github.com/JuliaLang/julia/issues/44401</a>\
    \ (may be an upstream LLVM bug:\n<a href=\"https://github.com/llvm/llvm-project/issues/53331\"\
    >https://github.com/llvm/llvm-project/issues/53331</a>)</li>\n<li>\n<a href=\"\
    https://github.com/JuliaLang/julia/issues/44263\">https://github.com/JuliaLang/julia/issues/44263</a>\
    \ (only in Julia v1.8+)</li>\n</ul>\n<p><em><strong>Note</strong></em>: Julia\
    \ v1.9, which is based on <a href=\"https://community.arm.com/arm-community-blogs/b/tools-software-ides-blog/posts/llvm-14\"\
    \ rel=\"nofollow\">LLVM\n14</a>,\nis able to natively autovectorise code for A64FX\
    \ <em>without</em> having to set\n<code>JULIA_LLVM_ARGS</code>, side stepping\
    \ the issues above altogether.</p>\n<h2><a id=\"user-content-mpijl\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#mpijl\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>MPI.jl</h2>\n<p><a href=\"https://github.com/JuliaParallel/MPI.jl\"\
    ><code>MPI.jl</code></a> with default JLL-provided MPICH works\nout of the box!\
    \  In order to\n<a href=\"https://juliaparallel.github.io/MPI.jl/stable/configuration/\"\
    \ rel=\"nofollow\">configure</a> <code>MPI.jl</code> v0.19 to\nuse system-provided\
    \ Fujitsu MPI (based on OpenMPI) you have to specify the <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/lang_latest/FujitsuCompiler/CompileCommands.html\"\
    \ rel=\"nofollow\">MPI C\ncompiler</a>\nfor A64FX with</p>\n<pre><code>julia --project\
    \ -e 'ENV[\"JULIA_MPI_BINARY\"]=\"system\"; ENV[\"JULIA_MPICC\"]=\"mpifcc\"; using\
    \ Pkg; Pkg.build(\"MPI\"; verbose=true)'\n</code></pre>\n<p><em><strong>Note #1</strong></em>:\
    \ <code>mpifcc</code> is available only on the compute nodes.  On the login nodes\
    \ that would be\n<code>mpifccpx</code>, but this is the cross compiler running\
    \ on Intel architecture, it's unlikely\nyou'll run an <code>aarch64</code> Julia\
    \ on there.  <a href=\"https://github.com/JuliaParallel/MPI.jl/issues/539\">Preliminary\n\
    tests</a> show that <code>MPI.jl</code> should work\nmostly fine with Fujitsu\
    \ MPI, but custom error handlers may not be available (read: trying\nto use them\
    \ causes segmentation faults).</p>\n<p><em><strong>Note #2</strong></em>: in <code>MPI.jl</code>\
    \ v0.20 Fujitsu MPI is a known ABI (it's the same as OpenMPI) and\nthere is nothing\
    \ special to do to configure it apart from <a href=\"https://juliaparallel.org/MPI.jl/dev/configuration/#Configuration-2\"\
    \ rel=\"nofollow\">choosing the system\nbinaries</a>.</p>\n<p><em><strong>Note\
    \ #3</strong></em>: we recommend using <code>MPI.jl</code>'s wrapper of <code>mpiexec</code>\
    \ to run MPI applications\nwith Julia:\n<a href=\"https://juliaparallel.org/MPI.jl/stable/configuration/#Julia-wrapper-for-mpiexec\"\
    \ rel=\"nofollow\"><code>mpiexecjl</code></a>.</p>\n<h3><a id=\"user-content-file-system-latency\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#file-system-latency\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>File system latency</h3>\n<p>Fugaku\
    \ has an advanced system to handle <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/use_latest/LyeredStorageAndLLIO/index.html\"\
    \ rel=\"nofollow\">parallel file system\nlatency</a>.\nIn order.  In order to\
    \ speed up parallel applications run through MPI you may want to\ndistribute it\
    \ to the cache area of the second-layer storage on the first-layer storage using\n\
    <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/use_latest/LyeredStorageAndLLIO/TheSecondLayerStrage.html#common-file-distribution-function-llio-transfer\"\
    \ rel=\"nofollow\"><code>llio_transfer</code></a>.\nIn particular, if you're using\
    \ Julia, you likely want to distribute the <code>julia</code> executable\nitself\
    \ together with its installation bundle.</p>\n<p>For example, assuming that you\
    \ are using the official binaries from the website, instead of\nthe Julia module\
    \ provided by Spack, you can do the following:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Directory for log of\
    \ `llio_transfer` and its wrapper `dir_transfer`</span>\nLOGDIR=<span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${TMPDIR}</span>/log<span\
    \ class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> Create the log directory if necessary</span>\nmkdir -p <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${LOGDIR}</span><span\
    \ class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> Get directory where Julia is placed</span>\nJL_BUNDLE=<span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-s\"><span class=\"pl-pds\"\
    >$(</span>dirname <span class=\"pl-s\"><span class=\"pl-pds\">$(</span>julia --startup-file=no\
    \ -O0 --compile=min -e <span class=\"pl-s\"><span class=\"pl-pds\">'</span>print(Sys.BINDIR)<span\
    \ class=\"pl-pds\">'</span></span><span class=\"pl-pds\">)</span></span><span\
    \ class=\"pl-pds\">)</span></span><span class=\"pl-pds\">\"</span></span>\n\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Move Julia installation to\
    \ fast LLIO directory</span>\n/home/system/tool/dir_transfer -l <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${LOGDIR}</span><span\
    \ class=\"pl-pds\">\"</span></span> <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span><span class=\"pl-smi\">${JL_BUNDLE}</span><span class=\"pl-pds\">\"\
    </span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Do not write\
    \ empty stdout/stderr files for MPI processes.</span>\n<span class=\"pl-k\">export</span>\
    \ PLE_MPI_STD_EMPTYFILE=off\n\nmpiexecjl --project=. -np ... julia ...\n\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> Remove Julia installation directory\
    \ from the cache.</span>\n/home/system/tool/dir_transfer -p -l <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${LOGDIR}</span><span\
    \ class=\"pl-pds\">\"</span></span> <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span><span class=\"pl-smi\">${JL_BUNDLE}</span><span class=\"pl-pds\">\"\
    </span></span></pre></div>\n<h2><a id=\"user-content-reverse-engineering-fujitsu-compiler-using-llvm-output\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#reverse-engineering-fujitsu-compiler-using-llvm-output\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Reverse\
    \ engineering Fujitsu compiler using LLVM output</h2>\n<p>The Fujitsu compiler\
    \ has <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/lang_latest/FujitsuCompiler/C/modeTradAndClangC.html\"\
    \ rel=\"nofollow\">two operation\nmodes</a>:\n\"trad\" (for \"traditional\") and\
    \ \"clang\" (enabled by the flag <code>-Nclang</code>).  In clang mode it's\n\
    based on LLVM (version 7 at the moment).  This means you can get it to emit LLVM\
    \ IR with\n<code>-emit-llvm</code>.  For example, with</p>\n<div class=\"highlight\
    \ highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\"><span class=\"pl-c1\"\
    >echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>int main(){}<span\
    \ class=\"pl-pds\">'</span></span> <span class=\"pl-k\">|</span> fcc -Nclang -x\
    \ c - -S -emit-llvm -o -</span></pre></div>\n<p>you get</p>\n<div class=\"highlight\
    \ highlight-source-llvm\"><pre><span class=\"pl-c\">; ModuleID = '-'</span>\n\
    source_filename = <span class=\"pl-s\">\"-\"</span>\n<span class=\"pl-k\">target</span>\
    \ <span class=\"pl-k\">datalayout</span> = <span class=\"pl-s\">\"e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128\"\
    </span>\n<span class=\"pl-k\">target</span> <span class=\"pl-k\">triple</span>\
    \ = <span class=\"pl-s\">\"aarch64-unknown-linux-gnu\"</span>\n\n<span class=\"\
    pl-c\">; Function Attrs: norecurse nounwind readnone uwtable</span>\n<span class=\"\
    pl-k\">define</span> dso_local <span class=\"pl-k\">i32</span> <span class=\"\
    pl-c1\">@main</span>() <span class=\"pl-k\">local_unnamed_addr</span> #<span class=\"\
    pl-c1\">0</span> <span class=\"pl-v\">!dbg</span> <span class=\"pl-v\">!8</span>\
    \ {\n  <span class=\"pl-k\">ret</span> <span class=\"pl-k\">i32</span> <span class=\"\
    pl-c1\">0</span>, <span class=\"pl-v\">!dbg</span> <span class=\"pl-v\">!11</span>\n\
    }\n\n<span class=\"pl-k\">attributes</span> #<span class=\"pl-c1\">0</span> =\
    \ { <span class=\"pl-k\">norecurse</span> <span class=\"pl-k\">nounwind</span>\
    \ <span class=\"pl-k\">readnone</span> <span class=\"pl-k\">uwtable</span> <span\
    \ class=\"pl-s\">\"correctly-rounded-divide-sqrt-fp-math\"</span>=<span class=\"\
    pl-s\">\"false\"</span> <span class=\"pl-s\">\"disable-tail-calls\"</span>=<span\
    \ class=\"pl-s\">\"false\"</span> <span class=\"pl-s\">\"less-precise-fpmad\"\
    </span>=<span class=\"pl-s\">\"false\"</span> <span class=\"pl-s\">\"no-frame-pointer-elim\"\
    </span>=<span class=\"pl-s\">\"true\"</span> <span class=\"pl-s\">\"no-frame-pointer-elim-non-leaf\"\
    </span> <span class=\"pl-s\">\"no-infs-fp-math\"</span>=<span class=\"pl-s\">\"\
    false\"</span> <span class=\"pl-s\">\"no-jump-tables\"</span>=<span class=\"pl-s\"\
    >\"false\"</span> <span class=\"pl-s\">\"no-nans-fp-math\"</span>=<span class=\"\
    pl-s\">\"false\"</span> <span class=\"pl-s\">\"no-signed-zeros-fp-math\"</span>=<span\
    \ class=\"pl-s\">\"false\"</span> <span class=\"pl-s\">\"no-trapping-math\"</span>=<span\
    \ class=\"pl-s\">\"false\"</span> <span class=\"pl-s\">\"stack-protector-buffer-size\"\
    </span>=<span class=\"pl-s\">\"8\"</span> <span class=\"pl-s\">\"target-cpu\"\
    </span>=<span class=\"pl-s\">\"a64fx\"</span> <span class=\"pl-s\">\"target-features\"\
    </span>=<span class=\"pl-s\">\"+crc,+crypto,+fp-armv8,+lse,+neon,+ras,+rdm,+sve,+v8.2a\"\
    </span> <span class=\"pl-s\">\"unsafe-fp-math\"</span>=<span class=\"pl-s\">\"\
    false\"</span> <span class=\"pl-s\">\"use-soft-float\"</span>=<span class=\"pl-s\"\
    >\"false\"</span> }\n\n<span class=\"pl-v\">!llvm.dbg.cu</span> = !{<span class=\"\
    pl-v\">!0</span>}\n<span class=\"pl-v\">!llvm.module.flags</span> = !{<span class=\"\
    pl-v\">!3</span>, <span class=\"pl-v\">!4</span>, <span class=\"pl-v\">!5</span>}\n\
    <span class=\"pl-v\">!llvm.ident</span> = !{<span class=\"pl-v\">!6</span>}\n\
    <span class=\"pl-v\">!llvm.compinfo</span> = !{<span class=\"pl-v\">!7</span>}\n\
    \n<span class=\"pl-v\">!0</span> = distinct <span class=\"pl-v\">!DICompileUnit</span>(language:\
    \ DW_LANG_C99, file: <span class=\"pl-v\">!1</span>, producer: <span class=\"\
    pl-s\">\"clang: Fujitsu C/C++ Compiler 4.7.0 (Nov  4 2021 10:55:52) (based on\
    \ LLVM 7.1.0)\"</span>, isOptimized: <span class=\"pl-k\">true</span>, runtimeVersion:\
    \ <span class=\"pl-c1\">0</span>, emissionKind: LineTablesOnly, enums: <span class=\"\
    pl-v\">!2</span>)\n<span class=\"pl-v\">!1</span> = <span class=\"pl-v\">!DIFile</span>(filename:\
    \ <span class=\"pl-s\">\"-\"</span>, directory: <span class=\"pl-s\">\"/home/ra000019/a04463\"\
    </span>)\n<span class=\"pl-v\">!2</span> = !{}\n<span class=\"pl-v\">!3</span>\
    \ = !{<span class=\"pl-k\">i32</span> <span class=\"pl-c1\">2</span>, !<span class=\"\
    pl-s\">\"Dwarf Version\"</span>, <span class=\"pl-k\">i32</span> <span class=\"\
    pl-c1\">4</span>}\n<span class=\"pl-v\">!4</span> = !{<span class=\"pl-k\">i32</span>\
    \ <span class=\"pl-c1\">2</span>, !<span class=\"pl-s\">\"Debug Info Version\"\
    </span>, <span class=\"pl-k\">i32</span> <span class=\"pl-c1\">3</span>}\n<span\
    \ class=\"pl-v\">!5</span> = !{<span class=\"pl-k\">i32</span> <span class=\"\
    pl-c1\">1</span>, !<span class=\"pl-s\">\"wchar_size\"</span>, <span class=\"\
    pl-k\">i32</span> <span class=\"pl-c1\">4</span>}\n<span class=\"pl-v\">!6</span>\
    \ = !{!<span class=\"pl-s\">\"clang: Fujitsu C/C++ Compiler 4.7.0 (Nov  4 2021\
    \ 10:55:52) (based on LLVM 7.1.0)\"</span>}\n<span class=\"pl-v\">!7</span> =\
    \ !{!<span class=\"pl-s\">\"C::clang\"</span>}\n<span class=\"pl-v\">!8</span>\
    \ = distinct <span class=\"pl-v\">!DISubprogram</span>(name: <span class=\"pl-s\"\
    >\"main\"</span>, scope: <span class=\"pl-v\">!9</span>, file: <span class=\"\
    pl-v\">!9</span>, line: <span class=\"pl-c1\">1</span>, type: <span class=\"pl-v\"\
    >!10</span>, isLocal: <span class=\"pl-k\">false</span>, isDefinition: <span class=\"\
    pl-k\">true</span>, scopeLine: <span class=\"pl-c1\">1</span>, isOptimized: <span\
    \ class=\"pl-k\">true</span>, unit: <span class=\"pl-v\">!0</span>, retainedNodes:\
    \ <span class=\"pl-v\">!2</span>)\n<span class=\"pl-v\">!9</span> = <span class=\"\
    pl-v\">!DIFile</span>(filename: <span class=\"pl-s\">\"&lt;stdin&gt;\"</span>,\
    \ directory: <span class=\"pl-s\">\"/home/ra000019/a04463\"</span>)\n<span class=\"\
    pl-v\">!10</span> = <span class=\"pl-v\">!DISubroutineType</span>(types: <span\
    \ class=\"pl-v\">!2</span>)\n<span class=\"pl-v\">!11</span> = <span class=\"\
    pl-v\">!DILocation</span>(line: <span class=\"pl-c1\">1</span>, column: <span\
    \ class=\"pl-c1\">12</span>, scope: <span class=\"pl-v\">!8</span>)</pre></div>\n\
    <h2><a id=\"user-content-systembenchmarksjl\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#systembenchmarksjl\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>SystemBenchmarks.jl</h2>\n<p>I ran <a href=\"https://github.com/IanButterworth/SystemBenchmark.jl\"\
    ><code>SystemBenchmarks.jl</code></a> on a\ncompute node.  Here are the results:\n\
    <a href=\"https://github.com/IanButterworth/SystemBenchmark.jl/issues/8#issuecomment-1039775968\"\
    >https://github.com/IanButterworth/SystemBenchmark.jl/issues/8#issuecomment-1039775968</a>.</p>\n\
    <h2><a id=\"user-content-blas\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #blas\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>BLAS</h2>\n\
    <p>OpenBLAS seems to have poor performance:</p>\n<div class=\"highlight highlight-source-julia\"\
    ><pre>julia<span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">using</span>\
    \ LinearAlgebra\n\njulia<span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\"\
    >peakflops</span>()\n<span class=\"pl-c1\">2.589865257047898e10</span></pre></div>\n\
    <p>Up to v1.7, Julia uses OpenBLAS v0.3.17, which actually doesn't support A64FX\
    \ at all, so\nit's probably using the generic kernels.\n<a href=\"https://github.com/xianyi/OpenBLAS/releases/tag/v0.3.19\"\
    ><code>v0.3.19</code></a> and\n<a href=\"https://github.com/xianyi/OpenBLAS/releases/tag/v0.3.20\"\
    ><code>v0.3.20</code></a> improved support for\nthis chip, you can find a build\
    \ of 0.3.20 at\n<a href=\"https://github.com/JuliaBinaryWrappers/OpenBLAS_jll.jl/releases/download/OpenBLAS-v0.3.20%2B0/OpenBLAS.v0.3.20.aarch64-linux-gnu-libgfortran5.tar.gz\"\
    >https://github.com/JuliaBinaryWrappers/OpenBLAS_jll.jl/releases/download/OpenBLAS-v0.3.20%2B0/OpenBLAS.v0.3.20.aarch64-linux-gnu-libgfortran5.tar.gz</a>,\n\
    but sadly there isn't a great performance improvement:</p>\n<div class=\"highlight\
    \ highlight-source-julia\"><pre>julia<span class=\"pl-k\">&gt;</span> BLAS<span\
    \ class=\"pl-k\">.</span><span class=\"pl-c1\">lbt_forward</span>(<span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>lib/libopenblas64_.so<span class=\"pl-pds\"\
    >\"</span></span>)\n<span class=\"pl-c1\">4856</span>\n\njulia<span class=\"pl-k\"\
    >&gt;</span> <span class=\"pl-c1\">peakflops</span>()\n<span class=\"pl-c1\">2.6362952057793587e10</span></pre></div>\n\
    <p>There is an <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/lang_latest/Library/BLASLAPACKScaLAPACKLibrary.html#how-to-dynamically-load-and-use-blas-lapack-and-scalapack\"\
    \ rel=\"nofollow\">optimised\nBLAS</a>\nprovided by Fujitsu, with support for\
    \ SVE (with both LP64 and ILP64).  In order to use it,\ninstall <a href=\"https://github.com/giordano/FujitsuBLAS.jl\"\
    ><code>FujitsuBLAS.jl</code></a></p>\n<div class=\"highlight highlight-source-julia\"\
    ><pre>julia<span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">using</span>\
    \ FujitsuBLAS, LinearAlgebra\n\njulia<span class=\"pl-k\">&gt;</span> BLAS<span\
    \ class=\"pl-k\">.</span><span class=\"pl-c1\">get_config</span>()\nLinearAlgebra<span\
    \ class=\"pl-k\">.</span>BLAS<span class=\"pl-k\">.</span>LBTConfig\nLibraries<span\
    \ class=\"pl-k\">:</span>\n\u2514 [ILP64] libfjlapackexsve_ilp64<span class=\"\
    pl-k\">.</span>so\n\njulia<span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\"\
    >peakflops</span>()\n<span class=\"pl-c1\">4.801227630694119e10</span></pre></div>\n\
    <p>The package <a href=\"https://github.com/carstenbauer/BLISBLAS.jl\"><code>BLISBLAS.jl</code></a>\
    \ similarly forwards\nBLAS calls to the <a href=\"https://github.com/flame/blis\"\
    >blis</a> library, which has optimised kernels\nfor A64FX.</p>\n<h2><a id=\"user-content-building-julia-from-source\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#building-julia-from-source\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building Julia\
    \ from source</h2>\n<h3><a id=\"user-content-with-gcc\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#with-gcc\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>with GCC</h3>\n<p>Building Julia from source with GCC (which is the\
    \ default if you don't set <code>CC</code> and <code>CXX</code>)\nworks fine,\
    \ it's just <em>slow</em>:</p>\n<pre><code>[...]\n    JULIA usr/lib/julia/corecompiler.ji\n\
    Core.Compiler \u2500\u2500\u2500\u2500 903.661 seconds\n[...]\nBase  \u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500271.257337 seconds\n\
    ArgTools  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 50.348227 seconds\n\
    Artifacts  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  1.193792 seconds\n\
    Base64  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  1.057241\
    \ seconds\nCRC32c  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500  0.097865 seconds\nFileWatching  \u2500\u2500\u2500\u2500\u2500  1.169747\
    \ seconds\nLibdl  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500  0.026215 seconds\nLogging  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500  0.411966 seconds\nMmap  \u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500  0.972844 seconds\nNetworkOptions \
    \ \u2500\u2500\u2500  1.159094 seconds\nSHA  \u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  2.067851 seconds\nSerialization\
    \  \u2500\u2500\u2500\u2500  2.942512 seconds\nSockets  \u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500  3.568797 seconds\nUnicode  \u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500  0.814165 seconds\nDelimitedFiles \
    \ \u2500\u2500\u2500  1.121546 seconds\nLinearAlgebra  \u2500\u2500\u2500\u2500\
    109.560774 seconds\nMarkdown  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500  7.977584 seconds\nPrintf  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500  1.635409 seconds\nRandom  \u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500 13.843395 seconds\nTar  \u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  3.146368 seconds\n\
    Dates  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \ 16.694863 seconds\nDistributed  \u2500\u2500\u2500\u2500\u2500\u2500  8.163152\
    \ seconds\nFuture  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500  0.060472 seconds\nInteractiveUtils  \u2500  5.245523 seconds\nLibGit2\
    \  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 15.469061 seconds\n\
    Profile  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  5.399918\
    \ seconds\nSparseArrays  \u2500\u2500\u2500\u2500\u2500 42.660136 seconds\nUUIDs\
    \  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  0.165799\
    \ seconds\nREPL  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500 40.149298 seconds\nSharedArrays  \u2500\u2500\u2500\u2500\u2500 \
    \ 5.476926 seconds\nStatistics  \u2500\u2500\u2500\u2500\u2500\u2500\u2500  2.130843\
    \ seconds\nSuiteSparse  \u2500\u2500\u2500\u2500\u2500\u2500 16.849304 seconds\n\
    TOML  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500  0.714203 seconds\nTest  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500  3.538098 seconds\nLibCURL  \u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500  3.547585 seconds\nDownloads  \u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500  3.657012 seconds\nPkg  \u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 54.053634 seconds\n\
    LazyArtifacts  \u2500\u2500\u2500\u2500  0.019103 seconds\nStdlibs total  \u2500\
    \u2500\u2500\u2500427.178257 seconds\nSysimage built. Summary:\nTotal \u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500 698.447219 seconds\nBase: \u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500 271.257337 seconds 38.8372%\nStdlibs: \u2500\u2500\u2500\u2500\
    \ 427.178257 seconds 61.1611%\n[...]\nPrecompilation complete. Summary:\nTotal\
    \ \u2500\u2500\u2500\u2500\u2500\u2500\u2500 1274.714700 seconds\nGeneration \u2500\
    \u2500 886.445205 seconds 69.5407%\nExecution \u2500\u2500\u2500 388.269495 seconds\
    \ 30.4593%\n</code></pre>\n<h3><a id=\"user-content-with-fujitsu-compiler\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#with-fujitsu-compiler\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>With Fujitsu compiler</h3>\n\
    <p><em>For reference, the version used for the last build I attempted was\n<a\
    \ href=\"https://github.com/JuliaLang/julia/commit/1ad2396f05fa63a71e5842c814791cd7c7715100\"\
    ><code>1ad2396f</code></a></em></p>\n<p>Compiling Julia from source with the Fujitsu\
    \ compiler is complicated.  In particular, it's\nan absolute pain to use the Fujitsu\
    \ compiler in trad mode.  You can have some more luck with\nclang mode.</p>\n\
    <p>Preparation.  Create the <code>Make.user</code> file with this content (I'm\
    \ not sure this file is\nactually necessary when using Clang mode, but it definitely\
    \ is with trad mode):</p>\n<div class=\"highlight highlight-source-makefile\"\
    ><pre><span class=\"pl-k\">override</span> <span class=\"pl-smi\">ARCH</span>\
    \ := aarch64\n<span class=\"pl-k\">override</span> <span class=\"pl-smi\">BUILD_MACHINE</span>\
    \ := aarch64-unknown-linux-gnu</pre></div>\n<p>Then you can compile with (<code>-Nclang</code>\
    \ is to select clang mode)</p>\n<pre><code>make -j50 CC=\"fcc -Nclang\" CFLAGS=\"\
    -Kopenmp\" CXX=\"FCC -Nclang\" CXXFLAGS=\"-Kopenmp\"\n</code></pre>\n<p>The compiler\
    \ in trad mode doesn't define the macro <code>__SIZEOF_POINTER__</code>, so compilation\n\
    would fail in\n<a href=\"https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/support/platform.h#L114-L115\"\
    >https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/support/platform.h#L114-L115</a>.\n\
    The solution is to set the macro <code>-D__SIZEOF_POINTER__=8</code> in the <code>CFLAGS</code>\
    \ (or just not use\ntrad mode).  Then, you may get errors like</p>\n<pre><code>/vol0003/ra000019/a04463/repo/julia/src/jltypes.c:2000:13:\
    \ error: initializer element is not a compile-time constant\n            jl_typename_type,\n\
    \            ^~~~~~~~~~~~~~~~\n./julia_internal.h:437:41: note: expanded from\
    \ macro 'jl_svec'\n                n == sizeof((void *[]){ __VA_ARGS__ })/sizeof(void\
    \ *),        \\\n                                        ^~~~~~~~~~~\n/usr/include/sys/cdefs.h:439:53:\
    \ note: expanded from macro '_Static_assert'\n      [!!sizeof (struct { int __error_if_negative:\
    \ (expr) ? 2 : -1; })]\n                                                    ^~~~\n\
    /vol0003/ra000019/a04463/repo/julia/src/jltypes.c:2025:43: error: initializer\
    \ element is not a compile-time constant\n    jl_typename_type-&gt;types = jl_svec(13,\
    \ jl_symbol_type, jl_any_type /*jl_module_type*/,\n                          \
    \                ^~~~~~~~~~~~~~\n./julia_internal.h:437:41: note: expanded from\
    \ macro 'jl_svec'\n                n == sizeof((void *[]){ __VA_ARGS__ })/sizeof(void\
    \ *),        \\\n                                        ^~~~~~~~~~~\n/usr/include/sys/cdefs.h:439:53:\
    \ note: expanded from macro '_Static_assert'\n      [!!sizeof (struct { int __error_if_negative:\
    \ (expr) ? 2 : -1; })]\n                                                    ^~~~\n\
    </code></pre>\n<p>This is the compiler's fault, which is supposed to be able to\
    \ handle this, but you can just\ndelete the assertions at lines\n<a href=\"https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/julia_internal.h#L427-L429\"\
    >https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/julia_internal.h#L427-L429</a>,\n\
    <a href=\"https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/julia_internal.h#L436-L438\"\
    >https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/julia_internal.h#L436-L438</a>,\n\
    <a href=\"https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/julia_internal.h#L444-L446\"\
    >https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/julia_internal.h#L444-L446</a>.</p>\n\
    <p>If you're lucky enough, with all these changes, you may be able to build <code>usr/bin/julia</code>.\n\
    Unfortunately, last time I tried, run this executable causes a segmentation fault\
    \ in\n<code>dl_init</code>:</p>\n<pre><code>(gdb) run\nStarting program: /vol0003/ra000019/a04463/repo/julia/julia\n\
    Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-151.el8.aarch64\n\
    [Thread debugging using libthread_db enabled]\nUsing host libthread_db library\
    \ \"/lib64/libthread_db.so.1\".\n\nProgram received signal SIGSEGV, Segmentation\
    \ fault.\n0x000040000000def4 in _dl_init () from /lib/ld-linux-aarch64.so.1\n\
    Missing separate debuginfos, use: yum debuginfo-install FJSVxoslibmpg-2.0.0-25.14.1.el8.aarch64\
    \ elfutils-libelf-0.182-3.el8.aarch64\n(gdb) bt\n#0  0x000040000000def4 in _dl_init\
    \ () from /lib/ld-linux-aarch64.so.1\n#1  0x000040000020adb0 in _dl_catch_exception\
    \ () from /lib64/libc.so.6\n#2  0x00004000000125e4 in dl_open_worker () from /lib/ld-linux-aarch64.so.1\n\
    #3  0x000040000020ad54 in _dl_catch_exception () from /lib64/libc.so.6\n#4  0x0000400000011aa8\
    \ in _dl_open () from /lib/ld-linux-aarch64.so.1\n#5  0x0000400000091094 in dlopen_doit\
    \ () from /lib64/libdl.so.2\n#6  0x000040000020ad54 in _dl_catch_exception ()\
    \ from /lib64/libc.so.6\n#7  0x000040000020ae20 in _dl_catch_error () from /lib64/libc.so.6\n\
    #8  0x00004000000917f0 in _dlerror_run () from /lib64/libdl.so.2\n#9  0x0000400000091134\
    \ in dlopen@@GLIBC_2.17 () from /lib64/libdl.so.2\n#10 0x0000400000291f34 in load_library\
    \ (rel_path=0x400001e900c6 &lt;dep_libs+30&gt; \"libjulia-internal.so.1\", src_dir=&lt;optimized\
    \ out&gt;, err=1) at /vol0003/ra000019/a04463/repo/julia/cli/loader_lib.c:65\n\
    #11 0x0000400000291c78 in jl_load_libjulia_internal () at /vol0003/ra000019/a04463/repo/julia/cli/loader_lib.c:200\n\
    #12 0x000040000000de04 in call_init.part () from /lib/ld-linux-aarch64.so.1\n\
    #13 0x000040000000df08 in _dl_init () from /lib/ld-linux-aarch64.so.1\n#14 0x0000400000001044\
    \ in _dl_start_user () from /lib/ld-linux-aarch64.so.1\nBacktrace stopped: previous\
    \ frame identical to this frame (corrupt stack?)\n</code></pre>\n"
  stargazers_count: 6
  subscribers_count: 2
  topics: []
  updated_at: 1670626282.0
haampie/spack-docker-bootstrap:
  data_format: 2
  description: Build optimized docker images for Spack
  filenames:
  - spack.yaml
  full_name: haampie/spack-docker-bootstrap
  latest_release: null
  readme: "<h1><a id=\"user-content-spack-in-docker-with-buildcache\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#spack-in-docker-with-buildcache\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Spack in Docker with buildcache</h1>\n\
    <p>This bootstraps Spack's own, optimized dependencies, as well as the\ncompiler\
    \ toolchain of the distro, so that in the end we just depend\non system libc.</p>\n\
    <p>See <a href=\"spack.yaml\">spack.yaml</a> for things that are built by Spack,\
    \ and\n<a href=\"Makefile\">Makefile</a> and <a href=\"Dockerfile\">Dockerfile</a>\
    \ for how it's built.</p>\n<p>Docker buildkit is required.</p>\n<p>Build with:</p>\n\
    <pre><code>DOCKER_BUILDKIT=1 docker build -f linux-ubuntu22.04-x86_64_v2/Dockerfile\
    \ -t spack-optimized --progress=plain .\n</code></pre>\n<p>Since this uses Python\
    \ 3.11 and clingo with some optimizations, it should\ngenerally be faster:</p>\n\
    <pre><code>Benchmark 1: docker run --rm spack-optimized spack spec hdf5\n  Time\
    \ (mean \xB1 \u03C3):      8.494 s \xB1  0.401 s    [User: 0.015 s, System: 0.008\
    \ s]\n  Range (min \u2026 max):    8.034 s \u2026  8.763 s    3 runs\n\nBenchmark\
    \ 2: docker run --rm spack/ubuntu-focal spec hdf5\n  Time (mean \xB1 \u03C3):\
    \     10.795 s \xB1  0.382 s    [User: 0.013 s, System: 0.009 s]\n  Range (min\
    \ \u2026 max):   10.355 s \u2026 11.030 s    3 runs\n\nSummary\n  'docker run\
    \ --rm spack-optimized spack spec hdf5' ran\n    1.27 \xB1 0.07 times faster than\
    \ 'docker run --rm spack/ubuntu-focal spec hdf5'\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1674134786.0
haampie/spack-pgo-lto-environment:
  data_format: 2
  description: Enable PGO and LTO in Spack software stacks
  filenames:
  - spack.yaml
  full_name: haampie/spack-pgo-lto-environment
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1653473825.0
haampie/spack-prune-specs:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: haampie/spack-prune-specs
  latest_release: null
  readme: '<p>Utilities:</p>

    <ul>

    <li>

    <code>make buildcache-minus-pipelines.filelist</code>: specs in buildcache no
    longer referenced by develop pipelines in the last x days.</li>

    <li>

    <code>make buildcache-intersect-pipelines.filelist</code>: specs both in buildcache
    referenced by develop pipelines in the last x days.</li>

    </ul>

    <p>Set <code>SINCE=yyy-mm-dd</code> to control the window (note that artifacts
    are removed after 30 days, so it can be max one month back).</p>

    <p><code>make</code> installs <code>aws</code>, <code>jq</code> and other utilities
    for you.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1678906710.0
hancheng2000/calcHW:
  data_format: 2
  description: null
  filenames:
  - environments/spack/spack.yaml
  full_name: hancheng2000/calcHW
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1676957739.0
hariharan-devarajan/unifyfs-bug:
  data_format: 2
  description: null
  filenames:
  - dependency/spack.yaml
  full_name: hariharan-devarajan/unifyfs-bug
  latest_release: null
  readme: "<h1><a id=\"user-content-unifyfs-bug\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#unifyfs-bug\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>unifyfs-bug</h1>\n<h2><a id=\"user-content-the-bug-comes-in-multi-node-case-only\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#the-bug-comes-in-multi-node-case-only\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>The bug\
    \ comes in multi-node case only.</h2>\n<h2><a id=\"user-content-instructions\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#instructions\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Instructions</h2>\n<ul>\n<li>\n\
    <p>update path of unifyfs on dependency/spack.yaml packages</p>\n</li>\n<li>\n\
    <p>activate dependency spack folder</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>spack activate -p dependency\nspack install</pre></div>\n</li>\n<li>\n<p>update\
    \ path of unifyfs install on line 3 of CMakeLists</p>\n</li>\n<li>\n<p>build code.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>cmake -DCMAKE_BUILD_TYPE=Debug\
    \ -DCMAKE_C_COMPILER=/usr/tce/packages/gcc/gcc-8.3.1/bin/gcc -DCMAKE_CXX_COMPILER=/usr/tce/packages/gcc/gcc-8.3.1/bin/g++\
    \ -G <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>CodeBlocks - Unix Makefiles<span\
    \ class=\"pl-pds\">\"</span></span> /g/g92/haridev/temp/unifyfs-bug\ncmake --build\
    \ /g/g92/haridev/temp/unifyfs-bug/cmake-build-debug --target all -- -j 128</pre></div>\n\
    </li>\n<li>\n<p>Run Unifyfs server</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> unifyfs-bug\n<span class=\"pl-k\">export</span>\
    \ UNIFYFS_LOG_VERBOSITY=3\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ SET ME</span>\n<span class=\"pl-k\">export</span> UNIFYFS_ROOT_DIR=/usr/workspace/iopp/software/tailorfs/dependency/.spack-env/view\
    \  \n<span class=\"pl-k\">export</span> UNIFYFS_LOG_DIR=<span class=\"pl-smi\"\
    >${HOME}</span>/unifyfs/logs\n<span class=\"pl-k\">export</span> pfs=/p/gpfs1/iopp\n\
    <span class=\"pl-k\">export</span> LD_LIBRARY_PATH=<span class=\"pl-smi\">${PWD}</span>/dependency/.spack-env/view/lib:<span\
    \ class=\"pl-smi\">${UNIFYFS_ROOT_DIR}</span>/lib\n<span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> ACTUAL RUN</span>\nUNIFYFS_LOG_DIR=<span class=\"pl-smi\"\
    >$UNIFYFS_LOG_DIR</span> UNIFYFS_SERVER_CORES=8 <span class=\"pl-smi\">${UNIFYFS_ROOT_DIR}</span>/bin/unifyfs\
    \ start --share-dir=<span class=\"pl-smi\">${pfs}</span>/unifyfs/share-dir -d</pre></div>\n\
    </li>\n<li>\n<p>Run code</p>\n<h2><a id=\"user-content-bug-1\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#bug-1\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Bug 1</h2>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>jsrun -r 1 -a 1 -c 1  -d packed <span class=\"pl-smi\">$PWD</span>/cmake-build-debug/unifyfs-bug\
    \ 1</pre></div>\n<h2><a id=\"user-content-bug-2\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#bug-2\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Bug 2</h2>\n<div class=\"highlight highlight-source-shell\"><pre>jsrun\
    \ -r 1 -a 1 -c 1  -d packed <span class=\"pl-smi\">$PWD</span>/cmake-build-debug/unifyfs-bug\
    \ 2</pre></div>\n</li>\n</ul>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1675706965.0
hepnos/HEPnOS:
  data_format: 2
  description: HEPnOS is a distributed object store for high energy physics applications,
    developed at Argonne National Laboratory.
  filenames:
  - spack.yaml
  full_name: hepnos/HEPnOS
  latest_release: v0.7.1
  readme: '<h1><a id="user-content-hepnos" class="anchor" aria-hidden="true" href="#hepnos"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HEPnOS</h1>

    <p>HEPnOS is the <em>High-Energy Physics''s new Object Store</em>, a distributed
    storage

    system specially designed for HEP experiments and workflows for the FermiLab.

    HEPnOS relies on libraries developed at Argonne National Laboratory within the

    context of the Mochi project (ANL, CMU, LANL, HDF Group).</p>

    <p>For information on copyright and licensing, see the COPYRIGHT file.

    For information on how to use, see the <a href="https://xgitlab.cels.anl.gov/sds/HEPnOS/wikis/home"
    rel="nofollow">wiki</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1641296454.0
hpc/mpifileutils:
  data_format: 2
  description: File utilities designed for scalability and performance.
  filenames:
  - spack.yaml
  full_name: hpc/mpifileutils
  latest_release: v0.11.1
  readme: '<h1><a id="user-content-mpifileutils" class="anchor" aria-hidden="true"
    href="#mpifileutils"><span aria-hidden="true" class="octicon octicon-link"></span></a>mpiFileUtils</h1>

    <p>mpiFileUtils provides both a library called <a href="src/common/README.md">libmfu</a>
    and a suite of MPI-based tools to manage large datasets, which may vary from large
    directory trees to large files. High-performance computing users often generate
    large datasets with parallel applications that run with many processes (millions
    in some cases). However those users are then stuck with single-process tools like
    cp and rm to manage their datasets. This suite provides MPI-based tools to handle
    typical jobs like copy, remove, and compare for such datasets, providing speedups
    of up to 20-30x.  It also provides a library that simplifies the creation of new
    tools or can be used in applications.</p>

    <p>Documentation is available on <a href="http://mpifileutils.readthedocs.io"
    rel="nofollow">ReadTheDocs</a>.</p>

    <h2><a id="user-content-daos-support" class="anchor" aria-hidden="true" href="#daos-support"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>DAOS Support</h2>

    <p>mpiFileUtils supports a DAOS backend for dcp, dsync, and dcmp. Custom serialization
    and deserialization for DAOS containers to and from a POSIX filesystem is provided
    with daos-serialize and daos-deserialize. Details and usage examples are provided
    in <a href="DAOS-Support.md">DAOS Support</a>.</p>

    <h2><a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributors</h2>

    <p>We welcome contributions to the project.  For details on how to help, see our
    <a href="CONTRIBUTING.md">Contributor Guide</a></p>

    <h3><a id="user-content-copyrights" class="anchor" aria-hidden="true" href="#copyrights"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Copyrights</h3>

    <p>Copyright (c) 2013-2015, Lawrence Livermore National Security, LLC.

    Produced at the Lawrence Livermore National Laboratory

    CODE-673838</p>

    <p>Copyright (c) 2006-2007,2011-2015, Los Alamos National Security, LLC.

    (LA-CC-06-077, LA-CC-10-066, LA-CC-14-046)</p>

    <p>Copyright (2013-2015) UT-Battelle, LLC under Contract No.

    DE-AC05-00OR22725 with the Department of Energy.</p>

    <p>Copyright (c) 2015, DataDirect Networks, Inc.</p>

    <p>All rights reserved.</p>

    <h2><a id="user-content-build-status" class="anchor" aria-hidden="true" href="#build-status"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build Status</h2>

    <p>The current status of the mpiFileUtils master branch is <a href="https://travis-ci.org/hpc/mpifileutils"
    rel="nofollow"><img src="https://camo.githubusercontent.com/76717f664d99534173ac7e9fb8e904b0e4bd14fbd51ac6969a88de2e6e86a94f/68747470733a2f2f7472617669732d63692e6f72672f6870632f6d706966696c657574696c732e706e673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/hpc/mpifileutils.png?branch=master"
    style="max-width: 100%;"></a>.</p>

    '
  stargazers_count: 136
  subscribers_count: 26
  topics: []
  updated_at: 1679676912.0
hppritcha/spack_ompix:
  data_format: 2
  description: null
  filenames:
  - gnu_master_x86_64/spack.yaml
  - intel_master_x86_64/spack.yaml
  - gnu_release_x86_64/spack.yaml
  full_name: hppritcha/spack_ompix
  latest_release: null
  readme: '<p>Project for using Gitlab CI to test spack builds of Open MPI master
    and release tarballs.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1640037910.0
iarspider/cms-spack-repo:
  data_format: 2
  description: null
  filenames:
  - environments/CMSSW_12_6_X/spack.yaml
  full_name: iarspider/cms-spack-repo
  latest_release: null
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1638894331.0
icl-utk-edu/hpfft:
  data_format: 2
  description: null
  filenames:
  - .github/CI/spack.yaml
  full_name: icl-utk-edu/hpfft
  latest_release: null
  readme: "<p><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/95208d9a920443b4cae72a2560512aad27c686017c663cb71aa8f434c86f0b08/68747470733a2f2f6269746275636b65742e6f72672f616179616c6133322f6c6f676f732f7261772f646530386466336333626664396435393535383762663834306633316166636234356436303139632f66696265722e706e67\"\
    ><img src=\"https://camo.githubusercontent.com/95208d9a920443b4cae72a2560512aad27c686017c663cb71aa8f434c86f0b08/68747470733a2f2f6269746275636b65742e6f72672f616179616c6133322f6c6f676f732f7261772f646530386466336333626664396435393535383762663834306633316166636234356436303139632f66696265722e706e67\"\
    \ alt=\"FBI_banner\" data-canonical-src=\"https://bitbucket.org/aayala32/logos/raw/de08df3c3bfd9d595587bf840f31afcb45d6019c/fiber.png\"\
    \ style=\"max-width: 100%;\"></a></p>\n<p><strong>FFT Benchmarking Initiative</strong></p>\n\
    <p><strong>Innovative Computing Laboratory</strong></p>\n<p><strong>University\
    \ of Tennessee</strong></p>\n<hr>\n<h1><a id=\"user-content-about\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#about\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>About</h1>\n<p>The FFT Infrastructure Benchmark for\
    \ Exascale Research (FIBER) provides a framework for Fast Fourier Transform (FFT)\
    \ benchmarks targeting exascale computing systems. It evaluates performance and\
    \ scalability of distributed FFTs on different architectures. Furthermore, it\
    \ analyzes the effect on applications that directly depend on FFTs. It can also\
    \ stress and test the overall network of a supercomputer, give an indication on\
    \ bisection bandwidth, noise, and other network and MPI collectives limitations\
    \ that are of interest to many other ECP applications.</p>\n<p>The current harness\
    \ software puts together FFT libraries supporting distributed 3-D complex-to-complex\
    \ and real-to-complex FFTs.</p>\n<hr>\n<h1><a id=\"user-content-publications\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#publications\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Publications</h1>\n<ul>\n<li><a\
    \ href=\"http://www.icl.utk.edu/publications/interim-report-benchmarking-fft-libraries-high-performance-systems\"\
    \ rel=\"nofollow\">Interim Report on Benchmarking FFT Libraries on High Performance\
    \ Systems</a></li>\n<li><a href=\"http://www.icl.utk.edu/publications/fft-benchmark-performance-experiments-systems-targeting-exascale\"\
    \ rel=\"nofollow\">FFT Benchmark Performance Experiments on Systems Targeting\
    \ Exascale</a></li>\n</ul>\n<hr>\n<h1><a id=\"user-content-setting-up\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#setting-up\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Setting up</h1>\n<p>Create a folder;\
    \ e.g., <code>Benchmarks_FFT</code>, and install the FFT libraries to benchmark;\
    \ or load them as modules.</p>\n<pre><code>-- Benchmarks_FFT\n        |-- heFFTe\n\
    \        |-- fftMPI\n        |-- AccFFT\n        |-- P3DFFT\n        |-- FFTE\n\
    \        |-- SWFFT\n        |-- 2DECOMP&amp;FFT\n        |-- nb3dFFT\n       \
    \ |-- FFTW\n        |-- FFTW++\n</code></pre>\n<p>Current libraries targeted by\
    \ FIBER:</p>\n<ul>\n<li>\n<p>CPU support: <a href=\"https://lammps.github.io/fftmpi/\"\
    \ rel=\"nofollow\">fftMPI</a>, <a href=\"https://xgitlab.cels.anl.gov/hacc/SWFFT\"\
    \ rel=\"nofollow\">SWFFT</a>,\n<a href=\"https://github.com/sdsc/p3dfft.3\">P3DFFT</a>,\n\
    <a href=\"https://gitlab.jsc.fz-juelich.de/goebbert/nb3dfft\" rel=\"nofollow\"\
    >nb3dFFT</a>,\n<a href=\"http://www.2decomp.org/download.html\" rel=\"nofollow\"\
    >2DECOMP&amp;FFT</a>, <a href=\"http://www.fftw.org/\" rel=\"nofollow\">FFTW</a>,\
    \ <a href=\"fftwpp.sourceforge.net/\">FFTW++</a></p>\n</li>\n<li>\n<p>CPU-GPU\
    \ support: <a href=\"https://bitbucket.org/icl/heffte\" rel=\"nofollow\">heFFTe</a>,\
    \ <a href=\"https://github.com/amirgholami/accfft\">AccFFT</a>,   <a href=\"http://www.ffte.jp/\"\
    \ rel=\"nofollow\">FFTE</a></p>\n</li>\n</ul>\n<h1><a id=\"user-content-compilation\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#compilation\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Compilation</h1>\n<p>Next clone\
    \ this repository and create  build folder, and execute the <code>cmake</code>\
    \ commands.\nIn the following example, we install FIBER with heFFTe and fftMPI\
    \ backends:</p>\n<pre><code>mkdir build; cd $_\nbuild/\ncmake -DFIBER_FFT_LIB_DIRS=\"\
    /home/Benchmarks_FFT/fftmpi/src;/home/heffte/build/lib\"\n-DFIBER_FFT_INCLUDE_DIRS=\"\
    /home/Benchmarks_FFT/fftmpi/src;/home/heffte/build/include\"\n-DFIBER_ENABLE_HEFFTE=ON\
    \ -DFIBER_ENABLE_FFTMPI=ON\n-DMPI_DIR=/sw/openmpi/4.0.0/ .. \nmake -j\n</code></pre>\n\
    <p>List the <code>lib</code> and <code>include</code> folders of libraries to\
    \ test, respectively, in <code>FIBER_FFT_LIB_DIRS</code> and <code>FIBER_FFT_INCLUDE_DIRS</code>.</p>\n\
    <h1><a id=\"user-content-testing-integration\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#testing-integration\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Testing integration</h1>\n<p>Run tests as follows:</p>\n\
    <pre><code>cd build/benchmarks\nmpirun -n 2 ./test3D_CPU_C2C &lt;library&gt;\n\
    mpirun -n 2 ./test3D_CPU_R2C &lt;library&gt;\n</code></pre>\n<p>If FIBER was build\
    \ linked to GPU enabled libraries:</p>\n<pre><code>cd build/benchmarks\nmpirun\
    \ -n 2 ./test3D_GPU_C2C &lt;gpu_library&gt;\nmpirun -n 2 ./test3D_GPU_R2C &lt;gpu_library&gt;\n\
    </code></pre>\n<h1><a id=\"user-content-running-benchmarks\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#running-benchmarks\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Running benchmarks</h1>\n<pre><code>cd\
    \ build/benchmarks\nmpirun -n $NUM_RANKS ./test3D_C2C -lib &lt;library&gt; -backend\
    \ &lt;1D_backend&gt; -size &lt;nx&gt; &lt;ny&gt; &lt;nz&gt; -pgrid &lt;p&gt; &lt;q&gt;\n\
    </code></pre>\n<p>where <code>library</code> has to be replaced by one of the\
    \ nine available libraries, provided user has it installed.\nOnce a parallel FFT\
    \ library has been correctly integrated to heFFTe, running these benchmarks should\
    \ report a correct validation output.</p>\n<h1><a id=\"user-content-documentation\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#documentation\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h1>\n<ul>\n<li>Installation\
    \ and a Doxygen documentation will be available shortly.</li>\n</ul>\n<hr>\n<h1><a\
    \ id=\"user-content-getting-help\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #getting-help\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Getting\
    \ Help</h1>\n<p>For assistance with the FIBER project, email <em><a href=\"mailto:fiber@icl.utk.edu\"\
    >fiber@icl.utk.edu</a></em> or start a GitHub issue.</p>\n<p>Contributions are\
    \ very welcome, please create a pull request.</p>\n<h1><a id=\"user-content-resources\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#resources\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Resources</h1>\n<ul>\n<li>Visit\
    \ the <a href=\"http://icl.utk.edu/fiber/\" rel=\"nofollow\">FIBER website</a>\
    \ for more information about the HeFFTe project.</li>\n<li>Visit the <a href=\"\
    https://exascaleproject.org\" rel=\"nofollow\">ECP website</a> to find out more\
    \ about the DOE Exascale Computing Initiative.</li>\n</ul>\n<hr>\n<h1><a id=\"\
    user-content-acknowledgments\" class=\"anchor\" aria-hidden=\"true\" href=\"#acknowledgments\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Acknowledgments</h1>\n\
    <p>This research was supported by the United States Exascale Computing Project.</p>\n\
    <hr>\n<h1><a id=\"user-content-license\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#license\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>License</h1>\n<pre><code>Copyright (c) 2022, University of Tennessee\n\
    All rights reserved.\n\nRedistribution and use in source and binary forms, with\
    \ or without\nmodification, are permitted provided that the following conditions\
    \ are met:\n    * Redistributions of source code must retain the above copyright\n\
    \      notice, this list of conditions and the following disclaimer.\n    * Redistributions\
    \ in binary form must reproduce the above copyright\n      notice, this list of\
    \ conditions and the following disclaimer in the\n      documentation and/or other\
    \ materials provided with the distribution.\n    * Neither the name of the University\
    \ of Tennessee nor the\n      names of its contributors may be used to endorse\
    \ or promote products\n      derived from this software without specific prior\
    \ written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND\
    \ CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT\
    \ NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\
    \ PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL UNIVERSITY OF TENNESSEE\
    \ BE LIABLE FOR ANY\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\
    \ DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\
    \ SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\
    \ CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\
    \ OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\
    \ OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n</code></pre>\n"
  stargazers_count: 3
  subscribers_count: 2
  topics: []
  updated_at: 1677703576.0
j-woz/SV-CP-2022-11-23:
  data_format: 2
  description: null
  filenames:
  - spack/spack.yaml
  full_name: j-woz/SV-CP-2022-11-23
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1669233200.0
jaykalinani/AsterX:
  data_format: 2
  description: AsterX is a GPU-accelerated GRMHD code for dynamical spacetimes
  filenames:
  - Docs/compile-notes/frontera-bitbucket/GPU/spack.yaml
  - Docs/compile-notes/frontera-github/CPU/spack.yaml
  - Docs/compile-notes/frontera-github/GPU/spack.yaml
  - Docs/compile-notes/frontera-bitbucket/CPU/spack.yaml
  full_name: jaykalinani/AsterX
  latest_release: null
  readme: '<p><a target="_blank" rel="noopener noreferrer" href="Docs/figures/asterx.png"><img
    align="top" src="Docs/figures/asterx.png" width="140" style="max-width: 100%;"></a></p>

    <p><strong>AsterX</strong> is a GPU-accelerated GRMHD code for dynamical spacetimes,
    written in C++. It is built upon the <a href="https://github.com/eschnett/CarpetX">CarpetX</a>
    driver, which is intended for the <a href="https://einsteintoolkit.org/" rel="nofollow">Einstein
    Toolkit</a>. <strong>CarpetX</strong> is based on <a href="https://amrex-codes.github.io"
    rel="nofollow">AMReX</a>, a software framework for block-structured AMR (adaptive
    mesh refinement).</p>

    <p>Full documentation will soon be available at <a href="https://asterx.readthedocs.io/en/latest/#"
    rel="nofollow">asterx.readthedocs.io</a>.</p>

    <ul>

    <li>

    <a href="https://github.com/jaykalinani/AsterX/actions"><img src="https://github.com/jaykalinani/AsterX/workflows/CI/badge.svg"
    alt="GitHub CI" style="max-width: 100%;"></a>  <a href="https://asterx.readthedocs.io/en/latest/?badge=latest"
    rel="nofollow"><img src="https://camo.githubusercontent.com/8257fa34c1c5b6c660b31bf16a6196859c354c9c503b7742e1cdee871fbb96c8/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6173746572782f62616467652f3f76657273696f6e3d6c6174657374"
    alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/asterx/badge/?version=latest"
    style="max-width: 100%;"></a> <a href="https://github.com/jaykalinani/AsterX/blob/main/LICENSE.md"><img
    src="https://camo.githubusercontent.com/b768fc44ae95216e4b53ff734978771466ba222596e760da27e9e60a0d47d6f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4c47504c5f76332d626c75652e737667"
    alt="License: LGPL v3" data-canonical-src="https://img.shields.io/badge/License-LGPL_v3-blue.svg"
    style="max-width: 100%;"></a>

    </li>

    </ul>

    <h2><a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h2>

    <ul>

    <li>Heavily derived from the GRMHD code <a href="https://zenodo.org/record/4350072"
    rel="nofollow">Spritz</a>.</li>

    <li>Solves the GRMHD equations in 3D Cartesian coordinates and on dynamical spacetimes
    using high-resolution shock capturing (HRSC) schemes.</li>

    <li>Based on the flux-conservative Valencia formulation.</li>

    <li>Directly evolves the staggered vector potential.</li>

    </ul>

    <h2><a id="user-content-available-modules" class="anchor" aria-hidden="true" href="#available-modules"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Available modules</h2>

    <ul>

    <li>

    <code>AsterX</code> - the core GRMHD module</li>

    <li>

    <code>AsterSeeds</code> - initial data module</li>

    <li>

    <code>Con2PrimFactory</code> - module providing different conservative-to-primitive
    variable recovery routines</li>

    <li>

    <code>EOSX</code> - equation of state driver</li>

    <li>

    <code>TOVSolver</code> - a modified version of the publicly available TOVSolver
    thorn used within the Einstein Toolkit</li>

    </ul>

    <h2><a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Getting started</h2>

    <p>Instructions for downloading and building the Einstein Toolkit including

    CarpetX can be found <a href="https://github.com/eschnett/CarpetX">here</a>.</p>

    <p>Details for building and running AsterX along with CarpetX will be added to
    <a href="https://asterx.readthedocs.io/en/latest/#" rel="nofollow">asterx.readthedocs.io</a>
    soon..</p>

    '
  stargazers_count: 8
  subscribers_count: 8
  topics: []
  updated_at: 1679282900.0
jaykalinani/AsterX-Docs:
  data_format: 2
  description: 'AsterX: a new open-source GPU-accelerated GRMHD code for dynamical
    spacetimes'
  filenames:
  - compile-notes/frontera-bitbucket/CPU/spack.yaml
  - compile-notes/frontera-bitbucket/GPU/spack.yaml
  - compile-notes/frontera-github/GPU/spack.yaml
  - compile-notes/frontera-github/CPU/spack.yaml
  full_name: jaykalinani/AsterX-Docs
  latest_release: null
  readme: '<h1><a id="user-content-asterx" class="anchor" aria-hidden="true" href="#asterx"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>AsterX</h1>

    <p>AsterX: a new open-source GPU-accelerated GRMHD code for dynamical spacetimes</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1677876939.0
jeffersonscientific/cees_spack_configs:
  data_format: 2
  description: CEES spack configurations (take 3). Focus on environments only (or
    mostly), and modular configs.
  filenames:
  - spack_env_files/cees_x86_64-beta_spack.yaml
  - spack_env_files/cees_skylake-beta_spack.yaml
  - spack_env_files/dev_cees-x86-intel_spack.yaml
  - spack_env_files/dev_py-bottleneck_spack.yaml
  - spack_env_files/cees_zen2-beta_spack.yaml
  - spack_env_files/cees_kimlab_spack.yaml
  - dev_configs/paraview_spack.yaml
  - spack_env_files/dev_paraview_spack.yaml
  - spack_env_files/cees-x86-oneapi_spack.yaml
  - spack_env_files/pflotran_ompi_sif_spack.yaml
  - spack_env_files/pflotran_sif_spack.yaml
  - configs/spack_petsc_mod.yaml
  - spack_env_files/paraview_spack.yaml
  - spack_env_files/cees_seissol_spack.yaml
  - spack_env_files/pflotran_spack.yaml
  - spack_env_files/dev_cees-x86-oneapi_spack.yaml
  - spack_env_files/py-bottleneck_spack.yaml
  - dev_configs/pflotran_spack.yaml
  - spack_env_files/cees-x86-intel_spack.yaml
  - spack_env_files/cees_compilers_spack.yaml
  full_name: jeffersonscientific/cees_spack_configs
  latest_release: null
  readme: "<h1><a id=\"user-content-cees_spack_configs\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#cees_spack_configs\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>cees_spack_configs</h1>\n<p>CEES spack configurations\
    \ (take 3). Focus on environments only (or mostly), and modular configs.</p>\n\
    <p>This constitutes a continued effort to find a way to Git-Support and modularize\
    \ Spack setups. While knowledge is much improved, success\nis arguably limited\
    \ -- alas. The idea is to be able to easily maintain a list of SW that is then\
    \ compiled over a suite of compiler, mpi,\narchitecture types.</p>\n<p>A few points:</p>\n\
    <ul>\n<li>Using environments is key.</li>\n<li>Using an <code>include:</code>\
    \ section can help. For example,</li>\n</ul>\n<pre><code>  include:\n  - $spack/../configs/packages_petsc.yaml\n\
    \  - $spack/../configs/compilers_sherlock_O2.yaml\n</code></pre>\n<p>might be\
    \ useful to build <code>petsc</code> environments for multiple architectures or\
    \ compilers. Unfortunatly -- at least at this time, not all sections\ncan be satisfied\
    \ as <code>include</code> files.</p>\n<ul>\n<li>Compilers remain a challenge...</li>\n\
    <li>If <code>providers</code> are specified, optimal (and functional) choices\
    \ will likely vary for different compilers.</li>\n</ul>\n<p>CONVENTIONS:</p>\n\
    <ul>\n<li>Configuration components indicated with <code>_inc</code> in name, eg\
    \ <code>packages_inc.yaml</code>. These files should stand alone for non-environment\
    \ builds\n(not recommended...) or can be included in an <code>include:</code>\
    \ clause of an environment.</li>\n<li>Environment files may be tagged with <code>_mod</code>\
    \ in the name, to indicate a \"modular\" envorinment, that uses an <code>include:</code>\
    \ section.</li>\n</ul>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1641864561.0
jeffersonscientific/seissol_compile:
  data_format: 2
  description: Compile (and similar) script for SeisSol
  filenames:
  - ss_spack_env_template.yaml
  full_name: jeffersonscientific/seissol_compile
  latest_release: null
  readme: '<h1><a id="user-content-seissol_compile" class="anchor" aria-hidden="true"
    href="#seissol_compile"><span aria-hidden="true" class="octicon octicon-link"></span></a>seissol_compile</h1>

    <p>Compile (and similar) script for SeisSol</p>

    <h1></h1>

    <p>To date, these scripts can be used to install SeisSol on Stanford Research
    Computing''s Sherlock HPC. The <code>compile_seissol_spack.sh</code> script primarily
    uses a <code>spack</code> built environment, and so can be adapted to another
    HPC relatively easily.</p>

    <p>The <code>compile_seissol_sherlock.sh</code> script might be refrenced as a
    template -- the idea being to use pre-built SW modules to build SeisSol, but it
    ultimately crashes and burns prety spetacularly. One issue is that the various
    components may have differend dependencies. Namely, some packages are built from
    a <code>gcc/10.1.0</code> toolchain and another from <code>gcc/12.1.0</code>.</p>

    <p>Files:</p>

    <ul>

    <li>

    <code>build_spack_env.sh</code>: a generic batchable bash script to build a spack
    environment.</li>

    <li>

    <code>ss_env.yaml</code>: Should be the einvironment file we use to define the
    <code>seissol</code> spack environment. Note that the environment includes some
    external package definitions and Sherlock''s built in <code>gcc</code> compilers,
    including the primary <code>gcc@12.1.0</code>. These will need to be modified
    to deploy on a different HPC. Compilers can be built natively in Spack, then automagically
    discovered and added, but ultimately it will still likely be neessary to modify
    their definition in the environment file.</li>

    <li>

    <code>compile_seissol_spack.sh</code>: Working (on Sherlock HPC) compile script.
    will build all the non-Spack components</li>

    <li>

    <code>compile_seissol_cees_sherlock</code>: An older compile script that attempts
    to use Sherlock''s standard SW to compile. It ultimately crashes and burns, but
    might be referenced as a template.</li>

    <li>

    <code>install_ss_spack.sh</code>: An early template to build the Spack environment
    from scratch, including building, and <code>find</code>ing compilers in Spack.</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1666893798.0
jerrygreenberg2/sdsc:
  data_format: 2
  description: null
  filenames:
  - share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/radiuss/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/tutorial/spack.yaml
  full_name: jerrygreenberg2/sdsc
  latest_release: null
  readme: '<h1><a id="user-content-sdsc-hpc-software-deployment-guide" class="anchor"
    aria-hidden="true" href="#sdsc-hpc-software-deployment-guide"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>SDSC HPC Software Deployment Guide</h1>

    <p>This document outlines the Spack-based software deployment process in

    use by the San Diego Supercomputer Center''s (SDSC) High-Performance

    Computing (HPC) User Services Group. All definitions, procedures,

    conventions, and policies defined within this guide are used by the

    group to build and maintain the custom Spack instances they deploy on

    SDSC''s HPC systems for end users.</p>

    <h2><a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Disclaimer</h2>

    <p>This is a new and evolving software deployment process in development

    and use by the SDSC HPC User Services Group to centrally manage HPC

    software on HPC systems with Spack. Please consider the status of the

    project as a pre-alpha release at this time. Use at your own risk.</p>

    <h2><a id="user-content-definitions-and-terminology" class="anchor" aria-hidden="true"
    href="#definitions-and-terminology"><span aria-hidden="true" class="octicon octicon-link"></span></a>Definitions
    and Terminology</h2>

    <ul>

    <li>A Spack <em><strong>instance</strong></em> is a unique, stand-alone installation
    of a

    specific version of <code>spack</code> that includes custom Spack configuration

    files, Spack packages, and a collection of Spack-installed software

    applications, libraries, and utilities.</li>

    <li>A Spack <em><strong>package</strong></em> is a set of instructions that defines
    how a

    specific piece of software is compiled and/or installed by Spack. For

    example, a Spack package specifies where to find and how to retrieve

    the software''s source code, its required (and/or optional) software

    dependencies, its compile-time options, any patches to apply, etc. A

    Spack package is primarily defined by it <em>package.py</em> file.</li>

    <li>A Spack <em><strong>spec</strong></em> is a string descriptor that specifies
    a particular

    build configuration of a Spack package. The full syntax of a spec

    may include the package name, its version, the compiler it should be

    built with, the compiler version, the system architecture it should be

    compiled for, any compile-time options for the package, and any

    requirements that should be enforced on its dependencies at build time.</li>

    <li>The <em><strong>core</strong></em> packages of a Spack instance are those
    software

    applications, libraries, and/or utilities compiled with Spack using

    the default system compiler. These packages form the foundation of the

    software environment upon which additional Spack packages are built.

    In general, the core packages of a Spack instance are a set of (core)

    compilers and other general software utilities. e.g., version control

    systems, data transfer tools, etc.</li>

    <li>A Spack package <em><strong>dependency chain</strong></em> is an explicitly-defined<br>

    ordered set of Spack specs that share a common (core) compiler and/or

    MPI library, may depend on one another (or share other software

    dependencies), and should be installed one after another, one at a

    time, as prescribed by their dependencies.</li>

    <li>A Spack <em><strong>deployment branch</strong></em> is a <em>trunk</em>-like
    branch for a specific

    version of <code>spack</code> that tracks all of the Spack configuration files,

    Spack packages, and Spack specs used to deploy a Spack instance (or a

    set of instances).</li>

    </ul>

    <h2><a id="user-content-github-repository" class="anchor" aria-hidden="true" href="#github-repository"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>GitHub Repository</h2>

    <p>The <a href="https://github.com/sdsc/spack">sdsc/spack</a> project is a custom
    fork

    of the Spack project''s main GitHub repository, which is referred to in

    this guide as the <a href="https://github.com/spack/spack">spack/spack</a> repo.

    The primary aim of the sdsc/spack repo is to manage and track all

    changes made to the custom Spack instances deployed by SDSC on its HPC

    systems.</p>

    <h3><a id="user-content-deployment-branches" class="anchor" aria-hidden="true"
    href="#deployment-branches"><span aria-hidden="true" class="octicon octicon-link"></span></a>Deployment
    Branches</h3>

    <p>The sdsc/spack repo and its use in practice are fundamentally structured

    around the concept of <em>deployment branches</em>. A deployment branch is a

    <em>trunk</em>-like branch created from an unmodifed, official release version

    of <code>spack</code> and is named accordingly, unless special circumstances

    require that an intermediate commit be used. For example, the

    <code>sdsc-0.17.3</code> deployment branch was created by checking out the

    <a href="https://github.com/spack/spack/releases/tag/v0.17.3">v0.17.3</a> release</p>

    <p>Once a version of Spack is selected and checked out, only a few minor

    changes and/or additions are made to the Spack release in order to

    initialize a deployment branch within the sdsc/spack repo. These

    modifications are as follows:</p>

    <ul>

    <li>The official version of the Spack <code>README.md</code> file is removed and

    replaced with the latest version of this document --- the <em>SDSC HPC

    Software Deployment Guide</em>.</li>

    <li>The latest version of the sdsc/spack <code>CONTRIBUTING.md</code> file is
    also

    included to provide information on how one may contribute to the

    sdsc/spack project and its deployment branches.</li>

    <li>A Spack package repository --- <code>var/spack/repos/sdsc</code> --- created
    to

    store all custom Spack packages created and/or maintained by SDSC,

    including all of SDSC''s custom modifications to Spack''s existing

    <code>builtin</code> packages.</li>

    <li>A Spack instance repository --- <code>etc/spack/sdsc</code> --- is created

    to  ...</li>

    </ul>

    <p>All other types of branches (see

    <a href="CONTRIBUTING.md">CONTRIBUTING.md</a>) should start from a deployment

    branch.</p>

    <h3><a id="user-content-package-repository" class="anchor" aria-hidden="true"
    href="#package-repository"><span aria-hidden="true" class="octicon octicon-link"></span></a>Package
    Repository</h3>

    <h3><a id="user-content-instance-repositories" class="anchor" aria-hidden="true"
    href="#instance-repositories"><span aria-hidden="true" class="octicon octicon-link"></span></a>Instance
    Repositories</h3>

    <h3><a id="user-content-access-control-and-permissions" class="anchor" aria-hidden="true"
    href="#access-control-and-permissions"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Access Control and Permissions</h3>

    <p>etc/spack/repos.yaml

    var/spack/repos/sdsc/repo.yaml

    var/spack/repos/sdsc/packages</p>

    <p>etc/spack/sdsc/expanse/0.17.3/cpu/specs

    etc/spack/sdsc/expanse/0.17.3/cpu/yamls</p>

    <h2><a id="user-content-deploying-hpc-software-via-spack" class="anchor" aria-hidden="true"
    href="#deploying-hpc-software-via-spack"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Deploying HPC Software via Spack</h2>

    <h3><a id="user-content-deploying-a-spack-instance" class="anchor" aria-hidden="true"
    href="#deploying-a-spack-instance"><span aria-hidden="true" class="octicon octicon-link"></span></a>Deploying
    a Spack Instance</h3>

    <ul>

    <li>start from existing deployment branch</li>

    </ul>

    <h3><a id="user-content-managing-changes-to-a-spack-instance" class="anchor" aria-hidden="true"
    href="#managing-changes-to-a-spack-instance"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Managing Changes to a Spack Instance</h3>

    <ul>

    <li>deploy change to configuration file</li>

    <li>add new package to sdsc package repository</li>

    <li>spack install a new spec</li>

    </ul>

    <h3><a id="user-content-setting-up-a-shared-instance-configuration" class="anchor"
    aria-hidden="true" href="#setting-up-a-shared-instance-configuration"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Setting up a Shared Instance Configuration</h3>

    <h3><a id="user-content-using-a-spack-mirror-and-build-caches-for-instance-backups"
    class="anchor" aria-hidden="true" href="#using-a-spack-mirror-and-build-caches-for-instance-backups"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Using a Spack Mirror
    and Build Caches for Instance Backup(s)</h3>

    <h2><a id="user-content-additional-notes" class="anchor" aria-hidden="true" href="#additional-notes"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Additional Notes</h2>

    <h3><a id="user-content-protecting-the-sdscspack-develop-branch" class="anchor"
    aria-hidden="true" href="#protecting-the-sdscspack-develop-branch"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Protecting the sdsc/spack <code>develop</code>
    branch</h3>

    <h3><a id="user-content-fetching-changes-and-re-syncing-the-develop-branch" class="anchor"
    aria-hidden="true" href="#fetching-changes-and-re-syncing-the-develop-branch"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Fetching changes and
    re-syncing the <code>develop</code> branch</h3>

    <h3><a id="user-content-creating-a-new-deployment-branch" class="anchor" aria-hidden="true"
    href="#creating-a-new-deployment-branch"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Creating a new deployment branch</h3>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1678670738.0
jerrypgreenberg/sdsc-tscc:
  data_format: 2
  description: null
  filenames:
  - share/spack/gitlab/cloud_pipelines/stacks/tutorial/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/build_systems/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/radiuss/spack.yaml
  full_name: jerrypgreenberg/sdsc-tscc
  latest_release: null
  readme: '<h1><a id="user-content-sdsc-hpc-software-deployment-guide" class="anchor"
    aria-hidden="true" href="#sdsc-hpc-software-deployment-guide"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>SDSC HPC Software Deployment Guide</h1>

    <p>This document outlines the Spack-based software deployment process in

    use by the San Diego Supercomputer Center''s (SDSC) High-Performance

    Computing (HPC) User Services Group. All definitions, procedures,

    conventions, and policies defined within this guide are used by the

    group to build and maintain the custom Spack instances they deploy on

    SDSC''s HPC systems for end users.</p>

    <h2><a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Disclaimer</h2>

    <p>This is a new and evolving software deployment process in development

    and use by the SDSC HPC User Services Group to centrally manage HPC

    software on HPC systems with Spack. Please consider the status of the

    project as a pre-alpha release at this time. Use at your own risk.</p>

    <h2><a id="user-content-definitions-and-terminology" class="anchor" aria-hidden="true"
    href="#definitions-and-terminology"><span aria-hidden="true" class="octicon octicon-link"></span></a>Definitions
    and Terminology</h2>

    <ul>

    <li>A Spack <em><strong>instance</strong></em> is a unique, stand-alone installation
    of a

    specific version of <code>spack</code> that includes custom Spack configuration

    files, Spack packages, and a collection of Spack-installed software

    applications, libraries, and utilities.</li>

    <li>A Spack <em><strong>package</strong></em> is a set of instructions that defines
    how a

    specific piece of software is compiled and/or installed by Spack. For

    example, a Spack package specifies where to find and how to retrieve

    the software''s source code, its required (and/or optional) software

    dependencies, its compile-time options, any patches to apply, etc. A

    Spack package is primarily defined by it <em>package.py</em> file.</li>

    <li>A Spack <em><strong>spec</strong></em> is a string descriptor that specifies
    a particular

    build configuration of a Spack package. The full syntax of a spec

    may include the package name, its version, the compiler it should be

    built with, the compiler version, the system architecture it should be

    compiled for, any compile-time options for the package, and any

    requirements that should be enforced on its dependencies at build time.</li>

    <li>The <em><strong>core</strong></em> packages of a Spack instance are those
    software

    applications, libraries, and/or utilities compiled with Spack using

    the default system compiler. These packages form the foundation of the

    software environment upon which additional Spack packages are built.

    In general, the core packages of a Spack instance are a set of (core)

    compilers and other general software utilities. e.g., version control

    systems, data transfer tools, etc.</li>

    <li>A Spack package <em><strong>dependency chain</strong></em> is an explicitly-defined<br>

    ordered set of Spack specs that share a common (core) compiler and/or

    MPI library, may depend on one another (or share other software

    dependencies), and should be installed one after another, one at a

    time, as prescribed by their dependencies.</li>

    <li>A Spack <em><strong>deployment branch</strong></em> is a <em>trunk</em>-like
    branch for a specific

    version of <code>spack</code> that tracks all of the Spack configuration files,

    Spack packages, and Spack specs used to deploy a Spack instance (or a

    set of instances).</li>

    </ul>

    <h2><a id="user-content-github-repository" class="anchor" aria-hidden="true" href="#github-repository"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>GitHub Repository</h2>

    <p>The <a href="https://github.com/sdsc/spack">sdsc/spack</a> project is a custom
    fork

    of the Spack project''s main GitHub repository, which is referred to in

    this guide as the <a href="https://github.com/spack/spack">spack/spack</a> repo.

    The primary aim of the sdsc/spack repo is to manage and track all

    changes made to the custom Spack instances deployed by SDSC on its HPC

    systems.</p>

    <h3><a id="user-content-deployment-branches" class="anchor" aria-hidden="true"
    href="#deployment-branches"><span aria-hidden="true" class="octicon octicon-link"></span></a>Deployment
    Branches</h3>

    <p>The sdsc/spack repo and its use in practice are fundamentally structured

    around the concept of <em>deployment branches</em>. A deployment branch is a

    <em>trunk</em>-like branch created from an unmodifed, official release version

    of <code>spack</code> and is named accordingly, unless special circumstances

    require that an intermediate commit be used. For example, the

    <code>sdsc-0.17.3</code> deployment branch was created by checking out the

    <a href="https://github.com/spack/spack/releases/tag/v0.17.3">v0.17.3</a> release</p>

    <p>Once a version of Spack is selected and checked out, only a few minor

    changes and/or additions are made to the Spack release in order to

    initialize a deployment branch within the sdsc/spack repo. These

    modifications are as follows:</p>

    <ul>

    <li>The official version of the Spack <code>README.md</code> file is removed and

    replaced with the latest version of this document --- the <em>SDSC HPC

    Software Deployment Guide</em>.</li>

    <li>The latest version of the sdsc/spack <code>CONTRIBUTING.md</code> file is
    also

    included to provide information on how one may contribute to the

    sdsc/spack project and its deployment branches.</li>

    <li>A Spack package repository --- <code>var/spack/repos/sdsc</code> --- created
    to

    store all custom Spack packages created and/or maintained by SDSC,

    including all of SDSC''s custom modifications to Spack''s existing

    <code>builtin</code> packages.</li>

    <li>A Spack instance repository --- <code>etc/spack/sdsc</code> --- is created

    to  ...</li>

    </ul>

    <p>All other types of branches (see

    <a href="CONTRIBUTING.md">CONTRIBUTING.md</a>) should start from a deployment

    branch.</p>

    <h3><a id="user-content-package-repository" class="anchor" aria-hidden="true"
    href="#package-repository"><span aria-hidden="true" class="octicon octicon-link"></span></a>Package
    Repository</h3>

    <h3><a id="user-content-instance-repositories" class="anchor" aria-hidden="true"
    href="#instance-repositories"><span aria-hidden="true" class="octicon octicon-link"></span></a>Instance
    Repositories</h3>

    <h3><a id="user-content-access-control-and-permissions" class="anchor" aria-hidden="true"
    href="#access-control-and-permissions"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Access Control and Permissions</h3>

    <p>etc/spack/repos.yaml

    var/spack/repos/sdsc/repo.yaml

    var/spack/repos/sdsc/packages</p>

    <p>etc/spack/sdsc/expanse/0.17.3/cpu/specs

    etc/spack/sdsc/expanse/0.17.3/cpu/yamls</p>

    <h2><a id="user-content-deploying-hpc-software-via-spack" class="anchor" aria-hidden="true"
    href="#deploying-hpc-software-via-spack"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Deploying HPC Software via Spack</h2>

    <h3><a id="user-content-deploying-a-spack-instance" class="anchor" aria-hidden="true"
    href="#deploying-a-spack-instance"><span aria-hidden="true" class="octicon octicon-link"></span></a>Deploying
    a Spack Instance</h3>

    <ul>

    <li>start from existing deployment branch</li>

    </ul>

    <h3><a id="user-content-managing-changes-to-a-spack-instance" class="anchor" aria-hidden="true"
    href="#managing-changes-to-a-spack-instance"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Managing Changes to a Spack Instance</h3>

    <ul>

    <li>deploy change to configuration file</li>

    <li>add new package to sdsc package repository</li>

    <li>spack install a new spec</li>

    </ul>

    <h3><a id="user-content-setting-up-a-shared-instance-configuration" class="anchor"
    aria-hidden="true" href="#setting-up-a-shared-instance-configuration"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Setting up a Shared Instance Configuration</h3>

    <h3><a id="user-content-using-a-spack-mirror-and-build-caches-for-instance-backups"
    class="anchor" aria-hidden="true" href="#using-a-spack-mirror-and-build-caches-for-instance-backups"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Using a Spack Mirror
    and Build Caches for Instance Backup(s)</h3>

    <h2><a id="user-content-additional-notes" class="anchor" aria-hidden="true" href="#additional-notes"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Additional Notes</h2>

    <h3><a id="user-content-protecting-the-sdscspack-develop-branch" class="anchor"
    aria-hidden="true" href="#protecting-the-sdscspack-develop-branch"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Protecting the sdsc/spack <code>develop</code>
    branch</h3>

    <h3><a id="user-content-fetching-changes-and-re-syncing-the-develop-branch" class="anchor"
    aria-hidden="true" href="#fetching-changes-and-re-syncing-the-develop-branch"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Fetching changes and
    re-syncing the <code>develop</code> branch</h3>

    <h3><a id="user-content-creating-a-new-deployment-branch" class="anchor" aria-hidden="true"
    href="#creating-a-new-deployment-branch"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Creating a new deployment branch</h3>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1679762434.0
jmellorcrummey/spack-configs:
  data_format: 2
  description: memoized spack configs for some DOE systems
  filenames:
  - ornl/summit/summit.spack.yaml
  - ornl/crusher/crusher.spack.yaml
  - anl/polaris/polaris.spack.yaml
  full_name: jmellorcrummey/spack-configs
  latest_release: null
  readme: "<p>This directory contains the various files, scripts, and instructions\
    \ for installing hpctoolkit\nat various sites, using Spack to do the install.</p>\n\
    <p>The top-level directory has an <a href=\"install.txt\">install.txt</a> script\
    \ with detailed,\nand hopefully, idiot-proof instructions for using Spack to install\
    \ hpctoolkit.</p>\n<p>It has a <code>bin</code> directory, containing a script\
    \ named <code>spacklink</code> that will set up a spack\nrepository to do the\
    \ installation at a specific <code>&lt;site&gt;</code> on a specific <code>&lt;machine&gt;</code>.</p>\n\
    <p>It has a number of sub-directories, named by <code>&lt;site&gt;</code>.\nEach\
    \ of those subdirectories contains one or more subdirectories, named by <code>&lt;machine&gt;</code>.</p>\n\
    <p>Each of those <code>&lt;site&gt;/&lt;machine&gt;</code> subdirectories contains\
    \ several files:</p>\n<ul>\n<li>\n<p><code>&lt;machine&gt;.config.yaml</code>:</p>\n\
    <p>specifies the directories in which to put the module files and packages for\
    \ a particular install.</p>\n</li>\n<li>\n<p><code>&lt;machine&gt;.modules.yaml</code>:</p>\n\
    <p>specifies information about the modules to be built</p>\n</li>\n<li>\n<p><code>&lt;machine&gt;.packages.yaml</code>:</p>\n\
    <p>this includes configuration for the software environment, including MPI, CUDA,\
    \ python, perl, etc.;\nspecifies the build-dependency version for installing hpctoolkit</p>\n\
    </li>\n<li>\n<p><code>&lt;machine&gt;.spack.yaml</code>:</p>\n<p>this specifies\
    \ a Spack environment file, which allows building several HPCToolkit configurations\n\
    with Spack in one go. If present, the <code>spacklink</code> script will create\
    \ a new directory parallel to\nthe Spack repository called <code>spack-env</code>.\
    \ The installation steps then become:</p>\n</li>\n</ul>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>  $ spack/bin/spack -e spack-env concretize <span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> add \"-f\" to re-concretize as\
    \ needed</span>\n  $ spack/bin/spack -e spack-env install</pre></div>\n"
  stargazers_count: 3
  subscribers_count: 3
  topics: []
  updated_at: 1658934301.0
jrood-nrel/spack-configs:
  data_format: 2
  description: Spack configuration files and scripts for use on machines at NREL
  filenames:
  - configs/ellis/utilities/spack.yaml
  - configs/ellis/software/spack.yaml
  - configs/ellis/compilers/spack.yaml
  full_name: jrood-nrel/spack-configs
  latest_release: null
  readme: '<h1><a id="user-content-spack-configuration-files-and-scripts-for-use-on-machines-at-nrel"
    class="anchor" aria-hidden="true" href="#spack-configuration-files-and-scripts-for-use-on-machines-at-nrel"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Spack configuration
    files and scripts for use on machines at NREL</h1>

    <p>These software installations are maintained by Jon Rood for the HPACF group
    at NREL and are tailored to the applications our group develops. The list of available
    modules can be seen in <a href="modules.txt">modules.txt</a>. They are open to
    anyone to use on our machines. The software installations are organized by date
    snapshots. The binaries, compilers, and utilties are not updated as often as the
    software modules, so dated symlinks might point to older dates for those. However,
    each date snapshot of the modules should be able to stand on its own so that older
    snapshots can be purged safely over time.</p>

    <ul>

    <li>"base" is just a newer version of GCC to replace the system GCC 4.8.5 which
    is far too old to build many recent projects.</li>

    <li>"binaries" are generally the binary downloads of Paraview and Visit.</li>

    <li>"compilers" are the latest set of compilers built using the base GCC.</li>

    <li>"utilities" are the latest set of utility programs that don''t rely on MPI
    and are built using the base GCC.</li>

    <li>"software" are the latest set of generally larger programs and dependencies
    that rely on MPI. Each date corresponds to a single MPI implementation so there
    is no confusion as to which MPI was used for the applications. These modules are
    built using a farily recent GCC, Clang, or Intel compiler provided from the "compilers"
    modules, using the highest optimization flags specific to the machine architecture.</li>

    </ul>

    <p>The Spack hierarchy is linked in the following manner where each installation
    is based on other upstream Spack installations. "software" depends on "utilities",
    which both depend on "compilers". This hierarchy allows Spack to point to packages
    it needs which are already built upstream. The "compilers" installation exposes
    only the modules for compilers, while the "utilities" modules inherit modules
    from itself as well as the dependency packages in the "compilers" installation
    except the compiler modules themselves.</p>

    <p>Currently there is no perfect way to advertise deprecation or addition, and
    evolution of these modules. I have an MOTD you can cat in your login script to
    see updates. Generally the latest 4 sets of modules will likely be kept and new
    sets have been showing up around every 3 to 6 months.</p>

    <p>To use these modules you can add the following to your <code>~/.bashrc</code>
    for example and choose the module set (date) you prefer, and the GCC or Intel
    compiled software modules:</p>

    <pre><code>#------------------------------------------


    #MPT 2.22

    #MODULES=modules-2020-07

    #COMPILER=gcc-8.4.0

    #COMPILER=clang-10.0.0

    #COMPILER=intel-18.0.4


    #MPICH 3.3.1

    #MODULES=modules-2019-10-08

    #COMPILER=gcc-7.4.0

    #COMPILER=clang-7.0.1

    #COMPILER=intel-18.0.4


    #MPICH 3.3

    #MODULES=modules-2019-05-23

    #COMPILER=gcc-7.4.0

    #COMPILER=intel-18.0.4


    #MPICH 3.3

    #MODULES=modules-2019-05-08

    #COMPILER=gcc-7.4.0

    #COMPILER=intel-18.0.4


    #MPICH 3.3

    #MODULES=modules-2019-01-10

    #COMPILER=gcc-7.3.0

    #COMPILER=intel-18.0.4


    #Recommended default according to where "modules" is currently symlinked

    MODULES=modules

    COMPILER=gcc-8.4.0

    #COMPILER=clang-10.0.0

    #COMPILER=intel-18.0.4


    module purge

    module unuse ${MODULEPATH}

    module use /nopt/nrel/ecom/hpacf/binaries/${MODULES}

    module use /nopt/nrel/ecom/hpacf/compilers/${MODULES}

    module use /nopt/nrel/ecom/hpacf/utilities/${MODULES}

    module use /nopt/nrel/ecom/hpacf/software/${MODULES}/${COMPILER}

    module load gcc

    module load git

    module load python

    #etc...


    #------------------------------------------

    </code></pre>

    <p>If <code>module avail</code> does not show the modules on Eagle, try removing
    the LMOD cache with <code>rm -rf ~/.lmod.d/.cache</code></p>

    <p>Also included in this directory is a recommended Spack configurations you can
    use to build your own packages on the machines supported at NREL. Once you have
    <code>SPACK_ROOT</code> set you can run <code>/nopt/nrel/ecom/hpacf/spack-configs/scripts/setup-spack.sh</code>
    which should copy the yaml files into your instance of Spack. Or you can copy
    the yaml files into your <code>${SPACK_ROOT}/etc</code> directory manually. <code>spack
    compilers</code> should then show you many available compilers. Source your Spack''s
    <code>setup-env.sh</code> after you do the <code>module unuse ${MODULEPATH}</code>
    in your <code>.bashrc</code> so that your Spack instance will add its own module
    path to MODULEPATH. Remove <code>~/.spack/linux</code> if it exists and <code>spack
    compilers</code> doesn''t show you the updated list of compilers. The <code>~/.spack</code>
    directory takes highest precendence in the Spack configuration.</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1666717629.0
key4hep/key4hep-spack:
  data_format: 2
  description: A Spack recipe repository of Key4hep software.
  filenames:
  - environments/key4hep-release-user/spack.yaml
  full_name: key4hep/key4hep-spack
  latest_release: '2021-10-29'
  readme: '<h1><a id="user-content-spack-package-repo-for-key4hep-software-packaging"
    class="anchor" aria-hidden="true" href="#spack-package-repo-for-key4hep-software-packaging"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>

    <a href="https://github.com/spack/spack">Spack</a> package repo for Key4HEP software
    packaging</h1>

    <p>This repository holds a set of Spack recipes for key4hep software. It grew
    out of <a href="https://github.com/HSF/hep-spack">https://github.com/HSF/hep-spack</a>,
    and many recipes habe been included in the upstream spack repostiory.</p>

    <p>Consult the <a href="https://spack.readthedocs.io/en/latest/" rel="nofollow">spack
    documentation</a> and the <a href="https://cern.ch/key4hep" rel="nofollow">key4hep
    documentation website</a> for more details.</p>

    <h3><a id="user-content-repository-contents" class="anchor" aria-hidden="true"
    href="#repository-contents"><span aria-hidden="true" class="octicon octicon-link"></span></a>Repository
    Contents</h3>

    <p>Apart from the recipes for key4hep packages in the folder <code>packages</code>,
    the repository contains some <code>scripts</code> used for publishing on cvmfs,
    and <code>config</code> files for spack.</p>

    <h3><a id="user-content-central-installations" class="anchor" aria-hidden="true"
    href="#central-installations"><span aria-hidden="true" class="octicon octicon-link"></span></a>Central
    Installations</h3>

    <p>Installations of the software stack can be found under <code>/cvmfs/sw.hsf.org/</code>,
    see:</p>

    <p><a href="https://key4hep.github.io/key4hep-doc/setup-and-getting-started/README.html"
    rel="nofollow">https://key4hep.github.io/key4hep-doc/setup-and-getting-started/README.html</a></p>

    '
  stargazers_count: 9
  subscribers_count: 9
  topics: []
  updated_at: 1673125688.0
lanl/CELLAR:
  data_format: 2
  description: The CELL Adaptive mesh Refinement (CELLAR) application provides cell-based
    adaptive mesh refinement data structures and execution for parallel computing
    architectures.
  filenames:
  - spack/snow/spack.yaml
  - spack/agaspar/spack.yaml
  - spack/darwin-power9/spack.yaml
  - spack/ci/spack.yaml
  - spack/default/spack.yaml
  full_name: lanl/CELLAR
  latest_release: null
  readme: '<h1><a id="user-content-cellar-----eap-core" class="anchor" aria-hidden="true"
    href="#cellar-----eap-core"><span aria-hidden="true" class="octicon octicon-link"></span></a>CELLAR  -  EAP
    Core</h1>

    <p>CELLAR is a C++ project that forms the foundation of cell based AMR for applications</p>

    <p>It provides the following:</p>

    <ul>

    <li>AMR Mesh Datastructure</li>

    <li>AMR Mesh Reconstruction</li>

    <li>Communication Patterns</li>

    <li>C++ Error Handling and Tracing</li>

    <li>Performance Monitoring</li>

    <li>C++/Fortran Interop</li>

    </ul>

    <h2><a id="user-content-building" class="anchor" aria-hidden="true" href="#building"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Building</h2>

    <p>The easiest way to install dependencies is using <a href="https://spack.io"
    rel="nofollow">Spack</a>.

    After

    <a href="https://spack.readthedocs.io/en/latest/getting_started.html" rel="nofollow">installing
    Spack</a>,

    you can start build dependencies.</p>

    <p>The following instructions assume that you have Spack 0.13 or newer. You can
    check your

    Spack version like so:</p>

    <pre><code>$ spack --version

    0.13.0

    </code></pre>

    <p>First, add <a href="https://github.com/lanl/cellar-spack">lanl/cellar-spack</a>

    to your list of spack repos.</p>

    <p>Once you have the <code>lanl/cellar-spack</code> installed, then you can install
    all

    dependencies using

    <a href="https://spack.readthedocs.io/en/latest/tutorial_environments.html#" rel="nofollow">Spack
    environments</a>.

    You''ll need to use a modern-ish C++ compiler that supports C++14:</p>

    <pre><code>$ module load gcc/9.3.0

    $ spack compiler find

    $ cd path/to/eap-core

    </code></pre>

    <p>Then issue the following commands. This will build all of eap-core''s dependencies.:</p>

    <pre><code>$ spack env create -d spack/default

    $ spack env activate -d $PWD/spack/default

    $ spack install

    </code></pre>

    <p>Any time you open a new shell, you''ll need to re-activate the Spack environment:</p>

    <pre><code>$ spack env activate -d $PWD/spack/default

    </code></pre>

    <p>Now you''re ready to build eap-core. First configure the project using CMake:</p>

    <pre><code>mkdir build &amp;&amp; cd build

    cmake ..

    </code></pre>

    <p>And then build:</p>

    <pre><code>make -j

    </code></pre>

    <p>For snow, substitute in spack/snow in the above instructions in place of spack/default.
    If you need

    to change the environment use "spack env deactivate".</p>

    <h2><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <p>Code contributors should read the <a href="DEVELOPERS.md">Developers Guide</a>
    prior to

    sending a pull request.</p>

    '
  stargazers_count: 2
  subscribers_count: 5
  topics: []
  updated_at: 1677256390.0
lcompilers/lpython:
  data_format: 2
  description: Python compiler
  filenames:
  - spack.yaml
  full_name: lcompilers/lpython
  latest_release: null
  readme: '<h1><a id="user-content-lpython" class="anchor" aria-hidden="true" href="#lpython"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>LPython</h1>

    <p>LPython is a Python compiler. It is in heavy development, currently in

    pre-alpha stage. Some of the goals of LPython:</p>

    <ul>

    <li>The best possible performance for numerical, array-oriented code</li>

    <li>Run on all platforms</li>

    <li>Compile a subset of Python yet be fully compatible with Python</li>

    <li>Explore designs so that LPython eventually can compile all Python code</li>

    <li>Fast compilation</li>

    <li>Excellent user-friendly diagnostic messages: error, warnings, hints, notes,

    etc.</li>

    <li>Ahead-of-Time compilation to binaries, plus interactive usage (Jupyter

    notebook)</li>

    <li>Transforming Python code to C++, Fortran and other languages</li>

    </ul>

    <p>And more.</p>

    <h1><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h1>

    <p>LPython works on Windows, macOS and Linux.</p>

    <h2><a id="user-content-install-conda" class="anchor" aria-hidden="true" href="#install-conda"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install Conda</h2>

    <p>Please follow the instructions here to install Conda on your platform:</p>

    <p><a href="https://github.com/conda-forge/miniforge/#download">https://github.com/conda-forge/miniforge/#download</a></p>

    <h3><a id="user-content-linux" class="anchor" aria-hidden="true" href="#linux"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Linux</h3>

    <div class="highlight highlight-source-shell"><pre>sudo apt install binutils-dev</pre></div>

    <h3><a id="user-content-windows" class="anchor" aria-hidden="true" href="#windows"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Windows</h3>

    <p>Install Visual Studio (MSVC), for example the version 2022, you can download
    the

    Community version for free from: <a href="https://visualstudio.microsoft.com/downloads/"
    rel="nofollow">https://visualstudio.microsoft.com/downloads/</a>.</p>

    <p>Launch the Miniforge prompt from the Desktop.</p>

    <p>In the shell, initialize the MSVC compiler using:</p>

    <div class="highlight highlight-source-shell"><pre>call <span class="pl-s"><span
    class="pl-pds">"</span>C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\VsDevCmd<span
    class="pl-pds">"</span></span> -arch=x64</pre></div>

    <p>You can optionally test MSVC via:</p>

    <div class="highlight highlight-source-shell"><pre>cl /<span class="pl-k">?</span>

    link /<span class="pl-k">?</span></pre></div>

    <p>Both commands must print several pages of help text.</p>

    <h2><a id="user-content-build-lpython" class="anchor" aria-hidden="true" href="#build-lpython"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build LPython</h2>

    <p>Clone LPython</p>

    <div class="highlight highlight-source-shell"><pre>git clone https://github.com/lcompilers/lpython.git

    <span class="pl-c1">cd</span> lpython</pre></div>

    <h3><a id="user-content-linux-and-macos" class="anchor" aria-hidden="true" href="#linux-and-macos"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Linux and MacOS</h3>

    <ul>

    <li>Create a Conda environment using the pre-existing file:</li>

    </ul>

    <div class="highlight highlight-source-shell"><pre>conda env create -f environment_unix.yml

    conda activate lp</pre></div>

    <ul>

    <li>Generate prerequisite files; build in Debug Mode:</li>

    </ul>

    <div class="highlight highlight-source-shell"><pre>./build0.sh

    ./build1.sh</pre></div>

    <h3><a id="user-content-windows-1" class="anchor" aria-hidden="true" href="#windows-1"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Windows</h3>

    <ul>

    <li>Create a Conda environment using the pre-existing file:</li>

    </ul>

    <div class="highlight highlight-source-shell"><pre>conda env create -f environment_win.yml

    conda activate lp</pre></div>

    <ul>

    <li>Generate prerequisite files; build in Release Mode:</li>

    </ul>

    <div class="highlight highlight-source-shell"><pre>call build0.bat

    call build1.bat</pre></div>

    <ul>

    <li>Tests and examples</li>

    </ul>

    <div class="highlight highlight-source-shell"><pre>ctest

    inst<span class="pl-cce">\b</span>in<span class="pl-cce">\l</span>python examples<span
    class="pl-cce">\e</span>xpr2.py

    inst<span class="pl-cce">\b</span>in<span class="pl-cce">\l</span>python examples<span
    class="pl-cce">\e</span>xpr2.py -o a.out

    a.out</pre></div>

    <ul>

    <li>After you update a test case file, you also need to update all the reference
    results associated with that test case:</li>

    </ul>

    <pre><code>python run_tests.py -u --skip-run-with-dbg

    </code></pre>

    <ul>

    <li>To see all the options associated with LPython test suite, use:</li>

    </ul>

    <pre><code>python run_tests.py --help

    </code></pre>

    <h2><a id="user-content-tests-linux-or-macos" class="anchor" aria-hidden="true"
    href="#tests-linux-or-macos"><span aria-hidden="true" class="octicon octicon-link"></span></a>Tests
    (Linux or MacOs):</h2>

    <p>Run tests:</p>

    <div class="highlight highlight-source-shell"><pre>ctest

    ./run_tests.py</pre></div>

    <p>Run integration tests:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span>
    integration_tests

    ./run_tests.py</pre></div>

    <h3><a id="user-content-speed-up-integration-test-on-macs" class="anchor" aria-hidden="true"
    href="#speed-up-integration-test-on-macs"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Speed up Integration Test on Macs</h3>

    <p>Integration tests run slowly because Apple checks the hash of each

    executable online before running. You can turn off that feature

    in the Privacy tab of the Security and Privacy item of System

    Preferences, Developer Tools, Terminal.app, "allow the apps below

    to run software locally that does not meet the system''s security

    policy."</p>

    <h2><a id="user-content-examples-linux-or-macos" class="anchor" aria-hidden="true"
    href="#examples-linux-or-macos"><span aria-hidden="true" class="octicon octicon-link"></span></a>Examples
    (Linux or MacOs)</h2>

    <p>You can run the following examples by hand in a terminal:</p>

    <div class="highlight highlight-source-shell"><pre>./src/bin/lpython examples/expr2.py

    ./src/bin/lpython examples/expr2.py -o expr

    ./expr

    ./src/bin/lpython --show-ast examples/expr2.py

    ./src/bin/lpython --show-asr examples/expr2.py

    ./src/bin/lpython --show-cpp examples/expr2.py

    ./src/bin/lpython --show-llvm examples/expr2.py

    ./src/bin/lpython --show-c examples/expr2.py</pre></div>

    <h2><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <p>We welcome contributions from anyone, even if you are new to compilers or to

    open source. It might sound daunting to contribute to a compiler at first, but

    please do, it is not complicated. We will help you with technical issues and

    help improve your contribution so that it can be merged.</p>

    <p>To contribute, submit a Pull Request (PR) against our repository at:</p>

    <p><a href="https://github.com/lcompilers/lpython">https://github.com/lcompilers/lpython</a></p>

    <p>and don''t forget to clean your history, see <a href="./doc/src/rebasing.md">example</a>.</p>

    <p>Please report any bugs you may find at our issue tracker:

    <a href="https://github.com/lcompilers/lpython/issues">https://github.com/lcompilers/lpython/issues</a>.
    Or, even better, fork the

    repository on GitHub and create a PR. We welcome all changes, big or small, and

    we will help you make a PR if you are new to git.</p>

    <p>If you have any questions or need help, please ask us at Zulip (<a href="https://lfortran.zulipchat.com/"
    rel="nofollow"><img src="https://camo.githubusercontent.com/11e6556bfe778e7cf7331cac9c44bd0616062722036cc0d9bb0b7909aaae8779/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7a756c69702d6a6f696e5f636861742d627269676874677265656e2e737667"
    alt="project chat" data-canonical-src="https://img.shields.io/badge/zulip-join_chat-brightgreen.svg"
    style="max-width: 100%;"></a>)

    or our <a href="https://groups.io/g/lfortran" rel="nofollow">mailinglist</a>.</p>

    <p>See the <a href="CONTRIBUTING.md">CONTRIBUTING</a> document for more information.</p>

    '
  stargazers_count: 116
  subscribers_count: 8
  topics: []
  updated_at: 1679815029.0
ma595/fenics-csd3-spack:
  data_format: 2
  description: Set up fenics spack on csd3
  filenames:
  - spack-icelake.yaml
  - spack-skylake.yaml
  full_name: ma595/fenics-csd3-spack
  latest_release: null
  readme: '<h1><a id="user-content-fenics-csd3-spack" class="anchor" aria-hidden="true"
    href="#fenics-csd3-spack"><span aria-hidden="true" class="octicon octicon-link"></span></a>fenics-csd3-spack</h1>

    <p>Follow instructions in icelake-spack-env.sh</p>

    <p>Or, copy existing <code>spack.yaml</code> files into cloned Spack repo. It
    is necessary to <code>module purge</code> environment first, otherwise the prepend
    path inside <code>spack.yaml</code> will lead to duplications.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1667830944.0
marcodelapierre/toy-cowsay-nf:
  data_format: 2
  description: Toy pipeline for simple Nextflow tests
  filenames:
  - scripts/spack/spack.yaml
  - scripts/containerize-spack/spack.yaml
  - spack.yaml
  full_name: marcodelapierre/toy-cowsay-nf
  latest_release: null
  readme: '<h2><a id="user-content-toy-pipeline-for-simple-nextflow-tests" class="anchor"
    aria-hidden="true" href="#toy-pipeline-for-simple-nextflow-tests"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Toy pipeline for simple Nextflow tests</h2>

    <p>The purpose of this repo is to have a pipeline with features including:</p>

    <p>General:</p>

    <ul>

    <li>Simple</li>

    <li>Small (including required software)</li>

    <li>Quick to setup and run</li>

    </ul>

    <p>Pipeline:</p>

    <ul>

    <li>Requires a small package, that can be installed with Conda or Spack

    <ul>

    <li>Conda: <code>cowpy</code> (from <code>conda-forge</code>)</li>

    <li>Spack: <code>cowsay</code>

    </li>

    </ul>

    </li>

    <li>Reads/writes files</li>

    </ul>

    <p>Software options:</p>

    <ul>

    <li>Host</li>

    <li>Containers</li>

    <li>Conda</li>

    <li>Conda with Wave</li>

    <li>Spack</li>

    </ul>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1676008940.0
markusrenoldner/MEHCscheme:
  data_format: 2
  description: A mass, energy, and helicty conserving dual-field discretization of
    the incompressible Navier-Stokes problem
  filenames:
  - extern/mfem-4.5/config/docker/spack.yaml
  full_name: markusrenoldner/MEHCscheme
  latest_release: null
  readme: '<h1><a id="user-content-mehcscheme" class="anchor" aria-hidden="true" href="#mehcscheme"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>MEHCscheme</h1>

    <p>A mass, energy, and helicty conserving dual-field Galerkin finite element discretization
    of the incompressible Navier-Stokes problem based on this paper <a href="https://arxiv.org/abs/2104.13023"
    rel="nofollow">https://arxiv.org/abs/2104.13023</a> implemented in MFEM, see <a
    href="https://mfem.org/" rel="nofollow">https://mfem.org/</a></p>

    <h1><a id="user-content-cmake-and-make" class="anchor" aria-hidden="true" href="#cmake-and-make"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>CMake and Make</h1>

    <p>TODO (basically download and unpack mfem and glvis into the extern folder,
    then install it there as described on mfem.org; then run the build.sh file to
    cmake, the compile.sh file to make, the run.sh file to run and the clean.sh file
    to delete the mesh and solution files afterwards)</p>

    <h1><a id="user-content-folder-structure" class="anchor" aria-hidden="true" href="#folder-structure"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>folder structure</h1>

    <ul>

    <li>/build will contain all files produced by cmake</li>

    <li>/extern contains the mfem (v4.5) and glvis (v4.2) library</li>

    <li>/out contains plots and data outputs</li>

    <li>/scripts contains some files necessary for plotting etc</li>

    <li>/src contains the cpp files that implement the mehc scheme</li>

    <li>.sh are all shell scripts, that contain some handy commands for building,
    compiling, running and cleaning</li>

    </ul>

    <h1><a id="user-content-run-the-scheme" class="anchor" aria-hidden="true" href="#run-the-scheme"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>run the scheme:</h1>

    <ol>

    <li>select the cpp file in src/CMakeLists.txt :

    <ul>

    <li>periodic-conv.cpp</li>

    <li>periodic-cons.cpp</li>

    <li>dirichlet-[placeholder].cpp</li>

    </ul>

    </li>

    <li>run ./build.sh</li>

    <li>run ./compilerun.sh</li>

    <li>produce a visualization using scripts/plots_[placeholder].ipynb</li>

    <li>find the visualizsations in out/plots</li>

    <li>find the raw data in out/rawdata</li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1678912809.0
mfem/mfem:
  data_format: 2
  description: Lightweight, general, scalable C++ library for finite element methods
  filenames:
  - config/docker/spack.yaml
  full_name: mfem/mfem
  latest_release: v4.5.2
  readme: "<pre><code>                Finite Element Discretization Library\n    \
    \                           __\n                   _ __ ___   / _|  ___  _ __\
    \ ___\n                  | '_ ` _ \\ | |_  / _ \\| '_ ` _ \\\n               \
    \   | | | | | ||  _||  __/| | | | | |\n                  |_| |_| |_||_|   \\___||_|\
    \ |_| |_|\n\n                           https://mfem.org\n</code></pre>\n<p><a\
    \ href=\"https://mfem.org\" rel=\"nofollow\">MFEM</a> is a modular parallel C++\
    \ library for finite element\nmethods. Its goal is to enable high-performance\
    \ scalable finite element\ndiscretization research and application development\
    \ on a wide variety of\nplatforms, ranging from laptops to supercomputers.</p>\n\
    <p>We welcome contributions and feedback from the community. Please see the file\n\
    <a href=\"CONTRIBUTING.md\">CONTRIBUTING.md</a> for additional details about our\
    \ development\nprocess.</p>\n<ul>\n<li>\n<p>For building instructions, see the\
    \ file <a href=\"INSTALL\">INSTALL</a>, or type \"make help\".</p>\n</li>\n<li>\n\
    <p>Copyright and licensing information can be found in files <a href=\"LICENSE\"\
    >LICENSE</a> and <a href=\"NOTICE\">NOTICE</a>.</p>\n</li>\n<li>\n<p>The best\
    \ starting point for new users interested in MFEM's features is to\nreview the\
    \ examples and miniapps at <a href=\"https://mfem.org/examples\" rel=\"nofollow\"\
    >https://mfem.org/examples</a>.</p>\n</li>\n<li>\n<p>Instructions for learning\
    \ with Docker are in <a href=\"config/docker\">config/docker</a>.</p>\n</li>\n\
    </ul>\n<p>Conceptually, MFEM can be viewed as a finite element toolbox that provides\
    \ the\nbuilding blocks for developing finite element algorithms in a manner similar\
    \ to\nthat of MATLAB for linear algebra methods. In particular, MFEM provides\
    \ support\nfor arbitrary high-order H1-conforming, discontinuous (L2), H(div)-conforming,\n\
    H(curl)-conforming and NURBS finite element spaces in 2D and 3D, as well as many\n\
    bilinear, linear and nonlinear forms defined on them. It enables the quick\nprototyping\
    \ of various finite element discretizations, including Galerkin\nmethods, mixed\
    \ finite elements, Discontinuous Galerkin (DG), isogeometric\nanalysis, hybridization\
    \ and Discontinuous Petrov-Galerkin (DPG) approaches.</p>\n<p>MFEM includes classes\
    \ for dealing with a wide range of mesh types: triangular,\nquadrilateral, tetrahedral\
    \ and hexahedral, as well as surface and topologically\nperiodical meshes. It\
    \ has general support for mesh refinement, including local\nconforming and non-conforming\
    \ (AMR) adaptive refinement. Arbitrary element\ntransformations, allowing for\
    \ high-order mesh elements with curved boundaries,\nare also supported.</p>\n\
    <p>When used as a \"finite element to linear algebra translator\", MFEM can take\
    \ a\nproblem described in terms of finite element-type objects, and produce the\n\
    corresponding linear algebra vectors and fully or partially assembled operators,\n\
    e.g. in the form of global sparse matrices or matrix-free operators. The library\n\
    includes simple smoothers and Krylov solvers, such as PCG, MINRES and GMRES, as\n\
    well as support for sequential sparse direct solvers from the SuiteSparse\nlibrary.\
    \ Nonlinear solvers (the Newton method), eigensolvers (LOBPCG), and\nseveral explicit\
    \ and implicit Runge-Kutta time integrators are also available.</p>\n<p>MFEM supports\
    \ MPI-based parallelism throughout the library, and can readily be\nused as a\
    \ scalable unstructured finite element problem generator. Starting with\nversion\
    \ 4.0, MFEM offers support for GPU acceleration, and programming models,\nsuch\
    \ as CUDA, HIP, OCCA, RAJA and OpenMP. MFEM-based applications require\nminimal\
    \ changes to switch from a serial to a highly-performant MPI-parallel\nversion\
    \ of the code, where they can take advantage of the integrated linear\nsolvers\
    \ from the hypre library. Comprehensive support for other external\npackages,\
    \ e.g. PETSc, SUNDIALS and libCEED is also included, giving access to\nadditional\
    \ linear and nonlinear solvers, preconditioners, time integrators, etc.</p>\n\
    <p>For examples of using MFEM, see the <a href=\"examples\">examples/</a> and\
    \ <a href=\"miniapps\">miniapps/</a>\ndirectories, as well as the OpenGL visualization\
    \ tool GLVis which is available\nat <a href=\"https://glvis.org\" rel=\"nofollow\"\
    >https://glvis.org</a>.</p>\n<h2><a id=\"user-content-license\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#license\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>License</h2>\n<p>MFEM is distributed under the terms\
    \ of the BSD-3 license. All new contributions\nmust be made under this license.\
    \ See <a href=\"LICENSE\">LICENSE</a> and <a href=\"NOTICE\">NOTICE</a> for\n\
    details.</p>\n<p>SPDX-License-Identifier: BSD-3-Clause <br>\nLLNL Release Number:\
    \ LLNL-CODE-806117 <br>\nDOI: 10.11578/dc.20171025.1248</p>\n"
  stargazers_count: 1156
  subscribers_count: 117
  topics:
  - finite-elements
  - high-order
  - high-performance-computing
  - parallel-computing
  - amr
  - computational-science
  - fem
  - scientific-computing
  - hpc
  - math-physics
  - radiuss
  updated_at: 1679803480.0
mochi-hpc-experiments/colza-experiments:
  data_format: 2
  description: Experiments using Colza for In Situ Analysis
  filenames:
  - theta/amr-wind/spack.yaml
  full_name: mochi-hpc-experiments/colza-experiments
  latest_release: ipdps2022
  readme: '<h1><a id="user-content-colza-experiments" class="anchor" aria-hidden="true"
    href="#colza-experiments"><span aria-hidden="true" class="octicon octicon-link"></span></a>Colza
    Experiments</h1>

    <p>This repository contains scripts to reproduce experiments

    related to the Colza elastic in situ analysis framework.

    These experiments were run on the Cori supercomputer.</p>

    <p>Each subfolder contains a README file explaining what the

    experiment in the subfolder does, how to install its

    dependencies, and how to run it.</p>

    <p>The ubuntu folder contains scripts that allow reproducing

    the most experiments on a single Linux workstation or a

    cluster of Linux machines.</p>

    '
  stargazers_count: 1
  subscribers_count: 3
  topics: []
  updated_at: 1655200495.0
mochi-hpc-experiments/platform-configurations:
  data_format: 2
  description: This repository provides a set of configuration files and example scripts
    for running Mochi experiments on various platforms.
  filenames:
  - ANL/Polaris/spack.yaml
  - ANL/ThetaGPU/spack.yaml
  - ANL/Polaris/spack-system-libfabric.yaml
  full_name: mochi-hpc-experiments/platform-configurations
  latest_release: null
  readme: '<h1><a id="user-content-platform-configurations-for-mochi" class="anchor"
    aria-hidden="true" href="#platform-configurations-for-mochi"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Platform configurations for Mochi</h1>

    <p>This repository provides Spack configuration files, example job scripts, and

    notes about building and running Mochi-based codes on various platforms.

    Please refer to the subdirectory for your platform of interest for more

    information.</p>

    <p>The <code>generic</code> subdirectory contains a minimal Spack environment
    example that

    can be used as a starting point for systems for which there is no existing

    recipe.</p>

    <h2><a id="user-content-using-spackyaml-files" class="anchor" aria-hidden="true"
    href="#using-spackyaml-files"><span aria-hidden="true" class="octicon octicon-link"></span></a>Using
    spack.yaml files</h2>

    <p>Each platform subdirectory in this repository provides a <code>spack.yaml</code>
    file.

    A <code>spack.yaml</code> file fully describes a Spack environment, including

    system-provided packages and compilers. It does so independently of any

    <code>compilers.yaml</code> or <code>packages.yaml</code> files installed in <code>~/.spack</code>,
    thereby

    preventing interference with user-specific spack configurations as much as

    possible.</p>

    <p>You may use <code>spack.yaml</code> files to create a

    <a href="https://spack.readthedocs.io/en/latest/environments.html" rel="nofollow">Spack
    environment</a>

    in which Mochi packages will be installed.</p>

    <p>If you don''t have Spack installed on your platform, clone it and set it up

    as follows.</p>

    <pre><code>$ git clone https://github.com/spack/spack.git

    $ . spack/share/spack/setup-env.sh

    </code></pre>

    <p>Remember that the second line needs to be executed every time you open a new

    terminal; it may be helpful to create an alias in your bashrc file as a

    shortcut.</p>

    <p>You will then need to clone <code>mochi-spack-packages</code>, which contains
    the Mochi packages.</p>

    <pre><code>$ git clone https://github.com/mochi-hpc/mochi-spack-packages.git

    $ spack repo add mochi-spack-packages

    </code></pre>

    <p>Now clone the present repository and <code>cd</code> into the subdirectory
    relevant

    to your platform. For example:</p>

    <pre><code>$ git clone https://github.com/mochi-hpc-experiments/platform-configurations.git

    $ cd platform-configurations/ANL/Bebop

    </code></pre>

    <p>Edit the path to <code>mochi-spack-packages</code> in the <code>repos</code>
    field of the <code>spack.yaml</code> file to

    match your installation.</p>

    <p>Then, execute the following command

    (changing <em>myenv</em> into an appropriate name for your environment).</p>

    <pre><code>$ spack env create myenv spack.yaml

    </code></pre>

    <p>Change to a directory outside of the <code>platform-configurations</code> folders

    and activate the environment as follows.</p>

    <pre><code>$ spack env activate myenv

    </code></pre>

    <p>You may now add specs to your environment. For instance if you want

    to install Margo, execute the following command.</p>

    <pre><code>$ spack add mochi-margo

    </code></pre>

    <p>If the <code>spack.yaml</code> file provides multiple compilers and you want

    to use another than the default one, specify the compiler explicitely,

    for example:</p>

    <pre><code>$ spack add mochi-margo %gcc@8.2.0

    </code></pre>

    <p>Note that the <code>spack.yaml</code> file you used may already have a spec

    added as an example (usually <code>mochi-margo</code>). You can remove it as

    follows.</p>

    <pre><code>$ spack rm mochi-margo

    </code></pre>

    <p>Once you have added the specs you need in your environment, install

    everything by executing the following command.</p>

    <pre><code>$ spack install

    </code></pre>

    <p>You may add more specs later on. For more information on how to manage

    Spack environments, please refer to the Spack documentation.</p>

    <h2><a id="user-content-contributing-to-this-repository" class="anchor" aria-hidden="true"
    href="#contributing-to-this-repository"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Contributing to this repository</h2>

    <p>Should you want to contribute a <code>spack.yaml</code> for a particular machine,

    please submit a merge request with it, and ensure the following.</p>

    <ul>

    <li>The <code>spack.yaml</code> file should contain the compiler(s) that have
    been tested

    and confirmed to work with Mochi packages.</li>

    <li>The <code>spack.yaml</code> file should try to list system-provided packages,

    in particular packages used for building (<code>cmake</code>, <code>autoconf</code>,
    etc.),

    and relevant system-provided MPI implementations.

    <ul>

    <li>Note that this must be done manually.  Spack provides a <code>spack external
    find</code> command that can be used to locate a subset of system packages,

    but it does not populate the <code>spack.yaml</code> file.</li>

    </ul>

    </li>

    <li>The <code>spack.yaml</code> file should contain the relevant variants for
    packages,

    in particular the transport methods to use with <code>libfabric</code>.</li>

    <li>The path to the <code>spack.yaml</code> file should be of the form

    <code>&lt;institution&gt;/&lt;platform&gt;/spack.yaml</code>.</li>

    <li>Please make sure that your <code>spack.yaml</code> is a reliable way to work
    with

    Mochi on the target platform, other people will rely on it!</li>

    </ul>

    <p>You can also contribute changes to existing <code>spack.yaml</code> files,
    in particular

    to add working compilers, system packages, etc. As always, please test that

    new setups work before creating a merge request.</p>

    '
  stargazers_count: 3
  subscribers_count: 3
  topics: []
  updated_at: 1677790084.0
mochi-hpc/margo-microservice-template:
  data_format: 2
  description: Template for a margo-based Mochi microservice.
  filenames:
  - spack.yaml
  full_name: mochi-hpc/margo-microservice-template
  latest_release: null
  readme: '<h1><a id="user-content-margo-microservice-template" class="anchor" aria-hidden="true"
    href="#margo-microservice-template"><span aria-hidden="true" class="octicon octicon-link"></span></a>Margo
    Microservice Template</h1>

    <p>This project is a template to start developing a Mochi microservice based on
    Margo.

    The complete documentation to get started using this template is available

    <a href="https://mochi.readthedocs.io/en/latest/templates/01_margo.html" rel="nofollow">here</a>.</p>

    <p>To use this template:</p>

    <ul>

    <li>Click on the green "Use this template" button at the top.</li>

    <li>Give a name to your project.</li>

    <li>Once your project repository is created, go to Settings &gt; Actions &gt;
    General and give "Read and write permissions" under <em>Workflow permissions</em>.</li>

    <li>Finally, edit the initial-setup.json file and push the changes to your repo.</li>

    </ul>

    <p>Editing the initial-setup.json file with trigger a github action that will

    cleanup your repository and rename files, namespaces, functions, etc. according

    to the name of your service and the resources it manages.</p>

    <p>Enjoy working with Mochi!</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1649078785.0
mochi-hpc/mobject:
  data_format: 2
  description: Mobject is a prototype Mochi object storage system based on RADOS
  filenames:
  - spack.yaml
  full_name: mochi-hpc/mobject
  latest_release: v0.6.1
  readme: "<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"mobject_logo.png\"\
    ><img src=\"mobject_logo.png\" alt=\"logo\" style=\"max-width: 100%;\"></a></p>\n\
    <h1><a id=\"user-content-mobject\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #mobject\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Mobject</h1>\n\
    <p><a href=\"https://github.com/mochi-hpc/mobject/actions/workflows/spell.yml\"\
    ><img src=\"https://github.com/mochi-hpc/mobject/actions/workflows/spell.yml/badge.svg\"\
    \ alt=\"check spelling\" style=\"max-width: 100%;\"></a>\n<a href=\"https://github.com/mochi-hpc/mobject/actions/workflows/spack.yml\"\
    ><img src=\"https://github.com/mochi-hpc/mobject/actions/workflows/spack.yml/badge.svg\"\
    \ alt=\"spack mobject\" style=\"max-width: 100%;\"></a>\n<a href=\"https://github.com/mochi-hpc/mobject/actions/workflows/spack_bedrock.yml\"\
    ><img src=\"https://github.com/mochi-hpc/mobject/actions/workflows/spack_bedrock.yml/badge.svg\"\
    \ alt=\"spack mobject+bedrock\" style=\"max-width: 100%;\"></a></p>\n<p>Mobject\
    \ is a distributed object storage system\nbuilt using a composition of <a href=\"\
    https://mochi.readthedocs.io\" rel=\"nofollow\">Mochi</a> components:</p>\n<ul>\n\
    <li>\n<a href=\"https://github.com/mochi-hpc/mochi-bake\">mochi-bake</a> (for\
    \ bulk storage)</li>\n<li>\n<a href=\"https://github.com/mochi-hpc/mochi-bedrock\"\
    >mochi-bedrock</a>\n(for configuration and bootstrapping)</li>\n<li>\n<a href=\"\
    https://github.com/mochi-hpc/mochi-sdskv\">mochi-sdskv</a>\n(for metadata and\
    \ log indexing)</li>\n<li>\n<a href=\"https://github.com/mochi-hpc/mochi-ssg\"\
    >mochi-ssg</a> (for group membership)</li>\n</ul>\n<h2><a id=\"user-content-installation\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#installation\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<p><a href=\"\
    https://mochi.readthedocs.io/en/latest/installing.html#installing-spack-and-the-mochi-repository\"\
    \ rel=\"nofollow\">Install Spack and Mochi Spack Repository</a>.</p>\n<p>Then,\
    \ run the following command to install mobject.</p>\n<pre><code>   spack install\
    \ mobject\n</code></pre>\n<h2><a id=\"user-content-hdf5-and-mobject\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#hdf5-and-mobject\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>HDF5 and Mobject</h2>\n<p><a\
    \ href=\"/include/librados-mobject-store.h\">Mobject API</a> is a subset of the\n\
    <a href=\"https://github.com/ceph/ceph/blob/main/src/include/rados/librados.h\"\
    >RADOS API</a>\nfrom Ceph\u2019s object storage layer.\nTherefore, <a href=\"\
    https://github.com/HDFGroup/vol-rados\">HDF5 RADOS VOL plugin-in</a>\ncan use\
    \ Mobject.</p>\n<h2><a id=\"user-content-faq\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#faq\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>FAQ</h2>\n<p>See <a href=\"doc/FAQ.md\">doc/FAQ.md</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1640785210.0
mochi-hpc/mochi-bedrock:
  data_format: 2
  description: Mochi bootstrapping service.
  filenames:
  - spack.yaml
  full_name: mochi-hpc/mochi-bedrock
  latest_release: v0.5.2
  readme: '<h1><a id="user-content-bedrock" class="anchor" aria-hidden="true" href="#bedrock"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Bedrock</h1>

    <p>Bedrock is Mochi''s service bootstrapping mechanism.

    For documentations and tutorials, please see

    <a href="https://mochi.readthedocs.io/en/latest/bedrock.html" rel="nofollow">here</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1640527359.0
mochi-hpc/mochi-poesie:
  data_format: 2
  description: POESIE is a Mochi microservice designed to run interpreters of various
    scripting languages (currently Lua and Python) and make them accessible remotely.
  filenames:
  - spack.yaml
  full_name: mochi-hpc/mochi-poesie
  latest_release: v0.2
  readme: "<h1><a id=\"user-content-poesie-embedding-scripting-languages-for-mochi-services\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#poesie-embedding-scripting-languages-for-mochi-services\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>POESIE:\
    \ Embedding Scripting Languages for Mochi Services</h1>\n<h2><a id=\"user-content-installation\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#installation\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<p>POESIE\
    \ can easily be installed using Spack:</p>\n<p><code>spack install mochi-poesie</code></p>\n\
    <p>This will install POESIE (and any required dependencies) with both\nPython\
    \ and Lua backends. Disabling one or the other can be done by\nappending <code>~lua</code>\
    \ or <code>~python</code>, for example:</p>\n<p><code>spack install poesie~lua</code></p>\n\
    <h2><a id=\"user-content-architecture\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#architecture\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Architecture</h2>\n<p>Like most mochi services, POESIE relies on a\
    \ client/provider architecture.\nA provider, identified by its <em>address</em>\
    \ and <em>provider id</em>, manages one or more\ninterpreters (called <em>virtual\
    \ machines</em>, or <em>VMs</em>), referenced externally\nby either their name\
    \ or their VM id.</p>\n<h2><a id=\"user-content-starting-a-daemon-using-bedrock\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#starting-a-daemon-using-bedrock\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Starting\
    \ a daemon using Bedrock</h2>\n<p>By installing POESIE with the <code>+bedrock</code>\
    \ variant, one can deploy a daemon\nby providing a JSON configuration like the\
    \ following to Bedrock.</p>\n<div class=\"highlight highlight-source-json\"><pre>{\n\
    \    <span class=\"pl-ent\">\"libraries\"</span>: [\n        <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>libpoesie-bedrock-module.so<span class=\"pl-pds\"\
    >\"</span></span>\n    ],\n    <span class=\"pl-ent\">\"providers\"</span>: [\n\
    \        {\n            <span class=\"pl-ent\">\"name\"</span>: <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>my_poesie_provider<span class=\"pl-pds\"\
    >\"</span></span>,\n            <span class=\"pl-ent\">\"provider_id\"</span>:\
    \ <span class=\"pl-c1\">0</span>,\n            <span class=\"pl-ent\">\"config\"\
    </span>: {\n                <span class=\"pl-ent\">\"vms\"</span>: {\n       \
    \             <span class=\"pl-ent\">\"my_vm\"</span>: {\n                   \
    \     <span class=\"pl-ent\">\"language\"</span>: <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>python<span class=\"pl-pds\">\"</span></span>\n            \
    \        }\n                }\n            }\n        }\n    ]\n}</pre></div>\n\
    <h2><a id=\"user-content-client-api\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #client-api\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Client\
    \ API</h2>\n<p>The client API is available in <em>poesie-client.h</em>.\nThe codes\
    \ in the <em>test</em> folder illustrate how to use it.</p>\n"
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1633975197.0
mochi-hpc/mochi-thallium:
  data_format: 2
  description: Thallium is a C++14 library wrapping Margo, Mercury, and Argobots and
    providing an object-oriented way to use these libraries.
  filenames:
  - spack.yaml
  full_name: mochi-hpc/mochi-thallium
  latest_release: v0.11.0
  readme: '<h1><a id="user-content-thallium" class="anchor" aria-hidden="true" href="#thallium"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Thallium</h1>

    <p>Thallium is a C++ interface to <a href="https://github.com/mochi-hpc/mochi-margo/">Margo</a>.

    It offers a modern, object-oriented way of developing HPC data services. More

    information can be found on <a href="https://mochi.readthedocs.io/en/latest/"
    rel="nofollow">Mochi''s readthedocs</a>

    website.</p>

    '
  stargazers_count: 9
  subscribers_count: 4
  topics: []
  updated_at: 1675839350.0
mochi-hpc/thallium-microservice-template:
  data_format: 2
  description: Template for a thallium-based Mochi microservice.
  filenames:
  - spack.yaml
  full_name: mochi-hpc/thallium-microservice-template
  latest_release: null
  readme: '<h1><a id="user-content-thallium-microservice-template" class="anchor"
    aria-hidden="true" href="#thallium-microservice-template"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Thallium Microservice Template</h1>

    <p>This project is a template to start developing a Mochi microservice based on
    Thallium.

    The complete documentation to get started using this template is available

    <a href="https://mochi.readthedocs.io/en/latest/templates/02_thallium.html" rel="nofollow">here</a>.</p>

    <p>To use this template:</p>

    <ul>

    <li>Click on the green "Use this template" button at the top.</li>

    <li>Give a name to your project.</li>

    <li>Once your project repository is created, go to Settings &gt; Actions &gt;
    General and give "Read and write permissions" under <em>Workflow permissions</em>.</li>

    <li>Finally, edit the initial-setup.json file and push the changes to your repo.</li>

    </ul>

    <p>Editing the initial-setup.json file with trigger a github action that will

    cleanup your repository and rename files, namespaces, functions, etc. according

    to the name of your service and the resources it manages.</p>

    <p>Enjoy working with Mochi!</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1649080284.0
nantes-m2-rps-exp/qqbar2mumu-2022:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: nantes-m2-rps-exp/qqbar2mumu-2022
  latest_release: null
  readme: "<h1><a id=\"user-content-projet-exp\xE9rimental---production-de-quarkonia\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#projet-exp\xE9rimental---production-de-quarkonia\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Projet exp\xE9\
    rimental - Production de quarkonia</h1>\n<blockquote>\n<p>Ce d\xE9pot git h\xE9\
    berge les fichiers n\xE9cessaires pour d\xE9marrer le projet \"Production de quarkonia\"\
    \ du Master 2 RPS de l'Universit\xE9 de Nantes. Il est principalement \xE0 destination\
    \ des \xE9tudiants qui r\xE9alisent ce projet. Le \"vous\" ci-dessous s'adresse\
    \ donc \xE0 ces \xE9tudiants.</p>\n</blockquote>\n<p>Pour ce projet le language\
    \ de programmation choisi est Python. Nous recommandons de l'utiliser par le biais\
    \ de <a href=\"https://jupyter.org\" rel=\"nofollow\">\"Notebooks Jupyter\"</a>\
    \ qui permettent de m\xE9langer le code, la documentation et les r\xE9sultats\
    \ de l'ex\xE9cution du code.</p>\n<p>Jupyter est un outil commun dans le domaine\
    \ de la science des donn\xE9es. Il y a bien des fa\xE7ons d'utiliser Jupyter et\
    \ de nombreux tutoriels sont disponibles en ligne pour aller plus loin, mais vous\
    \ trouverez ci-dessous trois m\xE9thodes pour d\xE9marrer :</p>\n<ol>\n<li>une\
    \ <a href=\"conda/README.md\">m\xE9thode locale bas\xE9e sur conda</a>\n</li>\n\
    <li>une <a href=\"cloud/README.md\">m\xE9thode cloud</a>\n</li>\n<li>une <a href=\"\
    multipass/README.md\">m\xE9thode locale bas\xE9e sur multipass</a>\n</li>\n</ol>\n\
    <p>A noter que seule la troisi\xE8me m\xE9thode permet, a priori, de r\xE9aliser\
    \ toutes les t\xE2ches n\xE9cessaires \xE0 ce projet, car elle offre des interfaces\
    \ Python de paquets C++ d\xE9velopp\xE9s sp\xE9cifiquement pour ce projet, alors\
    \ que les deux premi\xE8res ne permettent d'acc\xE9der qu'\xE0 des paquets Python\
    \ \"g\xE9n\xE9riques\". Les deux premi\xE8res m\xE9thodes permettent n\xE9anmoins\
    \ de d\xE9marrer assez rapidement.</p>\n<p>Pour ce projet, vous utiliserez \xE9\
    galement <a href=\"https://git.com\" rel=\"nofollow\">Git</a> et <a href=\"https://github.com\"\
    >GitHub</a>. Si ce n'est pas d\xE9j\xE0 le cas, il vous faudra <a href=\"https://git-scm.com/book/en/v2/Getting-Started-Installing-Git\"\
    \ rel=\"nofollow\">installer git sur votre machine</a> et vous <a href=\"https://fr.wikihow.com/cr%C3%A9er-un-compte-sur-GitHub\"\
    \ rel=\"nofollow\">cr\xE9\xE9r un compte GitHub</a>.</p>\n<p>Comme pour Jupyter,\
    \ un nombre important de ressources documentaires et tutoriels sont disponibles\
    \ sur le net pour commencer avec git si c'est votre premi\xE8re approche ou encore\
    \ pour approfondir votre ma\xEEtrise de cet outil si vous le connaissez d\xE9\
    j\xE0 un peu.</p>\n<p>Vous trouverez dans le <a href=\"git/README.md\">document\
    \ <code>git/README.md</code></a> les commandes de base pour d\xE9marrer avec ce\
    \ d\xE9p\xF4t git en particulier.</p>\n<p>Une fois la premi\xE8re installation\
    \ r\xE9alis\xE9e, commencez par vous familiariser avec Jupyter en utilisant le\
    \ <a href=\"notebooks/muon-eta-distribution.ipynb\">notebook d'exemple</a></p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1667463557.0
openPMD/openPMD-api:
  data_format: 2
  description: ':floppy_disk: C++ & Python API for Scientific I/O'
  filenames:
  - .github/ci/spack-envs/gcc7_py36_ompi_h5_ad1_ad2/spack.yaml
  - .github/ci/spack-envs/clang6_nopy_ompi_h5_ad1_ad2_bp3_libcpp/spack.yaml
  - .github/ci/spack-envs/clang7_nopy_ompi_h5_ad1_ad2/spack.yaml
  - .github/ci/spack-envs/clangtidy_nopy_ompi_h5_ad1_ad2/spack.yaml
  - .github/ci/spack-envs/clang6_nopy_nompi_h5_libcpp/spack.yaml
  full_name: openPMD/openPMD-api
  latest_release: 0.15.0
  readme: "<h1><a id=\"user-content-c--python-api-for-scientific-io-with-openpmd\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#c--python-api-for-scientific-io-with-openpmd\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>C++ &amp;\
    \ Python API for Scientific I/O with openPMD</h1>\n<p><a href=\"https://github.com/openPMD/openPMD-standard/releases\"\
    ><img src=\"https://camo.githubusercontent.com/5957dc11cb68f6705ef9ef6683152c6c4fcc057a24dff340e1e8bada1bee0d01/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6f70656e504d442d312e302e302d2d312e312e302d626c7565\"\
    \ alt=\"Supported openPMD Standard\" data-canonical-src=\"https://img.shields.io/badge/openPMD-1.0.0--1.1.0-blue\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://www.openpmd.org/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2d54fa5adcf7ca50d2cdb45823be5064379b613d526fa06f6e193ba2ce616318/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4150492d446f787967656e2d626c7565\"\
    \ alt=\"Doxygen\" data-canonical-src=\"https://img.shields.io/badge/API-Doxygen-blue\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://gitter.im/openPMD/API\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/f551141eea2176c34aa163092549d6fdde9a220d605d6efe9128b024c6e5932b/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f6f70656e504d442f415049\"\
    \ alt=\"Gitter chat\" data-canonical-src=\"https://img.shields.io/gitter/room/openPMD/API\"\
    \ style=\"max-width: 100%;\"></a>\n<a target=\"_blank\" rel=\"noopener noreferrer\
    \ nofollow\" href=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\"\
    ><img src=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\"\
    \ alt=\"Supported Platforms\" title=\"Supported Platforms\" data-canonical-src=\"\
    https://img.shields.io/badge/platforms-linux%20%7C%20osx%20%7C%20win-blue\" style=\"\
    max-width: 100%;\"></a>\n<a href=\"https://www.gnu.org/licenses/lgpl-3.0.html\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4fc8091716dbd967fa2a82eef3d29227110e5081f3128aaa38663e547c24f812/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4c47504c76332d626c7565\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/badge/license-LGPLv3-blue\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://doi.org/10.14278/rodare.27\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/104f6901ae04bff8385acc8273bb276ab84656a927a418108b940d09fd6e5d7c/68747470733a2f2f726f646172652e687a64722e64652f62616467652f444f492f31302e31343237382f726f646172652e32372e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://rodare.hzdr.de/badge/DOI/10.14278/rodare.27.svg\"\
    \ style=\"max-width: 100%;\"></a><br>\n<a href=\"https://www.codefactor.io/repository/github/openpmd/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0d568047fbbd300af56b766d9a4235a20534485f5827194e859d4f5c20e58334/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f6f70656e706d642f6f70656e706d642d6170692f6261646765\"\
    \ alt=\"CodeFactor\" data-canonical-src=\"https://www.codefactor.io/repository/github/openpmd/openpmd-api/badge\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://lgtm.com/projects/g/openPMD/openPMD-api/context:cpp\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/63a7f9e783999e3afc03ef38ee82e2048017e4e6d279ff4120ad8b8718480ccd/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f6370702f672f6f70656e504d442f6f70656e504d442d6170693f6c6f676f3d6c67746d266c6f676f57696474683d3138\"\
    \ alt=\"LGTM: C/C++\" data-canonical-src=\"https://img.shields.io/lgtm/grade/cpp/g/openPMD/openPMD-api?logo=lgtm&amp;logoWidth=18\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://lgtm.com/projects/g/openPMD/openPMD-api/context:python\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5046bf66a4612476a030d38de817c23fa03990183d2d74fa92c5f1379feb5d09/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f707974686f6e2f672f6f70656e504d442f6f70656e504d442d6170693f6c6f676f3d6c67746d266c6f676f57696474683d3138\"\
    \ alt=\"LGTM: Python\" data-canonical-src=\"https://img.shields.io/lgtm/grade/python/g/openPMD/openPMD-api?logo=lgtm&amp;logoWidth=18\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://lgtm.com/projects/g/openPMD/openPMD-api/alerts/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/85e32deb8face392eea9bfa2be4da4c11ca7c0f834fa069223fbc63758b68c4f/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f616c657274732f672f6f70656e504d442f6f70656e504d442d6170693f6c6f676f3d6c67746d266c6f676f57696474683d3138\"\
    \ alt=\"LGTM: Total alerts\" data-canonical-src=\"https://img.shields.io/lgtm/alerts/g/openPMD/openPMD-api?logo=lgtm&amp;logoWidth=18\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://coveralls.io/github/openPMD/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/f0487a8fc14d269210ae8ce80b556d4afcd93684f02e61386d2d02daa4423cbd/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f6f70656e504d442f6f70656e504d442d6170692f6261646765\"\
    \ alt=\"Coverage Status\" data-canonical-src=\"https://coveralls.io/repos/github/openPMD/openPMD-api/badge\"\
    \ style=\"max-width: 100%;\"></a><br>\n<a href=\"https://openpmd-api.readthedocs.io/en/latest/?badge=latest\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8f438ca4eaa308f982d0a28c48f05bf63ca1a86e52c3d2817f6d7559d1dee68e/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6f70656e706d642d6170692f62616467652f3f76657273696f6e3d6c6174657374\"\
    \ alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/openpmd-api/badge/?version=latest\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://travis-ci.com/openPMD/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8278d89e3634e25a818a2d673fe997c2aa5a09fd5995f716aeffa743388030ee/68747470733a2f2f7472617669732d63692e636f6d2f6f70656e504d442f6f70656e504d442d6170692e7376673f6272616e63683d646576\"\
    \ alt=\"Linux/OSX Build Status dev\" data-canonical-src=\"https://travis-ci.com/openPMD/openPMD-api.svg?branch=dev\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://ci.appveyor.com/project/ax3l/openpmd-api/branch/dev\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/30679331f3c6a5ae54970eda85f36a30347dee8a9111448d7aa48587fd524a1d/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f78393571346e36323070716b306530742f6272616e63682f6465763f7376673d74727565\"\
    \ alt=\"Windows Build Status dev\" data-canonical-src=\"https://ci.appveyor.com/api/projects/status/x95q4n620pqk0e0t/branch/dev?svg=true\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://github.com/openPMD/openPMD-api/actions?query=workflow%3Awheels\"\
    ><img src=\"https://github.com/openPMD/openPMD-api/workflows/wheels/badge.svg?branch=wheels&amp;event=push\"\
    \ alt=\"PyPI Wheel Release\" style=\"max-width: 100%;\"></a>\n<a href=\"https://dev.azure.com/axelhuebl/openPMD-api/_build/latest?definitionId=1&amp;branchName=azure_install\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/1fc9c860bb1fc8e7e145ea9dd6723b9ac6ac2743c195e5e899f6cfa3d38a5a69/68747470733a2f2f6465762e617a7572652e636f6d2f6178656c687565626c2f6f70656e504d442d6170692f5f617069732f6275696c642f7374617475732f6f70656e504d442e6f70656e504d442d6170693f6272616e63684e616d653d617a7572655f696e7374616c6c266c6162656c3d6e696768746c792532307061636b61676573\"\
    \ alt=\"Nightly Packages Status\" data-canonical-src=\"https://dev.azure.com/axelhuebl/openPMD-api/_apis/build/status/openPMD.openPMD-api?branchName=azure_install&amp;label=nightly%20packages\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://scan.coverity.com/projects/openpmd-openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0b068d7b5718aafccb46e2ee35f8249938ef189a36ba353c15222ed5e6f65e49/68747470733a2f2f7363616e2e636f7665726974792e636f6d2f70726f6a656374732f31373630322f62616467652e737667\"\
    \ alt=\"Coverity Scan Build Status\" data-canonical-src=\"https://scan.coverity.com/projects/17602/badge.svg\"\
    \ style=\"max-width: 100%;\"></a></p>\n<p>openPMD is an open meta-data schema\
    \ that provides meaning and self-description for data sets in science and engineering.\n\
    See <a href=\"https://github.com/openPMD/openPMD-standard\">the openPMD standard</a>\
    \ for details of this schema.</p>\n<p>This library provides a reference API for\
    \ openPMD data handling.\nSince openPMD is a schema (or markup) on top of portable,\
    \ hierarchical file formats, this library implements various backends such as\
    \ HDF5, ADIOS1, ADIOS2 and JSON.\nWriting &amp; reading through those backends\
    \ and their associated files are supported for serial and <a href=\"https://www.mpi-forum.org/docs/\"\
    \ rel=\"nofollow\">MPI-parallel</a> workflows.</p>\n<h2><a id=\"user-content-usage\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#usage\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Usage</h2>\n<h3><a id=\"user-content-c\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#c\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>C++</h3>\n<p><a href=\"https://isocpp.org/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8d47ea5fd5ff323ff5c76593ea37f2340533c73de5e6e37a2b27d7dc28070cd2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d4325324225324231372d79656c6c6f77677265656e\"\
    \ alt=\"C++17\" title=\"C++17 API\" data-canonical-src=\"https://img.shields.io/badge/language-C%2B%2B17-yellowgreen\"\
    \ style=\"max-width: 100%;\"></a> <a target=\"_blank\" rel=\"noopener noreferrer\
    \ nofollow\" href=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\"\
    ><img src=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\"\
    \ alt=\"C++17 API: Beta\" title=\"Status: Beta\" data-canonical-src=\"https://img.shields.io/badge/phase-beta-yellowgreen\"\
    \ style=\"max-width: 100%;\"></a></p>\n<div class=\"highlight highlight-source-c++\"\
    ><pre>#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"\
    pl-pds\">&lt;</span>openPMD/openPMD.hpp<span class=\"pl-pds\">&gt;</span></span>\n\
    #<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\"\
    >&lt;</span>iostream<span class=\"pl-pds\">&gt;</span></span>\n\n<span class=\"\
    pl-c\"><span class=\"pl-c\">//</span> ...</span>\n\n<span class=\"pl-k\">auto</span>\
    \ s = openPMD::Series(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>samples/git-sample/data%T.h5<span\
    \ class=\"pl-pds\">\"</span></span>, openPMD::Access::READ_ONLY);\n\n<span class=\"\
    pl-k\">for</span>( <span class=\"pl-k\">auto</span> <span class=\"pl-k\">const</span>\
    \ &amp; [step, it] : s.iterations ) {\n    std::cout &lt;&lt; <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>Iteration: <span class=\"pl-pds\">\"</span></span>\
    \ &lt;&lt; step &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span\
    \ class=\"pl-cce\">\\n</span><span class=\"pl-pds\">\"</span></span>;\n\n    <span\
    \ class=\"pl-k\">for</span>( <span class=\"pl-k\">auto</span> <span class=\"pl-k\"\
    >const</span> &amp; [name, mesh] : it.<span class=\"pl-smi\">meshes</span> ) {\n\
    \        std::cout &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>\
    \  Mesh '<span class=\"pl-pds\">\"</span></span> &lt;&lt; name &lt;&lt; <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>' attributes:<span class=\"pl-cce\"\
    >\\n</span><span class=\"pl-pds\">\"</span></span>;\n        <span class=\"pl-k\"\
    >for</span>( <span class=\"pl-k\">auto</span> <span class=\"pl-k\">const</span>&amp;\
    \ val : mesh.<span class=\"pl-c1\">attributes</span>() )\n            std::cout\
    \ &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    <span class=\"\
    pl-pds\">\"</span></span> &lt;&lt; val &lt;&lt; <span class=\"pl-s\"><span class=\"\
    pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>;\n\
    \    }\n\n    <span class=\"pl-k\">for</span>( <span class=\"pl-k\">auto</span>\
    \ <span class=\"pl-k\">const</span> &amp; [name, species] : it.<span class=\"\
    pl-smi\">particles</span> ) {\n        std::cout &lt;&lt; <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>  Particle species '<span class=\"pl-pds\">\"\
    </span></span> &lt;&lt; name &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>' attributes:<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\"\
    >\"</span></span>;\n        <span class=\"pl-k\">for</span>( <span class=\"pl-k\"\
    >auto</span> <span class=\"pl-k\">const</span>&amp; val : species.<span class=\"\
    pl-c1\">attributes</span>() )\n            std::cout &lt;&lt; <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>    <span class=\"pl-pds\">\"</span></span> &lt;&lt;\
    \ val &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"\
    pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>;\n    }\n}</pre></div>\n\
    <h3><a id=\"user-content-python\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #python\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Python</h3>\n\
    <p><a href=\"https://www.python.org/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/acd285afab5d7ddd4942e5215ade53e84551c9d7d635642ba92c19fde7d4345b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d507974686f6e332d79656c6c6f77677265656e\"\
    \ alt=\"Python3\" title=\"Python3 API\" data-canonical-src=\"https://img.shields.io/badge/language-Python3-yellowgreen\"\
    \ style=\"max-width: 100%;\"></a> <a target=\"_blank\" rel=\"noopener noreferrer\
    \ nofollow\" href=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\"\
    ><img src=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\"\
    \ alt=\"Python3 API: Beta\" title=\"Status: Beta\" data-canonical-src=\"https://img.shields.io/badge/phase-beta-yellowgreen\"\
    \ style=\"max-width: 100%;\"></a></p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">openpmd_api</span>\
    \ <span class=\"pl-k\">as</span> <span class=\"pl-s1\">io</span>\n\n<span class=\"\
    pl-c\"># ...</span>\n\n<span class=\"pl-s1\">series</span> <span class=\"pl-c1\"\
    >=</span> <span class=\"pl-s1\">io</span>.<span class=\"pl-v\">Series</span>(<span\
    \ class=\"pl-s\">\"samples/git-sample/data%T.h5\"</span>, <span class=\"pl-s1\"\
    >io</span>.<span class=\"pl-v\">Access</span>.<span class=\"pl-s1\">read_only</span>)\n\
    \n<span class=\"pl-k\">for</span> <span class=\"pl-s1\">k_i</span>, <span class=\"\
    pl-s1\">i</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">series</span>.<span\
    \ class=\"pl-s1\">iterations</span>.<span class=\"pl-en\">items</span>():\n  \
    \  <span class=\"pl-en\">print</span>(<span class=\"pl-s\">\"Iteration: {0}\"\
    </span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s1\">k_i</span>))\n\
    \n    <span class=\"pl-k\">for</span> <span class=\"pl-s1\">k_m</span>, <span\
    \ class=\"pl-s1\">m</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\"\
    >i</span>.<span class=\"pl-s1\">meshes</span>.<span class=\"pl-en\">items</span>():\n\
    \        <span class=\"pl-en\">print</span>(<span class=\"pl-s\">\"  Mesh '{0}'\
    \ attributes:\"</span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s1\"\
    >k_m</span>))\n        <span class=\"pl-k\">for</span> <span class=\"pl-s1\">a</span>\
    \ <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">m</span>.<span class=\"\
    pl-s1\">attributes</span>:\n            <span class=\"pl-en\">print</span>(<span\
    \ class=\"pl-s\">\"    {0}\"</span>.<span class=\"pl-en\">format</span>(<span\
    \ class=\"pl-s1\">a</span>))\n\n    <span class=\"pl-k\">for</span> <span class=\"\
    pl-s1\">k_p</span>, <span class=\"pl-s1\">p</span> <span class=\"pl-c1\">in</span>\
    \ <span class=\"pl-s1\">i</span>.<span class=\"pl-s1\">particles</span>.<span\
    \ class=\"pl-en\">items</span>():\n        <span class=\"pl-en\">print</span>(<span\
    \ class=\"pl-s\">\"  Particle species '{0}' attributes:\"</span>.<span class=\"\
    pl-en\">format</span>(<span class=\"pl-s1\">k_p</span>))\n        <span class=\"\
    pl-k\">for</span> <span class=\"pl-s1\">a</span> <span class=\"pl-c1\">in</span>\
    \ <span class=\"pl-s1\">p</span>.<span class=\"pl-s1\">attributes</span>:\n  \
    \          <span class=\"pl-en\">print</span>(<span class=\"pl-s\">\"    {0}\"\
    </span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s1\">a</span>))</pre></div>\n\
    <h3><a id=\"user-content-more\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #more\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>More!</h3>\n\
    <p>Curious?\nOur manual shows full <a href=\"https://openpmd-api.readthedocs.io/en/latest/usage/firstwrite.html\"\
    \ rel=\"nofollow\">read &amp; write examples</a>, both serial and MPI-parallel!</p>\n\
    <h2><a id=\"user-content-dependencies\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#dependencies\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Dependencies</h2>\n<p>Required:</p>\n<ul>\n<li>CMake 3.15.0+</li>\n\
    <li>C++17 capable compiler, e.g., g++ 7+, clang 7+, MSVC 19.15+, icpc 19+, icpx</li>\n\
    </ul>\n<p>Shipped internally in <code>share/openPMD/thirdParty/</code>:</p>\n\
    <ul>\n<li>\n<a href=\"https://github.com/catchorg/Catch2\">Catch2</a> 2.13.10+\
    \ (<a href=\"https://github.com/catchorg/Catch2/blob/master/LICENSE.txt\">BSL-1.0</a>)</li>\n\
    <li>\n<a href=\"https://github.com/pybind/pybind11\">pybind11</a> 2.10.1+ (<a\
    \ href=\"https://github.com/pybind/pybind11/blob/master/LICENSE\">new BSD</a>)</li>\n\
    <li>\n<a href=\"https://github.com/nlohmann/json\">NLohmann-JSON</a> 3.9.1+ (<a\
    \ href=\"https://github.com/nlohmann/json/blob/develop/LICENSE.MIT\">MIT</a>)</li>\n\
    <li>\n<a href=\"https://github.com/ToruNiina/toml11\">toml11</a> 3.7.1+ (<a href=\"\
    https://github.com/ToruNiina/toml11/blob/master/LICENSE\">MIT</a>)</li>\n</ul>\n\
    <p>I/O backends:</p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/JSON\"\
    \ rel=\"nofollow\">JSON</a></li>\n<li>\n<a href=\"https://support.hdfgroup.org/HDF5\"\
    \ rel=\"nofollow\">HDF5</a> 1.8.13+ (optional)</li>\n<li>\n<a href=\"https://www.olcf.ornl.gov/center-projects/adios\"\
    \ rel=\"nofollow\">ADIOS1</a> 1.13.1+ (optional, deprecated)</li>\n<li>\n<a href=\"\
    https://github.com/ornladios/ADIOS2\">ADIOS2</a> 2.7.0+ (optional)</li>\n</ul>\n\
    <p>while those can be built either with or without:</p>\n<ul>\n<li>MPI 2.1+, e.g.\
    \ OpenMPI 1.6.5+ or MPICH2</li>\n</ul>\n<p>Optional language bindings:</p>\n<ul>\n\
    <li>Python:\n<ul>\n<li>Python 3.7 - 3.11</li>\n<li>pybind11 2.10.1+</li>\n<li>numpy\
    \ 1.15+</li>\n<li>mpi4py 2.1+ (optional, for MPI)</li>\n<li>pandas 1.0+ (optional,\
    \ for dataframes)</li>\n<li>dask 2021+ (optional, for dask dataframes)</li>\n\
    </ul>\n</li>\n<li>CUDA C++ (optional, currently used only in tests)</li>\n</ul>\n\
    <h2><a id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#installation\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<p><a href=\"https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/580cdb6331254f66680bd967326d9630f5124291efd5ace18549fbb93cbae6db/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f737061636b2e696f2d6f70656e706d642d2d6170692d627269676874677265656e\"\
    \ alt=\"Spack Package\" data-canonical-src=\"https://img.shields.io/badge/spack.io-openpmd--api-brightgreen\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://anaconda.org/conda-forge/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3c3728308a9e416589fa8b3b8168967287c515037bfbd4b40f1b14f266de56fb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e64612e696f2d6f70656e706d642d2d6170692d627269676874677265656e\"\
    \ alt=\"Conda Package\" data-canonical-src=\"https://img.shields.io/badge/conda.io-openpmd--api-brightgreen\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://github.com/openPMD/homebrew-openPMD\"\
    ><img src=\"https://camo.githubusercontent.com/92b7b7bb69b2b17d1981ae5fdcdb32f971ee57f54a98ea4804e93fd0a2eb58ef/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f627265772e73682d6f70656e706d642d2d6170692d627269676874677265656e\"\
    \ alt=\"Brew Package\" data-canonical-src=\"https://img.shields.io/badge/brew.sh-openpmd--api-brightgreen\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://pypi.org/project/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4eeb2c48c0e554ea8dbe8e90f73a6f2d217e775e0bd5e5cb90ddf4619ef3a6a0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707970692e6f72672d6f70656e706d642d2d6170692d627269676874677265656e\"\
    \ alt=\"PyPI Package\" data-canonical-src=\"https://img.shields.io/badge/pypi.org-openpmd--api-brightgreen\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://cmake.org\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/2babf79954ea524c24f2f9b1cfa05728fdaccbe0a80c2da6a7a7595cc131894c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f66726f6d5f736f757263652d434d616b652d627269676874677265656e\"\
    \ alt=\"From Source\" data-canonical-src=\"https://img.shields.io/badge/from_source-CMake-brightgreen\"\
    \ style=\"max-width: 100%;\"></a></p>\n<p>Our community loves to help each other.\n\
    Please <a href=\"https://github.com/openPMD/openPMD-api/issues/new?labels=install&amp;template=install_problem.md\"\
    >report installation problems</a> in case you should get stuck.</p>\n<p>Choose\
    \ <em>one</em> of the install methods below to get started:</p>\n<h3><a id=\"\
    user-content-spack\" class=\"anchor\" aria-hidden=\"true\" href=\"#spack\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"https://spack.io\"\
    \ rel=\"nofollow\">Spack</a></h3>\n<p><a href=\"https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/becffbecb6a50502286f02ec86c4e62f239f3671148d11341152e0dc695a2fc9/68747470733a2f2f696d672e736869656c64732e696f2f737061636b2f762f6f70656e706d642d617069\"\
    \ alt=\"Spack Version\" data-canonical-src=\"https://img.shields.io/spack/v/openpmd-api\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://spack.io\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/ceb548d628bb13bf7eed61d8bb9292368f02fb952a549f53c7f34c8afad3ed11/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f73782532302d626c7565\"\
    \ alt=\"Spack Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20-blue\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b29fc54abf92a2a6d1d2c70159eb276df53b67a1294ae3f82b1b0bf22a3c586b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f253238432532422532422c5f70792532392c5f646576656c6f706d656e742c5f4850432d627269676874677265656e\"\
    \ alt=\"Spack Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28C%2B%2B,_py%29,_development,_HPC-brightgreen\"\
    \ style=\"max-width: 100%;\"></a></p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> optional:           \
    \    +python +adios1 -adios2 -hdf5 -mpi</span>\nspack install openpmd-api\nspack\
    \ load openpmd-api</pre></div>\n<h3><a id=\"user-content-conda\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#conda\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><a href=\"https://conda.io\" rel=\"nofollow\">Conda</a></h3>\n\
    <p><a href=\"https://anaconda.org/conda-forge/openpmd-api\" rel=\"nofollow\"><img\
    \ src=\"https://camo.githubusercontent.com/3ad2b345bb3589aa2d733ca20df72938ed54a48342b69f7cbe9eacab43d84549/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f766e2f636f6e64612d666f7267652f6f70656e706d642d617069\"\
    \ alt=\"Conda Version\" data-canonical-src=\"https://img.shields.io/conda/vn/conda-forge/openpmd-api\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://anaconda.org/conda-forge/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\"\
    \ alt=\"Conda Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20%7C%20win-blue\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://anaconda.org/conda-forge/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a422d06dfefa14bef9b5ad8fbd0df126763371e6f988172ee60ff850303a4fa4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f25323870792532392d627269676874677265656e\"\
    \ alt=\"Conda Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28py%29-brightgreen\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://anaconda.org/conda-forge/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a51d3cd2bfa3c6b01bd0f2e36abc206fb9db5700105fac2acd2c2105fced2ec4/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f646e2f636f6e64612d666f7267652f6f70656e706d642d617069\"\
    \ alt=\"Conda Downloads\" data-canonical-src=\"https://img.shields.io/conda/dn/conda-forge/openpmd-api\"\
    \ style=\"max-width: 100%;\"></a></p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> optional:           \
    \           OpenMPI support  =*=mpi_openmpi*</span>\n<span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> optional:                        MPICH support  =*=mpi_mpich*</span>\n\
    conda create -n openpmd -c conda-forge openpmd-api\nconda activate openpmd</pre></div>\n\
    <h3><a id=\"user-content-brew\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #brew\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a\
    \ href=\"https://brew.sh\" rel=\"nofollow\">Brew</a></h3>\n<p><a href=\"https://github.com/openPMD/homebrew-openPMD\"\
    ><img src=\"https://camo.githubusercontent.com/d873c8c473fece8abf1c34a8299a50b7ee0da9772eb50cce8649e7ca32468a6c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f627265772d6c61746573745f76657273696f6e2d6f72616e6765\"\
    \ alt=\"Brew Version\" data-canonical-src=\"https://img.shields.io/badge/brew-latest_version-orange\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://docs.brew.sh/Homebrew-on-Linux\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ceb548d628bb13bf7eed61d8bb9292368f02fb952a549f53c7f34c8afad3ed11/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f73782532302d626c7565\"\
    \ alt=\"Brew Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20-blue\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://brew.sh\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/7b767b67facc26db7d850ae5c3155fa949048991dde7a0f5471c1e6862f0a586/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f253238432532422532422c5f70792532392d627269676874677265656e\"\
    \ alt=\"Brew Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28C%2B%2B,_py%29-brightgreen\"\
    \ style=\"max-width: 100%;\"></a></p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>brew tap openpmd/openpmd\nbrew install openpmd-api</pre></div>\n<h3><a id=\"\
    user-content-pypi\" class=\"anchor\" aria-hidden=\"true\" href=\"#pypi\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"https://pypi.org\"\
    \ rel=\"nofollow\">PyPI</a></h3>\n<p><a href=\"https://pypi.org/project/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/fc28bd368523d0b8adab5f4b414aebb304743130eef42bca203f4e9eed428ae7/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6f70656e504d442d617069\"\
    \ alt=\"PyPI Version\" data-canonical-src=\"https://img.shields.io/pypi/v/openPMD-api\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://pypi.org/project/openPMD-api/#files\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\"\
    \ alt=\"PyPI Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20%7C%20win-blue\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://pypi.org/project/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a422d06dfefa14bef9b5ad8fbd0df126763371e6f988172ee60ff850303a4fa4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f25323870792532392d627269676874677265656e\"\
    \ alt=\"PyPI Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28py%29-brightgreen\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://pypi.org/project/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7cef25e887c028f65d5d7e20f3e7abe394ab328ecb499e34e47460afd2c78558/68747470733a2f2f696d672e736869656c64732e696f2f707970692f666f726d61742f6f70656e504d442d617069\"\
    \ alt=\"PyPI Format\" data-canonical-src=\"https://img.shields.io/pypi/format/openPMD-api\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://pypi.org/project/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/82acdd95515851a5ba4a3006871151f76cfe4d6583f6c24e80bd0fd29d391845/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6f70656e504d442d617069\"\
    \ alt=\"PyPI Downloads\" data-canonical-src=\"https://img.shields.io/pypi/dm/openPMD-api\"\
    \ style=\"max-width: 100%;\"></a></p>\n<p>On very old macOS versions (&lt;10.9)\
    \ or on exotic processor architectures, this install method <em>compiles from\
    \ source</em> against the found installations of HDF5, ADIOS1, ADIOS2, and/or\
    \ MPI (in system paths, from other package managers, or loaded via a module system,\
    \ ...).</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> we need pip 19 or newer</span>\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> optional:                   --user</span>\n\
    python3 -m pip install -U pip\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ optional:                        --user</span>\npython3 -m pip install openpmd-api</pre></div>\n\
    <p>If MPI-support shall be enabled, we always have to recompile:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> optional:                                    --user</span>\npython3\
    \ -m pip install -U pip setuptools wheel\npython3 -m pip install -U cmake\n\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> optional:                 \
    \                                                  --user</span>\nopenPMD_USE_MPI=ON\
    \ python3 -m pip install openpmd-api --no-binary openpmd-api</pre></div>\n<p>For\
    \ some exotic architectures and compilers, you might need to disable a compiler\
    \ feature called <a href=\"https://en.wikipedia.org/wiki/Interprocedural_optimization\"\
    \ rel=\"nofollow\">link-time/interprocedural optimization</a> if you encounter\
    \ linking problems:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-k\">export</span> CMAKE_INTERPROCEDURAL_OPTIMIZATION=OFF\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> optional:                               \
    \                 --user</span>\npython3 -m pip install openpmd-api --no-binary\
    \ openpmd-api</pre></div>\n<p>Additional CMake options can be passed via individual\
    \ environment variables, which need to be prefixed with <code>openPMD_CMAKE_</code>.</p>\n\
    <h3><a id=\"user-content-from-source\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #from-source\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>From\
    \ Source</h3>\n<p><a href=\"https://cmake.org\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0a40953107090abff3b564ec3436668756b873cd4d9e021738a046781a5a0e6b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d646576656c6f706d656e742d627269676874677265656e\"\
    \ alt=\"Source Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-development-brightgreen\"\
    \ style=\"max-width: 100%;\"></a></p>\n<p>openPMD-api can also be built and installed\
    \ from source using <a href=\"https://cmake.org/\" rel=\"nofollow\">CMake</a>:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>git clone https://github.com/openPMD/openPMD-api.git\n\
    \nmkdir openPMD-api-build\n<span class=\"pl-c1\">cd</span> openPMD-api-build\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> optional: for full tests,\
    \ with unzip</span>\n../openPMD-api/share/openPMD/download_samples.sh\n\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> for own install prefix append:</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span>   -DCMAKE_INSTALL_PREFIX=$HOME/somepath</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> for options append:</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span>   -DopenPMD_USE_...=...</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> e.g. for python support add:</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span>   -DopenPMD_USE_PYTHON=ON -DPython_EXECUTABLE=$(which\
    \ python3)</span>\ncmake ../openPMD-api\n\ncmake --build <span class=\"pl-c1\"\
    >.</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> optional</span>\n\
    ctest\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> sudo might be required\
    \ for system paths</span>\ncmake --build <span class=\"pl-c1\">.</span> --target\
    \ install</pre></div>\n<p>The following options can be added to the <code>cmake</code>\
    \ call to control features.\nCMake controls options with prefixed <code>-D</code>,\
    \ e.g. <code>-DopenPMD_USE_MPI=OFF</code>:</p>\n<table>\n<thead>\n<tr>\n<th>CMake\
    \ Option</th>\n<th>Values</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n\
    <tr>\n<td><code>openPMD_USE_MPI</code></td>\n<td>\n<strong>AUTO</strong>/ON/OFF</td>\n\
    <td>Parallel, Multi-Node I/O for clusters</td>\n</tr>\n<tr>\n<td><code>openPMD_USE_HDF5</code></td>\n\
    <td>\n<strong>AUTO</strong>/ON/OFF</td>\n<td>HDF5 backend (<code>.h5</code> files)</td>\n\
    </tr>\n<tr>\n<td><code>openPMD_USE_ADIOS1</code></td>\n<td>AUTO/ON/<strong>OFF</strong>\n\
    </td>\n<td>ADIOS1 backend (<code>.bp</code> files up to version BP3) - deprecated</td>\n\
    </tr>\n<tr>\n<td><code>openPMD_USE_ADIOS2</code></td>\n<td>\n<strong>AUTO</strong>/ON/OFF</td>\n\
    <td>ADIOS2 backend (<code>.bp</code> files in BP3, BP4 or higher)</td>\n</tr>\n\
    <tr>\n<td><code>openPMD_USE_PYTHON</code></td>\n<td>\n<strong>AUTO</strong>/ON/OFF</td>\n\
    <td>Enable Python bindings</td>\n</tr>\n<tr>\n<td><code>openPMD_USE_INVASIVE_TESTS</code></td>\n\
    <td>ON/<strong>OFF</strong>\n</td>\n<td>Enable unit tests that modify source code\
    \ <sup>1</sup>\n</td>\n</tr>\n<tr>\n<td><code>openPMD_USE_VERIFY</code></td>\n\
    <td>\n<strong>ON</strong>/OFF</td>\n<td>Enable internal VERIFY (assert) macro\
    \ independent of build type <sup>2</sup>\n</td>\n</tr>\n<tr>\n<td><code>openPMD_INSTALL</code></td>\n\
    <td>\n<strong>ON</strong>/OFF</td>\n<td>Add installation targets</td>\n</tr>\n\
    <tr>\n<td><code>openPMD_INSTALL_RPATH</code></td>\n<td>\n<strong>ON</strong>/OFF</td>\n\
    <td>Add RPATHs to installed binaries</td>\n</tr>\n<tr>\n<td><code>Python_EXECUTABLE</code></td>\n\
    <td>(newest found)</td>\n<td>Path to Python executable</td>\n</tr>\n</tbody>\n\
    </table>\n<p><sup>1</sup> <em>e.g. changes C++ visibility keywords, breaks MSVC</em>\n\
    <sup>2</sup> <em>this includes most pre-/post-condition checks, disabling without\
    \ specific cause is highly discouraged</em></p>\n<p>Additionally, the following\
    \ libraries are shipped internally.\nThe following options allow to switch to\
    \ external installs:</p>\n<table>\n<thead>\n<tr>\n<th>CMake Option</th>\n<th>Values</th>\n\
    <th>Library</th>\n<th>Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>openPMD_USE_INTERNAL_CATCH</code></td>\n\
    <td>\n<strong>ON</strong>/OFF</td>\n<td>Catch2</td>\n<td>2.13.10+</td>\n</tr>\n\
    <tr>\n<td><code>openPMD_USE_INTERNAL_PYBIND11</code></td>\n<td>\n<strong>ON</strong>/OFF</td>\n\
    <td>pybind11</td>\n<td>2.10.1+</td>\n</tr>\n<tr>\n<td><code>openPMD_USE_INTERNAL_JSON</code></td>\n\
    <td>\n<strong>ON</strong>/OFF</td>\n<td>NLohmann-JSON</td>\n<td>3.9.1+</td>\n\
    </tr>\n<tr>\n<td><code>openPMD_USE_INTERNAL_TOML11</code></td>\n<td>\n<strong>ON</strong>/OFF</td>\n\
    <td>toml11</td>\n<td>3.7.1+</td>\n</tr>\n</tbody>\n</table>\n<p>By default, this\
    \ will build as a shared library (<code>libopenPMD.[so|dylib|dll]</code>) and\
    \ installs also its headers.\nIn order to build a static library, append <code>-DBUILD_SHARED_LIBS=OFF</code>\
    \ to the <code>cmake</code> command.\nYou can only build a static or a shared\
    \ library at a time.</p>\n<p>By default, the <code>Release</code> version is built.\n\
    In order to build with debug symbols, pass <code>-DCMAKE_BUILD_TYPE=Debug</code>\
    \ to your <code>cmake</code> command.</p>\n<p>By default, tests, examples and\
    \ command line tools are built.\nIn order to skip building those, pass <code>OFF</code>\
    \ to these <code>cmake</code> options:</p>\n<table>\n<thead>\n<tr>\n<th>CMake\
    \ Option</th>\n<th>Values</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n\
    <tr>\n<td><code>openPMD_BUILD_TESTING</code></td>\n<td>\n<strong>ON</strong>/OFF</td>\n\
    <td>Build tests</td>\n</tr>\n<tr>\n<td><code>openPMD_BUILD_EXAMPLES</code></td>\n\
    <td>\n<strong>ON</strong>/OFF</td>\n<td>Build examples</td>\n</tr>\n<tr>\n<td><code>openPMD_BUILD_CLI_TOOLS</code></td>\n\
    <td>\n<strong>ON</strong>/OFF</td>\n<td>Build command-line tools</td>\n</tr>\n\
    <tr>\n<td><code>openPMD_USE_CUDA_EXAMPLES</code></td>\n<td>ON/<strong>OFF</strong>\n\
    </td>\n<td>Use CUDA in examples</td>\n</tr>\n</tbody>\n</table>\n<h2><a id=\"\
    user-content-linking-to-your-project\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #linking-to-your-project\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Linking to your project</h2>\n<p>The install will contain header files\
    \ and libraries in the path set with <code>-DCMAKE_INSTALL_PREFIX</code>.</p>\n\
    <h3><a id=\"user-content-cmake\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #cmake\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>CMake</h3>\n\
    <p>If your project is using CMake for its build, one can conveniently use our\
    \ provided <code>openPMDConfig.cmake</code> package which is installed alongside\
    \ the library.</p>\n<p>First set the following environment hint if openPMD-api\
    \ was <em>not</em> installed in a system path:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> optional: only needed\
    \ if installed outside of system paths</span>\n<span class=\"pl-k\">export</span>\
    \ CMAKE_PREFIX_PATH=<span class=\"pl-smi\">$HOME</span>/somepath:<span class=\"\
    pl-smi\">$CMAKE_PREFIX_PATH</span></pre></div>\n<p>Use the following lines in\
    \ your project's <code>CMakeLists.txt</code>:</p>\n<div class=\"highlight highlight-source-cmake\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> supports:           \
    \            COMPONENTS MPI NOMPI HDF5 ADIOS1 ADIOS2</span>\n<span class=\"pl-c1\"\
    >find_package</span>(openPMD 0.9.0 <span class=\"pl-k\">CONFIG</span>)\n\n<span\
    \ class=\"pl-k\">if</span>(openPMD_FOUND)\n    <span class=\"pl-c1\">target_link_libraries</span>(YourTarget\
    \ <span class=\"pl-k\">PRIVATE</span> openPMD::openPMD)\n<span class=\"pl-k\"\
    >endif</span>()</pre></div>\n<p><em>Alternatively</em>, add the openPMD-api repository\
    \ source directly to your project and use it via:</p>\n<div class=\"highlight\
    \ highlight-source-cmake\"><pre><span class=\"pl-c1\">add_subdirectory</span>(<span\
    \ class=\"pl-s\">\"path/to/source/of/openPMD-api\"</span>)\n\n<span class=\"pl-c1\"\
    >target_link_libraries</span>(YourTarget <span class=\"pl-k\">PRIVATE</span> openPMD::openPMD)</pre></div>\n\
    <p>For development workflows, you can even automatically download and build openPMD-api\
    \ from within a depending CMake project.\nJust replace the <code>add_subdirectory</code>\
    \ call with:</p>\n<div class=\"highlight highlight-source-cmake\"><pre><span class=\"\
    pl-c1\">include</span>(FetchContent)\n<span class=\"pl-c1\">set</span>(CMAKE_POLICY_DEFAULT_CMP0077\
    \ <span class=\"pl-k\">NEW</span>)\n<span class=\"pl-c1\">set</span>(openPMD_BUILD_CLI_TOOLS\
    \ <span class=\"pl-k\">OFF</span>)\n<span class=\"pl-c1\">set</span>(openPMD_BUILD_EXAMPLES\
    \ <span class=\"pl-k\">OFF</span>)\n<span class=\"pl-c1\">set</span>(openPMD_BUILD_TESTING\
    \ <span class=\"pl-k\">OFF</span>)\n<span class=\"pl-c1\">set</span>(openPMD_BUILD_SHARED_LIBS\
    \ <span class=\"pl-k\">OFF</span>)  <span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> precedence over BUILD_SHARED_LIBS if needed</span>\n<span class=\"pl-c1\"\
    >set</span>(openPMD_INSTALL <span class=\"pl-k\">OFF</span>)            <span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> or instead use:</span>\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> set(openPMD_INSTALL ${BUILD_SHARED_LIBS})\
    \  # only install if used as a shared library</span>\n<span class=\"pl-c1\">set</span>(openPMD_USE_PYTHON\
    \ <span class=\"pl-k\">OFF</span>)\nFetchContent_Declare(openPMD\n  GIT_REPOSITORY\
    \ <span class=\"pl-s\">\"https://github.com/openPMD/openPMD-api.git\"</span>\n\
    \  GIT_TAG        <span class=\"pl-s\">\"dev\"</span>)\nFetchContent_MakeAvailable(openPMD)</pre></div>\n\
    <h3><a id=\"user-content-manually\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #manually\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Manually</h3>\n\
    <p>If your (Linux/OSX) project is build by calling the compiler directly or uses\
    \ a manually written <code>Makefile</code>, consider using our <code>openPMD.pc</code>\
    \ helper file for <code>pkg-config</code> which are installed alongside the library.</p>\n\
    <p>First set the following environment hint if openPMD-api was <em>not</em> installed\
    \ in a system path:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> optional: only needed if installed\
    \ outside of system paths</span>\n<span class=\"pl-k\">export</span> PKG_CONFIG_PATH=<span\
    \ class=\"pl-smi\">$HOME</span>/somepath/lib/pkgconfig:<span class=\"pl-smi\"\
    >$PKG_CONFIG_PATH</span></pre></div>\n<p>Additional linker and compiler flags\
    \ for your project are available via:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> switch to check if openPMD-api\
    \ was build as static library</span>\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> (via BUILD_SHARED_LIBS=OFF) or as shared library (default)</span>\n\
    <span class=\"pl-k\">if</span> [ <span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span><span class=\"pl-s\"><span class=\"pl-pds\">$(</span>pkg-config --variable=static\
    \ openPMD<span class=\"pl-pds\">)</span></span><span class=\"pl-pds\">\"</span></span>\
    \ <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span>true<span class=\"pl-pds\">\"</span></span> ]\n<span class=\"pl-k\">then</span>\n\
    \    pkg-config --libs --static openPMD\n    <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> -L/usr/local/lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lopenPMD\
    \ -pthread /usr/lib/libmpi.so -pthread /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so\
    \ /usr/lib/libmpi.so /usr/lib/x86_64-linux-gnu/hdf5/openmpi/libhdf5.so /usr/lib/x86_64-linux-gnu/libsz.so\
    \ /usr/lib/x86_64-linux-gnu/libz.so /usr/lib/x86_64-linux-gnu/libdl.so /usr/lib/x86_64-linux-gnu/libm.so\
    \ -pthread /usr/lib/libmpi.so -pthread /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so\
    \ /usr/lib/libmpi.so</span>\n<span class=\"pl-k\">else</span>\n    pkg-config\
    \ --libs openPMD\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> -L${HOME}/somepath/lib\
    \ -lopenPMD</span>\n<span class=\"pl-k\">fi</span>\n\npkg-config --cflags openPMD\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> -I${HOME}/somepath/include</span></pre></div>\n\
    <h2><a id=\"user-content-author-contributions\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#author-contributions\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Author Contributions</h2>\n<p>openPMD-api is developed\
    \ by many people.\nIt was initially started by the <a href=\"https://hzdr.de/crp\"\
    \ rel=\"nofollow\">Computational Radiation Physics Group</a> at <a href=\"https://www.hzdr.de/\"\
    \ rel=\"nofollow\">HZDR</a> as successor to <a href=\"https://github.com/ComputationalRadiationPhysics/libSplash/\"\
    >libSplash</a>, generalizing the <a href=\"https://arxiv.org/abs/1706.00522\"\
    \ rel=\"nofollow\">successful HDF5 &amp; ADIOS1 implementations</a> in <a href=\"\
    https://github.com/ComputationalRadiationPhysics/picongpu\">PIConGPU</a>.\nThe\
    \ following people and institutions <a href=\"https://github.com/openPMD/openPMD-api/graphs/contributors\"\
    >contributed</a> to openPMD-api:</p>\n<ul>\n<li>\n<a href=\"https://github.com/ax3l\"\
    >Axel Huebl (HZDR, now LBNL)</a>:\nproject lead, releases, documentation, automated\
    \ CI/CD, Python bindings, Dask, installation &amp; packaging, prior reference\
    \ implementations</li>\n<li>\n<a href=\"https://github.com/franzpoeschel\">Franz\
    \ Poeschel (CASUS)</a>:\nJSON &amp; ADIOS2 backend, data staging/streaming, reworked\
    \ class design</li>\n<li>\n<a href=\"https://github.com/C0nsultant\">Fabian Koller\
    \ (HZDR)</a>:\ninitial library design and implementation with HDF5 &amp; ADIOS1\
    \ backend</li>\n<li>\n<a href=\"https://github.com/guj\">Junmin Gu (LBNL)</a>:\n\
    non-collective parallel I/O fixes, ADIOS improvements, benchmarks</li>\n</ul>\n\
    <p>Further thanks go to improvements and contributions from:</p>\n<ul>\n<li>\n\
    <a href=\"https://github.com/CFGrote\">Carsten Fortmann-Grote (EU XFEL GmbH, now\
    \ MPI-EvolBio)</a>:\ndraft of our Python unit tests</li>\n<li>\n<a href=\"https://github.com/StanczakDominik\"\
    >Dominik Sta\u0144czak (Warsaw University of Technology)</a>:\ndocumentation improvements</li>\n\
    <li>\n<a href=\"https://github.com/mingwandroid\">Ray Donnelly (Anaconda, Inc.)</a>:\n\
    support on conda packaging and libc++ quirks</li>\n<li>\n<a href=\"https://github.com/amundson\"\
    >James Amundson (FNAL)</a>:\ncompile fix for newer compilers</li>\n<li>\n<a href=\"\
    https://github.com/psychocoderHPC\">Ren\xE9 Widera (HZDR)</a>:\ndesign improvements\
    \ for initial API design</li>\n<li>\n<a href=\"https://github.com/erikzenker\"\
    >Erik Zenker (HZDR)</a>:\ndesign improvements for initial API design</li>\n<li>\n\
    <a href=\"https://github.com/sbastrakov\">Sergei Bastrakov (HZDR)</a>:\ndocumentation\
    \ improvements (windows)</li>\n<li>\n<a href=\"https://github.com/RemiLehe\">R\xE9\
    mi Lehe (LBNL)</a>:\npackage integration testing on macOS and Linux</li>\n<li>\n\
    <a href=\"https://github.com/LDAmorim\">L\xEDgia Diana Amorim (LBNL)</a>:\npackage\
    \ integration testing on macOS</li>\n<li>\n<a href=\"https://github.com/KseniaBastrakova\"\
    >Kseniia Bastrakova (HZDR)</a>:\ncompatibility testing</li>\n<li>\n<a href=\"\
    https://github.com/PrometheusPi\">Richard Pausch (HZDR)</a>:\ncompatibility testing,\
    \ documentation improvements</li>\n<li>\n<a href=\"https://github.com/pordyna\"\
    >Pawe\u0142 Ordyna (HZDR)</a>:\nreport on NVCC warnings</li>\n<li>\n<a href=\"\
    https://github.com/dmitry-ganyushin\">Dmitry Ganyushin (ORNL)</a>:\nDask prototyping\
    \ &amp; ADIOS2 benchmarking</li>\n<li>\n<a href=\"https://github.com/jakirkham\"\
    >John Kirkham (NVIDIA)</a>:\nDask guidance &amp; reviews</li>\n<li>\n<a href=\"\
    https://github.com/eschnett\">Erik Schnetter (PITP)</a>:\nC++ API bug fixes</li>\n\
    <li>\n<a href=\"https://github.com/jeanbez\">Jean Luca Bez (LBNL)</a>:\nHDF5 performance\
    \ tuning</li>\n<li>\n<a href=\"https://github.com/bernhardmgruber\">Bernhard Manfred\
    \ Gruber (CERN)</a>:\nCMake fix for parallel HDF5</li>\n</ul>\n<h3><a id=\"user-content-grants\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#grants\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Grants</h3>\n<p>The openPMD-api\
    \ authors acknowledge support via the following programs.\nThis project has received\
    \ funding from the European Unions Horizon 2020 research and innovation programme\
    \ under grant agreement No 654220.\nSupported by the Consortium for Advanced Modeling\
    \ of Particles Accelerators (CAMPA), funded by the U.S. DOE Office of Science\
    \ under Contract No. DE-AC02-05CH11231.\nSupported by the Exascale Computing Project\
    \ (17-SC-20-SC), a collaborative effort of two U.S. Department of Energy organizations\
    \ (Office of Science and the National Nuclear Security Administration).\nThis\
    \ work was partially funded by the Center of Advanced Systems Understanding (CASUS),\
    \ which is financed by Germany's Federal Ministry of Education and Research (BMBF)\
    \ and by the Saxon Ministry for Science, Culture and Tourism (SMWK) with tax funds\
    \ on the basis of the budget approved by the Saxon State Parliament.</p>\n<h3><a\
    \ id=\"user-content-transitive-contributions\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#transitive-contributions\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Transitive Contributions</h3>\n<p>openPMD-api\
    \ stands on the shoulders of giants and we are grateful for the following projects\
    \ included as direct dependencies:</p>\n<ul>\n<li>\n<a href=\"https://github.com/ornladios/ADIOS\"\
    >ADIOS1</a> and <a href=\"https://github.com/ornladios/ADIOS2\">ADIOS2</a> by\
    \ <a href=\"https://csmd.ornl.gov/adios\" rel=\"nofollow\">S. Klasky (ORNL), team,\
    \ collaborators</a> and <a href=\"https://github.com/ornladios/ADIOS2/graphs/contributors\"\
    >contributors</a>\n</li>\n<li>\n<a href=\"https://github.com/catchorg/Catch2\"\
    >Catch2</a> by <a href=\"https://github.com/philsquared\">Phil Nash</a>, <a href=\"\
    https://github.com/horenmar\">Martin Ho\u0159e\u0148ovsk\xFD</a> and <a href=\"\
    https://github.com/catchorg/Catch2/graphs/contributors\">contributors</a>\n</li>\n\
    <li>HDF5 by <a href=\"https://www.hdfgroup.org\" rel=\"nofollow\">the HDF group</a>\
    \ and community</li>\n<li>\n<a href=\"https://github.com/nlohmann/json\">json</a>\
    \ by <a href=\"https://github.com/nlohmann\">Niels Lohmann</a> and <a href=\"\
    https://github.com/nlohmann/json/graphs/contributors\">contributors</a>\n</li>\n\
    <li>\n<a href=\"https://github.com/ToruNiina/toml11\">toml11</a> by <a href=\"\
    https://github.com/ToruNiina\">Toru Niina</a> and <a href=\"https://github.com/ToruNiina/toml11#Contributors\"\
    >contributors</a>\n</li>\n<li>\n<a href=\"https://github.com/pybind/pybind11\"\
    >pybind11</a> by <a href=\"https://github.com/wjakob\">Wenzel Jakob (EPFL)</a>\
    \ and <a href=\"https://github.com/pybind/pybind11/graphs/contributors\">contributors</a>\n\
    </li>\n<li>all contributors to the evolution of modern C++ and early library preview\
    \ developers, e.g. <a href=\"https://github.com/mpark\">Michael Park (Facebook)</a>\n\
    </li>\n<li>the <a href=\"https://cmake.org\" rel=\"nofollow\">CMake build system</a>\
    \ and <a href=\"https://github.com/Kitware/CMake/blob/master/Copyright.txt\">contributors</a>\n\
    </li>\n<li>packaging support by the <a href=\"https://conda-forge.org\" rel=\"\
    nofollow\">conda-forge</a>, <a href=\"https://pypi.org\" rel=\"nofollow\">PyPI</a>\
    \ and <a href=\"https://spack.io\" rel=\"nofollow\">Spack</a> communities, among\
    \ others</li>\n<li>the <a href=\"https://github.com/openPMD/openPMD-standard\"\
    >openPMD-standard</a> by <a href=\"https://github.com/ax3l\">Axel Huebl (HZDR,\
    \ now LBNL)</a> and <a href=\"https://github.com/openPMD/openPMD-standard/blob/latest/AUTHORS.md\"\
    >contributors</a>\n</li>\n</ul>\n"
  stargazers_count: 102
  subscribers_count: 10
  topics:
  - openpmd
  - openscience
  - hdf5
  - adios
  - mpi
  - hpc
  - research
  - file-handling
  - python3
  - opendata
  - cpp14
  - metadata
  updated_at: 1679362144.0
range3/fio-practice:
  data_format: 2
  description: null
  filenames:
  - spack/envs/dev/spack.yaml
  full_name: range3/fio-practice
  latest_release: null
  readme: '<h1><a id="user-content-fio-practice" class="anchor" aria-hidden="true"
    href="#fio-practice"><span aria-hidden="true" class="octicon octicon-link"></span></a>fio-practice</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1665547201.0
range3/pegasus-lm:
  data_format: 2
  description: null
  filenames:
  - spack/envs/pegasus/spack.yaml
  full_name: range3/pegasus-lm
  latest_release: null
  readme: "<h1><a id=\"user-content-range3pegasus-lm\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#range3pegasus-lm\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>range3/pegasus-lm</h1>\n<h2><a id=\"user-content-setup\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#setup\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>setup</h2>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>module load cuda/11.8.0\nmodule load cudnn/8.6.0/cuda11\nmodule load openmpi/4.1.4/gcc9.4.0-cuda11.8.0\n\
    python3 -m venv .venv\n<span class=\"pl-c1\">source</span> .venv/bin/activate\n\
    pip install -U pip\npip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu118\n\
    pip install neologdn \\\n  prefetch-generator \\\n  datasets \\\n  sentencepiece\
    \ \\\n  transformers \\\n  scikit-learn \\\n  evaluate \\\n  tensorboard \\\n\
    \  accelerate \\\n  git+https://github.com/huggingface/transformers</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1678687089.0
robertu94/poorjit:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: robertu94/poorjit
  latest_release: null
  readme: '<h1><a id="user-content-poorjit" class="anchor" aria-hidden="true" href="#poorjit"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>poorjit</h1>

    <p>A poor man''s jit for C++ for when you want to instantiate and call templates
    at

    runtime. Is there a more efficient way to do this? sure, but this is ~100 lines

    of code I wrote in less than an afternoon. Almost certainly only works on Linux

    and with either clang or g++.</p>

    <p>See <code>test</code> for a usage example.</p>

    '
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1668715291.0
robertu94/sz-zfp-zchecker:
  data_format: 2
  description: container for the ISC/SC compression tutorial
  filenames:
  - spack.yaml
  full_name: robertu94/sz-zfp-zchecker
  latest_release: null
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1652997619.0
salotz/scopes-chipmunk2d:
  data_format: 2
  description: Scopes language wrapper of Chipmunk2D
  filenames:
  - spack.yaml
  full_name: salotz/scopes-chipmunk2d
  latest_release: null
  readme: "<h1><a id=\"user-content-scopes-chipmunk2d\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#scopes-chipmunk2d\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>scopes-chipmunk2d</h1>\n<h2><a id=\"user-content-installation\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#installation\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<p>The module\
    \ is under <code>src/chipmunk2d</code>. You can copy this subtree into your\n\
    project and then add it to the <code>package.path</code> in your Scopes\n<code>_project.sc</code>\
    \ file.</p>\n<h3><a id=\"user-content-with-spack\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#with-spack\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>With Spack</h3>\n<p>This module is available as the <code>scopes-chipmunk2d</code>\
    \ package in the\n<a href=\"https://github.com/salotz/snailpacks\">snailpacks</a>\
    \ repository. This will pull in the necessary dependencies\nincluding Scopes.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>  spack install scopes-chipmunk2d</pre></div>\n\
    <p>See the <a href=\"https://github.com/salotz/snailpacks\">snailpacks</a> documentation\
    \ for more best practices of installing.</p>\n<h2><a id=\"user-content-development-environment\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#development-environment\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Development Environment</h2>\n\
    <p>We use <a href=\"https://spack.io/\" rel=\"nofollow\">Spack</a> to install\
    \ dependencies. First install Spack.</p>\n<p>Then you'll need our custom repo\
    \ of build recipes:</p>\n<div class=\"highlight highlight-source-shell\"><pre>\
    \  mkdir -p <span class=\"pl-s\"><span class=\"pl-pds\">`</span>/.spack/repos</span>\n\
    <span class=\"pl-s\">  git clone git@github.com:salotz/snailpacks.git <span class=\"\
    pl-pds\">`</span></span>/.spack/repos/snailpacks\n  spack repo add <span class=\"\
    pl-s\"><span class=\"pl-pds\">`</span>/resources/spack-repos/snailpacks</span></pre></div>\n\
    <p>Then you need to create an environment in this folder that will\ncontain the\
    \ headers and libraries etc.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>  spack env create -d <span class=\"pl-c1\">.</span></pre></div>\n<p>Activate\
    \ the environment (i.e. set the environment variables\nappropriately) and install\
    \ the packages:</p>\n<div class=\"highlight highlight-source-shell\"><pre>  spacktivate\
    \ <span class=\"pl-c1\">.</span>\n  spack install</pre></div>\n<p>To exit the\
    \ environment (i.e. unset the env variables):</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>  despacktivate</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - scopes-lang
  - chipmunk2d
  updated_at: 1648788744.0
salotz/scopes-demos:
  data_format: 2
  description: null
  filenames:
  - template/{{name}}/spack.yaml
  - 001_chipmunk2d-hello-world/spack.yaml
  - 002_pong/spack.yaml
  full_name: salotz/scopes-demos
  latest_release: null
  readme: "<h2><a id=\"user-content-running-the-demos\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#running-the-demos\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Running the Demos</h2>\n<p>You will need Spack installed\
    \ as well as the <a href=\"\">snailpacks</a> repo. The\nquick bootstrap script\
    \ should be enough to get going if you don't have\nthis installed already:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>curl --proto <span class=\"\
    pl-s\"><span class=\"pl-pds\">'</span>=https<span class=\"pl-pds\">'</span></span>\
    \ --tlsv1.2 -sSf https://raw.githubusercontent.com/salotz/snailpacks/master/bootstrap.sh\
    \ <span class=\"pl-k\">|</span> sh</pre></div>\n<p>Then for each demo you can\
    \ build the environment, activate it, and run\nthem.</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>  <span class=\"pl-c1\">cd</span> XXX_demo-name\n\
    \  make env\n  spacktivate <span class=\"pl-c1\">.</span>\n  make run</pre></div>\n\
    <h2><a id=\"user-content-creating-a-new-demo\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#creating-a-new-demo\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Creating a New Demo</h2>\n<p>You can use the template\
    \ for a quick start (requires <code>copier</code> &gt; 6):</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>copier template</pre></div>\n<p>To update\
    \ the</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1657842137.0
scifihpc/scibuilder:
  data_format: 2
  description: New build system for building scientific software.
  filenames:
  - examples/without-image/spack_example_ubuntu22.04/spack.yaml
  - test/appl_test/spack.yaml
  - examples/build-image/spack_example_hy-alma8/spack.yaml
  - test/spack_env/spack.yaml
  full_name: scifihpc/scibuilder
  latest_release: null
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1678697646.0
simonpintarelli/acclapack-tests:
  data_format: 2
  description: null
  filenames:
  - spack-envs/rocm/spack.yaml
  - spack-envs/cuda/spack.yaml
  full_name: simonpintarelli/acclapack-tests
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1667393188.0
soma-monitoring-toolbox/soma-collector:
  data_format: 2
  description: soma-collector is the component exposing the core SOMA measurement
    API
  filenames:
  - examples/lulesh-inst/spack.yaml
  - spack.yaml
  full_name: soma-monitoring-toolbox/soma-collector
  latest_release: null
  readme: '<h2><a id="user-content-soma-collector" class="anchor" aria-hidden="true"
    href="#soma-collector"><span aria-hidden="true" class="octicon octicon-link"></span></a>soma-collector</h2>

    <p>soma-collector is the component exposing the core SOMA measurement API</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1677986895.0
spack/gitlab-runners:
  data_format: 2
  description: Images used to run Gitlab pipelines in the cloud
  filenames:
  - spack.yaml
  full_name: spack/gitlab-runners
  latest_release: v2023-03-09
  readme: '<p>This repository contains images that are used to run Gitlab pipelines
    to validate PRs in Spack.</p>

    <p>The recipes have been modified from ones in: <a href="https://github.com/UO-OACISS/e4s">https://github.com/UO-OACISS/e4s</a></p>

    '
  stargazers_count: 1
  subscribers_count: 10
  topics: []
  updated_at: 1675192423.0
spack/spack-configs:
  data_format: 2
  description: Share Spack configuration files with other HPC sites
  filenames:
  - OLCF/peak/spack.yaml
  - OLCF/summit/spack.yaml
  - OLCF/crusher/spack.yaml
  - OLCF/andes/spack.yaml
  - OLCF/spock/spack.yaml
  full_name: spack/spack-configs
  latest_release: null
  readme: '<h1><a id="user-content-spack-configs" class="anchor" aria-hidden="true"
    href="#spack-configs"><span aria-hidden="true" class="octicon octicon-link"></span></a>Spack
    Configs</h1>

    <p>This is a repository that sites can use to share their configuration

    files for Spack.  You can contribute your own configuration files, or

    browse around and look at what others have done.</p>

    <h2><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>Spack is distributed under the terms of both the MIT license and the

    Apache License (Version 2.0). Users may choose either license, at their

    option.</p>

    <p>All new contributions must be made under both the MIT and Apache-2.0

    licenses.</p>

    <p>See <a href="https://github.com/spack/spack-configs/blob/master/LICENSE-MIT">LICENSE-MIT</a>,

    <a href="https://github.com/spack/spack-configs/blob/master/LICENSE-APACHE">LICENSE-APACHE</a>,

    <a href="https://github.com/spack/spack-configs/blob/master/COPYRIGHT">COPYRIGHT</a>,
    and

    <a href="https://github.com/spack/spack-configs/blob/master/NOTICE">NOTICE</a>
    for details.</p>

    <p>SPDX-License-Identifier: (Apache-2.0 OR MIT)</p>

    <p>LLNL-CODE-811652</p>

    '
  stargazers_count: 47
  subscribers_count: 26
  topics: []
  updated_at: 1678806741.0
spack/spack-tutorial:
  data_format: 2
  description: Standalone Spack Tutorial Repository
  filenames:
  - outputs/stacks/examples/1.spack.stack.yaml
  - outputs/stacks/examples/3.spack.stack.yaml
  - outputs/stacks/examples/9.spack.stack.yaml
  - spack.yaml
  - outputs/stacks/examples/8.spack.stack.yaml
  - outputs/stacks/examples/0.spack.stack.yaml
  - outputs/stacks/examples/7.spack.stack.yaml
  - outputs/stacks/examples/2.spack.stack.yaml
  - outputs/stacks/examples/5.spack.stack.yaml
  - outputs/stacks/examples/6.spack.stack.yaml
  - outputs/stacks/examples/4.spack.stack.yaml
  full_name: spack/spack-tutorial
  latest_release: cineca23
  readme: '<h1><a id="user-content--spack-tutorial" class="anchor" aria-hidden="true"
    href="#-spack-tutorial"><span aria-hidden="true" class="octicon octicon-link"></span></a>

    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a01512f4480c4615a82f2b929789547a60d78e1f68d26be1a56e33d9258735d4/68747470733a2f2f63646e2e7261776769742e636f6d2f737061636b2f737061636b2f646576656c6f702f73686172652f737061636b2f6c6f676f2f737061636b2d6c6f676f2e737667"><img
    src="https://camo.githubusercontent.com/a01512f4480c4615a82f2b929789547a60d78e1f68d26be1a56e33d9258735d4/68747470733a2f2f63646e2e7261776769742e636f6d2f737061636b2f737061636b2f646576656c6f702f73686172652f737061636b2f6c6f676f2f737061636b2d6c6f676f2e737667"
    width="64" valign="middle" alt="Spack" data-canonical-src="https://cdn.rawgit.com/spack/spack/develop/share/spack/logo/spack-logo.svg"
    style="max-width: 100%;"></a> Spack Tutorial</h1>

    <p><a href="https://spack-tutorial.readthedocs.io" rel="nofollow"><img src="https://camo.githubusercontent.com/f38d9ceff55c2a7fea5d61861aa91b64fe00c220b83af1fd8af46da42ede70f5/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f737061636b2d7475746f7269616c2f62616467652f3f76657273696f6e3d6c6174657374"
    alt="Read the Docs" data-canonical-src="https://readthedocs.org/projects/spack-tutorial/badge/?version=latest"
    style="max-width: 100%;"></a></p>

    <p>Spack is a multi-platform package manager that builds and installs multiple
    versions and configurations of software. It works on Linux, macOS, and many supercomputers.
    Spack is non-destructive: installing a new version of a package does not break
    existing installations, so many configurations of the same package can coexist.</p>

    <p>This repository houses Spack''s <a href="https://spack-tutorial.readthedocs.io/en/latest/"
    rel="nofollow"><strong>hands-on tutorial</strong></a>, which is a subset of Spack''s
    <a href="https://spack.readthedocs.io/" rel="nofollow"><strong>full documentation</strong></a>
    (or you can run <code>spack help</code> or <code>spack help --all</code>).</p>

    <p>This tutorial covers basic to advanced usage, packaging, developer features,
    and large HPC deployments.  You can do all of the exercises on your own laptop
    using a Docker container. Feel free to use these materials to teach users at your
    organization about Spack.</p>

    <h2><a id="user-content-updating-the-tutorial" class="anchor" aria-hidden="true"
    href="#updating-the-tutorial"><span aria-hidden="true" class="octicon octicon-link"></span></a>Updating
    the tutorial</h2>

    <ol>

    <li>Create a new branch named for the event/milestone that corresponds to the
    new version you want to create.</li>

    <li>Upload screen shot of first slide (244px wide, .png) to <a href="https://github.com/spack/spack-tutorial/tree/master/tutorial/images">images
    directory</a> following existing file-naming convention.</li>

    <li>Upload PDF of slide deck to <a href="https://github.com/spack/spack-tutorial/tree/master/_static/slides">slides
    directory</a> following existing file-naming convention.</li>

    <li>Update <a href="https://github.com/spack/spack-tutorial/blob/master/index.rst">index.rst</a>
    with event name and date; full citation; and file paths for image and PDF.</li>

    <li>Update this README (lines 3 and 7) with link to new version''s URL.</li>

    <li>Build docs locally.</li>

    <li>Push changes to GitHub and active new tag/version on Read the Docs.</li>

    <li>Build new version on Read the Docs.</li>

    </ol>

    <h2><a id="user-content-updating-the-tutorial-container" class="anchor" aria-hidden="true"
    href="#updating-the-tutorial-container"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Updating the tutorial container</h2>

    <p>The spack tutorial container is built from another <a href="https://github.com/spack/spack-tutorial-container">repository</a>
    by an automated process.  For instructions on how to create an updated version
    of the tutorial container, see these <a href="https://github.com/spack/spack-tutorial-container/blob/master/UPDATING.md">instructions</a>.  For
    a general description of the automated process used to build the tutorial container,
    read the <a href="https://github.com/spack/spack-tutorial-container/blob/master/DESCRIPTION.md">description</a>.</p>

    <h2><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>Spack is distributed under the terms of both the MIT license and the Apache
    License (Version 2.0). Users may choose either license, at their option.</p>

    <p>All new contributions must be made under both the MIT and Apache-2.0 licenses.</p>

    <p>See <a href="https://github.com/spack/spack/blob/develop/LICENSE-MIT">LICENSE-MIT</a>,

    <a href="https://github.com/spack/spack/blob/develop/LICENSE-APACHE">LICENSE-APACHE</a>,

    <a href="https://github.com/spack/spack/blob/develop/COPYRIGHT">COPYRIGHT</a>,
    and

    <a href="https://github.com/spack/spack/blob/develop/NOTICE">NOTICE</a> for details.</p>

    <p>SPDX-License-Identifier: (Apache-2.0 OR MIT)</p>

    <p>LLNL-CODE-811652</p>

    '
  stargazers_count: 28
  subscribers_count: 37
  topics: []
  updated_at: 1679012892.0
sundials-codes/sundials-manyvector-demo:
  data_format: 2
  description: Demonstration application for Multirate+ManyVector capabilities
  filenames:
  - docker/spack-develop.yaml
  - spack/spack-summit.yaml
  - docker/spack-latest.yaml
  full_name: sundials-codes/sundials-manyvector-demo
  latest_release: null
  readme: "<h1><a id=\"user-content-sundials-manyvectormultirate-demonstration-code\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#sundials-manyvectormultirate-demonstration-code\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>SUNDIALS\
    \ ManyVector+Multirate Demonstration Code</h1>\n<p>[Note: this project is in active\
    \ development.]</p>\n<p>This is a <a href=\"https://github.com/LLNL/sundials\"\
    >SUNDIALS</a>-based demonstration\napplication to assess and demonstrate the large-scale\
    \ parallel performance of\nnew capabilities that have been added to SUNDIALS in\
    \ recent years. Namely:</p>\n<ol>\n<li>\n<p>The new SUNDIALS <a href=\"https://sundials.readthedocs.io/en/latest/nvectors/NVector_links.html#the-nvector-mpimanyvector-module\"\
    \ rel=\"nofollow\">MPIManyVector</a>\nimplementation, that enables flexibility\
    \ in how a solution data is\npartitioned across computational resources e.g.,\
    \ CPUs and GPUs.</p>\n</li>\n<li>\n<p>The new <a href=\"https://sundials.readthedocs.io/en/latest/arkode/index.html\"\
    \ rel=\"nofollow\">ARKODE</a>\nmultirate integration module, MRIStep, allowing\
    \ high-order accurate\ncalculations that subcycle \"fast\" processes within \"\
    slow\" ones.</p>\n</li>\n<li>\n<p>The new flexible SUNDIALS <a href=\"https://sundials.readthedocs.io/en/latest/sunlinsol/index.html\"\
    \ rel=\"nofollow\">SUNLinearSolver</a>\ninterfaces, to enable streamlined use\
    \ of problem specific and scalable\nlinear solver libraries e.g., SuiteSparse\
    \ and MAGMA.</p>\n</li>\n</ol>\n<h2><a id=\"user-content-model-equations\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#model-equations\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Model Equations</h2>\n<p>This code\
    \ simulates a 3D nonlinear inviscid compressible Euler equation with\nadvection\
    \ and reaction of chemical species,</p>\n<p>$$w_t = -\\nabla\\cdot F(w) + G(X,t,w),$$</p>\n\
    <p>for independent variables $(X,t) = (x,y,z,t) \\in \\Omega \\times [t_0, t_f]$\n\
    where the spatial domain is a three-dimensional cube,\n$\\Omega = [x_l, x_r] \\\
    times [y_l, y_r] \\times [z_l, z_r]$.</p>\n<p>The differential equation is completed\
    \ using initial condition\n$w(X,t_0) = w_0(X)$ and face-specific boundary conditions\
    \ may be periodic (0),\nhomogeneous Neumann (1), homogeneous Dirichlet (2), or\
    \ reflecting (3) under the\nrestriction that if any boundary is set to \"periodic\"\
    \ then the opposite face\nmust also indicate a periodic condition.</p>\n<p>The\
    \ system state vector $w$ is</p>\n<p>$$w = \\begin{bmatrix} \\rho &amp; \\rho\
    \ v_x &amp; \\rho v_y &amp; \\rho v_z &amp; e_t &amp; \\mathbf{c} \\end{bmatrix}^T\
    \ = \\begin{bmatrix} \\rho &amp; m_x &amp; m_y &amp; m_z &amp; e_t &amp; \\mathbf{c}\
    \ \\end{bmatrix}^T$$</p>\n<p>corresponding to the density, momentum in the x,\
    \ y, and z directions, total\nenergy per unit volume, and any number of chemical\
    \ densities\n$\\mathbf{c}\\in\\mathbb{R}^{nchem}$ that are advected along with\
    \ the fluid. The\nfluxes are given by</p>\n<p>$$F_x(w) = \\begin{bmatrix} \\rho\
    \ v_x &amp; \\rho v_x^2 + p &amp; \\rho v_x v_y &amp; \\rho v_x v_z &amp; v_x\
    \ (e_t+p) &amp; \\mathbf{c} v_x \\end{bmatrix}^T,$$</p>\n<p>$$F_y(w) = \\begin{bmatrix}\
    \ \\rho v_y &amp; \\rho v_x v_y &amp; \\rho v_y^2 + p &amp; \\rho v_y v_z &amp;\
    \ v_y (e_t+p) &amp; \\mathbf{c} v_y \\end{bmatrix}^T,$$</p>\n<p>$$F_z(w) = \\\
    begin{bmatrix} \\rho v_z &amp; \\rho v_x v_z &amp; \\rho v_y v_z &amp; \\rho v_z^2\
    \ + p &amp; v_z (e_t+p) &amp; \\mathbf{c} v_z \\end{bmatrix}^T.$$</p>\n<p>The\
    \ external force $G(X,t,w)$ is test-problem-dependent, and the ideal gas\nequation\
    \ of state gives $p = \\frac{R}{c_v}(e_t - \\frac{\\rho}{2}(v_x^2 + v_y^2 + v_z^2))$\n\
    and $e_t = \\frac{pc_v}{R} + \\frac{\\rho}{2}(v_x^2 + v_y^2 + v_z^2)$\nor equivalently,\
    \ $p = (\\gamma-1) (e_t - \\frac{\\rho}{2} (v_x^2 + v_y^2 + v_z^2))$\nand $e_t\
    \ = \\frac{p}{\\gamma - 1}\\frac{\\rho}{2}(v_x^2 + v_y^2 + v_z^2)$.</p>\n<p>We\
    \ have the physical parameters:</p>\n<ul>\n<li>\n<p>$R$ is the specific ideal\
    \ gas constant (287.14 J/kg/K),</p>\n</li>\n<li>\n<p>$c_v$ is the specific heat\
    \ capacity at constant volume (717.5 J/kg/K),</p>\n</li>\n<li>\n<p>$\\gamma =\
    \ c_p/c_v = 1 + R/c_v$ is the ratio of specific heats (1.4),</p>\n</li>\n</ul>\n\
    <p>corresponding to air (predominantly an ideal diatomic gas). The speed\nof sound\
    \ in the gas is then given by $c = \\sqrt{\\dfrac{\\gamma p}{\\rho}}$.</p>\n<p>The\
    \ fluid variables above are non-dimensionalized; in standard SI units\nthese would\
    \ be:</p>\n<ul>\n<li>\n<p>$[\\rho] = kg / m^3$,</p>\n</li>\n<li>\n<p>$[v_x] =\
    \ [v_y] = [v_z] = m/s$, which implies $[m_x] = [m_y] = [m_z] = kg / m^2 / s$</p>\n\
    </li>\n<li>\n<p>$[e_t] = kg / m / s^2$, and</p>\n</li>\n<li>\n<p>$[\\mathbf{c}_i]\
    \ = kg / m^3$</p>\n</li>\n</ul>\n<p>Note: the fluid portion of the description\
    \ above follows the presentation\n<a href=\"https://www.theoretical-physics.net/dev/fluid-dynamics/euler.html\"\
    \ rel=\"nofollow\">here</a>\nin sections 7.3.1 - 7.3.3.</p>\n<h2><a id=\"user-content-discretization\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#discretization\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Discretization</h2>\n<p>We discretize\
    \ this problem using the method of lines, where we first semi-discretize\nin space\
    \ using a regular finite volume grid with dimensions <code>nx</code> x <code>ny</code>\
    \ x <code>nz</code>, with\nfluxes at cell faces calculated using a 5th-order FD-WENO\
    \ reconstruction.  MPI\nparallelization is achieved using a standard 3D domain\
    \ decomposition, using <code>nprocs</code>\nMPI ranks, with layout <code>npx</code>\
    \ x <code>npy</code> x <code>npz</code> defined automatically via the\n<code>MPI_Dims_create</code>\
    \ utility routine.  The minimum size for any dimension is 3, so\nto run a two-dimensional\
    \ test in the yz-plane, one could specify <code>nx = 3</code> and\n<code>ny =\
    \ nz = 200</code>.  When run in parallel, only \"active\" spatial dimensions (those\n\
    with extent greater than 3) will be parallelized.</p>\n<p>The fluid fields $\\\
    rho$, $m_x$, $m_y$, $m_z$, and $e_t$ are stored in separate serial\n<code>N_Vector</code>\
    \ objects on each MPI rank. The chemical species at all spatial locations over\n\
    each MPI rank are collocated into a single serial or RAJA <code>N_Vector</code>\
    \ object when\nrunning on the CPU or GPU respectively. The five fluid vectors\
    \ and the chemical\nspecies vector are combined together to form the full \"solution\"\
    \ vector $w$ using\nthe <code>MPIManyVector</code> <code>N_Vector</code> module.</p>\n\
    <p>After spatial semi-discretization, we are faced with a large IVP system,</p>\n\
    <p>$$w'(t) = f_1(w) + f_2(w), \\quad w(t_0)=w_0,$$</p>\n<p>where $f_1(w)$ and\
    \ $f_2(w)$ contain the spatially discretized forms of\n$-\\nabla\\cdot F(w)$ and\
    \ $G(X,t,w)$, respectively.</p>\n<p>For non-reactive flows, the resulting initial-value\
    \ problem is evolved in time\nusing an adaptive step explicit Runge-Kutta method\
    \ from the ARKStep module in\nARKODE. For problems involving (typically stiff)\
    \ chemical reactions, the problem\nmay be solved using one of two approaches.</p>\n\
    <ol>\n<li>\n<p>It may be treated as a multirate initial-value problem, that is\
    \ solved using\nthe MRIStep module in ARKODE, wherein the gas dynamics equations\
    \ are evolved\nexplicitly at the slow time scale, while the chemical kinetics\
    \ are evolved\nat a faster time scale using a temporally-adaptive, diagonally-implicit\n\
    Runge-Kutta method from the ARKStep module.</p>\n</li>\n<li>\n<p>It may be treated\
    \ using mixed implicit-explicit (IMEX) methods at a single\ntime scale.  Here,\
    \ the gas dynamics equations are treated explicitly, while\nthe chemical kinetics\
    \ are treated implicitly, using an additive Runge-Kutta\nmethod from the ARKStep\
    \ module.</p>\n</li>\n</ol>\n<p>For (1) we use SUNDIALS' modified Newton solver\
    \ to handle the global nonlinear\nalgebraic systems arising at each implicit stage\
    \ of each time step.  Since only\n$f_2$ is treated implicitly and the reactions\
    \ are purely local in space, the\nNewton linear systems are block-diagonal. As\
    \ such, we provide a custom\n<code>SUNLinearSolver</code> implementation that\
    \ solves each MPI rank-local linear system\nindependently. The portion of the\
    \ Jacobian matrix on each rank is itself\nblock-diagonal. We further leverage\
    \ this structure by solving each rank-local\nlinear system using either the sparse\
    \ KLU (CPU-only) or batched dense MAGMA\n(GPU-enabled) SUNDIALS <code>SUNLinearSolver</code>\
    \ implementations.</p>\n<p>The multirate approach (2) can leverage the structure\
    \ of $f_2$ at a higher\nlevel. Since the MRI method applied to this problem evolves\
    \ \"fast\" sub-problems\nof the form</p>\n<p>$$v'(t) = f_2(t,v) + r_i(t), \\quad\
    \ i=2,\\ldots,s,$$</p>\n<p>and all MPI communication necessary to construct the\
    \ forcing functions, $r_i(t)$,\nhas already been performed, each sub-problem consists\
    \ of <code>nx</code> x <code>ny</code> x <code>nz</code>\nspatially-decoupled\
    \ fast IVPs. We construct a custom fast integrator that groups\nall the independent\
    \ fast IVPs on an MPI rank together as a single system evolved\nusing a rank-local\
    \ ARKStep instance.  The code for this custom integrator itself\nis minimal, primarily\
    \ consisting of steps to access the local subvectors in $w$\non a given MPI rank\
    \ and wrapping them in MPI-unaware ManyVectors provided to the\nlocal ARKStep\
    \ instance. The collection of independent local IVPs also leads to a\nblock diagonal\
    \ Jacobian, and we again utilize the <code>SUNLinearSolver</code> modules listed\n\
    above for linear systems that arise within the modified Newton iteration.</p>\n\
    <h2><a id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#installation\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<p>The following steps describe how to build the\
    \ demonstration code in a Linux or\nOS X environment.</p>\n<h3><a id=\"user-content-gettting-the-code\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#gettting-the-code\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Gettting the Code</h3>\n<p>To\
    \ obtain the code, clone this repository with Git:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>  git clone https://github.com/sundials-codes/sundials-manyvector-demo.git</pre></div>\n\
    <h3><a id=\"user-content-requirements\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#requirements\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Requirements</h3>\n<p>To compile the code you will need:</p>\n<ul>\n\
    <li>\n<p><a href=\"https://cmake.org\" rel=\"nofollow\">CMake</a> 3.20 or newer</p>\n\
    </li>\n<li>\n<p>modern C and C++ compilers</p>\n</li>\n<li>\n<p>the NVIDIA <a\
    \ href=\"https://developer.nvidia.com/cuda-toolkit\" rel=\"nofollow\">CUDA Toolkit</a>\
    \ (when\nusing the CUDA backend)</p>\n</li>\n<li>\n<p>an MPI library e.g., <a\
    \ href=\"https://www.open-mpi.org/\" rel=\"nofollow\">OpenMPI</a>,\n<a href=\"\
    https://www.mpich.org/\" rel=\"nofollow\">MPICH</a>, etc.</p>\n</li>\n<li>\n<p>the\
    \ <a href=\"https://www.hdfgroup.org/\" rel=\"nofollow\">HDF5</a> high-performance\
    \ data management and\nstorage suite</p>\n</li>\n<li>\n<p>the <a href=\"https://github.com/LLNL/RAJA\"\
    >RAJA</a> performance portability library</p>\n</li>\n<li>\n<p>the <a href=\"\
    https://computing.llnl.gov/projects/sundials\" rel=\"nofollow\">SUNDIALS</a> library\
    \ of time\nintegrators and nonlinear solvers</p>\n</li>\n<li>\n<p>the <a href=\"\
    https://people.engr.tamu.edu/davis/suitesparse.html\" rel=\"nofollow\">SuiteSparse</a>\
    \ library\nof sparse direct linear solvers (when using a CPU backend)</p>\n</li>\n\
    <li>\n<p>the <a href=\"https://icl.utk.edu/magma/\" rel=\"nofollow\">MAGMA</a>\
    \ dense linear solver library (when\nusing a GPU backend)</p>\n</li>\n</ul>\n\
    <h4><a id=\"user-content-installing-dependencies-with-spack\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#installing-dependencies-with-spack\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installing Dependencies with\
    \ Spack</h4>\n<p>Many of the above dependencies can be installed using the\n<a\
    \ href=\"https://spack.io/\" rel=\"nofollow\">Spack</a> package manager. For information\
    \ on using Spack see\nthe getting started <a href=\"https://spack.readthedocs.io/en/latest/getting_started.html#getting-started\"\
    \ rel=\"nofollow\">guide</a>.\nThe instructions below were formulated from Spack\
    \ v0.19.0, although newer versions should also work.</p>\n<p>Once Spack is setup,\
    \ we recommend creating a Spack <a href=\"https://spack.readthedocs.io/en/latest/environments.html#\"\
    \ rel=\"nofollow\">environment</a>\nwith the required dependencies e.g., on a\
    \ system with Pascal GPUs and CUDA\n11.4.2 installed:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>spack env create --with-view <span class=\"pl-k\"\
    >~</span>/views/sundials-demo sundials-demo\nspack env activate sundials-demo\n\
    spack add sundials@6.2.0 +openmp +mpi +logging-mpi +klu +magma +raja +cuda cuda_arch=60\
    \ ^cuda@11.4.2 ^magma@2.6.1 +cuda cuda_arch=60 ^raja@0.13.0 +cuda cuda_arch=60\
    \ ^suite-sparse@5.8.1\nspack add hdf5@1.10.7 +hl +mpi\nspack install</pre></div>\n\
    <p>To assist in building the dependencies on select systems the <a href=\"./spack\"\
    >spack</a>\ndirectory contains environment files leveraging software already available\
    \ on\nthe system. For example, on the OLCF Summit system:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>module load gcc/10.2.0 cuda/11.4.2 cmake/3.21.3\n\
    <span class=\"pl-c1\">cd</span> spack\nspack env create sundials-demo spack-summit.yaml\n\
    spack env activate sundials-demo\nspack install</pre></div>\n<h4><a id=\"user-content-using-docker-containers\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#using-docker-containers\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using Docker\
    \ Containers</h4>\n<p>It also possible to use the Docker containers from the <a\
    \ href=\"https://github.com/orgs/sundials-codes/packages?repo_name=sundials-manyvector-demo\"\
    >GitHub Container Registry</a>\nwith the necessary dependencies preinstalled for\
    \ CPU-only testing. Two images\nare provided:</p>\n<ul>\n<li>\n<p>sundials-demo-spack-latest\
    \ -- based on the latest Spack release (currently\nv0.19.0)</p>\n</li>\n<li>\n\
    <p>sundials-demo-spack-develop -- based on the Spack develop branch and updated\n\
    monthly</p>\n</li>\n</ul>\n<p>Pull the image(s) using <a href=\"https://www.docker.com/\"\
    \ rel=\"nofollow\">Docker</a> (or <a href=\"https://podman.io\" rel=\"nofollow\"\
    >Podman</a>).\nFor example, the <code>run</code> command below will pull the image\
    \ and start the container\nand the <code>exec</code> command will start a bash\
    \ shell inside the container.</p>\n<pre><code>docker run -t -d --name sundialsci-demo-spack-latest\
    \ ghcr.io/sundials-codes/sundials-demo-spack-latest:spack-latest\ndocker exec\
    \ -it sundials-demo-spack-lateset bash\n</code></pre>\n<p>Then clone this repository\
    \ with Git and configure/build the code as described\nbelow. The Spack installed\
    \ dependencies are available from the <code>/opt/view</code>\ndirectory.</p>\n\
    <h3><a id=\"user-content-configuration-options\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#configuration-options\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Configuration Options</h3>\n<p>Once the necessary\
    \ dependencies are installed, the following CMake variables can\nbe used to configure\
    \ the demonstration code build:</p>\n<ul>\n<li>\n<p><code>CMAKE_INSTALL_PREFIX</code>\
    \ - the path where executables and input files should be\ninstalled e.g., <code>my/install/path</code>.\
    \ The executables will be installed in the\n<code>bin</code> directory and input\
    \ files in the <code>tests</code> directory under the given path.</p>\n</li>\n\
    <li>\n<p><code>CMAKE_C_COMPILER</code> - the C compiler to use e.g., <code>mpicc</code>.\
    \ If not set, CMake\nwill attempt to automatically detect the C compiler.</p>\n\
    </li>\n<li>\n<p><code>CMAKE_C_FLAGS</code> - the C compiler flags to use e.g.,\
    \ <code>-g -O2</code>.</p>\n</li>\n<li>\n<p><code>CMAKE_C_STANDARD</code> - the\
    \ C standard to use, defaults to <code>99</code>.</p>\n</li>\n<li>\n<p><code>CMAKE_CXX_COMPILER</code>\
    \ - the C++ compiler to use e.g., <code>mpicxx</code>. If not set,\nCMake will\
    \ attempt to automatically detect the C++ compiler.</p>\n</li>\n<li>\n<p><code>CMAKE_CXX_FLAGS</code>\
    \ - the C++ flags to use e.g., <code>-g -O2</code>.</p>\n</li>\n<li>\n<p><code>CMAKE_CXX_STANDARD</code>\
    \ - the C++ standard to use, defaults to <code>11</code>.</p>\n</li>\n<li>\n<p><code>RAJA_ROOT</code>\
    \ - the root directory of the RAJA installation, defaults to the\nvalue of the\
    \ <code>RAJA_ROOT</code> environment variable. If not set, CMake will attempt\n\
    to automatically locate a RAJA install on the system.</p>\n</li>\n<li>\n<p><code>RAJA_BACKEND</code>\
    \ - the RAJA backend to use with the demonstration code, defaults\nto <code>SERIAL</code>.\
    \ Supported options are <code>SERIAL</code>, <code>OPENMP</code> and <code>CUDA</code>.\
    \  Note that this\nonly applies to on-node parallelism that is used when evaluating\
    \ chemistry-based\ncomponents associated with $f_2(w)$.</p>\n</li>\n<li>\n<p><code>SUNDIALS_ROOT</code>\
    \ - the root directory of the SUNDIALS installation, defaults to\nthe value of\
    \ the <code>SUNDIALS_ROOT</code> environment variable. If not set, CMake will\n\
    attempt to automatically locate a SUNDIALS install on the system.</p>\n</li>\n\
    <li>\n<p><code>ENABLE_HDF5</code> - build with HDF5 I/O support, defaults to <code>OFF</code>.</p>\n\
    </li>\n<li>\n<p><code>HDF5_ROOT</code> - the root directory of the HDF5 installation,\
    \ defaults to the\nvalue of the <code>HDF5_ROOT</code> environment variable. If\
    \ not set, CMake will attempt\nto automatically locate a HDF5 install on the system.</p>\n\
    </li>\n</ul>\n<p>When RAJA is installed with CUDA support enabled, the following\
    \ additional\nvariables may also be set:</p>\n<ul>\n<li>\n<p><code>CMAKE_CUDA_COMPILER</code>\
    \ - the CUDA compiler to use e.g., <code>nvcc</code>. If not set,\nCMake will\
    \ attempt to automatically detect the CUDA compiler.</p>\n</li>\n<li>\n<p><code>CMAKE_CUDA_FLAGS</code>\
    \ - the CUDA compiler flags to use.</p>\n</li>\n<li>\n<p><code>CMAKE_CUDA_ARCHITECTURES</code>\
    \ - the CUDA architecture to target e.g., <code>70</code>.</p>\n</li>\n</ul>\n\
    <h3><a id=\"user-content-building\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #building\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building</h3>\n\
    <p>In-source builds are not permitted, as such the code should be configured and\n\
    built from a separate build directory e.g.,</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>  <span class=\"pl-c1\">cd</span> sundials-manyvector-demo\n  mkdir build\n\
    \  <span class=\"pl-c1\">cd</span> build\n  cmake ../. \\\n    -DCMAKE_INSTALL_PREFIX=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>[install-path]<span class=\"\
    pl-pds\">\"</span></span> \\\n    -DRAJA_BACKEND=<span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>SERIAL<span class=\"pl-pds\">\"</span></span> \\\n    -DENABLE_HDF5=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>ON<span class=\"pl-pds\">\"</span></span>\
    \ \\\n    -DHDF5_ROOT=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>[spack-view-path]<span\
    \ class=\"pl-pds\">\"</span></span> \\\n    -DRAJA_ROOT=<span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span>[spack-view-path]<span class=\"pl-pds\">\"</span></span>\
    \ \\\n    -DSUNDIALS_ROOT=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>[spack-view-path]<span\
    \ class=\"pl-pds\">\"</span></span>\n  make\n  make install</pre></div>\n<p>where\
    \ <code>[install-path]</code> is the path to where the binary and test input files\n\
    should be installed and <code>[spack-view-path]</code> is the path to the Spack\
    \ environment\nview, <code>~/views/sundials-demo</code> when following the Spack\
    \ instructions above or\n<code>/opt/view</code> when using the Docker containers.</p>\n\
    <h2><a id=\"user-content-running\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #running\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running</h2>\n\
    <p>Several test cases are included with the code and the necessary input files\
    \ for\neach case are contained in the subdirectories within the <a href=\"./tests\"\
    >tests</a>\ndirectory. Each input file is internally documented to discuss all\
    \ possible\ninput parameters (in case some have been added since this <code>README</code>\
    \ was last\nupdated).</p>\n<p>The input files contain parameters to set up the\
    \ physical problem:</p>\n<ul>\n<li>\n<p>spatial domain, $\\Omega$ -- <code>xl</code>,\
    \ <code>xr</code>, <code>yl</code>, <code>yr</code>, <code>zl</code>, <code>zr</code></p>\n\
    </li>\n<li>\n<p>time interval, $[t_0, t_f]$ -- <code>t0</code>, <code>tf</code></p>\n\
    </li>\n<li>\n<p>the ratio of specific heats, $\\gamma$ -- <code>gamma</code></p>\n\
    </li>\n<li>\n<p>spatial discretization dimensions -- <code>nx</code>, <code>ny</code>,\
    \ <code>nz</code></p>\n</li>\n<li>\n<p>boundary condition types -- <code>xlbc</code>,\
    \ <code>xrbc</code>, <code>ylbc</code>, <code>yrbc</code>, <code>zlbc</code>,\
    \ <code>zrbc</code></p>\n</li>\n</ul>\n<p>Parameters to control the execution\
    \ of the code:</p>\n<ul>\n<li>\n<p>desired CFL fraction -- <code>cfl</code> (if\
    \ set to zero, then the time step is chosen purely using temporal adaptivity).</p>\n\
    </li>\n<li>\n<p>number of desired solution outputs -- <code>nout</code></p>\n\
    </li>\n<li>\n<p>a flag to enable optional output of RMS averages for each field\
    \ at the frequency specified via <code>nout</code> -- <code>showstats</code></p>\n\
    </li>\n</ul>\n<p>Numerous parameters are also provided to control how time integration\
    \ is\nperformed (these are passed directly to ARKODE). For further information\
    \ on the\nARKODE solver parameters and the meaning of individual values, see the\n\
    <a href=\"https://sundials.readthedocs.io/en/latest/index.html\" rel=\"nofollow\"\
    >ARKODE documentation</a>.</p>\n<p>To specify an input file to the executable,\
    \ the input filename should be\nprovided using the <code>-f</code> flag e.g.,</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>  <span class=\"pl-k\">&lt;</span>executable<span\
    \ class=\"pl-k\">&gt;</span> -f <span class=\"pl-k\">&lt;</span>input_file<span\
    \ class=\"pl-k\">&gt;</span></pre></div>\n<p>Additionally, any input parameters\
    \ may also be specified on the\ncommand line e.g.,</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>  <span class=\"pl-k\">&lt;</span>executable<span\
    \ class=\"pl-k\">&gt;</span> --nx=100 --ny=100 --nz=400</pre></div>\n<p>For example,\
    \ continuing with the Summit case from above, the primordial blast\ntest can be\
    \ run on one Summit node using four cores and four GPUs with the\nfollowing commands:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>  <span class=\"pl-c1\">cd</span>\
    \ <span class=\"pl-smi\">${MEMBERWORK}</span>/[projid]/sundials-demo/tests/primordial_blast\n\
    \  bsub -q debug -nnodes 1 -W 0:10 -P [projid] -Is <span class=\"pl-smi\">$SHELL</span>\n\
    \  jsrun -n4 -a1 -c1 -g1 ../../bin/primordial_blast_mr.exe -f input_primordial_blast_mr_gpu.txt</pre></div>\n\
    <p>The <code>bsub</code> command above will submit a request for an interactive\
    \ job to the\ndebug queue allocating one node for 10 minutes with the compute\
    \ time charged to\n<code>[projid]</code>. Once the interactive session starts\
    \ the test case is launched using\nthe <code>jsrun</code> command. Solutions are\
    \ output to disk using parallel HDF5, solution\nstatistics are optionally output\
    \ to the screen at specified frequencies, and run\nstatistics are printed at the\
    \ end of the simulation.</p>\n<p>The parallel HDF5 solution snapshots are written\
    \ at the frequency specified by\n<code>nout</code>.  Accompanying these <code>output-#######.hdf5</code>\
    \ files is an automatically\ngenerated input file, <code>restart_parameters.txt</code>\
    \ that stores a complete set of\ninput parameters to restart the simulation from\
    \ the most recently generated\noutput file. This is a \"warm\" restart, in that\
    \ it will pick up the calculation\nwhere the previous one left off, using the\
    \ same initial time step size as\nARKStep would use. This restart may differ slightly\
    \ from an uninterrupted run\nsince other internal ARKStep time adaptivity parameters\
    \ cannot be reused.  We\nnote that the restart must use the same spatial grid\
    \ size and number of chemical\ntracers as the original run, but it may use a different\
    \ number of MPI tasks if\ndesired.</p>\n<h2><a id=\"user-content-adding-new-tests\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#adding-new-tests\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Adding New Tests</h2>\n<p>Individual\
    \ test problems are uniquely specified through an input file and\nauxiliary source\
    \ code file(s) that should be linked with the main routine at\ncompile time. By\
    \ default, all codes are built with no chemical species; however,\nthis may be\
    \ controlled at compilation time using the <code>NVAR</code> preprocessor\ndirective,\
    \ corresponding to the number of unknowns at any spatial location.\nHence, the\
    \ (default) minimum value for <code>NVAR</code> is 5, so for a calculation with\
    \ 4\nchemical species the code should be compiled with the preprocessor directive\n\
    <code>NVAR=9</code>. See <a href=\"./src/CMakeLists.txt\">src/CMakeLists.txt</a>\
    \ for examples of how to\nspecify <code>NVAR</code> when adding a new test/executable.</p>\n\
    <p>The auxiliary source code files for creating a new test must contain three\n\
    functions. Each of these must return an integer flag indicating success (0) or\n\
    failure (nonzero). The initial condition function $w_0(X)$ must have the signature</p>\n\
    <div class=\"highlight highlight-source-c++\"><pre>  <span class=\"pl-k\">int</span>\
    \ <span class=\"pl-en\">initial_conditions</span>(<span class=\"pl-k\">const</span>\
    \ realtype&amp; t, N_Vector w, <span class=\"pl-k\">const</span> UserData&amp;\
    \ udata);</pre></div>\n<p>and the forcing function $G(X,t,w)$ must have the signature</p>\n\
    <div class=\"highlight highlight-source-c++\"><pre>  <span class=\"pl-k\">int</span>\
    \ <span class=\"pl-en\">external_forces</span>(<span class=\"pl-k\">const</span>\
    \ realtype&amp; t, N_Vector G, <span class=\"pl-k\">const</span> UserData&amp;\
    \ udata);</pre></div>\n<p>Additionally, a function must be supplied to compute/output\
    \ any\ndesired solution diagnostic information with the signature</p>\n<div class=\"\
    highlight highlight-source-c++\"><pre>  <span class=\"pl-k\">int</span> <span\
    \ class=\"pl-en\">output_diagnostics</span>(<span class=\"pl-k\">const</span>\
    \ realtype&amp; t, <span class=\"pl-k\">const</span> N_Vector w, <span class=\"\
    pl-k\">const</span> UserData&amp; udata);</pre></div>\n<p>If no diagnostics information\
    \ is desired, then this routine may just return 0.</p>\n<p>Here, the <code>initial_conditions</code>\
    \ routine will be called once when the simulation\nbegins, <code>external_forces</code>\
    \ will be called on every evaluation of the ODE\nright-hand side function for\
    \ the Euler equations (it is assumed that this does\nnot require the results from\
    \ (<code>UserData::ExchangeStart</code>\n/ <code>UserData::ExchangeEnd</code>),\
    \ and <code>output_diagnostics</code> will be called at the same\nfrequency as\
    \ the solution is output to disk.</p>\n<p>To add a new executable using these\
    \ auxiliary source code file(s), update\n<a href=\"./src/CMakeLists.txt\">src/CMakeLists.txt</a>\
    \ to include a new call to\n<code>sundemo_add_executable</code> in a similar manner\
    \ as the existing test problems e.g.,\n<code>hurricane_yz.exe</code>.</p>\n<h2><a\
    \ id=\"user-content-authors\" class=\"anchor\" aria-hidden=\"true\" href=\"#authors\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Authors</h2>\n\
    <p><a href=\"https://people.smu.edu/dreynolds\" rel=\"nofollow\">Daniel R. Reynolds</a>\
    \ and\n<a href=\"https://people.llnl.gov/gardner48\" rel=\"nofollow\">David J.\
    \ Gardner</a></p>\n"
  stargazers_count: 3
  subscribers_count: 4
  topics: []
  updated_at: 1655924113.0
supercontainers/sc-tutorials:
  data_format: 2
  description: SC Tutorials
  filenames:
  - exercises/spack_containerize/spack.yaml
  full_name: supercontainers/sc-tutorials
  latest_release: null
  readme: '<h1><a id="user-content-getting-started-with-containers-on-hpc" class="anchor"
    aria-hidden="true" href="#getting-started-with-containers-on-hpc"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Getting Started with Containers on HPC</h1>

    <p>View this on the <a href="https://supercontainers.github.io/sc-tutorials/"
    rel="nofollow">Tutorial Homepage</a>.</p>

    <h2><a id="user-content-hpc-containers-tutorial-session" class="anchor" aria-hidden="true"
    href="#hpc-containers-tutorial-session"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>HPC Containers Tutorial Session</h2>

    <p><a target="_blank" rel="noopener noreferrer" href="fig/ecp.jpg"><img src="fig/ecp.jpg"
    width="200" style="max-width: 100%;"></a><a target="_blank" rel="noopener noreferrer"
    href="fig/pawsey.png"><img src="fig/pawsey.png" width="200" style="max-width:
    100%;"></a><a target="_blank" rel="noopener noreferrer" href="fig/redhat.png"><img
    src="fig/redhat.png" width="200" style="max-width: 100%;"></a></p>

    <h2><a id="user-content-details" class="anchor" aria-hidden="true" href="#details"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Details</h2>

    <p>Full-day Tutorial Session</p>

    <p>Venue: Supercomputing Conference (SC 22)</p>

    <p>Date: Sunday November 13, 2022 8:30am - 5pm Central Standard Time (GMT -6)</p>

    <p>Location: Dallas TX, USA</p>

    <p>Link: <a href="https://sc22.supercomputing.org/presentation/?id=tut111&amp;sess=sess201"
    rel="nofollow">SC 2022 Tutorial Details</a></p>

    <p>Keywords: Containerized HPC, System Software and Runtime Systems, Scientific
    Software Development, DevOps</p>

    <h2><a id="user-content-abstract" class="anchor" aria-hidden="true" href="#abstract"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Abstract</h2>

    <p>Within just the past few years, the use of containers has revolutionized the
    way in which industries and enterprises have developed and deployed computational
    software and distributed systems. The containerization model has gained traction
    within the HPC community as well with the promise of improved reliability, reproducibility,
    portability, and levels of customization that were previously not possible on
    supercomputers. This adoption has been enabled by a number of HPC Container runtimes
    that have emerged including Singularity, Shifter, Enroot, Charliecloud and others.</p>

    <p>This hands-on tutorial looks to train users on the usability of containers
    on HPC resources. We will provide a detailed background on Linux containers, along
    with introductory hands-on experience building a container image, sharing the
    container and running it on a HPC cluster. Furthermore, the tutorial will provide
    more advanced information on how to run MPI-based and GPU-enabled HPC applications,
    how to optimize I/O intensive workflows, and how to setup GUI enabled interactive
    sessions. Cutting-edge examples will include machine learning and bioinformatics.
    Users will leave the tutorial with a solid foundational understanding of how to
    utilize containers with HPC resources through Shifter and Singularity, as well
    as an in-depth knowledge to deploy custom containers on their own resources.</p>

    <h2><a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>Please consult the website for prerequisites and recommended setup steps.</p>

    <h2><a id="user-content-questions" class="anchor" aria-hidden="true" href="#questions"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Questions</h2>

    <p>You can ask questions verbally or with this <a href="https://docs.google.com/document/d/11gMZ-T7iA5XiRWPLYIqX7Gqv7RMb-NF9kzGYHrnOi04/edit?usp=sharing"
    rel="nofollow">Google Doc</a>.

    Please append your question below the others in the document.</p>

    <p>We have also created a Slack Team for this.  The invitation link is <a href="https://join.slack.com/t/hpc-containers/shared_invite/enQtODI3NzY1NDU4OTk5LTUxOTgyOWJmYjIwOWI5YWU2MzBhZDI3Zjc1YmZmMjAxZjgzYzk4ZWEwNmFlNzlkOWI0MGNlZDNlMTBhYTBlOWY"
    rel="nofollow">here</a>.</p>

    <h2><a id="user-content-schedule---autogenerated-from-the-metadata" class="anchor"
    aria-hidden="true" href="#schedule---autogenerated-from-the-metadata"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Schedule - Autogenerated from the metadata</h2>

    '
  stargazers_count: 1
  subscribers_count: 7
  topics: []
  updated_at: 1649669614.0
tgamblin/cali-container:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: tgamblin/cali-container
  latest_release: null
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1667175253.0
thomas-bouvier/spack-envs:
  data_format: 2
  description: My Spack environments
  filenames:
  - local/spack.yaml
  - gemini/spack.yaml
  - cooley/spack.yaml
  - chifflot/p100/spack.yaml
  - chifflot/v100/spack.yaml
  - thetagpu/spack.yaml
  full_name: thomas-bouvier/spack-envs
  latest_release: null
  readme: '<h1><a id="user-content-spack-envs" class="anchor" aria-hidden="true" href="#spack-envs"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>spack-envs</h1>

    <pre><code>git clone -c feature.manyFiles=true https://github.com/spack/spack.git
    ~/spack

    git clone https://github.com/mochi-hpc/mochi-spack-packages.git ~/mochi-spack-packages

    git clone https://github.com/thomas-bouvier/spack-envs.git ~/spack-envs

    </code></pre>

    <h2><a id="user-content-locally" class="anchor" aria-hidden="true" href="#locally"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Locally</h2>

    <div class="highlight highlight-text-shell-session"><pre><span class="pl-c1">spack
    config --scope defaults edit config</span>

    <span class="pl-c1">install_tree: $spack/opt/spack</span>

    <span class="pl-c1">build_stage: $user_cache_path/stage</span>


    <span class="pl-c1">spack env activate ~/Dev/spack-envs/local</span>

    <span class="pl-c1">spack install</span></pre></div>

    <h2><a id="user-content-g5k" class="anchor" aria-hidden="true" href="#g5k"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>G5k</h2>

    <div class="highlight highlight-text-shell-session"><pre><span class="pl-c1">spack
    config --scope defaults edit config</span>

    <span class="pl-c1">install_tree: /mnt/spack</span>

    <span class="pl-c1">build_stage: /tmp/spack-stage</span></pre></div>

    <h2><a id="user-content-anl" class="anchor" aria-hidden="true" href="#anl"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>ANL</h2>

    <h3><a id="user-content-cooley" class="anchor" aria-hidden="true" href="#cooley"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Cooley</h3>

    <p>Before using Spack to compile stuff on Cooley, we recommend to run <code>use_build_cooley</code>
    to get access to newer gcc, cmake, and mvapich versions.</p>

    <h3><a id="user-content-thetagpu" class="anchor" aria-hidden="true" href="#thetagpu"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>ThetaGPU</h3>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1678891926.0
toxa81/se:
  data_format: 2
  description: Software environments
  filenames:
  - catalog-config/core/spack.yaml
  - catalog-config/compilers/nvhpc-22.9/spack.yaml
  - catalog-config/compilers/gcc-11.3.0/spack.yaml
  - catalog-config/libxc-5.2.3/spack.yaml
  full_name: toxa81/se
  latest_release: null
  readme: '<h1><a id="user-content-software-environments" class="anchor" aria-hidden="true"
    href="#software-environments"><span aria-hidden="true" class="octicon octicon-link"></span></a>Software
    environments</h1>

    <p>Deployment steps</p>

    <ul>

    <li>clone spack <code>git clone https://github.com/spack/spack.git</code>

    </li>

    <li>enable spack <code>source enable-spack</code>

    </li>

    <li>srun -N2 -n8 --partition=nvgpu spack -e ./env-spec/core install -j16</li>

    <li>install gcc-11.3.0 view <code>spack -e  ./env-spec/gcc-11.3.0/ install</code>

    </li>

    <li>install nvhpc-22.9 <code>srun -N1 --partition=nvgpu spack -e . install -j64</code>

    </li>

    </ul>

    <p>spack compiler find $(spack find --format {prefix.bin} gcc@11)</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1669195550.0
tpeterka/mpas-o-workflow:
  data_format: 2
  description: null
  filenames:
  - mpas_spack.yaml
  full_name: tpeterka/mpas-o-workflow
  latest_release: null
  readme: '<h1><a id="user-content-instructions-for-building-mpas-ocean-to-run-in-a-wilkins-workflow"
    class="anchor" aria-hidden="true" href="#instructions-for-building-mpas-ocean-to-run-in-a-wilkins-workflow"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Instructions for Building
    MPAS-Ocean to Run in a Wilkins Workflow</h1>

    <p>Installation is done through Spack. If you don''t have Spack installed or if
    Spack is new to you, go <a href="https://spack.readthedocs.io/en/latest/" rel="nofollow">here</a>
    first.</p>

    <p>Clone this repository and cd into it. These instructions assume there is a
    top-level directory called climate.</p>

    <pre><code>mkdir ~/climate

    cd ~/climate

    git clone https://github.com/tpeterka/mpas-o-workflow

    cd mpas-o-workflow

    </code></pre>

    <hr>

    <h2><a id="user-content-setting-up-spack-environment" class="anchor" aria-hidden="true"
    href="#setting-up-spack-environment"><span aria-hidden="true" class="octicon octicon-link"></span></a>Setting
    up Spack environment</h2>

    <h3><a id="user-content-first-time-create-and-load-the-spack-environment-for-mpas-ocean"
    class="anchor" aria-hidden="true" href="#first-time-create-and-load-the-spack-environment-for-mpas-ocean"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>First time: create
    and load the Spack environment for MPAS-Ocean</h3>

    <pre><code>cd ~/climate/mpas-o-workflow

    source ./create-mpas.sh     # requires being in the same directory to work properly

    </code></pre>

    <h3><a id="user-content-subsequent-times-load-the-spack-environment-for-mpas-ocean"
    class="anchor" aria-hidden="true" href="#subsequent-times-load-the-spack-environment-for-mpas-ocean"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Subsequent times: load
    the Spack environment for MPAS-Ocean</h3>

    <pre><code>source ~/climate/mpas-o-workflow/load-mpas.sh

    </code></pre>

    <hr>

    <h2><a id="user-content-building-mpas-ocean" class="anchor" aria-hidden="true"
    href="#building-mpas-ocean"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    MPAS-Ocean</h2>

    <h3><a id="user-content-first-time-clone-mpas-ocean" class="anchor" aria-hidden="true"
    href="#first-time-clone-mpas-ocean"><span aria-hidden="true" class="octicon octicon-link"></span></a>First
    time: clone MPAS-Ocean</h3>

    <pre><code>cd ~/climate

    git clone https://github.com/E3SM-Project/E3SM

    cd E3SM

    git submodule update --init --recursive

    </code></pre>

    <h2><a id="user-content-first-time-modify-mpas-ocean-makefiles-to-link-to-henson"
    class="anchor" aria-hidden="true" href="#first-time-modify-mpas-ocean-makefiles-to-link-to-henson"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>First time: modify
    MPAS-Ocean makefiles to link to Henson</h2>

    <p>Edit ~climate/E3SM/components/mpas-ocean/Makefile:</p>

    <p>Insert at line 596:

    <code>LIBS += -L $(HENSON)/lib -lhenson</code></p>

    <p>Insert at line 732:

    <code>LDFLAGS += -shared</code></p>

    <p>Edit line 1002 to add .so to executable name: <code>$(EXE_NAME).so</code></p>

    <h3><a id="user-content-build-mpas-ocean" class="anchor" aria-hidden="true" href="#build-mpas-ocean"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build MPAS-Ocean</h3>

    <pre><code>cd ~/climate/E3SM/components/mpas-ocean

    make clean              # if dirty

    make -j gfortran

    </code></pre>

    <p>This will take ~ 5 minutes to compile.</p>

    <h3><a id="user-content-create-a-run-script-for-mpas-ocean" class="anchor" aria-hidden="true"
    href="#create-a-run-script-for-mpas-ocean"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Create a run script for MPAS-Ocean</h3>

    <p>edit (create) <code>~/climate/E3SM/components/mpas-ocean/ocean_model</code>:

    <code>python3 ~/climate/mpas-o-workflow/mpas-henson.py</code></p>

    <p>Set permissions of <code>ocean_model</code> to executable:

    <code>chmod 755 ~/climate/E3SM/components/mpas-ocean/ocean_model</code></p>

    <hr>

    <h2><a id="user-content-setting-up-a-test-case-to-execute" class="anchor" aria-hidden="true"
    href="#setting-up-a-test-case-to-execute"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Setting up a test case to execute</h2>

    <p>Compass is an E3SM system for generating and running test cases for MPAS-Ocean,
    and relies on conda environments. The instructions below assume you have cond
    or miniconda already installed. If not, go <a href="https://docs.conda.io/en/latest/miniconda.html"
    rel="nofollow">here</a> first.</p>

    <h3><a id="user-content-first-time-install-compass-and-create-compass-environment"
    class="anchor" aria-hidden="true" href="#first-time-install-compass-and-create-compass-environment"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>First time: install
    Compass and create Compass environment</h3>

    <pre><code>cd ~

    git clone https://github.com/MPAS-Dev/compass.git compass-env-only

    cd ~/compass-env-only

    git submodule update --init --recursive

    ./conda/configure_compass_env.py --conda ~/miniconda3 --env_only

    source load_dev_compass_1.2.0-alpha.4.sh        # load_dev_compass-1.2.0-alpha.4.sh
    is the script created by the previous command

    </code></pre>

    <h3><a id="user-content-first-time-create-a-compass-configuration-file-for-a-new-machine"
    class="anchor" aria-hidden="true" href="#first-time-create-a-compass-configuration-file-for-a-new-machine"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>First time: create
    a compass configuration file for a new machine</h3>

    <p>Assumes the config file is named ~/compass-env-only/compass.cfg and has these
    contents, or similar (yours may vary)</p>

    <pre><code># This file contains some common config options you might want to set


    # The paths section describes paths to databases and shared compass environments

    [paths]


    # A root directory where MPAS standalone data can be found

    database_root = /home/tpeterka/compass/mpas_standalonedata


    # The parallel section describes options related to running tests in parallel

    [parallel]


    # parallel system of execution: slurm or single_node

    system = single_node


    # whether to use mpirun or srun to run the model

    parallel_executable = mpiexec


    # cores per node on the machine, detected automatically by default

    # cores_per_node = 4

    </code></pre>

    <h3><a id="user-content-first-time-create-test-case-for-the-executable" class="anchor"
    aria-hidden="true" href="#first-time-create-test-case-for-the-executable"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>First time: create
    test case for the executable</h3>

    <p>Assumes that <code>load_dev_compass_1.2.0-alpha.4.sh</code> is the name of
    the conda environment load script created initially</p>

    <pre><code>source ~/compass-env-only/load_dev_compass_1.2.0-alpha.4.sh

    compass setup -t ocean/baroclinic_channel/10km/default -w ~/spack-baroclinic-test
    -p ~/climate/E3SM/components/mpas-ocean -f ~/compass-env-only/compass.cfg

    </code></pre>

    <h3><a id="user-content-run-the-test-case" class="anchor" aria-hidden="true" href="#run-the-test-case"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Run the test case</h3>

    <p>Assumes that <code>load_dev_compass_1.2.0-alpha.4.sh</code> is the name of
    the conda environment load script created initially</p>

    <pre><code>source ~/compass-env-only/load_dev_compass_1.2.0-alpha.4.sh

    source ~/climate/mpas-o-workflow/load-mpas.sh

    cd ~/spack-baroclinic-test/ocean/baroclinic_channel/10km/default

    compass run

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1679076329.0
tvandera/spack-repos:
  data_format: 2
  description: null
  filenames:
  - var/spack/environments/karolina/bpmf-ompss-argo/spack.yaml
  - var/spack/environments/intelmpi/bpmf-ompss-argo/spack.yaml
  - var/spack/environments/karolina/bpmf-ompss-cluster/spack.yaml
  - var/spack/environments/intelmpi/bpmf-ompss-cluster/spack.yaml
  full_name: tvandera/spack-repos
  latest_release: null
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1635166163.0
ucdavis/spack-ucdavis:
  data_format: 2
  description: null
  filenames:
  - environments/hpccf/franklin/cluster-core/spack.yaml
  - environments/hpccf/franklin/general/spack.yaml
  - environments/hpccf/farm/general/spack.yaml
  full_name: ucdavis/spack-ucdavis
  latest_release: null
  readme: '<h1><a id="user-content-spack--uc-davis" class="anchor" aria-hidden="true"
    href="#spack--uc-davis"><span aria-hidden="true" class="octicon octicon-link"></span></a>Spack
    @ UC Davis</h1>

    <h2><a id="user-content-spack-repos-and-configs-for-uc-davis-hpccf-clusters" class="anchor"
    aria-hidden="true" href="#spack-repos-and-configs-for-uc-davis-hpccf-clusters"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Spack repos and configs
    for UC Davis HPCCF Clusters</h2>

    <p>This repo contains package specs, configurations, and utility scripts for

    <a href="https://spack.readthedocs.io/en/latest/index.html" rel="nofollow">spack</a>
    deployments on

    clusters managed by the UC Davis High Performance Computing Core Facility.</p>

    <p>The structure of this repo is as follows:</p>

    <ul>

    <li>

    <code>repos/hpccf</code>: Our spack package specifications. This includes both
    overrides

    of <code>builtin</code> and from-scratch specs. The packages are namespaced under
    <code>ucdavis.hpccf</code>.</li>

    <li>

    <code>templates/hpccf</code>: Template extensions for module management.</li>

    <li>

    <code>config/hpccf/[SITE]</code>: Site-specific configuration files. <code>[SITE]</code>
    directories

    correspond to cluster names. These are linked to <code>${SPACK_ROOT}/etc/spack/</code>
    when deployed.</li>

    <li>

    <code>bin</code>: Utility scripts.</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1676322811.0
ukri-excalibur/excalibur-tests:
  data_format: 2
  description: Performance benchmarks and regression tests for the ExCALIBUR project
  filenames:
  - benchmarks/spack/isambard-xci/compute-node/spack.yaml
  - benchmarks/spack/github-actions/default/spack.yaml
  - benchmarks/spack/isambard-a64fx/compute-node/spack.yaml
  - benchmarks/spack/dial3/compute-node/spack.yaml
  - benchmarks/spack/myriad/compute-node/spack.yaml
  - benchmarks/spack/csd3-icelake/compute-node/spack.yaml
  - benchmarks/spack/tesseract/compute-node/spack.yaml
  - benchmarks/spack/archer2/compute-node/spack.yaml
  - benchmarks/spack/cosma8/compute-node/spack.yaml
  - benchmarks/spack/isambard-cascadelake/compute-node/spack.yaml
  - benchmarks/spack/csd3-skylake/compute-node/spack.yaml
  - benchmarks/spack/tursa/cpu/spack.yaml
  full_name: ukri-excalibur/excalibur-tests
  latest_release: null
  readme: "<h1><a id=\"user-content-excalibur-tests\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#excalibur-tests\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>ExCALIBUR tests</h1>\n<p>Performance benchmarks and regression tests\
    \ for the ExCALIBUR project.</p>\n<p>These benchmarks are based on a similar project\
    \ by\n<a href=\"https://github.com/stackhpc/hpc-tests\">StackHPC</a>.</p>\n<p><em><strong>Note</strong>:\
    \ at the moment the ExCALIBUR benchmarks are a work-in-progress.</em></p>\n<h2><a\
    \ id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #installation\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <p>It is recommended to install the <strong>excalibur-tests</strong> package with\
    \ <code>pip</code> by</p>\n<div class=\"highlight highlight-source-shell\"><pre>pip\
    \ install <span class=\"pl-c1\">.</span></pre></div>\n<p>On most systems, it is\
    \ recommended to install the package in a virtual environment.\nFor example, using\
    \ the python3 <a href=\"https://docs.python.org/3/library/venv.html\" rel=\"nofollow\"\
    >built-in virtual environment tool <code>venv</code></a>,\ncreate an environment\
    \ called <code>my_environment</code> with</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>python3 -m venv ./my_environment</pre></div>\n<p>and activate it with</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c1\">source</span>\
    \ ./my_environment/bin/activate</pre></div>\n<p>For <a href=\"https://setuptools.pypa.io/en/latest/userguide/development_mode.html\"\
    \ rel=\"nofollow\">development</a>,\npass the <code>-e/--editable</code> flag\
    \ to <code>pip</code> to link the installed package to the files in the local\n\
    directory, instead of copying, to be able to make changes to the installed package.</p>\n\
    <h2><a id=\"user-content-requirements\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#requirements\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Requirements</h2>\n<p>The pip install will install a compatible version\
    \ of <strong>ReFrame</strong> from\n<a href=\"https://pypi.org/project/ReFrame-HPC/\"\
    \ rel=\"nofollow\">PyPi</a>. However, you will have to\nmanually provide an installation\
    \ of <strong>Spack</strong>.</p>\n<h3><a id=\"user-content-spack\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#spack\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Spack</h3>\n<p><a href=\"https://spack.io/\" rel=\"\
    nofollow\">Spack</a> is a package manager specifically designed for HPC\nfacilities.\
    \ In some HPC facilities there may be already a central Spack installation available.\n\
    However, the version installed is most likely too old to support all the features\n\
    used by this package. Therefore we recommend you install the latest version locally,\n\
    following the instructions below.</p>\n<p><em><strong>Note</strong>: if you have\
    \ already installed spack locally and you want to upgrade to\na newer version,\
    \ you might first have to clear the cache to avoid conflicts:\n<code>spack clean\
    \ -m</code></em></p>\n<p>Follow the <a href=\"https://spack.readthedocs.io/en/latest/getting_started.html\"\
    \ rel=\"nofollow\">official instructions</a>\nto install the latest version of\
    \ Spack (summarised here for convenience, but not guaranteed to be\nup-to-date):</p>\n\
    <ul>\n<li>git clone spack:\n<code>git clone -c feature.manyFiles=true https://github.com/spack/spack.git</code>\n\
    </li>\n<li>run spack setup script: <code>source ./spack/share/spack/setup-env.sh</code>\n\
    </li>\n<li>check spack is in <code>$PATH</code>, for example <code>spack --version</code>\n\
    </li>\n</ul>\n<p>In order to use Spack in ReFrame, the framework we use to run\
    \ the benchmarks\n(see below), the directory where the <code>spack</code> program\
    \ is installed needs to be in\nthe <code>PATH</code> environment variable. This\
    \ is taken care of by the <code>setup-env.sh</code>\nscript as above, and you\
    \ can have your shell init script (e.g. <code>.bashrc</code>)\ndo that automatically\
    \ in every session, by adding the following lines to it:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre><span class=\"pl-k\">export</span> SPACK_ROOT=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>/path/to/spack<span class=\"\
    pl-pds\">\"</span></span>\n<span class=\"pl-k\">if</span> [ <span class=\"pl-k\"\
    >-f</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"\
    pl-smi\">${SPACK_ROOT}</span>/share/spack/setup-env.sh<span class=\"pl-pds\">\"\
    </span></span> ]<span class=\"pl-k\">;</span> <span class=\"pl-k\">then</span>\n\
    \    <span class=\"pl-c1\">source</span> <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span><span class=\"pl-smi\">${SPACK_ROOT}</span>/share/spack/setup-env.sh<span\
    \ class=\"pl-pds\">\"</span></span>\n<span class=\"pl-k\">fi</span></pre></div>\n\
    <p>replacing <code>/path/to/spack</code> with the actual path to your Spack installation.</p>\n\
    <p>ReFrame also requires a <a href=\"https://spack.readthedocs.io/en/latest/environments.html\"\
    \ rel=\"nofollow\">Spack\nEnvironment</a>.  We\nprovide Spack environments for\
    \ some of the systems that are part of the\nExCALIBUR and DiRAC projects in\n\
    <a href=\"https://github.com/ukri-excalibur/excalibur-tests/tree/main/benchmarks/spack\"\
    >https://github.com/ukri-excalibur/excalibur-tests/tree/main/benchmarks/spack/</a>.\n\
    If you want to use a different Spack environment,\nset the environment variable\
    \ <code>EXCALIBUR_SPACK_ENV</code> to the path of the directory\nwhere the environment\
    \ is.  If this is not set, ReFrame will try to use the\nenvironment for the current\
    \ system if known, otherwise it will automatically\ncreate a very basic environment\
    \ (see \"Usage on unsupported systems\" section below).</p>\n<h3><a id=\"user-content-reframe\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#reframe\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>ReFrame</h3>\n<p><a href=\"https://reframe-hpc.readthedocs.io/en/stable/\"\
    \ rel=\"nofollow\">ReFrame</a> is a high-level\nframework for writing regression\
    \ tests for HPC systems.  For our tests we\nrequire ReFrame 3 version 3.11.0,\
    \ or later. We are currently not compatible\nwith ReFrame 4.</p>\n<p>If you need\
    \ to manually install ReFrame, follow the <a href=\"https://reframe-hpc.readthedocs.io/en/stable/started.html\"\
    \ rel=\"nofollow\">official\ninstructions</a> to\ninstall this package.  Note\
    \ that ReFrame requires Python 3.6: in your HPC system\nyou may need to load a\
    \ specific module to have this version of Python available.</p>\n<p>We provide\
    \ a ReFrame configuration file with the settings of some systems that\nare part\
    \ of the ExCALIBUR or DiRAC projects.  You can point ReFrame to this file by\n\
    setting the\n<a href=\"https://reframe-hpc.readthedocs.io/en/stable/manpage.html#envvar-RFM_CONFIG_FILE\"\
    \ rel=\"nofollow\"><code>RFM_CONFIG_FILE</code></a>\nenvironment variable:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span>\
    \ RFM_CONFIG_FILE=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"\
    pl-smi\">${PWD}</span>/benchmarks/reframe_config.py<span class=\"pl-pds\">\"</span></span></pre></div>\n\
    <p>If you want to use a different ReFrame configuration file, for example because\n\
    you use a different system, you can set this environment variable to the path\
    \ of\nthat file.</p>\n<p><strong>Note</strong>: in order to use the Spack build\
    \ system in ReFrame, the <code>spack</code>\nexecutable must be in the <code>PATH</code>\
    \ also on the compute nodes of a cluster, if\nyou want to run your benchmarks\
    \ on them. This is taken care of by adding it\nto your init file (see spack section\
    \ above).</p>\n<p>However, you will also need to set the\n<a href=\"https://reframe-hpc.readthedocs.io/en/stable/manpage.html#envvar-RFM_USE_LOGIN_SHELL\"\
    \ rel=\"nofollow\"><code>RFM_USE_LOGIN_SHELL</code></a>\nenvironment variable\
    \ (<code>export RFM_USE_LOGIN_SHELL=\"Yes\"</code>) in order to make ReFrame use</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">!</span><span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span>/bin/bash -l</span></pre></div>\n\
    <p>as <a href=\"https://en.wikipedia.org/wiki/Shebang_(Unix)\" rel=\"nofollow\"\
    >shebang</a> line, which would load\nthe user's init script.</p>\n<h2><a id=\"\
    user-content-usage\" class=\"anchor\" aria-hidden=\"true\" href=\"#usage\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage</h2>\n\
    <p>Once you have set up Spack and ReFrame, you can execute a benchmark with</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>reframe -c benchmarks/apps/BENCH_NAME\
    \ -r --performance-report</pre></div>\n<p>where <code>benchmarks/apps/BENCH_NAME</code>\
    \ is the directory where the benchmark is.  The command\nabove assumes you have\
    \ the program <code>reframe</code> in your PATH.  If you have followed the instructions\n\
    to install using <code>pip</code> into the default directory, it should have been\
    \ automatically added.\nIf it is not the case, call <code>reframe</code> with\
    \ its relative or absolute path.</p>\n<p>For example, to run the Sombrero benchmark\
    \ in the <code>benchmarks/apps/sombrero</code> directory you can\nuse</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>reframe -c benchmarks/apps/sombrero\
    \ -r --performance-report</pre></div>\n<p>For benchmarks that use the Spack build\
    \ system, the tests define a default Spack specification\nto be installed in the\
    \ environment, but users can change it when invoking ReFrame on the\ncommand line\
    \ with the\n<a href=\"https://reframe-hpc.readthedocs.io/en/stable/manpage.html#cmdoption-S\"\
    \ rel=\"nofollow\"><code>-S</code></a> option to set\nthe <code>spack_spec</code>\
    \ variable:</p>\n<pre><code>reframe -c benchmarks/apps/sombrero -r --performance-report\
    \ -S spack_spec='sombrero@2021-08-16%intel'\n</code></pre>\n<h3><a id=\"user-content-setting-environment-variables\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#setting-environment-variables\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Setting\
    \ environment variables</h3>\n<p>All the built-in fields of ReFrame regression\
    \ classes can be set on a per-job basis using the\n<code>-S</code> command-line\
    \ option. One useful such field is\n<a href=\"https://reframe-hpc.readthedocs.io/en/stable/regression_test_api.html#reframe.core.pipeline.RegressionTest.variables\"\
    \ rel=\"nofollow\"><code>variables</code></a>,\nwhich controls the environment\
    \ variables used in a job.\nThe syntax to set dictionary items, like for <code>variables</code>,\
    \ is a comma-separated list of <code>key:value</code> pairs: <code>-S dict=key_1:value_1,key_2:value_2</code>.\n\
    For example</p>\n<pre><code>reframe -c benchmarks/apps/sombrero -r --performance-report\
    \ -S variables=OMP_PLACES:threads\n</code></pre>\n<p>runs the <code>benchmarks/apps/sombrero</code>\
    \ benchmark setting the environment variable <code>OMP_PLACES</code>\nto <code>threads</code>.</p>\n\
    <h3><a id=\"user-content-selecting-system-and-queue-access-options\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#selecting-system-and-queue-access-options\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Selecting\
    \ system and queue access options</h3>\n<p>The provided ReFrame configuration\
    \ file contains the settings for multiple systems.  If you\nuse it, the automatic\
    \ detection of the system may fail, as some systems may use clashing\nhostnames.\
    \  To avoid this, you can use the flag <a href=\"https://reframe-hpc.readthedocs.io/en/stable/manpage.html#cmdoption-system\"\
    \ rel=\"nofollow\"><code>--system NAME:PARTITION</code></a>\nto specify the system\
    \ (and optionally the partition) to use.</p>\n<p>Additionally, if submitting jobs\
    \ to the compute nodes requires additional options, like for\nexample the resource\
    \ group you belong to (for example <code>--account=...</code> for Slurm), you\
    \ have\nto pass the command line flag\n<a href=\"https://reframe-hpc.readthedocs.io/en/stable/manpage.html#cmdoption-J\"\
    \ rel=\"nofollow\"><code>--job-option=...</code></a>\nto <code>reframe</code>\
    \ (e.g., <code>--job-option='--account=...'</code>).</p>\n<h3><a id=\"user-content-usage-on-unsupported-systems\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#usage-on-unsupported-systems\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage on\
    \ unsupported systems</h3>\n<p>The configuration provided in <a href=\"./reframe_config.py\"\
    ><code>reframe_config.py</code></a> lets you run the\nbenchmarks on pre-configured\
    \ HPC systems.  However you\ncan use this framework on any system by choosing\
    \ the \"generic\" system with <code>--system generic</code>, or by using your\
    \ own ReFrame configuration.  You can use the \"generic\" system to run\nbenchmarks\
    \ in ReFrame without using a queue manager or an MPI launcher (e.g. on a personal\
    \ workstation).</p>\n<p>If you choose the \"generic\" system and a benchmark using\
    \ the Spack build system,\na new empty Spack environment will be automatically\
    \ created in\n<code>spack-environments/generic</code> when ReFrame is launched\
    \ for the first time.\nYou should populate the environment with the packages already\
    \ installed on your system\nbefore running Spack to avoid excessively rebuilding\
    \ system packages. See the\n<em>Spack configuration</em> section of <a href=\"\
    ./CONTRIBUTING.md\"><code>CONTRIBUTING.md</code></a> for instructions on how\n\
    to set up a Spack environment.\nIn particular, make sure that at least a compiler\
    \ and an MPI library are added into the environment.\nAfter the Spack environment\
    \ is set up, tell ReFrame to use it by setting the environment\nvariable <code>EXCALIBUR_SPACK_ENV</code>,\
    \ as described above.</p>\n<h3><a id=\"user-content-system-specific-flags\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#system-specific-flags\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>System-specific flags</h3>\n\
    <p>While the aim is to automate as much system-specific configuration as possible,\
    \ there are some options that have to be provided by the user, such as accounting\
    \ details, and unfortunately the syntax can vary.\nThe file <a href=\"./SYSTEMS.md\"\
    ><code>SYSTEMS.md</code></a> contains information about the use of this framework\
    \ on specific systems.</p>\n<h2><a id=\"user-content-contributing-new-systems-or-benchmarks\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#contributing-new-systems-or-benchmarks\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contributing\
    \ new systems or benchmarks</h2>\n<p>Feel free to add new benchmark apps or support\
    \ new systems that are part of the\nExCALIBUR benchmarking collaboration.  Read\
    \ <a href=\"./CONTRIBUTING.md\"><code>CONTRIBUTING.md</code></a> for more details.</p>\n"
  stargazers_count: 9
  subscribers_count: 7
  topics: []
  updated_at: 1679052649.0
uturuncoglu/testing:
  data_format: 2
  description: It is used for component testing and GitHub Action implementation
  filenames:
  - spack.yaml
  full_name: uturuncoglu/testing
  latest_release: null
  readme: '<h1><a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>testing</h1>

    <p>It is used for component testing and GitHub Action implementation</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1657212117.0
waynemitchell/mfem:
  data_format: 2
  description: 'Mirror of MFEM - a lightweight, general, scalable C++ library for
    finite element methods. Please use the official repository, https://github.com/mfem/mfem,
    to create issues and pull requests. See also the MFEM website: '
  filenames:
  - config/docker/spack.yaml
  full_name: waynemitchell/mfem
  latest_release: null
  readme: "<pre><code>                Finite Element Discretization Library\n    \
    \                           __\n                   _ __ ___   / _|  ___  _ __\
    \ ___\n                  | '_ ` _ \\ | |_  / _ \\| '_ ` _ \\\n               \
    \   | | | | | ||  _||  __/| | | | | |\n                  |_| |_| |_||_|   \\___||_|\
    \ |_| |_|\n\n                           https://mfem.org\n</code></pre>\n<p><a\
    \ href=\"https://mfem.org\" rel=\"nofollow\">MFEM</a> is a modular parallel C++\
    \ library for finite element\nmethods. Its goal is to enable high-performance\
    \ scalable finite element\ndiscretization research and application development\
    \ on a wide variety of\nplatforms, ranging from laptops to supercomputers.</p>\n\
    <p>We welcome contributions and feedback from the community. Please see the file\n\
    <a href=\"CONTRIBUTING.md\">CONTRIBUTING.md</a> for additional details about our\
    \ development\nprocess.</p>\n<ul>\n<li>\n<p>For building instructions, see the\
    \ file <a href=\"INSTALL\">INSTALL</a>, or type \"make help\".</p>\n</li>\n<li>\n\
    <p>Copyright and licensing information can be found in files <a href=\"LICENSE\"\
    >LICENSE</a> and <a href=\"NOTICE\">NOTICE</a>.</p>\n</li>\n<li>\n<p>The best\
    \ starting point for new users interested in MFEM's features is to\nreview the\
    \ examples and miniapps at <a href=\"https://mfem.org/examples\" rel=\"nofollow\"\
    >https://mfem.org/examples</a>.</p>\n</li>\n<li>\n<p>Instructions for learning\
    \ with Docker are in <a href=\"config/docker\">config/docker</a>.</p>\n</li>\n\
    </ul>\n<p>Conceptually, MFEM can be viewed as a finite element toolbox that provides\
    \ the\nbuilding blocks for developing finite element algorithms in a manner similar\
    \ to\nthat of MATLAB for linear algebra methods. In particular, MFEM provides\
    \ support\nfor arbitrary high-order H1-conforming, discontinuous (L2), H(div)-conforming,\n\
    H(curl)-conforming and NURBS finite element spaces in 2D and 3D, as well as many\n\
    bilinear, linear and nonlinear forms defined on them. It enables the quick\nprototyping\
    \ of various finite element discretizations, including Galerkin\nmethods, mixed\
    \ finite elements, Discontinuous Galerkin (DG), isogeometric\nanalysis, hybridization\
    \ and Discontinuous Petrov-Galerkin (DPG) approaches.</p>\n<p>MFEM includes classes\
    \ for dealing with a wide range of mesh types: triangular,\nquadrilateral, tetrahedral\
    \ and hexahedral, as well as surface and topologically\nperiodical meshes. It\
    \ has general support for mesh refinement, including local\nconforming and non-conforming\
    \ (AMR) adaptive refinement. Arbitrary element\ntransformations, allowing for\
    \ high-order mesh elements with curved boundaries,\nare also supported.</p>\n\
    <p>When used as a \"finite element to linear algebra translator\", MFEM can take\
    \ a\nproblem described in terms of finite element-type objects, and produce the\n\
    corresponding linear algebra vectors and fully or partially assembled operators,\n\
    e.g. in the form of global sparse matrices or matrix-free operators. The library\n\
    includes simple smoothers and Krylov solvers, such as PCG, MINRES and GMRES, as\n\
    well as support for sequential sparse direct solvers from the SuiteSparse\nlibrary.\
    \ Nonlinear solvers (the Newton method), eigensolvers (LOBPCG), and\nseveral explicit\
    \ and implicit Runge-Kutta time integrators are also available.</p>\n<p>MFEM supports\
    \ MPI-based parallelism throughout the library, and can readily be\nused as a\
    \ scalable unstructured finite element problem generator. Starting with\nversion\
    \ 4.0, MFEM offers support for GPU acceleration, and programming models,\nsuch\
    \ as CUDA, HIP, OCCA, RAJA and OpenMP. MFEM-based applications require\nminimal\
    \ changes to switch from a serial to a highly-performant MPI-parallel\nversion\
    \ of the code, where they can take advantage of the integrated linear\nsolvers\
    \ from the hypre library. Comprehensive support for other external\npackages,\
    \ e.g. PETSc, SUNDIALS and libCEED is also included, giving access to\nadditional\
    \ linear and nonlinear solvers, preconditioners, time integrators, etc.</p>\n\
    <p>For examples of using MFEM, see the <a href=\"examples\">examples/</a> and\
    \ <a href=\"miniapps\">miniapps/</a>\ndirectories, as well as the OpenGL visualization\
    \ tool GLVis which is available\nat <a href=\"https://glvis.org\" rel=\"nofollow\"\
    >https://glvis.org</a>.</p>\n<h2><a id=\"user-content-license\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#license\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>License</h2>\n<p>MFEM is distributed under the terms\
    \ of the BSD-3 license. All new contributions\nmust be made under this license.\
    \ See <a href=\"LICENSE\">LICENSE</a> and <a href=\"NOTICE\">NOTICE</a> for\n\
    details.</p>\n<p>SPDX-License-Identifier: BSD-3-Clause <br>\nLLNL Release Number:\
    \ LLNL-CODE-806117 <br>\nDOI: 10.11578/dc.20171025.1248</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1679324110.0
xsdk-project/installxSDK:
  data_format: 2
  description: Bash shell script for installing xSDK and other IDEAS packages
  filenames:
  - platformFiles/lassen/spack.yaml
  - platformFiles/polaris/gcc-11.2.0/spack.yaml
  - platformFiles/summit/spack.yaml
  - platformFiles/crusher/PrgEnv-cray/spack.yaml
  - platformFiles/crusher/PrgEnv-gnu/spack.yaml
  full_name: xsdk-project/installxSDK
  latest_release: v0.1.1
  readme: '<h1><a id="user-content-useful-supplementary-materials-for-installing-the-xsdk"
    class="anchor" aria-hidden="true" href="#useful-supplementary-materials-for-installing-the-xsdk"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Useful supplementary
    materials for installing the xSDK</h1>

    <p>See <a href="https://xsdk.info/download/" rel="nofollow">https://xsdk.info/download/</a>
    for full directions on obtaining the xSDK.</p>

    <p>The primary content of this repository includes packages.yaml and

    compilers.yaml files, as well as other files useful for configuring builds of

    the xSDK through Spack for various platforms.</p>

    <p>The files are arranged generally as follows:</p>

    <p>installxSDK/platformFiles/&lt;platform description&gt;/&lt;files for platfom&gt;</p>

    <p>This repository is to be tagged for each major and minor release of the xSDK.</p>

    '
  stargazers_count: 5
  subscribers_count: 8
  topics: []
  updated_at: 1669065329.0
