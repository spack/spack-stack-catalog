ArjunaCluster/spack:
  data_format: 2
  description: Spack Repos and Configuration Files
  filenames:
  - environments/common/spack.yaml
  - environments/slurm/spack.yaml
  full_name: ArjunaCluster/spack
  latest_release: null
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1637623732.0
CivetWang/HPCHarryW:
  data_format: 2
  description: null
  filenames:
  - assignment/spack.yaml
  full_name: CivetWang/HPCHarryW
  latest_release: null
  readme: '<h1>

    <a id="user-content-hpcharryw" class="anchor" href="#hpcharryw" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HPCHarryW</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1653448958.0
E4S-Project/e4s:
  data_format: 2
  description: E4S for Spack
  filenames:
  - environments/22.02/spack-ppc64le.yaml
  - environments/21.11/spack-ppc64le.yaml
  - environments/22.02/spack-x86_64.yaml
  - environments/21.11/spack-x86_64.yaml
  full_name: E4S-Project/e4s
  latest_release: null
  readme: "<p><a href=\"https://github.com/E4S-Project/e4s/blob/master/logos/E4S-dark-green.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/E4S-Project/e4s/raw/master/logos/E4S-dark-green.png\"\
    \ width=\"200\" alt=\"E4S\" style=\"max-width:100%;\"></a></p> \n<p><a href=\"\
    https://camo.githubusercontent.com/58e7ffdceb32cd7a8facd6b6cd3920a56c15e0e2ef1d3398158ef4ec0d6ec886/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f4534532d50726f6a6563742f653473\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/58e7ffdceb32cd7a8facd6b6cd3920a56c15e0e2ef1d3398158ef4ec0d6ec886/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f4534532d50726f6a6563742f653473\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/E4S-Project/e4s\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/94cdab1cd9efc5521be1590b3d0b5dc5f707838e0d20015c14f23921ef2b7326/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6534732f62616467652f3f76657273696f6e3d6c6174657374\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/94cdab1cd9efc5521be1590b3d0b5dc5f707838e0d20015c14f23921ef2b7326/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6534732f62616467652f3f76657273696f6e3d6c6174657374\"\
    \ alt=\"Documentation\" data-canonical-src=\"https://readthedocs.org/projects/e4s/badge/?version=latest\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/c2e45205070ba0928aece78cf95f8658ef1cf69f7c113dc56f7d05e29e68755e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f4534532d50726f6a6563742f6534732e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c2e45205070ba0928aece78cf95f8658ef1cf69f7c113dc56f7d05e29e68755e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f4534532d50726f6a6563742f6534732e737667\"\
    \ alt=\"GitHub Issues\" data-canonical-src=\"https://img.shields.io/github/issues/E4S-Project/e4s.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/011fb27187d5b878949948752f73e86dea6828febca879ee69a5a5d52ce651ca/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f4534532d50726f6a6563742f653473\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/011fb27187d5b878949948752f73e86dea6828febca879ee69a5a5d52ce651ca/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f4534532d50726f6a6563742f653473\"\
    \ alt=\"GitHub pull requests\" data-canonical-src=\"https://img.shields.io/github/issues-pr/E4S-Project/e4s\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-e4s\" class=\"\
    anchor\" href=\"#e4s\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>E4S</h1>\n<p>The <a href=\"https://e4s-project.github.io/\"\
    \ rel=\"nofollow\">Extreme-scale Scientific Software Stack (E4S)</a> is a community\
    \ effort to provide open source\nsoftware packages for developing, deploying and\
    \ running scientific applications on high-performance\ncomputing (HPC) platforms.\
    \ E4S provides from-source builds and containers of a\n<a href=\"https://e4s-project.github.io/Resources/ProductInfo.html\"\
    \ rel=\"nofollow\">broad collection of HPC software packages</a>.</p>\n<p>E4S\
    \ is available to download in the following formats:</p>\n<ul>\n<li>\n<p>Containers:\
    \ Docker, Singularity, CharlieCloud, OVA</p>\n</li>\n<li>\n<p>Spack manifest (<code>spack.yaml</code>)\
    \ to install from source. These can be found in <a href=\"https://github.com/E4S-Project/e4s/tree/master/environments\"\
    >environments</a> directory.</p>\n</li>\n<li>\n<p><a href=\"http://aws.amazon.com/\"\
    \ rel=\"nofollow\">AWS EC2 image</a> with image name <code>ami-0db9d49091db1c25f</code>\
    \ in <strong>US-West-2 (Oregon)</strong></p>\n</li>\n<li>\n<p><a href=\"https://oaciss.uoregon.edu/e4s/inventory.html\"\
    \ rel=\"nofollow\">E4S Build Cache</a></p>\n</li>\n</ul>\n<p>Please see <a href=\"\
    https://github.com/E4S-Project/e4s/blob/master/E4S_Products.md\">E4S Product Dictionary</a>\
    \ for complete list of E4S products.</p>\n<h2>\n<a id=\"user-content-related-projects\"\
    \ class=\"anchor\" href=\"#related-projects\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Related Projects</h2>\n<ul>\n\
    <li>\n<p><a href=\"https://github.com/E4S-Project/E4S-Project.github.io\">E4S-Project/E4S-Project.github.io</a>\
    \ - E4S Documentation repo that is hosted on <a href=\"https://e4s-project.github.io/\"\
    \ rel=\"nofollow\">https://e4s-project.github.io/</a></p>\n</li>\n<li>\n<p><a\
    \ href=\"https://github.com/E4S-Project/testsuite\">E4S-Project/testsuite</a>\
    \ - E4S Testsuite with collection of validation tests that can be run post-install.</p>\n\
    </li>\n<li>\n<p><a href=\"https://github.com/E4S-Project/e4s-cl\">E4S-Project/e4s-cl</a>\
    \ - E4S Container Launcher is a tool to easily run MPI applications in E4S containers.</p>\n\
    </li>\n<li>\n<p><a href=\"https://github.com/E4S-Project/e4s-ci-badges\">E4S-Project/e4s-ci-badges</a>\
    \ - Display CI badges for E4S products that are available from <a href=\"https://shields.io/\"\
    \ rel=\"nofollow\">shields.io</a></p>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-license\"\
    \ class=\"anchor\" href=\"#license\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>License</h2>\n<p>E4S is released\
    \ as MIT license for more details see <a href=\"https://github.com/E4S-Project/e4s/blob/master/LICENSE\"\
    >LICENSE</a> file</p>\n<h2>\n<a id=\"user-content-contact\" class=\"anchor\" href=\"\
    #contact\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Contact</h2>\n<ul>\n<li>Mike Heroux (<a href=\"mailto:maherou@sandia.gov\"\
    >maherou@sandia.gov</a>)</li>\n<li>Sameer Shende (<a href=\"mailto:sameer@cs.uoregon.edu\"\
    >sameer@cs.uoregon.edu</a>)</li>\n</ul>\n"
  stargazers_count: 12
  subscribers_count: 9
  topics: []
  updated_at: 1652801638.0
FTHPC/Correlation_Compressibility:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: FTHPC/Correlation_Compressibility
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-compressibility-analysis-correlation_compressibility\"\
    \ class=\"anchor\" href=\"#compressibility-analysis-correlation_compressibility\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Compressibility Analysis (Correlation_Compressibility)</h1>\n<h2>\n\
    <a id=\"user-content-statement-of-purpose\" class=\"anchor\" href=\"#statement-of-purpose\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Statement of Purpose</h2>\n<p>This repo contains scripts to perform\
    \ compressibility analysis on several leading lossy compressors.\nThe compressibility\
    \ analysis relies on deriving statistics on scientific data and explore their\
    \ relationships to their compression ratios from various lossy compressors (based\
    \ on various compression scheme).\nThe extracted relationships between compression\
    \ ratios and statistical predictors are modeled via regression models, which provide\
    \ a statistical framework to predict compression ratios for the different studied\
    \ lossy compressors.</p>\n<p>This repo contains an automatic framework of scripts\
    \ that perform the compression of scientific datasets from 8 compressors (SZ2,\
    \ ZFP, MGARD, FPZIP, Digit Rounding and Bit Grooming), the derivation of the statistical\
    \ predictors of compression ratios (SVD, standard deviation, quantized entropy),\
    \ and scripts to perform the training of the regression models (linear and spline\
    \ regressions) as well as the validation of the regression predictions.\nA runtime\
    \ analysis is also performed and associated codes are provided.</p>\n<h3>\n<a\
    \ id=\"user-content-main-code-structures\" class=\"anchor\" href=\"#main-code-structures\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Main code structures</h3>\n<p>Compression metrics, including compression\
    \ ratios, and derivation of statistical predictors (SVD, standard deviation, quantized\
    \ entropy) codes are found in <code>compress_package</code> and are run via <code>scripts/run.sh</code>\
    \ as described in the section \"How to compute statistical predictors and compression\
    \ analysis on datasets\".\nLinear and spline regressions training and validation\
    \ (functions <code>cr_regression_linreg</code> and <code>cr_regression_gam</code>\
    \ from the script <code>replicate_figures/functions_paper.R</code>).\nCodes for\
    \ the different runtime analysis are found in the folder <code>runtime_analysis</code>\
    \ and are automated with the script <code>runtime.sh</code>, the study includes\
    \ compression time for SZ2, ZFP, MAGRD, FPZIP, data quantization, SVD, local (tiled)\
    \ variogram and local (tiled) variogram, and runtime for training and prediction\
    \ of the regressions.<br>\nFinally, the script <code>replicate_figures/graphs_paper_container.R</code>\
    \ replicates and saves all the figures from the paper ad as well as numbers from\
    \ the tables.</p>\n<p>For each dataset in the <code>dataset</code> folder, slicing\
    \ is performed for each variable field (e.g. density in Miranda), each slice is\
    \ stored in a class. The class is updated as compressions with the 8 compressors\
    \ is performed and updated as the statistical predictors are derived. Results\
    \ of each class are stored in a .csv file (example of csv files can be found at\
    \ <code>replicate_figures/generated_data/</code>).\nAll the datasets stored in\
    \ the <code>dataset</code> folder can be analyzed with the given set of codes,\
    \ one needs to source <code>scripts/config.json</code> with the appropriate dataset\
    \ name as described in the below section \"How to compute statistical predictors\
    \ and compression analysis on datasets\".\nThe regression analysis and its prediction\
    \ is then performed on R dataframes based on the aforementioned .csv files.</p>\n\
    <h2>\n<a id=\"user-content-system-information\" class=\"anchor\" href=\"#system-information\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>System Information</h2>\n<p>The hardware and software versions used\
    \ for the performance evaluations can be found in the table below. These nodes\
    \ come from Clemson University's Palmetto Cluster.</p>\n<p>These nodes have:</p>\n\
    <table>\n<thead>\n<tr>\n<th>component</th>\n<th>version</th>\n<th>component</th>\n\
    <th>version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>CPU</td>\n<td>Intel Xeon\
    \ 6148G (40 cores)</td>\n<td>sz2</td>\n<td>2.1.12.2</td>\n</tr>\n<tr>\n<td>GPU</td>\n\
    <td>2 Nvidia v100</td>\n<td>sz3</td>\n<td>3.1.3.1</td>\n</tr>\n<tr>\n<td>Memory</td>\n\
    <td>372GB</td>\n<td>zfp</td>\n<td>0.5.5</td>\n</tr>\n<tr>\n<td>Network</td>\n\
    <td>2 Mellanox MT27710 (HDR)</td>\n<td>mgard</td>\n<td>1.0.0</td>\n</tr>\n<tr>\n\
    <td>FileSystem</td>\n<td>BeeGFS 7.2.3 (24 targets)</td>\n<td>bit grooming</td>\n\
    <td>2.1.9</td>\n</tr>\n<tr>\n<td>Compiler</td>\n<td>GCC 8.4.1</td>\n<td>digit\
    \ rounding</td>\n<td>2.1.9</td>\n</tr>\n<tr>\n<td>OS</td>\n<td>CentOS 8.2.2004</td>\n\
    <td>R</td>\n<td>4.1.3</td>\n</tr>\n<tr>\n<td>MPI</td>\n<td>OpenMPI 4.0.5</td>\n\
    <td>Python</td>\n<td>3.9.12</td>\n</tr>\n<tr>\n<td>LibPressio</td>\n<td>0.83.4</td>\n\
    <td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h2>\n<a id=\"user-content-first-time-setup\"\
    \ class=\"anchor\" href=\"#first-time-setup\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>First time setup</h2>\n<h3>\n\
    <a id=\"user-content-container-installation-for-ease-of-setup\" class=\"anchor\"\
    \ href=\"#container-installation-for-ease-of-setup\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Container Installation\
    \ (for ease of setup)</h3>\n<p>We provide a container for <code>x86_64</code>\
    \ image for ease of installation.</p>\n<p>This container differs from our experimental\
    \ setup slightly. The production build used <code>-march=native -mtune=native</code>\
    \ for architecture optimized builds where as the container does not use these\
    \ flags to maximize compatibility across <code>x86_64</code> hardware.</p>\n<p>NOTE\
    \ this file is &gt;= 11 GB , download with caution.</p>\n<h4>\n<a id=\"user-content-docker\"\
    \ class=\"anchor\" href=\"#docker\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Docker</h4>\n<p>Many other systems\
    \ can use podman or docker.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>docker pull ghcr.io/fthpc/correlation_compressibility:latest\n\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span>most systems</span>\ndocker run -it --rm ghcr.io/fthpc/correlation_compressibility:latest\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> if running on a SeLinux enforcing\
    \ system</span>\ndocker run -it --rm --security-opt label=disable ghcr.io/fthpc/correlation_compressibility:latest</pre></div>\n\
    <h3>\n<a id=\"user-content-building-the-container\" class=\"anchor\" href=\"#building-the-container\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Building the Container</h3>\n<p>You can build the container yourself\
    \ as follows:\nNOTE this process takes 3+ hours on a modern laptop, and most clusters\
    \ do not\nprovide sufficient permissions to run container builds on the cluster.</p>\n\
    <p>Additionally compiling MGRAD -- one of the compressors we use takes &gt;= 4GB\
    \ RAM per core, be cautious\nwith systems with low RAM.  You may be able compensate\
    \ by using fewer cores by changing the spack install\ninstruction in the Dockerfile\
    \ to have a <code>-j N</code> where <code>N</code> is the number of cores you\
    \ wish to use</p>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> install/module load git-lfs, needed\
    \ to download example_data for building the container</span>\nsudo dnf install\
    \ git-lfs <span class=\"pl-c\"><span class=\"pl-c\">#</span>Fedora/CentOS Stream\
    \ 8</span>\nsudo apt-get install git-lfs <span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> Ubuntu</span>\nspack install git-lfs<span class=\"pl-k\">;</span> spack\
    \ load git-lfs <span class=\"pl-c\"><span class=\"pl-c\">#</span> using spack</span>\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> clone this repository</span>\n\
    git clone --recursive https://github.com/FTHPC/Correlation_Compressibility\n<span\
    \ class=\"pl-c1\">cd</span> Correlation_Compressibility\ndocker build <span class=\"\
    pl-c1\">.</span> -t correlation_compressibility</pre></div>\n<h3>\n<a id=\"user-content-manual-installation\"\
    \ class=\"anchor\" href=\"#manual-installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Manual Installation</h3>\n<p>By\
    \ default, it is recommended to follow the install locations that are indicated\
    \ on the top of <code>scripts/run.sh</code>\nand the top of <code>config.json</code>.\
    \ These two files provide the configuration options to get the program running.</p>\n\
    <p>Spack should be installed in the following location: <code>$HOME/spack/</code></p>\n\
    <p>This Github repo should be cloned in the following location: <code>$HOME/Correlation_Compressibility/</code>\n\
    This location is also referenced as the <code>COMPRESS_HOME</code> environment\
    \ variable.</p>\n<p>A dataset folder called 'datasets' should be in the following\
    \ location: <code>$HOME/Correlation_Compressibility/datasets/</code>.</p>\n<p>Clone\
    \ the repo but make sure to install or load <code>git-lfs</code> first.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> install/module load git-lfs, needed to download example_data\
    \ for building the container</span>\nsudo dnf install git-lfs <span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span>Fedora/CentOS Stream 8</span>\nsudo apt-get install\
    \ git-lfs <span class=\"pl-c\"><span class=\"pl-c\">#</span> Ubuntu</span>\nspack\
    \ install git-lfs<span class=\"pl-k\">;</span> spack load git-lfs <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> using spack</span>\n\n<span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> clone this repository</span>\ngit clone https://github.com/FTHPC/Correlation_Compressibility\
    \ <span class=\"pl-smi\">$HOME</span>/Correlation_Compressibility\n<span class=\"\
    pl-c1\">cd</span> <span class=\"pl-smi\">$HOME</span>/Correlation_Compressibility</pre></div>\n\
    <p>If you forgot to install <code>git-lfs</code> before and have an empty file\
    \ in the  <code>datasets</code> folder, you should install <code>git-lfs</code>\n\
    and then run the following:</p>\n<pre><code>git lfs fetch\ngit lfs checkout\n\
    </code></pre>\n<p>Once Spack is installed, there is a <code>spack.yaml</code>\
    \ configuration file containing the Spack environment necessary to run the program.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c1\">cd</span>\
    \ <span class=\"pl-smi\">$HOME</span>\ngit clone --depth=1 https://github.com/spack/spack\n\
    git clone --depth=1 https://github.com/robertu94/spack_packages \n<span class=\"\
    pl-c1\">source</span> ./spack/share/spack/setup-env.sh \nspack compiler find\n\
    spack external find \nspack repo add --scope=site ./spack_packages \n<span class=\"\
    pl-c1\">cd</span> <span class=\"pl-smi\">$HOME</span>/Correlation_Compressibility\
    \ \nspack env activate <span class=\"pl-c1\">.</span>\nspack install\n<span class=\"\
    pl-k\">export</span> COMPRESS_HOME=<span class=\"pl-smi\">$HOME</span>/Correlation_Compressibility\
    \ </pre></div>\n<p>These commands will install the environment. The environment\
    \ only needs to be installed once.\nIf you are using an older &lt; gcc11, then\
    \ you will need to add the following to the <code>spack.yaml</code> file:</p>\n\
    <pre><code>^libstdcompat+boost\n</code></pre>\n<p>after <code>^mgard@robertu94+cuda</code>\
    \ but before the <code>,</code>.</p>\n<h2>\n<a id=\"user-content-replication-of-results\"\
    \ class=\"anchor\" href=\"#replication-of-results\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Replication of\
    \ Results</h2>\n<h3>\n<a id=\"user-content-how-to-compute-statistical-predictors-and-compression-metrics-on-datasets\"\
    \ class=\"anchor\" href=\"#how-to-compute-statistical-predictors-and-compression-metrics-on-datasets\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>How to compute statistical predictors and compression metrics on datasets</h3>\n\
    <p>In order to run the statistical analysis that computes the statistical predictors\
    \ (SVD, standard deviation, quantized entropy) of compression ratios, a dataset\
    \ and a configuration file must be specified.\nTEST is a dataset that is specified\
    \ within the <code>config.json</code> file.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sh scripts/run.sh -c config.json -d TEST -n 2</pre></div>\n<p>The command\
    \ above performs the computation of statistical predictors and writes output to\
    \ the output file specified in the configuration file.\nThis will use local hardware\
    \ without a scheduler. Use <code>-n</code> to specify the MPI processes on your\
    \ local system. Default value is 32.\nIt is recommended that this value matches\
    \ your CPU core count.</p>\n<p>If one has the PBS scheduler and runs outside of\
    \ the container, feel free to use flags <code>-p</code> or <code>-s</code> for\
    \ job execution.\n<code>-p</code> will schedule multiple jobs based on the quantized\
    \ error bounds and error bound types for a specified dataset.\n<code>-s</code>\
    \ will schedule a single job grouping all the analysis for a specified dataset.</p>\n\
    <p>See <code>-h</code> for more options or help with syntax.</p>\n<p>If a dataset\
    \ is wanted to run, the <code>config.json</code> file provides options to add\
    \ datasets.\nThe following options must be added when adding another dataset in\
    \ the configuration file:</p>\n<div class=\"highlight highlight-source-json\"\
    ><pre><span class=\"pl-ent\">\"_comment\" </span>: \n{\n    <span class=\"pl-ent\"\
    >\"folder\"            </span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span>folder containing h5 or binary files<span class=\"pl-pds\">\"</span></span>,\n\
    \    <span class=\"pl-ent\">\"data_dimensions\"   </span>: <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>dimensions of the datasets within dataset_folder.\
    \ Either 1x2 or 1x3. EX: '1028, 1028'<span class=\"pl-pds\">\"</span></span>,\n\
    \    <span class=\"pl-ent\">\"slice_dimensions\"  </span>: <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>list of the dimensions wanted: EX: 'None' or\
    \ 'X, Y, Z'<span class=\"pl-pds\">\"</span></span>,\n    <span class=\"pl-ent\"\
    >\"output\"            </span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span>name of the output csv file: EX: 'test.csv'<span class=\"pl-pds\">\"</span></span>,\n\
    \    <span class=\"pl-ent\">\"dtype\"             </span>: <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>data type. can be 'float32' or 'float64'<span\
    \ class=\"pl-pds\">\"</span></span>,\n    <span class=\"pl-ent\">\"parse_info\"\
    \        </span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type of\
    \ parsing needed: 'None', 'slice', 'gaussian', 'gaussian_multi', 'spatialweight',\
    \ or 'scalarweight'<span class=\"pl-pds\">\"</span></span>,\n    <span class=\"\
    pl-ent\">\"dataset_name\"      </span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>necessary accessing 2D HDF5 files: 'standard' if not custom. custom\
    \ EX: 'Z'<span class=\"pl-pds\">\"</span></span>\n} </pre></div>\n<p>From this\
    \ section, .csv files are generated for each dataset and contain all the statistical\
    \ predictors described in the paper as well as compression metrcis including compresison\
    \ ratios for the 8 lossy compressors and 4 error bounds.</p>\n<h3>\n<a id=\"user-content-to-run-the-training-and-prediction-timing-analysis-demonstration\"\
    \ class=\"anchor\" href=\"#to-run-the-training-and-prediction-timing-analysis-demonstration\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>To run the training and prediction timing analysis demonstration</h3>\n\
    <p>In order to run the timing analysis, a dataset must be specified.\nThere are\
    \ two datasets setup within this demonstration.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sh runtime_analysis/runtime.sh -d [DATASET]</pre></div>\n<p>[DATASET] can\
    \ be either [NYX] or [SCALE]</p>\n<p>After running the above script, an *.RData\
    \ file(s) will be produced giving the approprirate timing information of\nthe\
    \ training and prediction for the regression models.</p>\n<p>Note: A quicker and\
    \ more efficient quantized entropy method is demonstrated in <code>qentropy.cc</code></p>\n\
    <h4>\n<a id=\"user-content-the-following-below-runs-qentropycc\" class=\"anchor\"\
    \ href=\"#the-following-below-runs-qentropycc\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>The following below runs <code>qentropy.cc</code>\n\
    </h4>\n<div class=\"highlight highlight-source-shell\"><pre>g++ -std=c++2a -O3\
    \ qentropy.cc -o qentropy -march=native -mtune=native\n./qentropy</pre></div>\n\
    <p>Note: Please run the runtime analysis for both datasets before running the\
    \ following section.</p>\n<h3>\n<a id=\"user-content-replication-of-figures-how-to-run-statistical-prediction-of-compression-ratios-and-the-prediction-validation\"\
    \ class=\"anchor\" href=\"#replication-of-figures-how-to-run-statistical-prediction-of-compression-ratios-and-the-prediction-validation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Replication of figures: how to run statistical prediction of compression\
    \ ratios and the prediction validation</h3>\n<p>The script <code>graphs_paper_container.R</code>\
    \  saves the graphs presented in the paper and provides associated validation\
    \ metrics (correlation and median absolute error percentage).</p>\n<p>The script\
    \ <code>graphs_paper_container.R</code> will source the scripts  <code>load_dataset_paper.R</code>\
    \ and <code>functions_paper.R</code> that respectively load the dataset of interest\
    \ and perform the regression analysis (training and prediction in cross-validation).\n\
    As a consequence the scripts  <code>load_dataset_paper.R</code> and <code>functions_paper.R</code>\
    \ do not need to be run by the user.</p>\n<p>The script <code>graphs_paper_container.R</code>\
    \  is run via the command:\n<code>bash sh replicate.sh</code></p>\n<p>From running\
    \ the script once, it will save all Figures 1, 3, 4 and 5 into .png files from\
    \ the paper as well as corresponding validation metrics.\nFigure 2 is not saved\
    \ as it provides a simple vizualization of slices of the datasets.\nSlices of\
    \ the datasets are generated in the Section \"How to compute statistical predictors\
    \ and compression metrics\" and can be stored, however we do not save them here\
    \ to save space in the container.\nNumbers for Tables 2, 3 and 5 are printed in\
    \ the R console.\nAll printed validation metrics are save into a file named <code>figure_replication.log</code>.\n\
    Figures and the log-file are saved in the same folder as the one where R script\
    \ is run and the filename structure is <code>figY_*.png</code> with Y is the figure\
    \ number reference in the paper and <code>*</code> provides additional informnation\
    \ about the data and the compressor.<br>\nNumbers for Table 4 are saved in the\
    \ last section in .txt files <code>statistic_benchmark_runtime_X.txt</code> with\
    \ X the studied dataset (NYX or SCALE).</p>\n<p>In order to limit the container\
    \ size to aid reproducibility, we only added a restricted number of scientific\
    \ datasets in the container and we rely on csv files from our production runs\
    \ (saved as described above in the Section \"How to compute statistical predictors\
    \ on datasets\").\nMore datasets are available on <a href=\"https://sdrbench.github.io\"\
    \ rel=\"nofollow\">SDRBench</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1648227729.0
FTHPC/pictorial-compressibility:
  data_format: 2
  description: LANL  LA-UR-21-32202 Compression Performance
  filenames:
  - spack.yaml
  full_name: FTHPC/pictorial-compressibility
  latest_release: null
  readme: '<h1>

    <a id="user-content-pictorial-compressibility" class="anchor" href="#pictorial-compressibility"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>pictorial-compressibility</h1>

    <p>LANL  LA-UR-21-32202 Compression Performance</p>

    <p>Tutorial Code using Libpressio to analyize compression performance.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1649450584.0
FluidNumerics/SELF:
  data_format: 2
  description: Spectral Element Library in Fortran
  filenames:
  - env/spack.yaml
  full_name: FluidNumerics/SELF
  latest_release: null
  readme: '<h1>

    <a id="user-content-spectral-element-libraries-in-fortran-self" class="anchor"
    href="#spectral-element-libraries-in-fortran-self" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Spectral Element Libraries in Fortran
    (SELF)</h1>

    <p>Copyright 2020-2022 Fluid Numerics LLC</p>

    <p><a href="https://self.readthedocs.io/en/latest/?badge=latest" rel="nofollow"><img
    src="https://camo.githubusercontent.com/2cdea5d87038eae2bd52034d42848bdf0381c26e2ffe70a7a973e360004a19f6/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f73656c662f62616467652f3f76657273696f6e3d6c6174657374"
    alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/self/badge/?version=latest"
    style="max-width:100%;"></a>

    <a href="https://codecov.io/gh/FluidNumerics/SELF" rel="nofollow"><img src="https://camo.githubusercontent.com/190632c16f2de9b4028909a9987ec0987590d74593c0133a6346d70373fb45ca/68747470733a2f2f636f6465636f762e696f2f67682f466c7569644e756d65726963732f53454c462f6272616e63682f6d61696e2f67726170682f62616467652e7376673f746f6b656e3d414b4b534c3543574b36"
    alt="codecov" data-canonical-src="https://codecov.io/gh/FluidNumerics/SELF/branch/main/graph/badge.svg?token=AKKSL5CWK6"
    style="max-width:100%;"></a>

    <a href="https://www.youtube.com/channel/UCW5e-TavOnw1AABGH-VMbRg?sub_confirmation=1"
    rel="nofollow"><img src="https://camo.githubusercontent.com/241f818a7c9fbbe538a27ae90073f54004dc794a7d2cab7ef50cb01c76e62cef/68747470733a2f2f696d672e736869656c64732e696f2f796f75747562652f6368616e6e656c2f73756273637269626572732f55435735652d5461764f6e773141414247482d564d6252673f7374796c653d736f6369616c"
    alt="Youtube" data-canonical-src="https://img.shields.io/youtube/channel/subscribers/UCW5e-TavOnw1AABGH-VMbRg?style=social"
    style="max-width:100%;"></a>

    <a href="https://www.reddit.com/r/FluidNumerics/" rel="nofollow"><img src="https://camo.githubusercontent.com/86acef9558f5e18573c4b9b4275d2eb6f59608130be921982dd5b0377650324a/68747470733a2f2f696d672e736869656c64732e696f2f7265646469742f7375627265646469742d73756273637269626572732f666c7569646e756d65726963733f7374796c653d736f6369616c"
    alt="Reddit" data-canonical-src="https://img.shields.io/reddit/subreddit-subscribers/fluidnumerics?style=social"
    style="max-width:100%;"></a></p>

    <p>SELF is licensed for use under the <a href="./LICENSE">Anti-Corporatist Software
    License</a>. For other licensure, reach out to <a href="mailto:support@fluidnumerics.com">support@fluidnumerics.com</a>.</p>

    <h2>

    <a id="user-content-about" class="anchor" href="#about" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>About</h2>

    <p>SELF is an object-oriented Fortran library that support the implementation
    of Spectral Element Methods for solving partial differential equations.</p>

    <p>The SELF API is designed based on the assumption that SEM developers and researchers
    need to be able to implement derivatives in 1-D and divergence, gradient, and
    curl in 2-D and 3-D on scalar, vector, and tensor functions using spectral collocation,
    continuous galerkin, and discontinuous galerkin spectral element methods. Additionally,
    as we enter the exascale era, we are currently faced with a zoo of compute hardware
    that is available. Because of this, SELF routines provide support for GPU acceleration
    through AMD''s HIP and support for multi-core, multi-node, and multi-GPU platforms
    with MPI.</p>

    <h2>

    <a id="user-content-support" class="anchor" href="#support" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Support</h2>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <ul>

    <li><a href="https://fluidnumerics.github.io/SELF/ford/" rel="nofollow"><strong>API
    Documentation</strong></a></li>

    <li><a href="https://self.readthedocs.io/en/latest/" rel="nofollow"><strong>ReadTheDocs</strong>
    <em>(Work in Progress)</em></a></li>

    </ul>

    <h3>

    <a id="user-content-community" class="anchor" href="#community" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Community</h3>

    <h4>

    <a id="user-content-open-collective" class="anchor" href="#open-collective" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Open Collective</h4>

    <p>SELF is part of the Higher Order Methods Collective, which is fiscally hosted
    by <a href="https://www.waterchange.org" rel="nofollow">WATERCHaNGE</a>.

    You can keep track of updates and announcements for livestreams and training events
    at the <a href="https://opencollective.com/higher-order-methods" rel="nofollow">**Higher
    Order Methods Open Collective **</a>.</p>

    <p>You can support SELF and related educational activities focused on numerical
    analysis and higher order methods for solving conservation laws by contributing
    to the Open Collective.

    <a href="https://opencollective.com/higher-order-methods/contribute" rel="nofollow"><img
    src="https://github.com/opencollective/opencollective-images/raw/main/src/static/images/contribute.svg"
    alt="Open Collective" style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-maintainers" class="anchor" href="#maintainers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Maintainers</h3>

    <ul>

    <li><a href="https://fluidnumerics.com/people/joe-schoonover" rel="nofollow">Joseph
    Schoonover, Fluid Numerics LLC</a></li>

    <li>

    <strong>You</strong> Want to become a maintainer ? Reach out to <a href="mailto:support@fluidnumerics.com">support@fluidnumerics.com</a>

    </li>

    </ul>

    <p>If you''d like to contribute, see <a href="./CONTRIBUTING.md">CONTRIBUTING.md</a>
    to get started.</p>

    <p>If you need help, <a href="https://github.com/FluidNumerics/SELF/issues/new">open
    an issue</a></p>

    '
  stargazers_count: 19
  subscribers_count: 5
  topics:
  - spectral-element-method
  - gpu-acceleration
  - gpu-computing
  - hpc
  - pde-solver
  updated_at: 1652476825.0
HEPonHPC/hepnos_eventselection:
  data_format: 2
  description: null
  filenames:
  - docker/hepnos/spack.yaml
  full_name: HEPonHPC/hepnos_eventselection
  latest_release: null
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1639159950.0
LLNL/radiuss-spack-testing:
  data_format: 2
  description: Gitlab CI automation of Spack testing with RADIUSS projects builds.
  filenames:
  - spack-environments/empty/spack.yaml
  full_name: LLNL/radiuss-spack-testing
  latest_release: null
  readme: '<h1>

    <a id="user-content-radiuss-spack-testing" class="anchor" href="#radiuss-spack-testing"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>RADIUSS
    Spack Testing</h1>

    <p>The RADIUSS project promotes and supports key High Performance Computing (HPC)
    open-source software developed at the LLNL. These tools and libraries cover a
    wide range of features a team would need to develop a modern simulation code targeting
    HPC plaftorms.</p>

    <p>RADIUSS Spack Testing is a sub-project from the RADIUSS initiative providing
    a

    testing infrastructure to test Spack Packages automatically in GitLab while

    tracking changes in Spack.</p>

    <p>Access the <a href="https://radiuss-spack-testing.readthedocs.io/" rel="nofollow">documentation</a>.</p>

    <h2>

    <a id="user-content-getting-started" class="anchor" href="#getting-started" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Getting Started</h2>

    <p>The primary goal of this repo is to be used in Gitlab. The Gitlab CI configuration
    is such that it will use Spack pipeline feature to generate and run a pipeline
    that builds one of the environments in the <code>spack-environments</code> directory.</p>

    <p>The specific environment to be built is controlled by the CI variable <code>ENV_NAME</code>.</p>

    <h3>

    <a id="user-content-installing" class="anchor" href="#installing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installing</h3>

    <p>This project requires no installation.</p>

    <h2>

    <a id="user-content-contributing" class="anchor" href="#contributing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <p>Please read <a href="https://github.com/LLNL/radiuss-spack-testing/CONTRIBUTING.md">CONTRIBUTING.md</a>
    for details on our code of conduct, and the process for submitting pull requests
    to us.</p>

    <h2>

    <a id="user-content-versioning" class="anchor" href="#versioning" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Versioning</h2>

    <p>version: 1.0.0</p>

    <p>TODO: Not even sure how to handle versioning here.</p>

    <h2>

    <a id="user-content-authors" class="anchor" href="#authors" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Authors</h2>

    <p>Adrien M Bernede</p>

    <p>See also the list of <a href="https://github.com/LLNL/radiuss-spack-testing/contributors">contributors</a>
    who participated in this project.</p>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>This project is licensed under the MIT License - see the <a href="LICENSE">LICENSE</a>
    file for details</p>

    <p>All new contributions must be made under the MIT License.</p>

    <p>See <a href="https://github.com/LLNL/radiuss-spack-testing/blob/master/LICENSE">LICENSE</a>,

    <a href="https://github.com/LLNL/radiuss-spack-testing/blob/master/COPYRIGHT">COPYRIGHT</a>,
    and

    <a href="https://github.com/LLNL/radiuss-spack-testing/blob/master/NOTICE">NOTICE</a>
    for details.</p>

    <p>SPDX-License-Identifier: (MIT)</p>

    <p>LLNL-CODE-793462</p>

    <h2>

    <a id="user-content-acknowledgments" class="anchor" href="#acknowledgments" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Acknowledgments</h2>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics:
  - radiuss
  updated_at: 1638908320.0
LLNL/sundials:
  data_format: 2
  description: Official development repository for SUNDIALS - a SUite of Nonlinear
    and DIfferential/ALgebraic equation Solvers. Pull requests are welcome for bug
    fixes and minor changes.
  filenames:
  - test/spack/spack.yaml
  full_name: LLNL/sundials
  latest_release: v6.2.0
  readme: '<h1>

    <a id="user-content-sundials-suite-of-nonlinear-and-differentialalgebraic-equation-solvers"
    class="anchor" href="#sundials-suite-of-nonlinear-and-differentialalgebraic-equation-solvers"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SUNDIALS:
    SUite of Nonlinear and DIfferential/ALgebraic equation Solvers</h1>

    <h3>

    <a id="user-content-version-620-apr-2022" class="anchor" href="#version-620-apr-2022"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Version
    6.2.0 (Apr 2022)</h3>

    <p><strong>Center for Applied Scientific Computing, Lawrence Livermore National
    Laboratory</strong></p>

    <p>SUNDIALS is a family of software packages providing robust and efficient time

    integrators and nonlinear solvers that can easily be incorporated into existing

    simulation codes. The packages are designed to require minimal information from

    the user, allow users to supply their own data structures underneath the

    packages, and enable interfacing with user-supplied or third-party algebraic

    solvers and preconditioners.</p>

    <p>The SUNDIALS suite consists of the following packages for ordinary differential

    equation (ODE) systems, differential-algebraic equation (DAE) systems, and

    nonlinear algebraic systems:</p>

    <ul>

    <li>

    <p>ARKODE - for integrating stiff, nonstiff, and multirate ODEs of the form</p>

    <p><code>M(t) y'' = f1(t,y) + f2(t,y), y(t0) = y0</code></p>

    </li>

    <li>

    <p>CVODE - for integrating stiff and nonstiff ODEs of the form</p>

    <p><code>y'' = f(t,y), y(t0) = y0</code></p>

    </li>

    <li>

    <p>CVODES - for integrating and sensitivity analysis (forward and adjoint) of

    ODEs of the form</p>

    <p><code>y'' = f(t,y,p), y(t0) = y0(p)</code></p>

    </li>

    <li>

    <p>IDA - for integrating DAEs of the form</p>

    <p><code>F(t,y,y'') = 0, y(t0) = y0, y''(t0) = y0''</code></p>

    </li>

    <li>

    <p>IDAS - for integrating and sensitivity analysis (forward and adjoint) of DAEs

    of the form</p>

    <p><code>F(t,y,y'',p) = 0, y(t0) = y0(p), y''(t0) = y0''(p)</code></p>

    </li>

    <li>

    <p>KINSOL - for solving nonlinear algebraic systems of the form</p>

    <p><code>F(u) = 0</code> or <code>G(u) = u</code></p>

    </li>

    </ul>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>For installation directions see the <a href="https://sundials.readthedocs.io/en/latest/Install_link.html"
    rel="nofollow">online documentation</a>,

    the <a href="./INSTALL_GUIDE.pdf">INSTALL_GUIDE</a>, or the installation chapter
    in any of

    the package user guides.</p>

    <p>Warning to users who receive more than one of the individual packages at

    different times: Mixing old and new versions of SUNDIALS may fail. To avoid

    such failures, obtain all desired package at the same time.</p>

    <h2>

    <a id="user-content-support" class="anchor" href="#support" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Support</h2>

    <p>Full user guides for all of the SUNDIALS packages are available <a href="https://sundials.readthedocs.io"
    rel="nofollow">online</a>

    and in the <a href="./doc">doc</a> directory. Additionally, the <a href="./doc">doc</a>
    directory

    contains documentation for the package example programs.</p>

    <p>For information on recent changes to SUNDIALS see the <a href="./CHANGELOG.md">CHANGELOG</a>

    or the introduction chapter of any package user guide.</p>

    <p>A list of Frequently Asked Questions on build and installation procedures as

    well as common usage issues is available on the SUNDIALS <a href="https://computing.llnl.gov/projects/sundials/faq"
    rel="nofollow">FAQ</a>.

    For dealing with systems with unphysical solutions or discontinuities see the

    SUNDIALS <a href="https://computing.llnl.gov/projects/sundials/usage-notes" rel="nofollow">usage
    notes</a>.</p>

    <p>If you have a question not covered in the FAQ or usage notes, please submit

    your question to the SUNDIALS <a href="https://computing.llnl.gov/projects/sundials/mailing-list"
    rel="nofollow">mailing list</a>.</p>

    <h2>

    <a id="user-content-contributing" class="anchor" href="#contributing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <p>Bug fixes or minor changes are preferred via a pull request to the

    <a href="https://github.com/LLNL/sundials">SUNDIALS GitHub repository</a>. For
    more

    information on contributing see the <a href="./CONTRIBUTING.md">CONTRIBUTING</a>
    file.</p>

    <h2>

    <a id="user-content-citing" class="anchor" href="#citing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Citing</h2>

    <p>See the <a href="https://sundials.readthedocs.io/en/latest/index.html#citing"
    rel="nofollow">online documentation</a>

    or <a href="./CITATIONS.md">CITATIONS</a> file for information on how to cite
    SUNDIALS in

    any publications reporting work done using SUNDIALS packages.</p>

    <h2>

    <a id="user-content-authors" class="anchor" href="#authors" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Authors</h2>

    <p>The SUNDIALS library has been developed over many years by a number of

    contributors. The current SUNDIALS team consists of Cody J. Balos,

    David J. Gardner, Alan C. Hindmarsh, Daniel R. Reynolds, and Carol S. Woodward.

    We thank Radu Serban for significant and critical past contributions.</p>

    <p>Other contributors to SUNDIALS include: James Almgren-Bell, Lawrence E. Banks,

    Peter N. Brown, George Byrne, Rujeko Chinomona, Scott D. Cohen, Aaron Collier,

    Keith E. Grant, Steven L. Lee, Shelby L. Lockhart, John Loffeld, Daniel McGreer,

    Slaven Peles, Cosmin Petra, H. Hunter Schwartz, Jean M. Sexton,

    Dan Shumaker, Steve G. Smith, Allan G. Taylor, Hilari C. Tiedeman, Chris White,

    Ting Yan, and Ulrike M. Yang.</p>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>SUNDIALS is released under the BSD 3-clause license. See the <a href="./LICENSE">LICENSE</a>

    and <a href="./NOTICE">NOTICE</a> files for details. All new contributions must
    be made

    under the BSD 3-clause license.</p>

    <p><strong>Please Note</strong> If you are using SUNDIALS with any third party
    libraries linked

    in (e.g., LAPACK, KLU, SuperLU_MT, PETSc, or <em>hypre</em>), be sure to review
    the

    respective license of the package as that license may have more restrictive

    terms than the SUNDIALS license.</p>

    <pre><code>SPDX-License-Identifier: BSD-3-Clause


    LLNL-CODE-667205  (ARKODE)

    UCRL-CODE-155951  (CVODE)

    UCRL-CODE-155950  (CVODES)

    UCRL-CODE-155952  (IDA)

    UCRL-CODE-237203  (IDAS)

    LLNL-CODE-665877  (KINSOL)

    </code></pre>

    '
  stargazers_count: 258
  subscribers_count: 37
  topics:
  - ode-solver
  - dae-solver
  - nonlinear-equation-solver
  - sensitivity-analysis
  - time-integration
  - scientific-computing
  - parallel-computing
  - hpc
  - math-physics
  - radiuss
  - solver
  - high-performance-computing
  updated_at: 1653858906.0
MichaelBrim/unify-olcf-scripts:
  data_format: 2
  description: null
  filenames:
  - crusher/spack-env/spack.yaml
  - summit/spack-env/spack.yaml
  full_name: MichaelBrim/unify-olcf-scripts
  latest_release: null
  readme: '<h1>

    <a id="user-content-unify-olcf-scripts" class="anchor" href="#unify-olcf-scripts"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>unify-olcf-scripts</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1649778838.0
NERSC/spack-infrastructure:
  data_format: 2
  description: null
  filenames:
  - spack-configs/cori-e4s-21.02/prod/spack.yaml
  - spack-configs/cori-e4s-22.02/ci/spack.yaml
  - spack-configs/cori-e4s-20.10/spack.yaml
  - spack-configs/cori-e4s-21.02/spack.yaml
  - spack-configs/cori-e4s-20.10/prod/spack.yaml
  - spack-configs/perlmutter-e4s-21.11/spack.yaml
  - spack-configs/perlmutter-spack-develop/spack.yaml
  - spack-configs/perlmutter-e4s-21.11/ci/spack.yaml
  - spack-configs/cori-e4s-22.02/ci/gerty/spack.yaml
  - spack-configs/cori-e4s-22.02/spack.yaml
  - spack-configs/cori-e4s-21.05/spack.yaml
  - spack-configs/cori-spack-develop/spack.yaml
  - spack-configs/perlmutter-e4s-21.11/ci/muller/spack.yaml
  - spack-configs/perlmutter-e4s-22.02/spack.yaml
  full_name: NERSC/spack-infrastructure
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-spack-infrastructure\" class=\"anchor\" href=\"\
    #spack-infrastructure\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Spack Infrastructure</h1>\n<p>The spack infrastructure\
    \ repository contains spack configuration in the form of <code>spack.yaml</code>\
    \ required to build spack stacks on Cori and Perlmutter system. We leverage gitlab\
    \ to automate software stack deployment which is configured using the <a href=\"\
    https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/.gitlab-ci.yml\"\
    \ rel=\"nofollow\">.gitlab-ci.yml</a> file.</p>\n<h2>\n<a id=\"user-content-spack-configuration\"\
    \ class=\"anchor\" href=\"#spack-configuration\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Spack Configuration</h2>\n<p>The\
    \ spack configuration can be found in <a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/tree/main/spack-configs\"\
    \ rel=\"nofollow\">spack-configs</a> directory with subdirectory for each deployment.\n\
    Each pipeline can be run if one sets the variable <code>PIPELINE_NAME</code> to\
    \ a unique value in order to run a pipeline. You can check the <a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/.gitlab-ci.yml\"\
    \ rel=\"nofollow\">.gitlab-ci.yml</a> for the gitlab configuration. The pipeline\
    \ can be run via <a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/pipelines/new\"\
    \ rel=\"nofollow\">web interface</a>, if you chose this route, you must set <code>PIPELINE_NAME</code>\
    \ to the appropriate value.</p>\n<p>If you want to trigger pipeline via <a href=\"\
    https://software.nersc.gov/NERSC/spack-infrastructure/-/pipelines/new\" rel=\"\
    nofollow\">web-interface</a> you will need to define PIPELINE_NAME variable to\
    \ trigger the appropriate pipeline.</p>\n<table>\n<thead>\n<tr>\n<th>system</th>\n\
    <th>status</th>\n<th>PIPELINE_NAME</th>\n<th>description</th>\n<th>spack.yaml</th>\n\
    </tr>\n</thead>\n<tbody>\n<tr>\n<td>Perlmutter</td>\n<td><strong>IN-PROGRESS</strong></td>\n\
    <td><code>PERLMUTTER_SPACK_DEVELOP</code></td>\n<td>This spack configuration is\
    \ based on <code>spack@develop</code> branch to see what packages can be built.\
    \ We expect this pipeline will fail and we are not expected to fix build failure.\
    \ The main purpose of this project is to build as many packages across all the\
    \ compilers, mpi, blas providers of interest and see what works. Since we don't\
    \ know which package works during deployment, we will leverage data from this\
    \ pipeline to make informed decision what packages should be picked with given\
    \ compilers. This pipeline is our development and we should use this to experiment\
    \ new compilers. Note that we won't hardcode versions for packages since we want\
    \ to build with latest release. However we will hardcode externals depending on\
    \ how system is configured.</td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-spack-develop/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-spack-develop/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Cori</td>\n<td><strong>IN-PROGRESS</strong></td>\n<td><code>CORI_SPACK_DEVELOP</code></td>\n\
    <td>This spack configuration will build E4S stack using spack <code>develop</code>\
    \ branch on Cori.</td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-spack-develop/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-spack-develop/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Cori</td>\n<td><strong>COMPLETE</strong></td>\n<td><code>CORI_E4S_22.02</code></td>\n\
    <td>This spack configuration will build E4S/22.02 on Cori using a scheduled pipeline.</td>\n\
    <td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-22.02/ci/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-22.02/ci/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Gerty</td>\n<td><strong>COMPLETE</strong></td>\n<td><code>GERTY_E4S_22.02</code></td>\n\
    <td>This spack configuration will build E4S/22.02 on gerty using a scheduled pipeline.</td>\n\
    <td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-22.02/ci/gerty/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-22.02/ci/gerty/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Perlmutter</td>\n<td><strong>COMPLETE</strong></td>\n<td><code>PERLMUTTER_E4S_21.11_DEPLOY</code></td>\n\
    <td>This spack configuration is deployment configuration for E4S/21.11. For more\
    \ details on this stack see  <a href=\"https://docs.nersc.gov/applications/e4s/perlmutter/21.11/\"\
    \ rel=\"nofollow\">https://docs.nersc.gov/applications/e4s/perlmutter/21.11/</a>\n\
    </td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-e4s-21.11/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-e4s-21.11/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Perlmutter</td>\n<td><strong>COMPLETE</strong></td>\n<td><code>PERLMUTTER_E4S_21.11</code></td>\n\
    <td>This spack configuration is used for development for building E4S/21.11 using\
    \ scheduled pipeline.</td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-e4s-21.11/ci/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-e4s-21.11/ci/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Muller</td>\n<td><strong>COMPLETE</strong></td>\n<td><code>MULLER_E4S_21.11</code></td>\n\
    <td>This spack configuration was used to build E4S/21.11 on Muller using scheduled\
    \ pipeline. Once e4s/21.11 was built on Muller we followed up with building the\
    \ same spack configuration on Perlmutter.</td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-e4s-21.11/ci/muller/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-e4s-21.11/ci/muller/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Cori</td>\n<td><strong>COMPLETE</strong></td>\n<td></td>\n<td>E4S/21.05\
    \ spack stack based on <a href=\"https://github.com/spack/spack/tree/e4s-21.05\"\
    >e4s-21.05</a> branch of spack. This stack can be accessed via <code>module load\
    \ e4s/21.05</code>.</td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-21.05/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-21.05/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Cori</td>\n<td><strong>COMPLETE</strong></td>\n<td></td>\n<td>E4S/21.02\
    \ spack configuration used for deployment purposes, this can be accessed via <code>module\
    \ load e4s/21.02</code> on Cori. For more details see <a href=\"https://docs.nersc.gov/applications/e4s/cori/21.02/\"\
    \ rel=\"nofollow\">https://docs.nersc.gov/applications/e4s/cori/21.02/</a>\n</td>\n\
    <td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/tree/main/spack-configs/cori-e4s-21.02/prod/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/tree/main/spack-configs/cori-e4s-21.02/prod/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Cori</td>\n<td><strong>COMPLETE</strong></td>\n<td></td>\n<td>E4S/21.02\
    \ spack configuration that push to buildcache.</td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-21.02/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-21.02/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Cori</td>\n<td><strong>COMPLETE</strong></td>\n<td></td>\n<td>E4S/20.10\
    \ spack configuration that push to build cache using <code>spack ci</code>.  This\
    \ project lives in <a href=\"https://software.nersc.gov/NERSC/e4s-2010\" rel=\"\
    nofollow\">https://software.nersc.gov/NERSC/e4s-2010</a> and configuration was\
    \ copied over here.</td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-20.10/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-20.10/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Cori</td>\n<td><strong>COMPLETE</strong></td>\n<td></td>\n<td>E4S/20.10\
    \ spack configuration for Cori used for deployment purpose. This stack can be\
    \ accessed via <code>module load e4s/20.10</code>. This is documented at <a href=\"\
    https://docs.nersc.gov/applications/e4s/cori/20.10/\" rel=\"nofollow\">https://docs.nersc.gov/applications/e4s/cori/20.10/</a>\n\
    </td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-20.10/prod/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-20.10/prod/spack.yaml</a></td>\n\
    </tr>\n</tbody>\n</table>\n<h2>\n<a id=\"user-content-running-ci-pipelines\" class=\"\
    anchor\" href=\"#running-ci-pipelines\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running CI Pipelines</h2>\n<p>This\
    \ project is configured with several <a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/pipeline_schedules\"\
    \ rel=\"nofollow\">scheduled pipelines</a> that will run at different times.</p>\n\
    <p>Currently, we have a shell runner installed on Perlmutter using <code>e4s</code>\
    \ account which is configured with following settings. You can find list of runners\
    \ and their runner status under <a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/settings/ci_cd\"\
    \ rel=\"nofollow\">Settings &gt; CI/CD &gt; Runners</a>.</p>\n<table>\n<thead>\n\
    <tr>\n<th>System</th>\n<th>Runner Name</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n\
    <td>perlmutter</td>\n<td><code>perlmutter-e4s</code></td>\n</tr>\n<tr>\n<td>cori</td>\n\
    <td><code>cori-e4s</code></td>\n</tr>\n<tr>\n<td>muller</td>\n<td><code>muller-e4s</code></td>\n\
    </tr>\n<tr>\n<td>gerty</td>\n<td><code>gerty-e4s</code></td>\n</tr>\n</tbody>\n\
    </table>\n<p>The runner configuration files are located in <code>~/.gitlab-runner</code>\
    \ for user <strong>e4s</strong>.</p>\n<p>The production pipelines are triggered\
    \ via web-interface which requires approval from a project maintainer. Production\
    \ pipelines should be run when we need to do full redeployment of stack.</p>\n\
    <h2>\n<a id=\"user-content-troubleshooting-gitlab-runner\" class=\"anchor\" href=\"\
    #troubleshooting-gitlab-runner\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Troubleshooting gitlab runner</h2>\n\
    <p>You will need to login as <code>e4s</code> user via <code>collabsu</code> command.\
    \ This will prompt you for password which is your <strong>NERSC password</strong>\
    \ for your username not <strong>e4s</strong> user.</p>\n<pre><code>collabsu e4s\n\
    </code></pre>\n<p>Once you are logged in, you can login to the desired system\
    \ to restart the runner. You can check the runner status by navigating to <a href=\"\
    https://software.nersc.gov/NERSC/spack-infrastructure/-/settings/ci_cd\" rel=\"\
    nofollow\">Settings &gt; CI/CD &gt; Runners</a>. If gitlab runner is down you\
    \ will need to restart the runner which is located in <code>$HOME/cron</code>\
    \ directory for e4s user.</p>\n<p>For instance, to access muller you will need\
    \ to login to Cori/DTN nodes and run <code>ssh login.muller.nersc.gov</code>.</p>\n\
    <p>The <code>gitlab-runner</code> command should be accessible with e4s user.\
    \ To register a runner you can run <code>gitlab-runner register</code> and follow\
    \ the prompt. The runner configuration will be written to <code>~/.gitlab-runner/config.toml</code>\
    \ however we recommend you create a separate config.toml or copy the file to separate\
    \ file. For instance if you want to register a runner for muller you can set <code>gitlab-runner\
    \ register -c ~/.gitlab-runner/muller.config.toml</code> when registering the\
    \ runner and it will write the runner configuration to <code>~/.gitlab-runner/muller.config.toml</code>.\
    \ For more details regarding runner register please see <a href=\"https://docs.gitlab.com/runner/register/\"\
    \ rel=\"nofollow\">https://docs.gitlab.com/runner/register/</a></p>\n<p>To restart\
    \ a runner you can run the script based on runner type</p>\n<pre><code># restart\
    \ gerty runner\nbash $HOME/cron/restart-gerty.sh\n\n# restart muller runner\n\
    bash $HOME/cron/restart-muller.sh\n\n# restart perlmutter runner\nbash $HOME/cron/restart-perlmutter.sh\n\
    \n# restart cori runner\nbash $HOME/cron/restart-cori.sh\n</code></pre>\n<p>In\
    \ order to access gerty, you will need to login to data transfer node and then\
    \ login to gerty as follows</p>\n<pre><code>ssh dtn01.nersc.gov\ncollabsu e4s\n\
    ssh gerty\n</code></pre>\n<h2>\n<a id=\"user-content-current-challenges\" class=\"\
    anchor\" href=\"#current-challenges\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Current Challenges</h2>\n<p>There\
    \ are several challenges with building spack stack at NERSC which can be summarized\
    \ as follows</p>\n<ul>\n<li>\n<p><strong>System OS + Cray Programming Environment\
    \ (CPE) changes</strong>: A system upgrade such as change to <code>glibc</code>\
    \ or upgrades in CPE can lead to full software stack rebuild, especially if you\
    \ have externals set to packages like <code>cray-mpich</code>, <code>cray-libsci</code>\
    \ which generally change between versions</p>\n</li>\n<li>\n<p><strong>Incompatibile\
    \ compilers</strong>: Some packages can't be built with certain compilers (<code>nvhpc</code>,\
    \ <code>aocc</code>) which could be due to several factors.</p>\n<ul>\n<li>An\
    \ application doesn't have support though it was be added in newer version but\
    \ you don't have it in your spack release used for deployment</li>\n<li>Lack of\
    \ support in spack package recipe or spack-core base including spack-cray detection.\
    \ This may require getting fix and cherry-pick commit or waiting for new version</li>\n\
    <li>Spack Cray detection is an important part in build errors including how one\
    \ specifies externals via <code>modules</code> vs <code>prefix</code> both could\
    \ be provided and it requires experimentation. An example of this is trying to\
    \ get <code>cray-mpich</code> external one could set something like this with\
    \ modules or prefix</li>\n</ul>\n<div class=\"highlight highlight-source-yaml\"\
    ><pre>  <span class=\"pl-ent\">cray-mpich</span>:\n    <span class=\"pl-ent\"\
    >buildable</span>: <span class=\"pl-c1\">false</span>\n    <span class=\"pl-ent\"\
    >externals</span>:\n    - <span class=\"pl-ent\">spec</span>: <span class=\"pl-s\"\
    >cray-mpich@8.1.11 %gcc@9.3.0</span>\n      <span class=\"pl-ent\">prefix</span>:\
    \ <span class=\"pl-s\">/opt/cray/pe/mpich/8.1.11/ofi/gnu/9.1</span>\n      <span\
    \ class=\"pl-ent\">modules</span>:\n      - <span class=\"pl-s\">cray-mpich/8.1.11</span>\n\
    \      - <span class=\"pl-s\">cudatoolkit/21.9_11.4</span></pre></div>\n<ul>\n\
    <li>\n<strong>Spack concretizer</strong> prevent one from chosing a build configration\
    \ for a spec. This requires a few troubleshooting step but usually boils down\
    \ to:\n<ul>\n<li>Read the spack package file <code>spack edit &lt;package&gt;</code>\
    \ for conflicts and try <code>spack spec</code> to see concretized spec.</li>\n\
    <li>Try different version, different compiler, different dependency. Some packages\
    \ have conflicting variant for instance one can't enable <code>+openmp</code>\
    \ and <code>+pthread</code> it is mutually exclusive.</li>\n</ul>\n</li>\n</ul>\n\
    </li>\n</ul>\n<p>There is a document <a href=\"https://docs.google.com/document/d/1jWrCcK8LgpNDMytXhLdBYpIusidkoowrZAH1zos7zIw/edit?usp=sharing\"\
    \ rel=\"nofollow\">Spack E4S Issues on Permlutter</a> outlining current issues\
    \ with spack. If you need access to document please contact <strong>Shahzeb Siddiqui</strong>.</p>\n\
    <h2>\n<a id=\"user-content-contact\" class=\"anchor\" href=\"#contact\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contact</h2>\n\
    <p>If you need elevated privledge or assistance with this project please contact\
    \ one of the maintainers:</p>\n<ul>\n<li><strong>Shahzeb Siddiqui (<a href=\"\
    mailto:shahzebsiddiqui@lbl.gov\">shahzebsiddiqui@lbl.gov</a>)</strong></li>\n\
    <li>E4S Team: <strong>Sameer Shende (<a href=\"mailto:sameer@cs.uoregon.edu\"\
    >sameer@cs.uoregon.edu</a>)</strong>, <strong>Christopher Peyralans (<a href=\"\
    mailto:lpeyrala@uoregon.edu\">lpeyrala@uoregon.edu</a>)</strong>, <strong>Wyatt\
    \ Spear (<a href=\"mailto:wspear@cs.uoregon.edu\">wspear@cs.uoregon.edu</a>)</strong>,\
    \ <strong>Nicholas Chaimov (<a href=\"mailto:nchaimov@paratools.com\">nchaimov@paratools.com</a>)</strong>\n\
    </li>\n</ul>\n"
  stargazers_count: 2
  subscribers_count: 14
  topics: []
  updated_at: 1652355879.0
NOAA-EMC/WW3:
  data_format: 2
  description: WAVEWATCH III
  filenames:
  - model/ci/spack.yaml
  full_name: NOAA-EMC/WW3
  latest_release: 6.07.1
  readme: "<h1>\n<a id=\"user-content-the-wavewatch-iii-framework\" class=\"anchor\"\
    \ href=\"#the-wavewatch-iii-framework\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>The WAVEWATCH III Framework</h1>\n\
    <p>WAVEWATCH III<sup>\xAE</sup>  is a community wave modeling framework that includes\
    \ the\nlatest scientific advancements in the field of wind-wave modeling and dynamics.</p>\n\
    <h2>\n<a id=\"user-content-general-features\" class=\"anchor\" href=\"#general-features\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>General Features</h2>\n<p>WAVEWATCH III<sup>\xAE</sup> solves the\
    \ random phase spectral action density\nbalance equation for wavenumber-direction\
    \ spectra. The model includes options\nfor shallow-water (surf zone) applications,\
    \ as well as wetting and drying of\ngrid points. Propagation of a wave spectrum\
    \ can be solved using regular\n(rectilinear or curvilinear) and unstructured (triangular)\
    \ grids. See\n<a href=\"https://github.com/NOAA-EMC/WW3/wiki/About-WW3\">About\
    \ WW3</a> for a\ndetailed description of WAVEWATCH III<sup>\xAE</sup> .</p>\n\
    <h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<p>The WAVEWATCH III<sup>\xAE</sup>  framework\
    \ package has two parts that need to be combined so\nall runs smoothly: the GitHub\
    \ repo itself, and a binary data file bundle that\nneeds to be obtained from our\
    \ ftp site. Steps to successfully acquire and install\nthe framework are outlined\
    \ in our <a href=\"https://github.com/NOAA-EMC/WW3/wiki/Quick-Start\">Quick Start</a>\n\
    guide.</p>\n<h2>\n<a id=\"user-content-disclaimer\" class=\"anchor\" href=\"#disclaimer\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Disclaimer</h2>\n<p>The United States Department of Commerce (DOC)\
    \ GitHub project code is provided\non an 'as is' basis and the user assumes responsibility\
    \ for its use. DOC has\nrelinquished control of the information and no longer\
    \ has responsibility to\nprotect the integrity, confidentiality, or availability\
    \ of the information. Any\nclaims against the Department of Commerce stemming\
    \ from the use of its GitHub\nproject will be governed by all applicable Federal\
    \ law. Any reference to\nspecific commercial products, processes, or services\
    \ by service mark,\ntrademark, manufacturer, or otherwise, does not constitute\
    \ or imply their\nendorsement, recommendation or favoring by the Department of\
    \ Commerce. The\nDepartment of Commerce seal and logo, or the seal and logo of\
    \ a DOC bureau,\nshall not be used in any manner to imply endorsement of any commercial\
    \ product\nor activity by DOC or the United States Government.</p>\n"
  stargazers_count: 175
  subscribers_count: 42
  topics: []
  updated_at: 1653885544.0
NOAA-EMC/spack-stack:
  data_format: 2
  description: null
  filenames:
  - configs/templates/ufs-weather-model/spack.yaml
  - configs/templates/empty/spack.yaml
  full_name: NOAA-EMC/spack-stack
  latest_release: null
  readme: '<h1>

    <a id="user-content-spack-stack" class="anchor" href="#spack-stack" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>spack-stack</h1>

    <p>spack-stack is a collaborative effort between the NOAA Environmental Modeling
    Center (EMC), the UCAR Joint Center for Satellite Data Assimilation (JCSDA), and
    the Earth Prediction Innovation Center (EPIC). spack-stack is designed to support
    the various applications of the supporting agencies such as the Unified Forecast
    System (UFS) or the Joint Effort for Data assimilation Integration (JEDI). The
    stack can be installed on a range of platforms, from Linux and macOS laptops to
    HPC systems, and comes pre-configured for many systems. Users can install the
    necessary packages for a particular application and later add the missing packages
    for another application without having to rebuild the entire stack.</p>

    <p><a href="https://github.com/spack/spack">spack</a> is a community-supported,
    multi-platform, Python-based package manager originally developed by the Lawrence
    Livermore National Laboratory (LLNL; <a href="https://computing.llnl.gov/projects/spack-hpc-package-manager"
    rel="nofollow">https://computing.llnl.gov/projects/spack-hpc-package-manager</a>).
    It is provided as a submodule so that a stable version can be referenced. <a href="https://spack.readthedocs.io/en/latest/"
    rel="nofollow">See the Spack Documentation for more information</a></p>

    <p>spack-stack is mainly a collection of Spack configuration files, but provides
    a Spack extension to simplify the installation process:</p>

    <ul>

    <li>

    <code>spack stack create</code> is provided to copy common, site-specific, and
    application-specific configuration files into a coherent Spack environment and
    to create container recipes</li>

    <li>

    <code>spack stack setup-meta-modules</code> creates compiler, MPI and Python meta-modules
    for a convenient setup of a user environment using modules (lua and tcl)</li>

    </ul>

    <p>spack-stack is maintained by:</p>

    <ul>

    <li>Kyle Gerheiser (@kgerheiser), NOAA-EMC</li>

    <li>Dom Heinzeller (@climbfuji), JCSDA</li>

    <li>not yet appointed, EPIC</li>

    </ul>

    <p>Ready-to-use spack-stack installations are available on the following platforms:</p>

    <p><strong>Note: this versions are for early testers - use at your own risk</strong></p>

    <table>

    <thead>

    <tr>

    <th>System</th>

    <th>Location</th>

    <th>Maintained by (temporary)</th>

    <th>jedi-ewok tested</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>MSU Orion</td>

    <td><code>/work/noaa/gsd-hpcs/dheinzel/spack-stack-20220411-ewok-tmp</code></td>

    <td>Dom Heinzeller</td>

    <td>yes</td>

    </tr>

    <tr>

    <td>NASA Discover</td>

    <td><code>/discover/swdev/jcsda/spack-stack/spack-stack-v0.0.1/envs/jedi-all-intel-2022.0.1/install</code></td>

    <td>Dom Heinzeller</td>

    <td>yes</td>

    </tr>

    <tr>

    <td>NCAR-Wyoming Cheyenne</td>

    <td><code>/glade/work/jedipara/cheyenne/spack-stack/spack-stack-v0.0.1/envs/jedi-all-intel-2022.0.2/install</code></td>

    <td>Dom Heinzeller</td>

    <td>yes</td>

    </tr>

    <tr>

    <td>NOAA NCO WCOSS2</td>

    <td></td>

    <td></td>

    <td></td>

    </tr>

    <tr>

    <td>NOAA RDHPCS Gaea</td>

    <td><code>/lustre/f2/pdata/esrl/gsd/spack-stack/spack-stack-v0.0.1</code></td>

    <td>Dom Heinzeller</td>

    <td>yes</td>

    </tr>

    <tr>

    <td>NOAA RDHPCS Hera</td>

    <td></td>

    <td></td>

    <td></td>

    </tr>

    <tr>

    <td>NOAA RDHPCS Jet</td>

    <td></td>

    <td></td>

    <td></td>

    </tr>

    </tbody>

    </table>

    <p>For questions or problems, please consult the currently open <a href="https://github.com/noaa-emc/spack-stack/issues">issues</a>
    and the <a href="https://github.com/noaa-emc/spack-stack/discussions">current
    and past discussions</a> first.</p>

    <p><strong>Note. spack-stack is in early development and not yet ready for use.
    Instructions may be incomplete or invalid.</strong></p>

    <h2>

    <a id="user-content-quickstart" class="anchor" href="#quickstart" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quickstart</h2>

    <pre><code>git clone https://github.com/NOAA-EMC/spack-stack.git

    cd spack-stack


    # Ensure Python 3.7+ is available and the default before sourcing spack


    # Sources Spack from submodule and sets ${SPACK_STACK_DIR}

    source setup.sh


    # Basic usage of create.py

    spack stack create -h

    </code></pre>

    <h3>

    <a id="user-content-create-local-environment" class="anchor" href="#create-local-environment"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Create
    local environment</h3>

    <pre><code># See a list of sites and apps

    spack stack create env -h


    # Create a pre-configured Spack environment in envs/&lt;app&gt;.&lt;site&gt;

    # (copies site-specific, application-specific, and common config files into the
    environment directory)

    spack stack create env --site hera --specs jedi-fv3-env --name jedi-fv3.hera


    # Activate the newly created environment

    # Optional: decorate the command line prompt using -p

    #     Note: in some cases, this can mess up long lines in bash

    #     because color codes are not escaped correctly. In this

    #     case, use export SPACK_COLOR=''never'' first.

    spack env activate [-p] envs/jedi-fv3.hera


    # Optionally edit config files (spack.yaml, packages.yaml compilers.yaml, site.yaml)

    emacs envs/jedi-fv3.hera/spack.yaml

    emacs envs/jedi-fv3.hera/common/*.yaml

    emacs envs/jedi-fv3.hera/site/*.yaml


    # Process the specs and install

    # Note: both steps will take some time!

    spack concretize

    spack install


    # Create lua module files

    spack module lmod refresh


    # Create meta-modules for compiler, mpi, python

    spack stack setup-meta-modules

    </code></pre>

    <h3>

    <a id="user-content-create-container" class="anchor" href="#create-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Create
    container</h3>

    <pre><code># See a list of preconfigured containers

    spack stack create container -h


    # Create container spack definition (spack.yaml) in directory envs/&lt;spec&gt;.&lt;config&gt;

    spack stack create container docker-ubuntu-gcc-openmpi --app ufs-weather-model


    # Descend into container environment directory

    cd envs/docker-ubuntu-gcc-openmpi.ufs-weather-model


    # Optionally edit config file

    emacs spack.yaml


    # Docker: create Dockerfile and build container

    # See section "container" in spack.yaml for additional information

    spack containerize &gt; Dockerfile

    docker build -t myimage .

    docker run -it myimage

    </code></pre>

    <h2>

    <a id="user-content-generating-new-site-config" class="anchor" href="#generating-new-site-config"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Generating
    new site config</h2>

    <p>Recommended: Start with an empty (default) site config. Then run <code>spack
    external find</code> to locate common external packages such as git, Perl, CMake,
    etc., and run <code>spack compiler find</code> to locate compilers in your path.
    Compilers or external packages with modules need to be added manually.</p>

    <pre><code>spack stack create env --site default --specs jedi-ufs-env --name jedi-ufs.mysite


    # Descend into site config directory

    cd envs/jedi-ufs.mysite/site


    # Find external packages and compilers, output the files here

    # (overwrites packages.yaml and compilers.yaml)

    SPACK_SYSTEM_CONFIG_PATH=`pwd` spack external find --all --scope system

    SPACK_SYSTEM_CONFIG_PATH=`pwd` spack compiler find --scope system


    # Optionally edit config files as above in the quickstart section


    # Optionally attempt to find additional packages by name,

    # for example: "spack external find wget"

    </code></pre>

    <p>It is also instructive to peruse the GitHub actions scripts in <code>.github/workflows</code>
    and <code>.github/actions</code> to see how automated spack-stack builds are configured
    for CI testing, as well as the existing site configs in <code>configs/sites</code>.</p>

    <h2>

    <a id="user-content-known-issues" class="anchor" href="#known-issues" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Known issues</h2>

    <h3>

    <a id="user-content-general" class="anchor" href="#general" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>General</h3>

    <ol>

    <li>First call to <code>spack concretize</code> fails with <code>[Errno 2] No
    such file or directory: ... .json</code>

    This can happen when <code>spack concretize</code> is called the very first time
    in a new spack-stack clone, during which the boostrapping (installation of clingo)
    is done first. Simply rerunning the command should solve the problem.</li>

    </ol>

    <h2>

    <a id="user-content-disclaimer" class="anchor" href="#disclaimer" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Disclaimer</h2>

    <p>The United States Department of Commerce (DOC) GitHub project code is

    provided on an "as is" basis and the user assumes responsibility for

    its use. DOC has relinquished control of the information and no longer

    has responsibility to protect the integrity, confidentiality, or

    availability of the information. Any claims against the Department of

    Commerce stemming from the use of its GitHub project will be governed

    by all applicable Federal law. Any reference to specific commercial

    products, processes, or services by service mark, trademark,

    manufacturer, or otherwise, does not constitute or imply their

    endorsement, recommendation or favoring by the Department of

    Commerce. The Department of Commerce seal and logo, or the seal and

    logo of a DOC bureau, shall not be used in any manner to imply

    endorsement of any commercial product or activity by DOC or the United

    States Government.</p>

    '
  stargazers_count: 1
  subscribers_count: 6
  topics: []
  updated_at: 1653600474.0
NOAA-GFDL/AM4:
  data_format: 2
  description: null
  filenames:
  - container/spack_intel_gfdl_model.yaml
  full_name: NOAA-GFDL/AM4
  latest_release: '2021.03'
  readme: '<h1>

    <a id="user-content-gfdl-am4-model" class="anchor" href="#gfdl-am4-model" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>GFDL AM4 Model</h1>

    <p><a href="https://zenodo.org/badge/latestdoi/102487636" rel="nofollow"><img
    src="https://camo.githubusercontent.com/878db836b9000fd7d9ff531257cade7343f3a3fdf8f764b5a7f1e8ef6ccc6abe/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3130323438373633362e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/102487636.svg" style="max-width:100%;"></a></p>

    <p>This repository includes the public release of the GFDL AM4 model

    code.  The AM4 model is described in the

    <a href="https://doi.org/10.1002/2017MS001208" rel="nofollow">two</a>

    <a href="https://doi.org/10.1002/2017MS001209" rel="nofollow">articles</a> published
    in the

    <a href="https://agupubs.onlinelibrary.wiley.com/journal/19422466" rel="nofollow">Journal
    of Advances in Modeling Earth Systems

    (JAMES)</a>.

    More information on the model and access to the output is available on

    the <a href="http://data1.gfdl.noaa.gov/nomads/forms/am4.0/" rel="nofollow">AM4
    data and code

    site</a> at the

    <a href="https://www.gfdl.noaa.gov" rel="nofollow">Geophysical Fluid Dynamics
    Laboratory

    (GFDL)</a>.</p>

    <p>The layout of this package includes the following directories:</p>

    <ul>

    <li>src - The source code for the AM4 model</li>

    <li>exec - The build directory with Makefiles for building the AM4 model executable</li>

    <li>idealized_exec - The build directory with Makefiles for building the aquaplanet

    and doubly periodic executable</li>

    <li>run - Sample run script and updated files needed for running</li>

    <li>analysis - Sample analysis scripts</li>

    </ul>

    <h2>

    <a id="user-content-cloning-instructions" class="anchor" href="#cloning-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Cloning
    Instructions</h2>

    <p>This repository uses <a href="https://git-scm.com/book/en/v2/Git-Tools-Submodules"
    rel="nofollow">git

    submodules</a> to

    point to other repositories.  Thus, care should be taken when cloning,

    and updating the source to ensure all source.  To obtain all source,

    use the following git command</p>

    <pre><code>git clone --recursive https://github.com/NOAA-GFDL/AM4.git

    </code></pre>

    <p>The <code>--recursive</code> option to <code>git clone</code> instructs git
    to recursively

    clone all submodules.  In the event the repository was not cloned

    using the <code>--recursive</code> option, the following step must be taken to

    obtain all sources:</p>

    <pre><code># From within the AM4 parent directory

    git submodule update --init --recursive

    </code></pre>

    <h2>

    <a id="user-content-source-code" class="anchor" href="#source-code" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Source Code</h2>

    <p>All model source is contained in the <a href="src">src</a> directory.  GFDL

    tracks code using the git version control system.  This package

    includes a single version of the following GFDL model components.  The

    git hash listed corresponds to the commit hash in the internal GFDL

    git repository.</p>

    <table>

    <thead>

    <tr>

    <th>Component</th>

    <th>Commit Hash</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>atmos_drivers</td>

    <td>5ee95d6abf0879594551dd7e6635dff4004c4010</td>

    </tr>

    <tr>

    <td>atmos_param</td>

    <td>2e94acfd8621e85216bf822c395a8c3f15a511a5</td>

    </tr>

    <tr>

    <td>atmos_shared</td>

    <td>a557d4d7bab033ef1ad1d400a62fe07a97ccb477</td>

    </tr>

    <tr>

    <td>ice_param</td>

    <td>1553c8bc4f9a66791c89367b6f327147523155ed</td>

    </tr>

    <tr>

    <td>ice_sis</td>

    <td>ccc7328dcd79706dd5c17c8bab660222886fc80b</td>

    </tr>

    <tr>

    <td>land_lad2</td>

    <td>a220288ecb289bf9d793d051fc5076072874ce07</td>

    </tr>

    </tbody>

    </table>

    <p>The following components are available in the

    <a href="https://github.com/NOAA-GFDL">NOAA-GFDL</a> github organization:</p>

    <ul>

    <li><a href="https://github.com/NOAA-GFDL/MOM6">MOM6</a></li>

    <li><a href="https://github.com/NOAA-GFDL/coupler">coupler</a></li>

    <li>

    <a href="https://github.com/NOAA-GFDL/FMS">FMS</a> (as <a href="src/shared">shared</a>)</li>

    <li>

    <a href="https://github.com/NOAA-GFDL/GFDL_atmos_cubed_sphere">GFDL_atmos_cubed_sphere
    (tag AM4.0)</a> (as <a href="src/atmos_cubed_sphere">atmos_cubed_sphere</a>)</li>

    </ul>

    <h2>

    <a id="user-content-building-am4" class="anchor" href="#building-am4" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Building AM4</h2>

    <p>###Containers

    The <a href="container">container folder</a> provides example Dockerfiles and
    Signularity

    definition files to use to build AM4 containers using either GCC/GFORTAN or

    Intel oneAPI. There is a script that can be used to build the intel

    singularity containers, and the first step of this script can be used with the

    other GFDL climate models.</p>

    <h3>

    <a id="user-content-from-source" class="anchor" href="#from-source" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>From source</h3>

    <p>The <a href="exec">exec</a> directory contains Makefiles that can be used to

    build the AM4 executable.  These Makefiles were generated using the

    <a href="https://github.com/NOAA-GFDL/mkmf">Make Makefile (mkmf)</a> program.

    Included in the exec direcgtory is a sample make template file for the

    Intel compilers (<a href="exec/templates/intel.mk">intel.mk</a>).  This make

    template can be used on any system with a relatively recent version of

    the Intel compilers, the netCDF 4 library and the MPICH2 MPI library.

    Included in the <a href="exec/templates/intel.mk">intel.mk</a> file are

    additional settings that can be modified during the build.</p>

    <p>To run the default build (-O3 -msse2), go to the exec directory and

    enter the command</p>

    <pre><code>make

    </code></pre>

    <p>If you would like to change some of the compiler options, there are several
    different

    options to add to the make command.  For example</p>

    <pre><code>make ISA=-xhost BLD_TYPE=REPRO

    </code></pre>

    <p>will replace -msse with -xhost and -O3 with -O2.  The three options for

    <code>BLD_TYPE</code> are<br>

    <code>PROD</code> (-O3)<br>

    <code>REPRO</code> (-O2)<br>

    <code>DEBUG</code> (-O0 and other traps)<br>

    All of the make line options can be

    found in the <a href="exec/templates/intel.mk">intel.mk</a> file.</p>

    <h2>

    <a id="user-content-obtaining-the-input-data" class="anchor" href="#obtaining-the-input-data"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Obtaining
    the input data</h2>

    <p>The input data required for running the AM4 model can be found on

    <a href="http://data1.gfdl.noaa.gov/nomads/forms/am4.0/" rel="nofollow">GFDL''s
    data

    portal</a> .</p>

    <p>The file <code>AM4.tar.gz</code> contains a configured run directory to run
    a

    sample experiment of the AM4 model.  Included in the tar file is a

    README.AM4_run with more instructions on how to configure the AM4 run

    directory.</p>

    <p>On Linux systems, the <code>wget</code> command is usually sufficient to download
    the data

    file:</p>

    <pre><code>wget ftp://nomads.gfdl.noaa.gov/users/Ming.Zhao/AM4Documentation/GFDL-AM4.0/inputData/AM4_run.tar.gz

    </code></pre>

    <p>To ensure the file downloaded is complete and not corrupted, download one of
    the two files:</p>

    <pre><code>wget ftp://nomads.gfdl.noaa.gov/users/Ming.Zhao/AM4Documentation/GFDL-AM4.0/inputData/AM4_run.tar.gz.sha256

    wget ftp://nomads.gfdl.noaa.gov/users/Ming.Zhao/AM4Documentation/GFDL-AM4.0/inputData/AM4_run.tar.gz.sig

    </code></pre>

    <p>and run the following command that corresponds to the signature file downloaded:</p>

    <pre><code>sha256sum -c AM4_run.tar.gz.sha256

    </code></pre>

    <pre><code>gpg --verify AM4_run.tar.gz.sig

    </code></pre>

    <h2>

    <a id="user-content-running-am4" class="anchor" href="#running-am4" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running AM4</h2>

    <p>Included in the run directory is a sample run script for reference.

    To run the AM4 sample experiment, first download the data file

    mentioned in <a href="#obtaining-the-input-data">Obtaining the Input data</a>

    section.  Replace diag_table and input.nml in the top level of the

    untar''d directory with the corresponding files in the run directory

    of this repository. Modify the variables in the configuration section

    in the sample run script, and then run the script.</p>

    <p>The sample data and run script are configured to run on 216

    processors.  To run on a different number of processors, or modify the

    experiment, refer to the <code>README.AM4_run</code> file included in the AM4

    data tarball.</p>

    <p>Note: The <code>input.nml</code> file (found in the AM4 data tarball) contains

    Fortran namelists and namelist variables that modify, at run time, the

    model.  To learn more about the settings in the <code>input.nml</code> file,

    please refer to source code where the namelist/variable are defined.</p>

    <h2>

    <a id="user-content-analysis-scripts" class="anchor" href="#analysis-scripts"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Analysis
    Scripts</h2>

    <p>Some of the climate analysis scripts run at NOAA GFDL and used in the

    AM4 documentation papers are located in the analysis directory.

    Within each analysis suite, is a <a href="https://jupyter-notebook.readthedocs.io/en/stable/"
    rel="nofollow">jupyter

    notebook</a>, both

    readable and runnable from your local jupyter environment, provided

    all dependencies are installed.</p>

    <p>E.g.</p>

    <ul>

    <li><a href="analysis/cjs1/radiation_atmos_av_mon/radiation_atmos_av_mon.ipynb">Radiation
    processor</a></li>

    <li><a href="analysis/bw/bw_atmos_cru_ts_a1r/bw_atmos_monthly_cru_ts.1980-2014.ipynb">Long-term
    DJF seasonal mean</a></li>

    <li><a href="analysis/bw/bw_atmos_zm_atl_pac_a1r/bw_atmos_atl_pac.1980-2014.ipynb">Zonal_mean_zonal_wind_stress</a></li>

    <li><a href="analysis/pcmdimetrics/portraitPlot-AM4.AMIP.ipynb">PCMDI Metrics
    Portrait Plot</a></li>

    </ul>

    <h2>

    <a id="user-content-model-output-and-other-references" class="anchor" href="#model-output-and-other-references"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Model
    output and Other References</h2>

    <p>Please refer to the <a href="http://data1.gfdl.noaa.gov/nomads/forms/am4.0/"
    rel="nofollow">AM4 data and code

    site</a> for details

    about where to find model and OBS data used in the papers.</p>

    <p>For all analysis figures and pertaining data, please use the AM4

    documentation papers as the original reference.</p>

    <p>Please direct your questions and feedback to

    <a href="mailto:gfdl.climate.model.info@noaa.gov">gfdl.climate.model.info@noaa.gov</a></p>

    <h2>

    <a id="user-content-disclaimer" class="anchor" href="#disclaimer" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Disclaimer</h2>

    <p>The United States Department of Commerce (DOC) GitHub project code is

    provided on an ''as is'' basis and the user assumes responsibility for

    its use.  DOC has relinquished control of the information and no

    longer has responsibility to protect the integrity, confidentiality,

    or availability of the information.  Any claims against the Department

    of Commerce stemming from the use of its GitHub project will be

    governed by all applicable Federal law.  Any reference to specific

    commercial products, processes, or services by service mark,

    trademark, manufacturer, or otherwise, does not constitute or imply

    their endorsement, recommendation or favoring by the Department of

    Commerce.  The Department of Commerce seal and logo, or the seal and

    logo of a DOC bureau, shall not be used in any manner to imply

    endorsement of any commercial product or activity by DOC or the United

    States Government.</p>

    <p>This project code is made available through GitHub but is managed by

    NOAA-GFDL at <a href="https://gitlab.gfdl.noaa.gov" rel="nofollow">https://gitlab.gfdl.noaa.gov</a>.</p>

    '
  stargazers_count: 11
  subscribers_count: 7
  topics:
  - fortran
  - jupyter-notebook
  - shell-script
  - ncl
  updated_at: 1645229928.0
NOAA-GFDL/HPC-ME:
  data_format: 2
  description: null
  filenames:
  - spack_gnu.yaml
  - apps/spack_intel_gfdl_model.yaml
  - apps/spack_intel_ufs_model.yaml
  - spack_intel_ubuntu20.04_e4s.yaml
  full_name: NOAA-GFDL/HPC-ME
  latest_release: null
  readme: '<h1>

    <a id="user-content-hpc-me-hpc-portable-containers-for-model-environments" class="anchor"
    href="#hpc-me-hpc-portable-containers-for-model-environments" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HPC-ME: HPC Portable
    Containers for Model Environments</h1>

    <h2>

    <a id="user-content-contents" class="anchor" href="#contents" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contents</h2>

    <ul>

    <li><a href="#what-is-hpc-me">What is HPC-ME</a></li>

    <li><a href="#list-of-current-compilers">List of current compilers/MPI/OS</a></li>

    <li><a href="#list-of-current-libraries">List of current libraries</a></li>

    <li><a href="#how-to-build">How to build</a></li>

    <li><a href="#how-to-use">How to use</a></li>

    <li><a href="#gfdl-example">GFDL example</a></li>

    <li><a href="#planned-improvements">Planned improvements</a></li>

    </ul>

    <h2>

    <a id="user-content-what-is-hpc-me" class="anchor" href="#what-is-hpc-me" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>What is HPC-ME</h2>

    <p>HPC Portable Container - Model Environments is a set of Dockerfiles, Singularity
    Definition files, and containers to provide portable model environments for scientific
    applications that require the same set of libraries.  The ultimate goal is to
    have a community-based list of libraries that are needed for compiling, executing,
    and post-processing earth science models.  We all use many of the same underlying
    libraries, and by working together we can agree upon a community-based approach
    to making container usage as standardized as possible.</p>

    <h2>

    <a id="user-content-list-of-current-compilersmpios" class="anchor" href="#list-of-current-compilersmpios"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>List
    of current compilers/MPI/OS</h2>

    <p>For each container, there is a full version that contains the programming environment
    and a smaller runtime environment that can be used to run compiled executables.
    (The runtime container definition files will be added soon.)

    #- <a href="Dockerfile_gnu_ubuntu20.04">gcc 8/mpich/ubuntu 20.04</a></p>

    <ul>

    <li><a href="Dockerfile_gnu_rhel8">gcc 8/mpich/RHEL8</a></li>

    <li>

    <a href="Dockerfile_intel_ubuntu18.04">intel oneAPI 2022.1/mpich(impi)/ubuntu
    18.04</a>

    #- <a href="Dockerfile_intel_centos8">intel oneAPI 2021.4/mpich(impi)/centos 8</a>

    </li>

    </ul>

    <h2>

    <a id="user-content-list-of-current-libraries" class="anchor" href="#list-of-current-libraries"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>List
    of current libraries</h2>

    <p>This is the current list of most of the libraries used in the HPC-ME containers
    (We are trying to keep this up-to-date).

    The complete lit should be found in the respective YAML file.</p>

    <ul>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#automake"
    rel="nofollow">automake@1.16.3</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#bacio" rel="nofollow">bacio@2.4.1</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#berkeley-db"
    rel="nofollow">berkeley-db@18.1.40</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#bison" rel="nofollow">bison@3.7.6</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#bzip2" rel="nofollow">bzip2@1.0.8</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#cmake" rel="nofollow">cmake@3.21.2</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#crtm" rel="nofollow">crtm@2.3.0</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#curl" rel="nofollow">curl@7.78.0</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#diffutils"
    rel="nofollow">diffutils@3.7</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#esmf" rel="nofollow">esmf@8.1.1</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#expat" rel="nofollow">expat@2.4.1</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#g2" rel="nofollow">g2@3.4.3</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#g2tmpl"
    rel="nofollow">g2tmpl@1.10.0</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#gdbm" rel="nofollow">gdbm@1.19</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#gsl" rel="nofollow">gsl@2.7</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#hdf5" rel="nofollow">hdf5@1.10.7</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#intel-mpi"
    rel="nofollow">intel-mpi@2019.10.317</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#ip" rel="nofollow">ip@3.3.3</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#ip2" rel="nofollow">ip2@1.1.2</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#jasper"
    rel="nofollow">jasper@2.0.32</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#libbsd"
    rel="nofollow">libbsd@0.11.3</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#libiconv"
    rel="nofollow">libiconv@1.16</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#libjpeg-turbo"
    rel="nofollow">libjpeg-turbo@2.1.0</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#libmd" rel="nofollow">libmd@1.0.3</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#libpng"
    rel="nofollow">libpng@1.6.37</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#libsigsegv"
    rel="nofollow">libsigsegv@2.13</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#libxml2"
    rel="nofollow">libxml2@2.9.12</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#libyaml"
    rel="nofollow">libyaml@0.2.5</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#m4" rel="nofollow">m4@1.4.19</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#nasm" rel="nofollow">nasm@2.15.05</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#ncurses"
    rel="nofollow">ncurses@6.2</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#nemsio"
    rel="nofollow">nemsio@2.5.2</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#netcdf-c"
    rel="nofollow">netcdf-c@4.8.0</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#netcdf-fortran"
    rel="nofollow">netcdf-fortran@4.5.3</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#numactl"
    rel="nofollow">numactl@2.0.14</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#openssl"
    rel="nofollow">openssl@1.1.1l</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#parallel-netcdf"
    rel="nofollow">parallel-netcdf@1.12.2</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#perl" rel="nofollow">perl@5.34.0</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#pkgconf"
    rel="nofollow">pkgconf@1.8.0</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#readline"
    rel="nofollow">readline@8.1</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#sfcio" rel="nofollow">sfcio@1.4.1</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#sigio" rel="nofollow">sigio@2.3.2</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#sp" rel="nofollow">sp@2.3.3</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#udunits"
    rel="nofollow">udunits@2.2.28</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#w3emc" rel="nofollow">w3emc@2.9.0</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#w3nco" rel="nofollow">w3nco@2.4.1</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#wrf-io"
    rel="nofollow">wrf-io@1.2.0</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#xerces-c"
    rel="nofollow">xerces-c@3.2.3</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#xz" rel="nofollow">xz@5.2.5</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#zlib" rel="nofollow">zlib@1.2.11</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#lmod" rel="nofollow">lmod@8.5.6</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#nccmp" rel="nofollow">nccmp@1.8.6.5</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#nco" rel="nofollow">nco@4.7.9</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#cray-netcdf"
    rel="nofollow">cray-netcdf@4.6.3.2</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#cray-hdf5"
    rel="nofollow">cray-hdf5@1.10.5.2</a></li>

    <li><a href="https://spack.readthedocs.io/en/latest/package_list.html#uberftp"
    rel="nofollow">uberftp</a></li>

    </ul>

    <h2>

    <a id="user-content-how-to-build" class="anchor" href="#how-to-build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>How to build</h2>

    <p><strong>We plan to make this step optional soon.</strong> In order to build
    the Docker images, you will need access to a computer with root-like access, and
    either docker or singularity installed. If you do not have root-like access to
    a suitable machine, you can still run images that were already created (e.g. on
    Docker hub), and we plan on hosting runnable Docker images along with the Dockerfiles
    in this repository soon. If you have root-like access and docker, start by choosing
    one of the currently supported model environments from the list above. Then build
    the Docker container from the Dockerfile using docker build; for example, to build
    the gcc8/mpich/ubuntu18 container:</p>

    <pre><code>docker build --file Dockerfile_gnu_ubuntu20.04 . --tag hpc-me.ubuntu.gnu

    </code></pre>

    <p>The build process takes approximately 2-3 hours, as the packages are downloaded
    and compiled using Spack. After a successful build, you will see that the image
    was built and tagged successfully:</p>

    <pre><code>Successfully built 90a878af77b4

    Successfully tagged hpc-me.rhel8.gnu:latest

    </code></pre>

    <p>Then, you may run the container using docker or singularity on the same host.
    To run the image on a different machine, pushing the image to Docker Hub is recommended.
    Note that you will need a DockerHub account to do this (replace USER with your
    Docker user ID in the examples below). For example:</p>

    <pre><code>docker tag hpc-me.rhel8.gnu USER/hpc-me.rhel8.gnu

    docker login

    docker push USER/hpc-me.rhel8.gnu:latest

    </code></pre>

    <h2>

    <a id="user-content-how-to-use" class="anchor" href="#how-to-use" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>How to use</h2>

    <p>We plan to make improvements on this process. Also, while we plan on making
    Docker images available on the GitHub container registry, currently you must build
    the images yourself. Please start with the <a href="#how-to-build">Build instructions</a>
    to generate a Docker image with your desired OS/compiler HPC-ME environment. Then
    you may run the container using docker or singularity; singularity is more likely
    than docker to be available on HPC environments.</p>

    <p>The usage documentation consists of some general notes on serial/parallel usage,
    files inside and outside the container, downloading the containers, and then specific
    usage scenarios:</p>

    <ul>

    <li><a href="#serial-applications-using-docker">Serial applications using docker</a></li>

    <li><a href="#serial-applications-using-singularity">Serial applications using
    singularity</a></li>

    <li><a href="#parallel-applications-using-singularity">Parallel applications using
    singularity</a></li>

    </ul>

    <h3>

    <a id="user-content-serial-and-parallel-usage" class="anchor" href="#serial-and-parallel-usage"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Serial
    and parallel usage</h3>

    <p>HPC-ME containers are intended for both serial and parallel applications. Serial
    applications include compiling model executables, generating input grids, and
    post-processing model output. Earth system, climate, and weather models require
    parallelism to run efficiently, and use one of the Message Passage Interface (MPI)
    implementations OpenMPI, Intel MPI, or mpich. GCC-based HPC-ME containers use
    the mpich-based MPI library, which is widely available on most HPC sites, and
    the Intel-based containers contain both mpich and Intel MPI.</p>

    <h3>

    <a id="user-content-notes-on-filesystems-and-writing-files" class="anchor" href="#notes-on-filesystems-and-writing-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Notes
    on filesystems and writing files</h3>

    <p>We recommend not saving or modifying files within the environment container,
    and instead create and modify files on your regular filesystem. To do this, you
    will need to connect your filesystem to your container using bind mounts.</p>

    <h3>

    <a id="user-content-downloading-containers-and-managing-images-on-the-filesystem"
    class="anchor" href="#downloading-containers-and-managing-images-on-the-filesystem"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Downloading
    containers and managing images on the filesystem</h3>

    <p>Once you have pushed your images to DockerHub, you will need to download them
    before using. In the examples below, replace USER with your Docker Hub ID. If
    using docker,</p>

    <pre><code>docker pull USER/hpc-me.rhel8.gnu:latest

    </code></pre>

    <p>If using singularity,</p>

    <pre><code>singularity pull docker://USER/hpc-me.rhel8.gnu:latest

    </code></pre>

    <p>If using singularity, the image file (SIF format) is saved to the current working
    directory</p>

    <pre><code>&gt; ls *.sif

    -rwxr-xr-x 532M Dec 10 16:09 hpc-me.rhel8.gnu_latest.sif*

    </code></pre>

    <p>If using docker, the downloaded image is handled by the central docker service.</p>

    <h3>

    <a id="user-content-serial-applications-using-docker" class="anchor" href="#serial-applications-using-docker"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Serial
    applications using docker</h3>

    <p>You may activate an interactive shell within the desired HPC-ME container using
    docker. After running the container, the compilers and tools available within
    the container will be accessible in your PATH; e.g.</p>

    <pre><code>&gt; docker run -it hpc-me.rhel8.gnu:latest


    [root@0d2cf64e1175 /]# which nf-config

    /opt/view/bin/nf-config


    [root@0d2cf64e1175 /]# nf-config --version

    netCDF-Fortran 4.5.3


    [root@0d2cf64e1175 /]# nf-config --cflags

    -I/opt/software/linux-rhel8-x86_64/gcc-8.4.1/netcdf-fortran-4.5.3-g5qfkdlp36unt2s4j4wyrc6heh2sa64n/include

    </code></pre>

    <h3>

    <a id="user-content-serial-applications-using-singularity" class="anchor" href="#serial-applications-using-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Serial
    applications using singularity</h3>

    <p>Singularity can run Docker images and is more likely to be available on HPC
    environments. As with docker run, the HPC-ME tools and compilers are available
    in the shell, somewhat similar to loading a set of Environment Modules prepared
    by site administrators.</p>

    <pre><code>&gt;singularity run hpc-me.rhel8.gnu_latest.sif


    Singularity&gt; which nf-config

    /opt/view/bin/nf-config


    Singularity&gt; nf-config --version

    netCDF-Fortran 4.5.3

    </code></pre>

    <h3>

    <a id="user-content-parallel-applications-using-singularity" class="anchor" href="#parallel-applications-using-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Parallel
    applications using singularity</h3>

    <p>HPC-ME containers can provide the runtime environment for MPI applications.
    For instance, one could compile an MPI application using the instructions above
    using one of the HPC-ME development containers; and then run the application using
    the corresponding runtime HPC-ME container.</p>

    <p>Please note that we are continuing to improve the usability of HPC-ME containers
    as well as provide more usage examples.</p>

    <p>Usually, GFDL climate models are run on gaea by submitting a runscript to the
    Slurm scheduler. The runscript loads needed runtime Environment Modules, prepares
    input directories and files, and executes the MPI executable using srun. The HPC-ME
    containers provide the necessary runtime environment, obviating the need for loading
    Environment Modules. Currently, our approach for using the HPC-ME containers is
    as follows:</p>

    <ol>

    <li>Create a new container, starting with the desired HPC-ME runtime container</li>

    <li>Add the MPI-compiled executable to the container filesystem</li>

    <li>Set the MPI-compiled executable to as the container''s command (so that when
    the container is run the MPI executable within the container runs)</li>

    <li>Run the singularity container SIF file using srun within the runscript, replacing
    the traditional MPI executable.</li>

    </ol>

    <ul>

    <li>Replace "srun executable.x" with "srun singularity run container.SIF"</li>

    <li>Add --mpi=pmi2 to the srun call, which connects the system MPI to the container
    MPI to the singularity run call</li>

    <li>Bind the working directory so that the container has access to the input files
    and can write output files (singularity run -B=/path/to/workdir)</li>

    </ul>

    <ol start="5">

    <li>Submit the modified runscript to the scheduler</li>

    </ol>

    <p>We plan to provide more examples and usage scenarios, such as using the HPC-ME
    containers as-is (i.e. not creating a new container as described above)</p>

    <h2>

    <a id="user-content-gfdl-example" class="anchor" href="#gfdl-example" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>GFDL example</h2>

    <p>An example of using an HPC-ME container with the GFDL FRE workflow can be found
    <a href="GFDL_EXAMPLE.md">here</a></p>

    <h2>

    <a id="user-content-planned-improvements" class="anchor" href="#planned-improvements"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Planned
    improvements</h2>

    <p>HPC-ME is a work in progress under active development, so please check back
    or follow the repository for more updates.</p>

    <h3>

    <a id="user-content-build-cache" class="anchor" href="#build-cache" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build cache</h3>

    <p>We are working to create a build cache for the libraries listed so that building
    the containers is quick and easy.</p>

    <h3>

    <a id="user-content-github-container-registry" class="anchor" href="#github-container-registry"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Github
    container registry</h3>

    <p>We are working to add CI capability to this repository, so that the containers
    will be automatically built and stored in the github container registry. This
    will make building unnecessary for most cases, though users may build the containers
    themselves if they wish (e.g. for custom modifications).</p>

    <h3>

    <a id="user-content-more-usage-examples-and-documentation-especially-for-mpi-applications"
    class="anchor" href="#more-usage-examples-and-documentation-especially-for-mpi-applications"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>More
    usage examples and documentation, especially for MPI applications</h3>

    <p>We are still learning how to best use the HPC-ME containers with MPI appliations,
    so please check back.</p>

    <h3>

    <a id="user-content-disclaimer" class="anchor" href="#disclaimer" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Disclaimer</h3>

    <p>The United States Department of Commerce (DOC) GitHub project code is provided

    on an ''as is'' basis and the user assumes responsibility for its use. DOC has

    relinquished control of the information and no longer has responsibility to

    protect the integrity, confidentiality, or availability of the information. Any

    claims against the Department of Commerce stemming from the use of its GitHub

    project will be governed by all applicable Federal law. Any reference to

    specific commercial products, processes, or services by service mark,

    trademark, manufacturer, or otherwise, does not constitute or imply their

    endorsement, recommendation or favoring by the Department of Commerce. The

    Department of Commerce seal and logo, or the seal and logo of a DOC bureau,

    shall not be used in any manner to imply endorsement of any commercial product

    or activity by DOC or the United States Government.</p>

    <p>This project code is made available through GitHub but is managed by NOAA-GFDL

    at <a href="https://gitlab.gfdl.noaa.gov" rel="nofollow">https://gitlab.gfdl.noaa.gov</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1650907447.0
PawseySC/hpc-container-training:
  data_format: 2
  description: 'Training material on using containers in an HPC setting. '
  filenames:
  - demos/spack_blast/spack.yaml
  full_name: PawseySC/hpc-container-training
  latest_release: null
  readme: '<h1>

    <a id="user-content-readme" class="anchor" href="#readme" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Readme</h1>

    '
  stargazers_count: 1
  subscribers_count: 5
  topics:
  - docker
  - singularity
  - hpc
  - pawsey
  - training-materials
  updated_at: 1650946836.0
PawseySC/pawsey-spack-config:
  data_format: 2
  description: Configuration files for Spack at Pawsey
  filenames:
  - setonix/environments/env_roms/spack.yaml
  - setonix/environments/env_benchmarking/spack.yaml
  - setonix/environments/env_apps/spack.yaml
  - setonix/environments/env_devel/spack.yaml
  - setonix/environments/env_s3_clients/spack.yaml
  - setonix/environments/env_io_libs/spack.yaml
  - setonix/environments/env_bio/spack.yaml
  - setonix/environments/env_langs/spack.yaml
  - setonix/environments/env_vis/spack.yaml
  - setonix/environments/env_astro/spack.yaml
  - setonix/environments/env_python/spack.yaml
  - setonix/environments/env_wrf/spack.yaml
  full_name: PawseySC/pawsey-spack-config
  latest_release: null
  readme: '<h1>

    <a id="user-content-pawsey-spack-config" class="anchor" href="#pawsey-spack-config"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>pawsey-spack-config</h1>

    <p>Configuration files for Spack at Pawsey.</p>

    <h2>

    <a id="user-content-setonix-setup" class="anchor" href="#setonix-setup" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Setonix setup</h2>

    <p>This can be found in the <code>setonix/</code> directory.<br>

    See <code>README.md</code> in there for further information.</p>

    <h2>

    <a id="user-content-other-setups" class="anchor" href="#other-setups" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Other setups</h2>

    <ul>

    <li>

    <code>joey/</code>: test deployment for the Setonix test system</li>

    <li>Current Pawsey systems

    <ul>

    <li><code>askapingest/</code></li>

    <li><code>garrawarla/</code></li>

    <li><code>magnus/</code></li>

    <li><code>topaz/</code></li>

    <li><code>zeus/</code></li>

    </ul>

    </li>

    <li>

    <code>examples/</code>: deployment examples and tests</li>

    <li>

    <code>deprecated/</code>: legacy deployments</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 10
  topics: []
  updated_at: 1641801068.0
PawseySC/performance-modelling-tools:
  data_format: 2
  description: This repository hosts configuration files for HPC Toolkit, ROCprof,
    NVprof and ERT, and scripts to help us create roofline and instruction based roofline
    diagrams (performance models) for applications
  filenames:
  - spack/mulan/spack.yaml
  full_name: PawseySC/performance-modelling-tools
  latest_release: null
  readme: '<h1>

    <a id="user-content-performance-modeling-tools" class="anchor" href="#performance-modeling-tools"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Performance
    Modeling Tools</h1>

    <p>This repository hosts configuration files and scripts to support performance
    modeling of research application on Pawsey Supercomputing Centre systems.</p>

    <p>We have provided</p>

    <ul>

    <li>

    <a href="./spack">Spack environments</a> that provide performance modeling toolchains
    for Pawsey systems</li>

    <li>

    <a href="./examples/">Examples</a> that illustrate how to use this repository
    for active research projects</li>

    <li>

    <a href="./bin/">Scripts</a> for processing profiler data to create graphics and
    other post-processed data to support performance modeling</li>

    </ul>

    <h2>

    <a id="user-content-about-this-repository" class="anchor" href="#about-this-repository"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>About
    this Repository</h2>

    <h3>

    <a id="user-content-scaffolding" class="anchor" href="#scaffolding" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Scaffolding</h3>

    <p>This repository is organized with the following top-level directories</p>

    <ul>

    <li>

    <code>bin/</code> : Contains scripts for post-processing profiler data</li>

    <li>

    <code>docs/</code> : Contains mkdocs documentation</li>

    <li>

    <code>examples/</code> : Contains example configurations for creating profile
    data and processing output with the tools in this repository.</li>

    <li>

    <code>spack/</code> : Contains spack environments for various Pawsey systems that
    provide hpc-toolkit and other useful tools.</li>

    </ul>

    <h2>

    <a id="user-content-getting-help" class="anchor" href="#getting-help" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Getting Help</h2>

    <p>Currently, you can request help by <a href="https://github.com/PawseySC/performance-modelling-tools/issues">submitting
    an issue</a></p>

    '
  stargazers_count: 1
  subscribers_count: 9
  topics: []
  updated_at: 1652665803.0
PawseySC/sc-tutorials:
  data_format: 2
  description: SC Tutorials
  filenames:
  - exercises/spack_containerize/spack.yaml
  full_name: PawseySC/sc-tutorials
  latest_release: null
  readme: '<h1>

    <a id="user-content-getting-started-with-containers-on-hpc" class="anchor" href="#getting-started-with-containers-on-hpc"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Getting
    Started with Containers on HPC</h1>

    <p>View this on the <a href="https://supercontainers.github.io/sc-tutorials/"
    rel="nofollow">Tutorial Homepage</a>.</p>

    <h2>

    <a id="user-content-ecp-supercontainers-tutorial-session" class="anchor" href="#ecp-supercontainers-tutorial-session"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>ECP
    Supercontainers Tutorial Session</h2>

    <p><a href="fig/ecp.jpg" target="_blank" rel="noopener noreferrer"><img src="fig/ecp.jpg"
    width="200" style="max-width:100%;"></a><a href="fig/pawsey.png" target="_blank"
    rel="noopener noreferrer"><img src="fig/pawsey.png" width="200" style="max-width:100%;"></a><a
    href="fig/redhat.png" target="_blank" rel="noopener noreferrer"><img src="fig/redhat.png"
    width="200" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-details" class="anchor" href="#details" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Details</h2>

    <p>Full-day Tutorial Session</p>

    <p>Venue: Supercomputing Conference (SC 21)</p>

    <p>Date: Monday, 15 November 2021 8am - 5pm Central Standard Time (GMT -6)</p>

    <p>Location: Virtual, St. Louis MO, USA</p>

    <p>Link: <a href="https://sc21.supercomputing.org/presentation/?id=tut114&amp;sess=sess185"
    rel="nofollow">SC 2021 Tutorial Details</a></p>

    <p>Keywords: Containerized HPC, System Software and Runtime Systems, Scientific
    Software Development, DevOps</p>

    <h2>

    <a id="user-content-ec2-login" class="anchor" href="#ec2-login" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>EC2 Login</h2>

    <p>These will be provided the day of the tutorial.</p>

    <h2>

    <a id="user-content-abstract" class="anchor" href="#abstract" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Abstract</h2>

    <p>Within just the past few years, the use of containers has revolutionized the
    way in which industries and enterprises have developed and deployed computational
    software and distributed systems. The containerization model has gained traction
    within the HPC community as well with the promise of improved reliability, reproducibility,
    portability, and levels of customization that were previously not possible on
    supercomputers. This adoption has been enabled by a number of HPC Container runtimes
    that have emerged including Singularity, Shifter, Enroot, Charliecloud and others.</p>

    <p>This hands-on tutorial looks to train users on the usability of containers
    on HPC resources. We will provide a detailed background on Linux containers, along
    with introductory hands-on experience building a container image, sharing the
    container and running it on a HPC cluster. Furthermore, the tutorial will provide
    more advanced information on how to run MPI-based and GPU-enabled HPC applications,
    how to optimize I/O intensive workflows, and how to setup GUI enabled interactive
    sessions. Cutting-edge examples will include machine learning and bioinformatics.
    Users will leave the tutorial with a solid foundational understanding of how to
    utilize containers with HPC resources through Shifter and Singularity, as well
    as an in-depth knowledge to deploy custom containers on their own resources.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>This is a hands-on tutorial.  Participants should bring a laptop and load or
    pre-install a terminal and/or ssh client in advance to make best use of time during
    the tutorial.  We will be providing training user accounts to both pre-configured
    EC2 instances.</p>

    <div><a href="fig/AWS_logo.png" target="_blank" rel="noopener noreferrer"><img
    src="fig/AWS_logo.png" width="250" style="max-width:100%;"></a></div>

    <p>This tutorial is supported by the Amazon AWS Machine Learning Research Awards.  EC2
    images and temporary login credentials will be distributed onsite at the tutorial.</p>

    <p>After the tutorial, you can boot our tutorial image yourself on Amazon EC2
    to run through the tutorial again. We recommend you use your own EC2 key and change
    the password.</p>

    <p>US-West-Oregon: ami-0fe12765123c6a840</p>

    <h3>

    <a id="user-content-optional-prerequisites" class="anchor" href="#optional-prerequisites"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Optional
    Prerequisites</h3>

    <p>Users can also install Docker and Singularity prior to attending the tutorial
    session.  Here, it may be beneficial to create Docker and Sylabs (Singularity)
    accounts in advance at <a href="https://cloud.docker.com/" rel="nofollow">https://cloud.docker.com/</a>
    and <a href="https://cloud.sylabs.io/" rel="nofollow">https://cloud.sylabs.io/</a>.  These
    accounts will be needed to create images on Docker Cloud/Dockerhub and Sylabs
    Cloud.</p>

    <p><a href="https://sylabs.io/guides/3.7/user-guide/" rel="nofollow">Install Singularity
    on Linux</a></p>

    <p><a href="https://repo.sylabs.io/desktop/" rel="nofollow">Install Singularity
    on Mac</a> (Alpha)</p>

    <p><a href="https://www.docker.com/products/docker-desktop" rel="nofollow">Install
    Docker for Desktop</a></p>

    <h2>

    <a id="user-content-questions" class="anchor" href="#questions" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Questions</h2>

    <p>You can ask questions verbally or with this <a href="https://docs.google.com/document/d/11gMZ-T7iA5XiRWPLYIqX7Gqv7RMb-NF9kzGYHrnOi04/edit?usp=sharing"
    rel="nofollow">Google Doc</a>.

    Please append your question below the others in the document.</p>

    <p>We have also created a Slack Team for this.  The invitation link is <a href="https://join.slack.com/t/hpc-containers/shared_invite/enQtODI3NzY1NDU4OTk5LTUxOTgyOWJmYjIwOWI5YWU2MzBhZDI3Zjc1YmZmMjAxZjgzYzk4ZWEwNmFlNzlkOWI0MGNlZDNlMTBhYTBlOWY"
    rel="nofollow">here</a>.</p>

    <h2>

    <a id="user-content-schedule" class="anchor" href="#schedule" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Schedule</h2>

    <p>8:00 - 8:20 Introduction and update on Linux containers - SLIDES (Shane)</p>

    <p>8:20 - 8:50 Building and running Docker containers (Shane)</p>

    <p>8:50 - 9:30 Advanced container builds (Eduardo)</p>

    <p>9:30 - 9:55 Container images best practices (Shane)</p>

    <p>9:55 - 10:00 Interactive Q &amp; A session</p>

    <p>10:00 - 10:30 MORNING BREAK</p>

    <p>10:30 - 10:50 HPC and containers - SLIDES (Shane)</p>

    <p>10:50 - 11:10 Installing a container engine - SLIDES (Marco)</p>

    <p>11:10 - 11:50 Running HPC jobs with containers (Marco)</p>

    <p>11:50 - 12:00 Interactive Q &amp; A session</p>

    <p>12:00 - 13:00 LUNCH BREAK</p>

    <p>13:00 - 13:30 Optional Q &amp; A session (including Slurm)</p>

    <p>13:30 - 14:20 Advanced HPC use cases (Marco)</p>

    <p>14:20 - 15:00 Container services and Kubernetes (multiple presenters)</p>

    <p>15:00 - 15:30 AFTERNOON BREAK</p>

    <p>15:30 - 16:20 Containers with E4S (Sameer)</p>

    <p>16:20 - 16:40 Success stories and use cases (Shane)</p>

    <p>16:40 - 17:00 Final Q &amp; A, wrap-up, and feedback survey</p>

    '
  stargazers_count: 1
  subscribers_count: 4
  topics: []
  updated_at: 1637641196.0
SC-SGS/CPPuddle:
  data_format: 2
  description: Utility library to handle small, reusable pools of both device memory
    buffers (via allocators) and device executors (with multiple scheduling policies).
  filenames:
  - spack.yaml
  full_name: SC-SGS/CPPuddle
  latest_release: v0.1.0
  readme: "<h3>\n<a id=\"user-content-cppuddle\" class=\"anchor\" href=\"#cppuddle\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>CPPuddle</h3>\n<p><a href=\"https://github.com/SC-SGS/CPPuddle/actions/workflows/cmake.yml\"\
    ><img src=\"https://github.com/SC-SGS/CPPuddle/actions/workflows/cmake.yml/badge.svg\"\
    \ alt=\"ctest\" style=\"max-width:100%;\"></a>\n<a href=\"https://simsgs.informatik.uni-stuttgart.de/jenkins/view/Octo-Tiger%20and%20Dependencies/job/CPPuddle/job/master/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/f85055028a87ff41032206704d36fede5e5cc779d3369a6650f02d9d46676a45/68747470733a2f2f73696d7367732e696e666f726d6174696b2e756e692d7374757474676172742e64652f6a656e6b696e732f6275696c645374617475732f69636f6e3f6a6f623d4350507564646c652532466d617374657226636f6e6669673d616c6c6275696c6473\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://simsgs.informatik.uni-stuttgart.de/jenkins/buildStatus/icon?job=CPPuddle%2Fmaster&amp;config=allbuilds\"\
    \ style=\"max-width:100%;\"></a></p>\n<h4>\n<a id=\"user-content-purpose\" class=\"\
    anchor\" href=\"#purpose\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Purpose</h4>\n<p>This repository was initially\
    \ created to explore how to best use HPX and Kokkos together!\nFor fine-grained\
    \ GPU tasks, we needed a way to avoid excessive allocations of one-usage GPU buffers\
    \ (as allocations block the device for all streams) and creation/deletion of GPU\
    \ executors (as those are usually tied to a stream which is expensive to create\
    \ as well).</p>\n<p>We currently test/use CPPuddle in <a href=\"https://github.com/STEllAR-GROUP/octotiger\"\
    >Octo-Tiger</a>, together with <a href=\"https://github.com/STEllAR-GROUP/hpx-kokkos\"\
    >HPX-Kokkos</a>.\nIn this use-case, allocating GPU buffers for all sub-grids in\
    \ advance would have wasted a lot of memory. On the other hand, unified memory\
    \ would have caused unnecessary GPU to CPU page migrations (as the old input data\
    \ gets overwritten anyway). Allocating buffers on-the-fly would have blocked the\
    \ device. Hence, we currently test this buffer management solution!</p>\n<h4>\n\
    <a id=\"user-content-tools-provided-by-this-repository\" class=\"anchor\" href=\"\
    #tools-provided-by-this-repository\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Tools provided by this repository</h4>\n\
    <ul>\n<li>Allocators that reuse previousely allocated buffers if available (works\
    \ with normal heap memory, pinned memory, aligned memory, CUDA/HIP device memory,\
    \ and Kokkos Views). Note that separate buffers do not coexist on a single chunk\
    \ of continuous memory, but use different allocations.</li>\n<li>Executor pools\
    \ and various scheduling policies (round robin, priority queue, multi-gpu), which\
    \ rely on reference counting to gauge the current load of a executor instead of\
    \ querying the device itself. Tested with CUDA, HIP and Kokkos executors provided\
    \ by HPX / HPX-Kokkos.</li>\n</ul>\n<h4>\n<a id=\"user-content-requirements\"\
    \ class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Requirements</h4>\n<ul>\n<li>C++14</li>\n\
    <li>CMake (&gt;= 3.11)</li>\n<li>Optional (for the header-only utilities / test):\
    \ CUDA, Boost, <a href=\"https://github.com/STEllAR-GROUP/hpx\">HPX</a>, <a href=\"\
    https://github.com/kokkos/kokkos\">Kokkos</a>, <a href=\"https://github.com/STEllAR-GROUP/hpx-kokkos\"\
    >HPX-Kokkos</a>\n</li>\n</ul>\n<p>The submodules can be used to obtain the optional\
    \ dependencies which are required for testing the header-only utilities. If these\
    \ tests are not required, the submodule (and the respective buildscripts in /scripts)\
    \ can be ignored safely.</p>\n<h4>\n<a id=\"user-content-build--install\" class=\"\
    anchor\" href=\"#build--install\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Build / Install</h4>\n<pre><code>\
    \  cmake -H/path/to/source -B$/path/to/build -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/path/to/install/cppuddle\
    \ -DCPPUDDLE_WITH_TESTS=OFF -DCPPUDDLE_WITH_COUNTERS=OFF                     \
    \                                        \n  cmake --build /path/to/build -- -j4\
    \ VERBOSE=1                                                                  \
    \                                                                            \
    \                                                            \n  cmake --build\
    \ /path/to/build --target install  \n</code></pre>\n<p>If installed correctly,\
    \ cppuddle can be used in other cmake-based projects via</p>\n<pre><code>find_package(CPPuddle\
    \ REQUIRED)\n</code></pre>\n"
  stargazers_count: 2
  subscribers_count: 4
  topics: []
  updated_at: 1652160432.0
SCOREC/rhel7-spack-config:
  data_format: 2
  description: rhel7 spack configuration and scripts
  filenames:
  - v0.15.4/spack.yaml
  full_name: SCOREC/rhel7-spack-config
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-setup-on-scorec\" class=\"anchor\" href=\"#setup-on-scorec\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>setup on SCOREC</h1>\n<pre><code>cd /opt/scorec/spack/rhel7-spack-config/\n\
    source setupSpack.sh\n</code></pre>\n<h1>\n<a id=\"user-content-rhel7-spack-config\"\
    \ class=\"anchor\" href=\"#rhel7-spack-config\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>rhel7-spack-config</h1>\n<p>rhel7\
    \ spack configuration and scripts</p>\n<p>The <code>install.sh</code> script maintained\
    \ in this repo is for documentation purposes (e.g., in case we had to reinstall\
    \ the entire stack from scratch) and should not be executed as it will not use\
    \ all of our existing package installs.  More discussion of package installation\
    \ is below.</p>\n<h2>\n<a id=\"user-content-useful-commands\" class=\"anchor\"\
    \ href=\"#useful-commands\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>useful commands</h2>\n<p>regenerate lmod module\
    \ tree:</p>\n<pre><code>spack module lmod refresh\n</code></pre>\n<h2>\n<a id=\"\
    user-content-installing-new-packages\" class=\"anchor\" href=\"#installing-new-packages\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>installing new packages</h2>\n<p>Our spack repo is tracking the master\
    \ spack branch.  Spack package updates could result in additional installation\
    \ of packages with little or no package source code changes.  These additional\
    \ installs can be avoided when installing new packages by first examining the\
    \ output of the <code>spack spec -I</code> command.  If a utility/infrastructure\
    \ level package, such as cmake or mpich, is marked with a <code>[+]</code> symbol\
    \ in the leftmost column then it means that the existing install will be used.\
    \  If spack does not default to using the existing install you can append the\
    \ hash of the package to the spec command.</p>\n<p>For example, lets see what\
    \ happens when we ask for a pumi install using gcc 7.3.0</p>\n<pre><code>$ spack\
    \ spec -I pumi@develop%gcc@7.3.0\nInput spec\n--------------------------------\n\
    \ -   pumi@develop%gcc@7.3.0\n\nConcretized\n--------------------------------\n\
    \ -   pumi@develop%gcc@7.3.0 build_type=RelWithDebInfo ~fortran~shared simmodsuite=none\
    \ ~zoltan arch=linux-rhel7-x86_64 \n[+]      ^cmake@3.13.1%gcc@7.3.0~doc+ncurses+openssl+ownlibs~qt\
    \ arch=linux-rhel7-x86_64 \n[+]          ^ncurses@6.1%gcc@7.3.0~symlinks~termlib\
    \ arch=linux-rhel7-x86_64 \n[+]              ^pkgconf@1.5.4%gcc@7.3.0 arch=linux-rhel7-x86_64\
    \ \n[+]          ^openssl@1.1.1%gcc@7.3.0+systemcerts arch=linux-rhel7-x86_64\
    \ \n[+]              ^perl@5.16.3%gcc@7.3.0+cpanm patches=0eac10ed90aeb0459ad8851f88081d439a4e41978e586ec743069e8b059370ac\
    \ +shared+threads arch=linux-rhel7-x86_64 \n[+]              ^zlib@1.2.11%gcc@7.3.0+optimize+pic+shared\
    \ arch=linux-rhel7-x86_64 \n -       ^mpich@3.3%gcc@7.3.0 device=ch3 +hydra netmod=tcp\
    \ +pmi+romio~verbs arch=linux-rhel7-x86_64 \n[+]          ^findutils@4.6.0%gcc@7.3.0\
    \ patches=84b916c0bf8c51b7e7b28417692f0ad3e7030d1f3c248ba77c42ede5c1c5d11e,bd9e4e5cc280f9753ae14956c4e4aa17fe7a210f55dd6c84aa60b12d106d47a2\
    \ arch=linux-rhel7-x86_64 \n[+]              ^autoconf@system%gcc@7.3.0 arch=linux-rhel7-x86_64\
    \ \n[+]              ^automake@system%gcc@7.3.0 arch=linux-rhel7-x86_64 \n[+]\
    \              ^libtool@system%gcc@7.3.0 arch=linux-rhel7-x86_64 \n[+]       \
    \       ^m4@1.4.16%gcc@7.3.0 patches=c0a408fbffb7255fcc75e26bd8edab116fc81d216bfd18b473668b7739a4158e\
    \ +sigsegv arch=linux-rhel7-x86_64 \n[+]              ^texinfo@6.5%gcc@7.3.0 arch=linux-rhel7-x86_64\n\
    </code></pre>\n<p>Spack wants to install mpich 3.3, but we don't want to change\
    \ to the new mpich version yet.  So, we will get the hash of the existing mpich\
    \ 3.2.1 install:</p>\n<pre><code>$ spack find -ldv mpich%gcc@7.3.0\n==&gt; 1 installed\
    \ package\n-- linux-rhel7-x86_64 / gcc@7.3.0 -------------------------------\n\
    niuhmad    mpich@3.2.1 device=ch3 +hydra netmod=tcp +pmi+romio~verbs\n</code></pre>\n\
    <p>then append the hash <code>niuhmad</code> to the spec for pumi using the <code>^</code>\
    \ syntax to specify it as a dependency:</p>\n<pre><code>$ spack spec -I pumi@develop%gcc@7.3.0\
    \ ^/niuhmad\nInput spec\n--------------------------------\n -   pumi@develop%gcc@7.3.0\n\
    [+]      ^mpich@3.2.1%gcc@7.3.0 device=ch3 +hydra netmod=tcp +pmi+romio~verbs\
    \ arch=linux-rhel7-x86_64 \n\nConcretized\n--------------------------------\n\
    \ -   pumi@develop%gcc@7.3.0 build_type=RelWithDebInfo ~fortran~shared simmodsuite=none\
    \ ~zoltan arch=linux-rhel7-x86_64 \n[+]      ^cmake@3.13.1%gcc@7.3.0~doc+ncurses+openssl+ownlibs~qt\
    \ arch=linux-rhel7-x86_64 \n[+]          ^ncurses@6.1%gcc@7.3.0~symlinks~termlib\
    \ arch=linux-rhel7-x86_64 \n[+]              ^pkgconf@1.5.4%gcc@7.3.0 arch=linux-rhel7-x86_64\
    \ \n[+]          ^openssl@1.1.1%gcc@7.3.0+systemcerts arch=linux-rhel7-x86_64\
    \ \n[+]              ^perl@5.16.3%gcc@7.3.0+cpanm patches=0eac10ed90aeb0459ad8851f88081d439a4e41978e586ec743069e8b059370ac\
    \ +shared+threads arch=linux-rhel7-x86_64 \n[+]              ^zlib@1.2.11%gcc@7.3.0+optimize+pic+shared\
    \ arch=linux-rhel7-x86_64 \n[+]      ^mpich@3.2.1%gcc@7.3.0 device=ch3 +hydra\
    \ netmod=tcp +pmi+romio~verbs arch=linux-rhel7-x86_64 \n</code></pre>\n<p>And\
    \ see that in the Concretized spec it is now using the existing mpich 3.2.1 install.</p>\n\
    <h2>\n<a id=\"user-content-contents\" class=\"anchor\" href=\"#contents\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>contents</h2>\n\
    <p>compilers.yaml - compiler list\nconfig.yaml - global config\ninstall.sh - package\
    \ installation commands\nmodules.yaml - hierarchical layout for lua modules\n\
    packages.yaml - system installed packages\nREADME.md - this file\nsetupSpack.sh\
    \ - env needed for executing spack commands</p>\n"
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1643231013.0
SouthernMethodistUniversity/msds_hpc:
  data_format: 2
  description: null
  filenames:
  - classes/03_2/spack.yaml
  - classes/04_2/spack_containers/spack.yaml
  full_name: SouthernMethodistUniversity/msds_hpc
  latest_release: null
  readme: '<h1>

    <a id="user-content-ds-7347-high-performance-computing-hpc-and-data-science" class="anchor"
    href="#ds-7347-high-performance-computing-hpc-and-data-science" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>DS 7347 High-Performance
    Computing (HPC) and Data Science</h1>

    <p>Welcome!</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1652826983.0
UO-OACISS/e4s:
  data_format: 2
  description: E4S Spack environments and container recipes
  filenames:
  - docker-recipes/archived/special/superlu-sc/spack.yaml
  - docker-recipes/ubuntu20.04-runner-ppc64le/spack.yaml
  - docker-recipes/ubuntu20.04-runner-x86_64/spack.yaml
  full_name: UO-OACISS/e4s
  latest_release: null
  readme: '<p>This is a collection of configurations for building ECP SDK

    containers with combinations of packages, including the full

    E4S set.</p>

    <p>These are the set of stacks that are targeted for the first release:</p>

    <p><a href="figures/SDKdefinition1.png" target="_blank" rel="noopener noreferrer"><img
    src="figures/SDKdefinition1.png" alt="SDK definitions" style="max-width:100%;"></a></p>

    <p>The configuration files for each container platform will be specified under
    each directory.  For example, the Docker configurations are under the "docker"
    subdirectory.  Each subdirectory will have a README.md file to explain how to
    build the container image for each stack.</p>

    '
  stargazers_count: 18
  subscribers_count: 6
  topics: []
  updated_at: 1644561389.0
akarmas/sample-ebrains-component:
  data_format: 2
  description: showcase how to mirror from github to EBRAINS Gitlab
  filenames:
  - .ebrains/spack/component-name_spack.yaml
  full_name: akarmas/sample-ebrains-component
  latest_release: v0.1
  readme: '<h1>

    <a id="user-content-sample-ebrains-component" class="anchor" href="#sample-ebrains-component"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>sample-ebrains-component</h1>

    <p>The project aims to showcase i) how to set up a mirror code repository from
    Github to

    EBRAINS Gitlab and ii) the necessary configurations to allow the automated update
    of

    the mirror on certain events.

    It can be used to facilitate the initial integration requirements that an EBRAINS
    component

    team has to fulfill.

    The steps that need to be followed to achieve this are detailed below and all
    the files in

    the present project can be used as an example and reference.

    Let''s assume that we want to mirror a code repository from Github (source_repo)
    to EBRAINS

    Gitlab (mirror=destination_repo).</p>

    <p>The goal is to set up the destination_repo and configure the source_repo to
    automatically

    update the destination_repo when certain events occur.

    In this example the event that triggers the automated update is a push event in
    the master branch.</p>

    <p>At the EBRAINS Gitlab perform the following steps:</p>

    <ol>

    <li>Create an empty project at gitlab.ebrains.eu (destination_repo_name)</li>

    <li>Create a gitlab service account on the new project (detailed documentation
    <a href="https://docs.gitlab.com/ee/user/project/settings/project_access_tokens.html"
    rel="nofollow">here</a>)

    From the left-side menu navigate to:<br>

    Settings &gt; Access tokens and<br>

    i) set the name variable of the service account (here, Name: ghpusher)<br>

    ii) set the expiration date of the project access token to be created (here, Expire
    Date: leave empty to never expire)<br>

    iii) select all the scopes<br>

    and then click the "Create project access token" button<br>

    The new project access token will be created and you need to save the new project
    access

    token because you will not be able to access it again (you will need the project
    access token

    later in the process of setting up the mirror)</li>

    <li>Then navigate again from the left-side menu to<br>

    Settings &gt; Repository &gt; Protected branches<br>

    and set "Allow force push" to On, for the branches you want to sync from the source_repo
    to

    the destination_repo (for this particular example only the master branch will
    be available)</li>

    </ol>

    <p>Then at Github perform the following steps:</p>

    <ol start="4">

    <li>Navigate to the source_repo that you want to mirror to EBRAINS</li>

    <li>Navigate from the horizontal menu to:<br>

    Settings &gt; Secrets &gt; New repository secret<br>

    i) Set the name of the secret (here EBRAINS_GITLAB_ACCESS_TOKEN)<br>

    ii) Set as the value of the secret the token that you created and saved at step
    2.<br>

    iii) Click the "Add secret" button</li>

    <li>Create the .github/workflows directories in the source_repo (detailed documentation
    <a href="https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions">here</a>)</li>

    <li>In the .github/workflows directory create a yml file (here ebrains.yml)

    and define the rules for synching the destination_repo with Github Actions</li>

    </ol>

    <p>Note that the file ebrains_explanation.yml aims to explain the ebrains.yml
    and

    facilitate the reader to use it as a template.</p>

    <h2>

    <a id="user-content-acknowledgments" class="anchor" href="#acknowledgments" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Acknowledgments</h2>

    <p>Credits to the Arbor team for initially implementing the flow (ebrains mirror
    <a href="https://gitlab.ebrains.eu/arbor-sim/arbor" rel="nofollow">here</a>).</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1633429913.0
akhilred36/gameOfLifeKokkos:
  data_format: 2
  description: Game of Life implementation in Kokkos.
  filenames:
  - spack.yaml
  full_name: akhilred36/gameOfLifeKokkos
  latest_release: null
  readme: '<h1>

    <a id="user-content-gameoflifekokkos" class="anchor" href="#gameoflifekokkos"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>gameOfLifeKokkos</h1>

    <p>Game of Life implementation in Kokkos with MPI.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <ul>

    <li>GCC&gt;=7.5.0</li>

    <li>CUDA&gt;=11.2.0</li>

    <li>Kokkos&gt;=3.5.00 with cuda and nvcc_wrapper</li>

    <li>OpenMPI&gt;=4.1.2 with cuda</li>

    <li>CMake&gt;=3.10</li>

    </ul>

    <h2>

    <a id="user-content-install-prerequisites-into-a-spack-environment" class="anchor"
    href="#install-prerequisites-into-a-spack-environment" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install prerequisites
    into a spack environment:</h2>

    <div class="highlight highlight-source-shell"><pre>spack env create <span class="pl-k">&lt;</span>name<span
    class="pl-k">&gt;</span> spack.yaml</pre></div>

    <div class="highlight highlight-source-shell"><pre>spack env activate <span class="pl-k">&lt;</span>name<span
    class="pl-k">&gt;</span></pre></div>

    <div class="highlight highlight-source-shell"><pre>spack install</pre></div>

    <h2>

    <a id="user-content-building" class="anchor" href="#building" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Building</h2>

    <h3>

    <a id="user-content-initial-setup" class="anchor" href="#initial-setup" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Initial setup:</h3>

    <div class="highlight highlight-source-shell"><pre>mkdir build <span class="pl-k">&amp;&amp;</span>
    mkdir data</pre></div>

    <h3>

    <a id="user-content-compile" class="anchor" href="#compile" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Compile:</h3>

    <div class="highlight highlight-source-shell"><pre>make clean <span class="pl-k">&amp;&amp;</span>
    make</pre></div>

    <h3>

    <a id="user-content-clean-generated-log-files" class="anchor" href="#clean-generated-log-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Clean
    generated log files:</h3>

    <div class="highlight highlight-source-shell"><pre>make cleandata</pre></div>

    <h2>

    <a id="user-content-running" class="anchor" href="#running" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running</h2>

    <p>Run the following in the build directory:</p>

    <h3>

    <a id="user-content-format" class="anchor" href="#format" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Format:</h3>

    <div class="highlight highlight-source-shell"><pre>mpirun -n <span class="pl-k">&lt;</span>numTasks<span
    class="pl-k">&gt;</span> ./gol <span class="pl-k">&lt;</span>meshSize<span class="pl-k">&gt;</span>
    <span class="pl-k">&lt;</span>numIterations<span class="pl-k">&gt;</span> <span
    class="pl-k">&lt;</span>coord1X coord1Y<span class="pl-k">&gt;</span> <span class="pl-k">&lt;</span>coord2X
    coord2Y<span class="pl-k">&gt;</span> .... <span class="pl-k">&lt;</span>coordNX
    coordNY<span class="pl-k">&gt;</span> <span class="pl-k">&lt;</span>print<span
    class="pl-k">&gt;</span></pre></div>

    <h3>

    <a id="user-content-example-runs" class="anchor" href="#example-runs" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Example runs:</h3>

    <p>4 tasks, mesh size 32, 100 iterations, (10, 10) (10, 11) (10, 13) as active
    cells, and log enabled:</p>

    <div class="highlight highlight-source-shell"><pre>mpirun -n 4 ./gol 32 100 10
    10 10 11 10 13 1</pre></div>

    <p>4 tasks, mesh size 32, 100 iterations, (10, 10) (10, 11) (10, 13) as active
    cells, and log disabled:</p>

    <div class="highlight highlight-source-shell"><pre>mpirun -n 4 ./gol 32 100 10
    10 10 11 10 13 0</pre></div>

    <h2>

    <a id="user-content-interpreting-logs" class="anchor" href="#interpreting-logs"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Interpreting
    logs:</h2>

    <p>The logs are written into the directory data/. The naming convention used is
    <code>&lt;rank&gt;_&lt;iter&gt;</code>. For example, to view the state of a mesh
    of rank 0 and iteration 12, run:</p>

    <div class="highlight highlight-source-shell"><pre>cat 0_12</pre></div>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1649796861.0
alexpacheco/spackenv:
  data_format: 2
  description: 'Spack Environments '
  filenames:
  - cent8/envs/avx/lusoft/spack.yaml
  - cent7/bioinformatics/spack.yaml
  - cent7/bioinformatics_default/spack.yaml
  - cent7/bio_old/spack.yaml
  - cent8/envs/avx2/lusoft/spack.yaml
  - cent7/libs_old/spack.yaml
  - cent7/library/spack.yaml
  - cent8/envs/solhawk/spack.yaml
  - cent7/py_376/spack.yaml
  - cent8/envs/avx512/python/spack.yaml
  - cent7/library/bak/spack.yaml
  - compilers/envs/compilers/spack.yaml
  - cent8/envs/avx512/rproject/spack.yaml
  - cent7/python_376/spack.yaml
  - cent8/envs/avx/rproject/spack.yaml
  - cent8/envs/avx512/lusoft/spack.yaml
  - cent7/mpis/spack.yaml
  - cent8/envs/x86_64/spack.yaml
  - cent8/envs/avx2/python/spack.yaml
  - cent7/apps/spack.yaml
  - cent7/ece_hpc/spack.yaml
  - cent8/envs/avx2/rproject/spack.yaml
  - cent8/envs/avx/python/spack.yaml
  full_name: alexpacheco/spackenv
  latest_release: null
  readme: '<h1>

    <a id="user-content-spack-environments" class="anchor" href="#spack-environments"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SPACK
    Environments</h1>

    <p>This repo contains the environment definitions to deploy site-software on Lehigh
    University''s Research Computing resources via SPACK environments.</p>

    <h2>

    <a id="user-content-software-deployment-for-centos-8x" class="anchor" href="#software-deployment-for-centos-8x"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Software
    deployment for CentOS 8.x</h2>

    <p>Software is deployed using two Spack installations.</p>

    <ol>

    <li>For compilers and module environments</li>

    <li>Site software for general use</li>

    </ol>

    <h3>

    <a id="user-content-compilers" class="anchor" href="#compilers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Compilers</h3>

    <p>This spack installation provides the gcc, nvhpc and cuda compilers, and lmod
    software for module management. In the future, this installation will also provide
    intel-oneapi compilers. For legacy reasons, intel@19.0.3 and intel@20.0.3 were
    installed in /share/Apps/intel with older intel compilers. This installation should
    not be used for deploying site software nor should the software provided be made
    available using the module environment.</p>

    <p>To reproduce installation</p>

    <pre><code>git clone https://github.com/alexpacheco/spackenv.git

    cd spackenv/compilers/envs/compilers

    spack env activate -d .

    spack concretize -f # optional

    spack install

    </code></pre>

    <p>The directory <code>etc/lmod</code> contains the LMOD configuration to switch
    between avx, avx2 and avx512 enabled <code>MODULEPATHS</code></p>

    <h3>

    <a id="user-content-lu-software" class="anchor" href="#lu-software" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>LU Software</h3>

    <p>This spack installation provides the deployed site-software on Sol and Hawk.</p>

    <p>To reproduce this installation, you need to first copy the site configuration
    files from <code>etc/spack</code> to your spack install tree. This assumes that
    SLURM and the compiler environment above is already installed. Edit the <code>packages.yaml</code>
    file to point to the location of slurm (/usr/local), rmda-core (/usr), gcc, intel,
    cuda, and nvhpc. The file <code>repo.yaml</code> is hardwired with  location of
    the lubio repository and should be changed to your location. The directory <code>templates</code>
    contains the template lua file for a few modules as defined in the <code>modules.yaml</code>
    file  and should be copied to the <code>etc</code> directory in your spack installation
    tree.</p>

    <p>On Sol, these files are available at <code>/share/Apps/lusoft/etc/spack</code>.</p>

    <h4>

    <a id="user-content-available-environments" class="anchor" href="#available-environments"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Available
    Environments</h4>

    <h5>

    <a id="user-content-solhawk" class="anchor" href="#solhawk" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>solhawk</h5>

    <p>This environment builds the entire software except the various python and r
    packages for ivybridge, haswell and skylake_avx512 architectures. This environment
    also builds the tcl environment modules that is not currently used. This should
    be build first and any new packages should be added to this environment.</p>

    <pre><code>cd spackenv/cent8/envs/solhawk

    spack env activate -d .

    spack concretize -f # optional

    spack install

    </code></pre>

    <h4>

    <a id="user-content-avxavx2avx512" class="anchor" href="#avxavx2avx512" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>avx/avx2/avx512</h4>

    <p>These environment builds the software stack except the various python and r
    packages for ivybridge/haswell/skylake_avx512 architectures. If software in the
    <code>solhawk</code> environment is already built, then these environments are
    only setting up the installation root for the LMOD module files <code>/share/Apps/lusoft/share/modules/lmod/{avx,avx2,avx512}</code>.
    The only reason these environments exist is due to SPACK''s inability to built
    a architecture based LMOD module tree similar to the TCL module tree.

    <em>Note</em>: If you change the path of the installation root, make sure that
    you change the corresponding path in <code>compilers/etc/SitePackage.lua</code>.</p>

    <pre><code>cd spackenv/cent8/envs/avx2/lusoft

    spack env activate -d .

    spack concretize -f # optional

    spack install

    </code></pre>

    <h4>

    <a id="user-content-python-and-r-packages" class="anchor" href="#python-and-r-packages"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Python
    and R packages</h4>

    <p>Rather than building module files for various python and r packages, a single
    module is created for a filesystem view of all python and r packages respectively.
    The path to the r filesystem is setup as <code>R_LIBS_SITE</code> so that any
    application such as <code>trinity</code> that requires many R packages only need
    to load the r module. If new packages added to the above environments require
    a dependent R package, then that dependency should be added to the rpoject environment
    and concretized. The python environment uses a <code>concretization: together</code>
    and may not provide the same python package as the above software environments.
    The filesystem views are hardwired as <code>/share/Apps/py_spack/3.8.6/{avx,avx2,avx512}</code>
    and <code>/share/Apps/r_spack/4.0.3/{avx,avx2,avx512}</code>.</p>

    <pre><code>cd spackenv/cent8/envs/avx/python

    spack env activate -d .

    spack concretize -f # optional

    spack install

    </code></pre>

    <pre><code>cd spackenv/cent8/envs/avx512/rproject

    spack env activate -d .

    spack concretize -f # optional

    spack install

    </code></pre>

    <h4>

    <a id="user-content-x86_64" class="anchor" href="#x86_64" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>x86_64</h4>

    <p>This environment builds unoptimized software such as anaconda python, gnu parallel,
    scree, tmux, etc for generic x86_64 processor.</p>

    <h2>

    <a id="user-content-centos-7x-software" class="anchor" href="#centos-7x-software"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>CentOS
    7.x software</h2>

    <p>This just collects the various environments for building software before the
    CentOS 8.x upgrade.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1629131122.0
aminaramoon/config:
  data_format: 2
  description: null
  filenames:
  - packages/spack.yaml
  full_name: aminaramoon/config
  latest_release: null
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1637295946.0
arcaneframework/containers:
  data_format: 2
  description: Containers with Arcane and Alien
  filenames:
  - spack/envs/dev-tools/spack.yaml
  - spack/envs/alien/spack.yaml
  - spack/envs/arcane/spack.yaml
  - spack/envs/all/spack.yaml
  full_name: arcaneframework/containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-containers" class="anchor" href="#containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>containers</h1>

    <p>Containers with Arcane and Alien</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1637878998.0
cayrols/internal_fiber:
  data_format: 2
  description: 'Purpose: PR'
  filenames:
  - .github/CI/spack.yaml
  full_name: cayrols/internal_fiber
  latest_release: null
  readme: "<p><a href=\"https://camo.githubusercontent.com/95208d9a920443b4cae72a2560512aad27c686017c663cb71aa8f434c86f0b08/68747470733a2f2f6269746275636b65742e6f72672f616179616c6133322f6c6f676f732f7261772f646530386466336333626664396435393535383762663834306633316166636234356436303139632f66696265722e706e67\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/95208d9a920443b4cae72a2560512aad27c686017c663cb71aa8f434c86f0b08/68747470733a2f2f6269746275636b65742e6f72672f616179616c6133322f6c6f676f732f7261772f646530386466336333626664396435393535383762663834306633316166636234356436303139632f66696265722e706e67\"\
    \ alt=\"FBI_banner\" data-canonical-src=\"https://bitbucket.org/aayala32/logos/raw/de08df3c3bfd9d595587bf840f31afcb45d6019c/fiber.png\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><strong>FFT Benchmarking Initiative</strong></p>\n\
    <p><strong>Innovative Computing Laboratory</strong></p>\n<p><strong>University\
    \ of Tennessee</strong></p>\n<hr>\n<h1>\n<a id=\"user-content-about\" class=\"\
    anchor\" href=\"#about\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>About</h1>\n<p>The FFT Infrastructure Benchmark\
    \ for Exascale Research (FIBER) provides a framework for Fast Fourier Transform\
    \ (FFT) benchmarks targeting exascale computing systems. It evaluates performance\
    \ and scalability of distributed FFTs on different architectures. Furthermore,\
    \ it analyzes the effect on applications that directly depend on FFTs. It can\
    \ also stress and test the overall network of a supercomputer, give an indication\
    \ on bisection bandwidth, noise, and other network and MPI collectives limitations\
    \ that are of interest to many other ECP applications.</p>\n<p>The current harness\
    \ software puts together FFT libraries supporting distributed 3-D complex-to-complex\
    \ and real-to-complex FFTs.</p>\n<hr>\n<h1>\n<a id=\"user-content-setting-up\"\
    \ class=\"anchor\" href=\"#setting-up\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Setting up</h1>\n<p>Create a\
    \ folder; e.g., <code>Benchmarks_FFT</code>, and install the FFT libraries to\
    \ benchmark; or load them as modules.</p>\n<pre><code>-- Benchmarks_FFT\n    \
    \    |-- heFFTe\n        |-- fftMPI\n        |-- AccFFT\n        |-- P3DFFT\n\
    \        |-- FFTE\n        |-- SWFFT\n        |-- 2DECOMP&amp;FFT\n        |--\
    \ nb3dFFT\n        |-- FFTW\n        |-- FFTW++\n</code></pre>\n<p>Current libraries\
    \ targeted by FIBER:</p>\n<ul>\n<li>\n<p>CPU support: <a href=\"https://lammps.github.io/fftmpi/\"\
    \ rel=\"nofollow\">fftMPI</a>, <a href=\"https://xgitlab.cels.anl.gov/hacc/SWFFT\"\
    \ rel=\"nofollow\">SWFFT</a>,\n<a href=\"https://github.com/sdsc/p3dfft.3\">P3DFFT</a>,\n\
    <a href=\"https://gitlab.jsc.fz-juelich.de/goebbert/nb3dfft\" rel=\"nofollow\"\
    >nb3dFFT</a>,\n<a href=\"http://www.2decomp.org/download.html\" rel=\"nofollow\"\
    >2DECOMP&amp;FFT</a>, <a href=\"http://www.fftw.org/\" rel=\"nofollow\">FFTW</a>,\
    \ <a href=\"fftwpp.sourceforge.net/\">FFTW++</a></p>\n</li>\n<li>\n<p>CPU-GPU\
    \ support: <a href=\"https://bitbucket.org/icl/heffte\" rel=\"nofollow\">heFFTe</a>,\
    \ <a href=\"https://github.com/amirgholami/accfft\">AccFFT</a>,   <a href=\"http://www.ffte.jp/\"\
    \ rel=\"nofollow\">FFTE</a></p>\n</li>\n</ul>\n<h1>\n<a id=\"user-content-compilation\"\
    \ class=\"anchor\" href=\"#compilation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Compilation</h1>\n<p>Next clone\
    \ this repository and create  build folder, and execute the <code>cmake</code>\
    \ commands.\nIn the following example, we install FIBER with heFFTe and fftMPI\
    \ backends:</p>\n<pre><code>mkdir build; cd $_\nbuild/\ncmake -DFIBER_FFT_LIB_DIRS=\"\
    /home/Benchmarks_FFT/fftmpi/src;/home/heffte/build/lib\"\n-DFIBER_FFT_INCLUDE_DIRS=\"\
    /home/Benchmarks_FFT/fftmpi/src;/home/heffte/build/include\"\n-DFIBER_ENABLE_HEFFTE=ON\
    \ -DFIBER_ENABLE_FFTMPI=ON\n-DMPI_DIR=/sw/openmpi/4.0.0/ .. \nmake -j\n</code></pre>\n\
    <p>List the <code>lib</code> and <code>include</code> folders of libraries to\
    \ test, respectively, in <code>FIBER_FFT_LIB_DIRS</code> and <code>FIBER_FFT_INCLUDE_DIRS</code>.</p>\n\
    <h1>\n<a id=\"user-content-testing-integration\" class=\"anchor\" href=\"#testing-integration\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Testing integration</h1>\n<p>Run tests as follows:</p>\n<pre><code>cd\
    \ build/benchmarks\nmpirun -n 2 ./test3D_CPU_C2C &lt;library&gt;\nmpirun -n 2\
    \ ./test3D_CPU_R2C &lt;library&gt;\n</code></pre>\n<p>If FIBER was build linked\
    \ to GPU enabled libraries:</p>\n<pre><code>cd build/benchmarks\nmpirun -n 2 ./test3D_GPU_C2C\
    \ &lt;gpu_library&gt;\nmpirun -n 2 ./test3D_GPU_R2C &lt;gpu_library&gt;\n</code></pre>\n\
    <h1>\n<a id=\"user-content-running-benchmarks\" class=\"anchor\" href=\"#running-benchmarks\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Running benchmarks</h1>\n<pre><code>cd build/benchmarks\nmpirun -n\
    \ $NUM_RANKS ./test3D_C2C -lib &lt;library&gt; -backend &lt;1D_backend&gt; -size\
    \ &lt;nx&gt; &lt;ny&gt; &lt;nz&gt; -pgrid &lt;p&gt; &lt;q&gt;\n</code></pre>\n\
    <p>where <code>library</code> has to be replaced by one of the nine available\
    \ libraries, provided user has it installed.\nOnce a parallel FFT library has\
    \ been correctly integrated to heFFTe, running these benchmarks should report\
    \ a correct validation output.</p>\n<h1>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h1>\n<ul>\n<li>Installation\
    \ and a Doxygen documentation will be available shortly.</li>\n</ul>\n<hr>\n<h1>\n\
    <a id=\"user-content-getting-help\" class=\"anchor\" href=\"#getting-help\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Getting\
    \ Help</h1>\n<p>For assistance with the FIBER project, email <em><a href=\"mailto:fiber@icl.utk.edu\"\
    >fiber@icl.utk.edu</a></em> or start a GitHub issue.</p>\n<p>Contributions are\
    \ very welcome, please create a pull request.</p>\n<h1>\n<a id=\"user-content-resources\"\
    \ class=\"anchor\" href=\"#resources\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Resources</h1>\n<ul>\n<li>Visit\
    \ the <a href=\"http://icl.utk.edu/fiber/\" rel=\"nofollow\">FIBER website</a>\
    \ for more information about the HeFFTe project.</li>\n<li>Visit the <a href=\"\
    https://exascaleproject.org\" rel=\"nofollow\">ECP website</a> to find out more\
    \ about the DOE Exascale Computing Initiative.</li>\n</ul>\n<hr>\n<h1>\n<a id=\"\
    user-content-acknowledgments\" class=\"anchor\" href=\"#acknowledgments\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Acknowledgments</h1>\n\
    <p>This research was supported by the United States Exascale Computing Project.</p>\n\
    <hr>\n<h1>\n<a id=\"user-content-license\" class=\"anchor\" href=\"#license\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>License</h1>\n<pre><code>Copyright (c) 2022, University of Tennessee\n\
    All rights reserved.\n\nRedistribution and use in source and binary forms, with\
    \ or without\nmodification, are permitted provided that the following conditions\
    \ are met:\n    * Redistributions of source code must retain the above copyright\n\
    \      notice, this list of conditions and the following disclaimer.\n    * Redistributions\
    \ in binary form must reproduce the above copyright\n      notice, this list of\
    \ conditions and the following disclaimer in the\n      documentation and/or other\
    \ materials provided with the distribution.\n    * Neither the name of the University\
    \ of Tennessee nor the\n      names of its contributors may be used to endorse\
    \ or promote products\n      derived from this software without specific prior\
    \ written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND\
    \ CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT\
    \ NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\
    \ PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL UNIVERSITY OF TENNESSEE\
    \ BE LIABLE FOR ANY\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\
    \ DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\
    \ SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\
    \ CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\
    \ OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\
    \ OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1645724748.0
charmoniumQ/astrophysics-project:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: charmoniumQ/astrophysics-project
  latest_release: null
  readme: '<h1>

    <a id="user-content-neural-network-superresolving-for-cosmological-simulations"
    class="anchor" href="#neural-network-superresolving-for-cosmological-simulations"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Neural
    Network Superresolving for Cosmological Simulations</h1>

    <p>In this repository, I attempt to reproduce the analysis of <a href="https://arxiv.org/pdf/2111.06393.pdf"
    rel="nofollow">Schaurecker et

    al. 2021</a> on Enzo data (they use Illustris).</p>

    <h1>

    <a id="user-content-to-reproduce" class="anchor" href="#to-reproduce" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>To reproduce</h1>

    <p>The code <code>main.py</code> is intended to be run locally. It sends commands
    to the

    remote. You will need to modify this with your site-specific parameters. It

    should be the only file you need to modify.</p>

    <p>To set up the remote machine (should be capable of Slurm):</p>

    <div class="highlight highlight-source-shell"><pre>remote$ <span class="pl-c"><span
    class="pl-c">#</span> Install Spack on the remote</span>

    remote$ git clone -c feature.manyFiles=true https://github.com/spack/spack.git


    remote$ <span class="pl-c"><span class="pl-c">#</span> Copy spack.lock to the
    remote</span>

    remote$ spack/bin/spack environment create main4 spack.lock

    remote$ spack/bin/spack environment activate main4

    remote$ spack/bin/spack concretize

    remote$ spack/bin/spack install


    remote$ <span class="pl-c"><span class="pl-c">#</span> Copy envirment.yaml ot
    the remote</span>

    remote$ spack/bin/spack activate main4

    remote$ conda install --name main3 --file environment,yaml


    remote$ <span class="pl-c"><span class="pl-c">#</span> Ensure that Slurm works</span>

    remote$ sbatch --help</pre></div>

    <p>To set up the local machine:</p>

    <div class="highlight highlight-source-shell"><pre>locla$ <span class="pl-c"><span
    class="pl-c">#</span> Install conda</span>

    locla$ <span class="pl-c"><span class="pl-c">#</span> Install conda environment</span>

    local$ conda install --name main3 --file environment,yaml</pre></div>

    <p>You will need to configure SSH keys to the remote.</p>

    <p>Then you should be to run <code>main.py</code>. <code>main.py</code> runs the
    entire workflow. It is

    smart about not running a certain step if the data already exists. It also

    hashes the input parameters in the filename of the data, so it is unlikely to

    return stale data.</p>

    <p>The end result will end up in <code>output</code>.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1650312591.0
cinemascienceworkflows/miniapp:
  data_format: 2
  description: Ascent-based miniapp workflows
  filenames:
  - inputs/spack/spack.yaml
  full_name: cinemascienceworkflows/miniapp
  latest_release: null
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1646254492.0
eugeneswalker/exawind-cacher:
  data_format: 2
  description: null
  filenames:
  - compiler-spack.yaml
  - exawind-spack.yaml
  full_name: eugeneswalker/exawind-cacher
  latest_release: null
  readme: '<h2>

    <a id="user-content-instructions" class="anchor" href="#instructions" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Instructions</h2>

    <ol>

    <li>Set the appropriate values in secrets.env.tpl</li>

    <li>Rename secrets.env.tpl -&gt; secrets.env</li>

    </ol>

    <p>...instructions to be completed later</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1626409254.0
eugeneswalker/exawind-containers:
  data_format: 2
  description: null
  filenames:
  - amr-wind-container/spack.yaml
  full_name: eugeneswalker/exawind-containers
  latest_release: null
  readme: '<h2>

    <a id="user-content-working-with-the-docker-image-ecpe4sexawindlatest" class="anchor"
    href="#working-with-the-docker-image-ecpe4sexawindlatest" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Working with the Docker
    image (ecpe4s/exawind:latest)</h2>

    <ol>

    <li>Build the Docker image</li>

    </ol>

    <pre><code>$&gt; ./build-docker-image.sh

    </code></pre>

    <ol start="2">

    <li>Launch a container from the image</li>

    </ol>

    <pre><code>$&gt; docker run -it --rm ecpe4s/exawind


    root@8df184bdac63:/# which naluX

    /opt/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/nalu-wind-master-zjlelnq6lbetgsvmpabyqe5krlwl43vq/bin/naluX


    root@8df184bdac63:/# which amr_wind

    /opt/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/amr-wind-main-ehzusqf26dxsz7tbjykhubyegyzvinkh/bin/amr_wind

    </code></pre>

    <h2>

    <a id="user-content-working-with-the-singularity-image-exawindsif" class="anchor"
    href="#working-with-the-singularity-image-exawindsif" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Working with the Singularity
    image (exawind.sif)</h2>

    <ol>

    <li>Build the Docker image:</li>

    </ol>

    <pre><code>$&gt; ./build-docker-image.sh

    </code></pre>

    <ol start="2">

    <li>Save the Docker image as a docker-archive</li>

    </ol>

    <pre><code>$&gt; docker save -o exawind.tar ecpe4s/exawind:latest

    </code></pre>

    <ol start="3">

    <li>Build the Singularity image:</li>

    </ol>

    <pre><code>$&gt; ./build-singularity-image.sh

    </code></pre>

    <ol start="4">

    <li>Run the Singularity image:</li>

    </ol>

    <pre><code>$&gt; ./exawind.sif


    Exawind Singularity&gt; which naluX

    /opt/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/nalu-wind-master-zjlelnq6lbetgsvmpabyqe5krlwl43vq/bin/naluX


    Exawind Singularity&gt; which amr_wind

    /opt/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/amr-wind-main-ehzusqf26dxsz7tbjykhubyegyzvinkh/bin/amr_wind

    </code></pre>

    <h2>

    <a id="user-content-run-selected-exawind-regression-tests" class="anchor" href="#run-selected-exawind-regression-tests"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    Selected ExaWind Regression Tests</h2>

    <ol>

    <li>

    <p>Launch a container using either the Docker or Singularity image (see above)</p>

    </li>

    <li>

    <p>Clone this repository in the newly launched container and run the tests (here
    illustrated with Singularity)</p>

    </li>

    </ol>

    <pre><code>Exawind Singularity&gt; git clone https://github.com/eugeneswalker/exawind-containers
    ~/exawind-containers

    Exawind Singularity&gt; cd ~/exawind-containers/demo



    Exawind Singularity&gt; ./run-nonIsoEdgeOpenJet.sh

    PASS: nonIsoEdgeOpenJet.......................     6.2260s 8.1315e-19 5.7732e-15



    Exawind Singularity&gt; ./run-nalu-wind-tests.sh

    PASS: ablHill3d_ii............................    10.3820s 8.1955e-16 3.6451e-11

    PASS: ablHill3d_ip............................    10.0905s 2.7485e-17 2.3703e-13

    ...



    Exawind Singularity&gt; ./run-amr-wind-tests.sh

    finished abl_bndry_output

    finished abl_godunov

    ...

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1626420401.0
eugeneswalker/facility-spack:
  data_format: 2
  description: null
  filenames:
  - crusher/develop/PrgEnv-cray/failures/spack.yaml
  - uo-containers/22.05/production/e4s-22.05-cuda-noextern.spack.yaml
  - uo-containers/22.05/archives/spack-full-clang.spack.yaml
  - arcticus/22.05/failures/spack.yaml
  - uo-containers/22.05/production/e4s-22.05-cuda-ppc64le.spack.yaml
  - arcticus/develop/spack.yaml
  - crusher/experimental/rocm-4.5.2/spack.yaml
  - crusher/develop/PrgEnv-cray/spack.yaml
  - crusher/22.02/PrgEnv-amd/spack.yaml
  - uo-containers/develop/spack.yaml
  - crusher/experimental/rocm-4.5.0/spack.yaml
  - crusher/develop/PrgEnv-cray/full-wrappers/spack.yaml
  - arcticus/22.02/spack.yaml
  - uo-containers/22.05/failed-oneapi/spack.yaml
  - arcticus/experimental/spack.yaml
  - perlmutter/develop/failures/spack.yaml
  - uo-containers/22.05/archives/spack-cpu-ppc64le.spack.yaml
  - crusher/develop/PrgEnv-gnu/spack.yaml
  - crusher/22.02/PrgEnv-gnu/spack.yaml
  - crusher/22.02/PrgEnv-cray/spack.yaml
  - uo-containers/22.05/archives/spack-cpu.spack.yaml
  - uo-containers/22.05/production/e4s-22.05-cuda-ppc64le-noextern.spack.yaml
  - uo-containers/22.05/archives/spack-cuda-external-ppc64le.spack.yaml
  - crusher/experimental/rocm-5.0.2/spack.yaml
  - uo-containers/22.05/archives/spack-cuda-external.spack.yaml
  - uo-containers/22.05/archives/spack-rocm.spack.yaml
  - crusher/develop/PrgEnv-cray/full-wrappers/failures/spack.yaml
  - uo-containers/22.05/production/save/e4s-22.05-oneapi.spack.yaml
  - uo-containers/22.05/production/e4s-22.05-rocm-noextern.spack.yaml
  - perlmutter/21.11/spack.yaml
  - uo-containers/22.05/production/e4s-22.05-cuda.spack.yaml
  - uo-containers/22.05/archives/spack-rocm-external.spack.yaml
  - uo-containers/22.05/production/e4s-22.05-oneapi.spack.yaml
  - uo-containers/22.05/production/e4s-22.05-rocm.spack.yaml
  - uo-containers/22.05/archives/spack-cuda-ppc64le.spack.yaml
  - uo-containers/22.05/archives/spack-cuda.spack.yaml
  - crusher/22.05/PrgEnv-gnu/failures/spack.yaml
  - arcticus/22.05/spack.yaml
  - perlmutter/develop/spack.yaml
  - applications/exago/container/spack.yaml
  - applications/exago/crusher/spack.yaml
  - crusher/experimental/rocm-5.1.0-sameer/spack.yaml
  - uo-containers/22.05/components/e4s-oneapi-builder.spack.yaml
  - uo-containers/22.05/components/save/e4s-oneapi-builder.spack.yaml
  - crusher/experimental/rocm-5.1.0-develop/spack.yaml
  - crusher/22.02/PrgEnv-cray/base/spack.yaml
  - arcticus/experimental/failures/spack.yaml
  - crusher/develop/PrgEnv-cray/base/spack.yaml
  - arcticus/develop/failures-only/spack.yaml
  full_name: eugeneswalker/facility-spack
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1647987034.0
floquet/builds:
  data_format: 2
  description: Notes and scripts for building applications on HPCs
  filenames:
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/shell-scripts/2022-03-24_12,17/yamls/share/spack/gitlab/cloud_pipelines/stacks/tutorial/spack.yaml
  - results-spack/echo/Macmini8,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-20_11,45/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - results-spack/linux-centos7-haswell/Macmini8,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-03_10,10/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml
  - results-spack/echo/MacBookPro16,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-13_18,23/yamls/share/spack/gitlab/cloud_pipelines/stacks/build_systems/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/shell-scripts/2022-03-24_12,17/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-mac/spack.yaml
  - results-spack/echo/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/dantopa/2022-03-20_11,43/yamls/share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml
  - results-spack/linux-centos7-haswell/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-07_16,22/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-13_18,08/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/shell-scripts/2022-03-24_12,17/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-26_00,09/yamls/share/spack/gitlab/cloud_pipelines/stacks/build_systems/spack.yaml
  - results-spack/linux-centos7-haswell/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-07_16,22/yamls/share/spack/gitlab/cloud_pipelines/stacks/radiuss/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-13_18,08/yamls/share/spack/gitlab/cloud_pipelines/stacks/build_systems/spack.yaml
  - results-spack/echo/Macmini8,1-(xiuhcoatl)/linux/centos-7.9.2009/build-logs/2022-03-20_18,24/yamls/share/spack/gitlab/cloud_pipelines/stacks/radiuss/spack.yaml
  - results-spack/linux-centos7-haswell/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-07_16,22/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-13_18,08/yamls/share/spack/gitlab/cloud_pipelines/stacks/radiuss/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-13_18,08/yamls/share/spack/gitlab/cloud_pipelines/stacks/tutorial/spack.yaml
  - results-spack/echo/Macmini8,1-(xiuhcoatl)/linux/centos-7.9.2009/build-logs/2022-03-20_18,24/yamls/share/spack/gitlab/cloud_pipelines/stacks/tutorial/spack.yaml
  - results-spack/echo/MacBookPro16,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-13_18,23/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml
  - results-spack/linux-centos7-haswell/Macmini8,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-03_10,10/yamls/share/spack/gitlab/cloud_pipelines/stacks/tutorial/spack.yaml
  - results-spack/echo/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-25_10,39/yamls/share/spack/gitlab/cloud_pipelines/stacks/build_systems/spack.yaml
  - results-spack/echo/Macmini8,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-20_11,45/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml
  - results-spack/echo/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-25_10,39/yamls/share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-26_00,09/yamls/share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml
  - results-spack/echo/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-25_10,39/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/shell-scripts/2022-03-24_12,17/yamls/share/spack/gitlab/cloud_pipelines/stacks/radiuss/spack.yaml
  - results-spack/echo/Macmini8,1-(xiuhcoatl)/linux/centos-7.9.2009/build-logs/2022-03-20_18,24/yamls/share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml
  - results-spack/linux-centos7-haswell/Macmini8,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-03_10,10/yamls/share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml
  - results-spack/echo/Macmini8,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-20_11,45/yamls/share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-26_00,09/yamls/share/spack/gitlab/cloud_pipelines/stacks/tutorial/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-26_00,09/yamls/share/spack/gitlab/cloud_pipelines/stacks/radiuss/spack.yaml
  - results-spack/echo/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/dantopa/2022-03-20_11,43/yamls/share/spack/gitlab/cloud_pipelines/stacks/tutorial/spack.yaml
  - results-spack/echo/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/dantopa/2022-03-20_11,43/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - results-spack/linux-centos7-haswell/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-07_16,22/yamls/share/spack/gitlab/cloud_pipelines/stacks/tutorial/spack.yaml
  - results-spack/linux-centos7-haswell/Macmini8,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-03_10,10/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - results-spack/echo/Macmini8,1-(xiuhcoatl)/linux/centos-7.9.2009/build-logs/2022-03-20_18,24/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - results-spack/darwin-monterey-skylake/MacBookPro16,1-(ehecoatl)/darwin-21.3.0/Monterey-12.2.1/ehecoatl-internal-spack/2022-03-07_16,29/yamls/share/spack/gitlab/cloud_pipelines/stacks/build_systems/spack.yaml
  - results-spack/echo/MacBookPro16,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-13_18,23/yamls/share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml
  - results-spack/echo/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-25_10,39/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-mac/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-26_00,09/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-13_18,08/yamls/share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml
  - results-spack/darwin-monterey-skylake/MacBookPro16,1-(ehecoatl)/darwin-21.3.0/Monterey-12.2.1/ehecoatl-internal-spack/2022-03-07_16,29/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - results-spack/echo/Macmini8,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-20_11,45/yamls/share/spack/gitlab/cloud_pipelines/stacks/radiuss/spack.yaml
  - results-spack/echo/MacBookPro16,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-13_18,23/yamls/share/spack/gitlab/cloud_pipelines/stacks/tutorial/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-26_00,09/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-13_18,08/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml
  - results-spack/echo/MacBookPro16,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-13_18,23/yamls/share/spack/gitlab/cloud_pipelines/stacks/radiuss/spack.yaml
  - results-spack/echo/Macmini8,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-20_11,45/yamls/share/spack/gitlab/cloud_pipelines/stacks/tutorial/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/shell-scripts/2022-03-24_12,17/yamls/share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml
  - results-spack/linux-centos7-haswell/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-07_16,22/yamls/share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml
  - results-spack/linux-centos7-haswell/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-07_16,22/yamls/share/spack/gitlab/cloud_pipelines/stacks/build_systems/spack.yaml
  - results-spack/echo/MacBookPro16,1-(quaxolotl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-26_00,09/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-mac/spack.yaml
  - results-spack/echo/MacBookPro16,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-13_18,23/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - results-spack/echo/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-25_10,39/yamls/share/spack/gitlab/cloud_pipelines/stacks/radiuss/spack.yaml
  - results-spack/echo/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-25_10,39/yamls/share/spack/gitlab/cloud_pipelines/stacks/tutorial/spack.yaml
  - results-spack/linux-centos7-haswell/Macmini8,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-03_10,10/yamls/share/spack/gitlab/cloud_pipelines/stacks/radiuss/spack.yaml
  - results-spack/echo/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/dantopa/2022-03-20_11,43/yamls/share/spack/gitlab/cloud_pipelines/stacks/radiuss/spack.yaml
  - results-spack/echo/MacBookPro16,1-(ehecoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-25_10,39/yamls/share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - results-spack/linux-centos7-haswell/Macmini8,1-(xiuhcoatl)/linux/centos-7.9.2009/centos-7.9.2009-dantopa-docker-spack/2022-03-03_10,10/yamls/share/spack/gitlab/cloud_pipelines/stacks/build_systems/spack.yaml
  full_name: floquet/builds
  latest_release: null
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1641760136.0
gyselax/gyselalibxx:
  data_format: 2
  description: Gyselalib++ is a collection of C++ components for writing gyrokinetic
    semi-lagrangian codes and similar
  filenames:
  - spack.yaml
  full_name: gyselax/gyselalibxx
  latest_release: null
  readme: '<h1>

    <a id="user-content-gyselalib" class="anchor" href="#gyselalib" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Gyselalib++</h1>

    <p>Gyselalib++ is a collection of C++ components for writing gyrokinetic semi-lagrangian
    codes and

    similar as well as a collection of such codes.</p>

    <h2>

    <a id="user-content-compilation" class="anchor" href="#compilation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Compilation</h2>

    <p>to compile voice++:</p>

    <pre><code>git clone --recurse-submodules git@gitlab.maisondelasimulation.fr:gysela-developpers/voicexx.git

    cd voicexx

    mkdir build

    cd build

    cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS="-Wall -Wno-sign-compare" ..

    make

    </code></pre>

    <h2>

    <a id="user-content-execution" class="anchor" href="#execution" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Execution</h2>

    <p>to run the tests:</p>

    <pre><code>ctest --output-on-failure

    </code></pre>

    <p>Then, just have a look at <code>tests/landau/growthrate_t0.0to45.0.png</code>:</p>

    <p><a href="https://camo.githubusercontent.com/b767f6df1712f338f85a7b0b813f7452888524845e8a67bf6223a02a8ca6dc83/68747470733a2f2f6769746c61622e6d6169736f6e64656c6173696d756c6174696f6e2e66722f677973656c612d646576656c6f70706572732f766f69636578782f2d2f6a6f62732f6172746966616374732f6d61696e2f7261772f6275696c642f74657374732f6c616e6461752f6666742f67726f777468726174655f74302e30746f34352e302e706e673f6a6f623d636d616b655f74657374735f52656c65617365"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/b767f6df1712f338f85a7b0b813f7452888524845e8a67bf6223a02a8ca6dc83/68747470733a2f2f6769746c61622e6d6169736f6e64656c6173696d756c6174696f6e2e66722f677973656c612d646576656c6f70706572732f766f69636578782f2d2f6a6f62732f6172746966616374732f6d61696e2f7261772f6275696c642f74657374732f6c616e6461752f6666742f67726f777468726174655f74302e30746f34352e302e706e673f6a6f623d636d616b655f74657374735f52656c65617365"
    alt="tests/landau/fft/growthrate_t0.0to45.0.png" title="Landau damping rate" data-canonical-src="https://gitlab.maisondelasimulation.fr/gysela-developpers/voicexx/-/jobs/artifacts/main/raw/build/tests/landau/fft/growthrate_t0.0to45.0.png?job=cmake_tests_Release"
    style="max-width:100%;"></a></p>

    <p>and <code>tests/landau/frequency_t0.0to45.0.png</code>:</p>

    <p><a href="https://camo.githubusercontent.com/4aa67726f68146816ee08dc5994ec66f3ac47f1de0f4ef1693d513c69ff71aee/68747470733a2f2f6769746c61622e6d6169736f6e64656c6173696d756c6174696f6e2e66722f677973656c612d646576656c6f70706572732f766f69636578782f2d2f6a6f62732f6172746966616374732f6d61696e2f7261772f6275696c642f74657374732f6c616e6461752f6666742f6672657175656e63795f74302e30746f34352e302e706e673f6a6f623d636d616b655f74657374735f52656c65617365"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/4aa67726f68146816ee08dc5994ec66f3ac47f1de0f4ef1693d513c69ff71aee/68747470733a2f2f6769746c61622e6d6169736f6e64656c6173696d756c6174696f6e2e66722f677973656c612d646576656c6f70706572732f766f69636578782f2d2f6a6f62732f6172746966616374732f6d61696e2f7261772f6275696c642f74657374732f6c616e6461752f6666742f6672657175656e63795f74302e30746f34352e302e706e673f6a6f623d636d616b655f74657374735f52656c65617365"
    alt="tests/landau/fft/frequency_t0.0to45.0.png" title="Landau damping frequency"
    data-canonical-src="https://gitlab.maisondelasimulation.fr/gysela-developpers/voicexx/-/jobs/artifacts/main/raw/build/tests/landau/fft/frequency_t0.0to45.0.png?job=cmake_tests_Release"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h2>

    <p>To install dependencies through spack, first follow the the 3 first steps of

    <a href="https://github.com/pdidev/spack">https://github.com/pdidev/spack</a></p>

    <p>Then execute the following:</p>

    <div class="highlight highlight-source-shell"><pre>spack env create voice spack.yaml

    spack env activate voice

    spack concretize --reuse

    spack install</pre></div>

    <p>For example, you can find a Dockerfile installing these dependencies on ubuntu
    in

    <code>voicexx_env/Dockerfile</code>.</p>

    '
  stargazers_count: 3
  subscribers_count: 1
  topics:
  - hpc
  - numerical-simulation
  - gyrokinetic
  - poisson-solver
  - vlasov-solver
  - plasma-physics
  - ddc
  updated_at: 1646130547.0
haampie/software-stack-manali:
  data_format: 2
  description: fast spack builds on slow filesystem
  filenames:
  - pkgs-gcc/spack.yaml
  - nvhpc/spack.yaml
  - gcc/spack.yaml
  - pkgs-nvhpc/spack.yaml
  full_name: haampie/software-stack-manali
  latest_release: null
  readme: "<p>Bootstrap GCC and NVHPC, and build an HPC software stack based on OpenMPI,\
    \ with a few\nunique features:</p>\n<ol>\n<li>parallel package builds with single\
    \ jobserver for all builds;</li>\n<li>building on a fast filesystem, targeting\
    \ a slower filesystem, without worrying\nabout relocation issues.</li>\n</ol>\n\
    <p><strong>Requirements</strong>:</p>\n<ul>\n<li><code>spack</code></li>\n<li>\n\
    <code>bwrap</code> (optionally)</li>\n</ul>\n<p><strong>Usage</strong>:</p>\n\
    <ol>\n<li>Copy <code>Make.user.example</code> to <code>Make.user</code> and possibly\
    \ change some variables.</li>\n<li>Run <code>make -j$(nproc)</code>\n</li>\n</ol>\n\
    <p>This outputs two files:</p>\n<ul>\n<li>\n<code>store.tar.zst</code>: a ZStandard\
    \ compressed tarball of the software stack;</li>\n<li>\n<code>store.squashfs</code>:\
    \ a SquashFS file of the software stack.</li>\n</ul>\n<p>Installing the software\
    \ stack can be done by extracting the tarball or mounting the\nSquashFS file at\
    \ the <code>STORE</code> location.</p>\n<p>A few variables can be set in <code>Make.user</code>:</p>\n\
    <ul>\n<li>\n<code>STORE</code>: where to install packages;</li>\n<li>\n<code>SPACK</code>:\
    \ what <code>spack</code> to use;</li>\n<li>\n<code>SPACK_INSTALL_FLAGS</code>:\
    \ specify more install flags, like <code>--verbose</code>.</li>\n</ul>\n<p>To\
    \ build packages in parallel with nice output, use <code>-O</code> (requires GNU\
    \ make &gt;= 4.3):</p>\n<div class=\"highlight highlight-text-shell-session\"\
    ><pre><span class=\"pl-c1\">make -j&lt;N&gt; -Orecurse</span></pre></div>\n<p>To\
    \ build on a fast filesystem, use <code>bwrap</code>, for example:</p>\n<div class=\"\
    highlight highlight-text-shell-session\"><pre><span class=\"pl-c1\">./bwrap.sh\
    \ make -j&lt;N&gt; -Orecurse</span></pre></div>\n<p>This allows you to map the\
    \ directory <code>/dev/shm/$(STORE) -&gt; $(STORE)</code>, so that the Spack\n\
    install directory is fast. Note that <code>bwrap.sh</code> has some hard-coded\
    \ paths you need to\nchange.</p>\n<p><strong>Generating modules</strong></p>\n\
    <p>There's no <code>modules.yaml</code> file right now, but generating modules\
    \ goes along those lines:</p>\n<div class=\"highlight highlight-source-yaml\"\
    ><pre><span class=\"pl-ent\">modules</span>:\n  <span class=\"pl-s\"><span class=\"\
    pl-pds\">'</span><span class=\"pl-ent\">default:</span><span class=\"pl-pds\"\
    >'</span></span>:\n    <span class=\"pl-ent\">arch_folder</span>: <span class=\"\
    pl-c1\">false</span>\n    <span class=\"pl-ent\">roots</span>:\n      <span class=\"\
    pl-ent\">tcl</span>: <span class=\"pl-s\">/path/to/tcl/modules</span>\n    <span\
    \ class=\"pl-ent\">enable</span>:\n    - <span class=\"pl-s\">tcl</span>\n   \
    \ <span class=\"pl-ent\">tcl</span>:\n      <span class=\"pl-ent\">projections</span>:\n\
    \        <span class=\"pl-ent\">all</span>: <span class=\"pl-s\"><span class=\"\
    pl-pds\">'</span>{name}/{version}-{compiler.name}-{compiler.version}<span class=\"\
    pl-pds\">'</span></span>\n      <span class=\"pl-ent\">all</span>:\n        <span\
    \ class=\"pl-ent\">autoload</span>: <span class=\"pl-s\">none</span>\n       \
    \ <span class=\"pl-ent\">filter</span>:\n          <span class=\"pl-ent\">environment_blacklist</span>:\
    \ <span class=\"pl-s\">['LD_LIBRARY_PATH', 'LIBRARY_PATH', 'CPATH']</span></pre></div>\n\
    <div class=\"highlight highlight-text-shell-session\"><pre><span class=\"pl-c1\"\
    >spack module tcl refresh</span>\n<span class=\"pl-c1\">spack module tcl setdefault\
    \ gcc@11</span></pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1652256031.0
haampie/spack-pgo-lto-environment:
  data_format: 2
  description: Enable PGO and LTO in Spack software stacks
  filenames:
  - spack.yaml
  full_name: haampie/spack-pgo-lto-environment
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1653473825.0
hepnos/HEPnOS:
  data_format: 2
  description: HEPnOS is a distributed object store for high energy physics applications,
    developed at Argonne National Laboratory.
  filenames:
  - spack.yaml
  full_name: hepnos/HEPnOS
  latest_release: v0.6.4
  readme: '<h1>

    <a id="user-content-hepnos" class="anchor" href="#hepnos" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HEPnOS</h1>

    <p>HEPnOS is the <em>High-Energy Physics''s new Object Store</em>, a distributed
    storage

    system specially designed for HEP experiments and workflows for the FermiLab.

    HEPnOS relies on libraries developed at Argonne National Laboratory within the

    context of the Mochi project (ANL, CMU, LANL, HDF Group).</p>

    <p>For information on copyright and licensing, see the COPYRIGHT file.

    For information on how to use, see the <a href="https://xgitlab.cels.anl.gov/sds/HEPnOS/wikis/home"
    rel="nofollow">wiki</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1641296454.0
hepnos/HEPnOS-Dataloader:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: hepnos/HEPnOS-Dataloader
  latest_release: v0.5.2
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1643644851.0
hepnos/HEPnOS-PEP-Benchmark:
  data_format: 2
  description: Benchmark exercizing the ParallelEventProcessor feature of HEPnOS.
  filenames:
  - spack.yaml
  full_name: hepnos/HEPnOS-PEP-Benchmark
  latest_release: v0.6
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1643644897.0
hppritcha/spack_ompix:
  data_format: 2
  description: null
  filenames:
  - gnu_master_x86_64/spack.yaml
  - gnu_release_x86_64/spack.yaml
  - intel_master_x86_64/spack.yaml
  full_name: hppritcha/spack_ompix
  latest_release: null
  readme: '<p>Project for using Gitlab CI to test spack builds of Open MPI master
    and release tarballs.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1640037910.0
icl-utk-edu/fiber:
  data_format: 2
  description: null
  filenames:
  - .github/CI/spack.yaml
  full_name: icl-utk-edu/fiber
  latest_release: null
  readme: "<p><a href=\"https://camo.githubusercontent.com/95208d9a920443b4cae72a2560512aad27c686017c663cb71aa8f434c86f0b08/68747470733a2f2f6269746275636b65742e6f72672f616179616c6133322f6c6f676f732f7261772f646530386466336333626664396435393535383762663834306633316166636234356436303139632f66696265722e706e67\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/95208d9a920443b4cae72a2560512aad27c686017c663cb71aa8f434c86f0b08/68747470733a2f2f6269746275636b65742e6f72672f616179616c6133322f6c6f676f732f7261772f646530386466336333626664396435393535383762663834306633316166636234356436303139632f66696265722e706e67\"\
    \ alt=\"FBI_banner\" data-canonical-src=\"https://bitbucket.org/aayala32/logos/raw/de08df3c3bfd9d595587bf840f31afcb45d6019c/fiber.png\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><strong>FFT Benchmarking Initiative</strong></p>\n\
    <p><strong>Innovative Computing Laboratory</strong></p>\n<p><strong>University\
    \ of Tennessee</strong></p>\n<hr>\n<h1>\n<a id=\"user-content-about\" class=\"\
    anchor\" href=\"#about\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>About</h1>\n<p>The FFT Infrastructure Benchmark\
    \ for Exascale Research (FIBER) provides a framework for Fast Fourier Transform\
    \ (FFT) benchmarks targeting exascale computing systems. It evaluates performance\
    \ and scalability of distributed FFTs on different architectures. Furthermore,\
    \ it analyzes the effect on applications that directly depend on FFTs. It can\
    \ also stress and test the overall network of a supercomputer, give an indication\
    \ on bisection bandwidth, noise, and other network and MPI collectives limitations\
    \ that are of interest to many other ECP applications.</p>\n<p>The current harness\
    \ software puts together FFT libraries supporting distributed 3-D complex-to-complex\
    \ and real-to-complex FFTs.</p>\n<hr>\n<h1>\n<a id=\"user-content-publications\"\
    \ class=\"anchor\" href=\"#publications\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Publications</h1>\n<ul>\n<li><a\
    \ href=\"http://www.icl.utk.edu/publications/interim-report-benchmarking-fft-libraries-high-performance-systems\"\
    \ rel=\"nofollow\">Interim Report on Benchmarking FFT Libraries on High Performance\
    \ Systems</a></li>\n<li><a href=\"http://www.icl.utk.edu/publications/fft-benchmark-performance-experiments-systems-targeting-exascale\"\
    \ rel=\"nofollow\">FFT Benchmark Performance Experiments on Systems Targeting\
    \ Exascale</a></li>\n</ul>\n<hr>\n<h1>\n<a id=\"user-content-setting-up\" class=\"\
    anchor\" href=\"#setting-up\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Setting up</h1>\n<p>Create a folder;\
    \ e.g., <code>Benchmarks_FFT</code>, and install the FFT libraries to benchmark;\
    \ or load them as modules.</p>\n<pre><code>-- Benchmarks_FFT\n        |-- heFFTe\n\
    \        |-- fftMPI\n        |-- AccFFT\n        |-- P3DFFT\n        |-- FFTE\n\
    \        |-- SWFFT\n        |-- 2DECOMP&amp;FFT\n        |-- nb3dFFT\n       \
    \ |-- FFTW\n        |-- FFTW++\n</code></pre>\n<p>Current libraries targeted by\
    \ FIBER:</p>\n<ul>\n<li>\n<p>CPU support: <a href=\"https://lammps.github.io/fftmpi/\"\
    \ rel=\"nofollow\">fftMPI</a>, <a href=\"https://xgitlab.cels.anl.gov/hacc/SWFFT\"\
    \ rel=\"nofollow\">SWFFT</a>,\n<a href=\"https://github.com/sdsc/p3dfft.3\">P3DFFT</a>,\n\
    <a href=\"https://gitlab.jsc.fz-juelich.de/goebbert/nb3dfft\" rel=\"nofollow\"\
    >nb3dFFT</a>,\n<a href=\"http://www.2decomp.org/download.html\" rel=\"nofollow\"\
    >2DECOMP&amp;FFT</a>, <a href=\"http://www.fftw.org/\" rel=\"nofollow\">FFTW</a>,\
    \ <a href=\"fftwpp.sourceforge.net/\">FFTW++</a></p>\n</li>\n<li>\n<p>CPU-GPU\
    \ support: <a href=\"https://bitbucket.org/icl/heffte\" rel=\"nofollow\">heFFTe</a>,\
    \ <a href=\"https://github.com/amirgholami/accfft\">AccFFT</a>,   <a href=\"http://www.ffte.jp/\"\
    \ rel=\"nofollow\">FFTE</a></p>\n</li>\n</ul>\n<h1>\n<a id=\"user-content-compilation\"\
    \ class=\"anchor\" href=\"#compilation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Compilation</h1>\n<p>Next clone\
    \ this repository and create  build folder, and execute the <code>cmake</code>\
    \ commands.\nIn the following example, we install FIBER with heFFTe and fftMPI\
    \ backends:</p>\n<pre><code>mkdir build; cd $_\nbuild/\ncmake -DFIBER_FFT_LIB_DIRS=\"\
    /home/Benchmarks_FFT/fftmpi/src;/home/heffte/build/lib\"\n-DFIBER_FFT_INCLUDE_DIRS=\"\
    /home/Benchmarks_FFT/fftmpi/src;/home/heffte/build/include\"\n-DFIBER_ENABLE_HEFFTE=ON\
    \ -DFIBER_ENABLE_FFTMPI=ON\n-DMPI_DIR=/sw/openmpi/4.0.0/ .. \nmake -j\n</code></pre>\n\
    <p>List the <code>lib</code> and <code>include</code> folders of libraries to\
    \ test, respectively, in <code>FIBER_FFT_LIB_DIRS</code> and <code>FIBER_FFT_INCLUDE_DIRS</code>.</p>\n\
    <h1>\n<a id=\"user-content-testing-integration\" class=\"anchor\" href=\"#testing-integration\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Testing integration</h1>\n<p>Run tests as follows:</p>\n<pre><code>cd\
    \ build/benchmarks\nmpirun -n 2 ./test3D_CPU_C2C &lt;library&gt;\nmpirun -n 2\
    \ ./test3D_CPU_R2C &lt;library&gt;\n</code></pre>\n<p>If FIBER was build linked\
    \ to GPU enabled libraries:</p>\n<pre><code>cd build/benchmarks\nmpirun -n 2 ./test3D_GPU_C2C\
    \ &lt;gpu_library&gt;\nmpirun -n 2 ./test3D_GPU_R2C &lt;gpu_library&gt;\n</code></pre>\n\
    <h1>\n<a id=\"user-content-running-benchmarks\" class=\"anchor\" href=\"#running-benchmarks\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Running benchmarks</h1>\n<pre><code>cd build/benchmarks\nmpirun -n\
    \ $NUM_RANKS ./test3D_C2C -lib &lt;library&gt; -backend &lt;1D_backend&gt; -size\
    \ &lt;nx&gt; &lt;ny&gt; &lt;nz&gt; -pgrid &lt;p&gt; &lt;q&gt;\n</code></pre>\n\
    <p>where <code>library</code> has to be replaced by one of the nine available\
    \ libraries, provided user has it installed.\nOnce a parallel FFT library has\
    \ been correctly integrated to heFFTe, running these benchmarks should report\
    \ a correct validation output.</p>\n<h1>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h1>\n<ul>\n<li>Installation\
    \ and a Doxygen documentation will be available shortly.</li>\n</ul>\n<hr>\n<h1>\n\
    <a id=\"user-content-getting-help\" class=\"anchor\" href=\"#getting-help\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Getting\
    \ Help</h1>\n<p>For assistance with the FIBER project, email <em><a href=\"mailto:fiber@icl.utk.edu\"\
    >fiber@icl.utk.edu</a></em> or start a GitHub issue.</p>\n<p>Contributions are\
    \ very welcome, please create a pull request.</p>\n<h1>\n<a id=\"user-content-resources\"\
    \ class=\"anchor\" href=\"#resources\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Resources</h1>\n<ul>\n<li>Visit\
    \ the <a href=\"http://icl.utk.edu/fiber/\" rel=\"nofollow\">FIBER website</a>\
    \ for more information about the HeFFTe project.</li>\n<li>Visit the <a href=\"\
    https://exascaleproject.org\" rel=\"nofollow\">ECP website</a> to find out more\
    \ about the DOE Exascale Computing Initiative.</li>\n</ul>\n<hr>\n<h1>\n<a id=\"\
    user-content-acknowledgments\" class=\"anchor\" href=\"#acknowledgments\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Acknowledgments</h1>\n\
    <p>This research was supported by the United States Exascale Computing Project.</p>\n\
    <hr>\n<h1>\n<a id=\"user-content-license\" class=\"anchor\" href=\"#license\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>License</h1>\n<pre><code>Copyright (c) 2022, University of Tennessee\n\
    All rights reserved.\n\nRedistribution and use in source and binary forms, with\
    \ or without\nmodification, are permitted provided that the following conditions\
    \ are met:\n    * Redistributions of source code must retain the above copyright\n\
    \      notice, this list of conditions and the following disclaimer.\n    * Redistributions\
    \ in binary form must reproduce the above copyright\n      notice, this list of\
    \ conditions and the following disclaimer in the\n      documentation and/or other\
    \ materials provided with the distribution.\n    * Neither the name of the University\
    \ of Tennessee nor the\n      names of its contributors may be used to endorse\
    \ or promote products\n      derived from this software without specific prior\
    \ written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND\
    \ CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT\
    \ NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\
    \ PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL UNIVERSITY OF TENNESSEE\
    \ BE LIABLE FOR ANY\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\
    \ DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\
    \ SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\
    \ CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\
    \ OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\
    \ OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n</code></pre>\n"
  stargazers_count: 4
  subscribers_count: 3
  topics: []
  updated_at: 1649709600.0
jrood-nrel/spack-configs:
  data_format: 2
  description: Spack configuration files and scripts for use on machines at NREL
  filenames:
  - configs/eagle/utilities/spack.yaml
  full_name: jrood-nrel/spack-configs
  latest_release: null
  readme: '<h1>

    <a id="user-content-spack-configuration-files-and-scripts-for-use-on-machines-at-nrel"
    class="anchor" href="#spack-configuration-files-and-scripts-for-use-on-machines-at-nrel"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Spack
    configuration files and scripts for use on machines at NREL</h1>

    <p>These software installations are maintained by Jon Rood for the HPACF group
    at NREL and are tailored to the applications our group develops. The list of available
    modules can be seen in <a href="modules.txt">modules.txt</a>. They are open to
    anyone to use on our machines. The software installations are organized by date
    snapshots. The binaries, compilers, and utilties are not updated as often as the
    software modules, so dated symlinks might point to older dates for those. However,
    each date snapshot of the modules should be able to stand on its own so that older
    snapshots can be purged safely over time.</p>

    <ul>

    <li>"base" is just a newer version of GCC to replace the system GCC 4.8.5 which
    is far too old to build many recent projects.</li>

    <li>"binaries" are generally the binary downloads of Paraview and Visit.</li>

    <li>"compilers" are the latest set of compilers built using the base GCC.</li>

    <li>"utilities" are the latest set of utility programs that don''t rely on MPI
    and are built using the base GCC.</li>

    <li>"software" are the latest set of generally larger programs and dependencies
    that rely on MPI. Each date corresponds to a single MPI implementation so there
    is no confusion as to which MPI was used for the applications. These modules are
    built using a farily recent GCC, Clang, or Intel compiler provided from the "compilers"
    modules, using the highest optimization flags specific to the machine architecture.</li>

    </ul>

    <p>The Spack hierarchy is linked in the following manner where each installation
    is based on other upstream Spack installations. "software" depends on "utilities",
    which both depend on "compilers". This hierarchy allows Spack to point to packages
    it needs which are already built upstream. The "compilers" installation exposes
    only the modules for compilers, while the "utilities" modules inherit modules
    from itself as well as the dependency packages in the "compilers" installation
    except the compiler modules themselves.</p>

    <p>Currently there is no perfect way to advertise deprecation or addition, and
    evolution of these modules. I have an MOTD you can cat in your login script to
    see updates. Generally the latest 4 sets of modules will likely be kept and new
    sets have been showing up around every 3 to 6 months.</p>

    <p>To use these modules you can add the following to your <code>~/.bashrc</code>
    for example and choose the module set (date) you prefer, and the GCC or Intel
    compiled software modules:</p>

    <pre><code>#------------------------------------------


    #MPT 2.22

    #MODULES=modules-2020-07

    #COMPILER=gcc-8.4.0

    #COMPILER=clang-10.0.0

    #COMPILER=intel-18.0.4


    #MPICH 3.3.1

    #MODULES=modules-2019-10-08

    #COMPILER=gcc-7.4.0

    #COMPILER=clang-7.0.1

    #COMPILER=intel-18.0.4


    #MPICH 3.3

    #MODULES=modules-2019-05-23

    #COMPILER=gcc-7.4.0

    #COMPILER=intel-18.0.4


    #MPICH 3.3

    #MODULES=modules-2019-05-08

    #COMPILER=gcc-7.4.0

    #COMPILER=intel-18.0.4


    #MPICH 3.3

    #MODULES=modules-2019-01-10

    #COMPILER=gcc-7.3.0

    #COMPILER=intel-18.0.4


    #Recommended default according to where "modules" is currently symlinked

    MODULES=modules

    COMPILER=gcc-8.4.0

    #COMPILER=clang-10.0.0

    #COMPILER=intel-18.0.4


    module purge

    module unuse ${MODULEPATH}

    module use /nopt/nrel/ecom/hpacf/binaries/${MODULES}

    module use /nopt/nrel/ecom/hpacf/compilers/${MODULES}

    module use /nopt/nrel/ecom/hpacf/utilities/${MODULES}

    module use /nopt/nrel/ecom/hpacf/software/${MODULES}/${COMPILER}

    module load gcc

    module load git

    module load python

    #etc...


    #------------------------------------------

    </code></pre>

    <p>If <code>module avail</code> does not show the modules on Eagle, try removing
    the LMOD cache with <code>rm -rf ~/.lmod.d/.cache</code></p>

    <p>Also included in this directory is a recommended Spack configurations you can
    use to build your own packages on the machines supported at NREL. Once you have
    <code>SPACK_ROOT</code> set you can run <code>/nopt/nrel/ecom/hpacf/spack-configs/scripts/setup-spack.sh</code>
    which should copy the yaml files into your instance of Spack. Or you can copy
    the yaml files into your <code>${SPACK_ROOT}/etc</code> directory manually. <code>spack
    compilers</code> should then show you many available compilers. Source your Spack''s
    <code>setup-env.sh</code> after you do the <code>module unuse ${MODULEPATH}</code>
    in your <code>.bashrc</code> so that your Spack instance will add its own module
    path to MODULEPATH. Remove <code>~/.spack/linux</code> if it exists and <code>spack
    compilers</code> doesn''t show you the updated list of compilers. The <code>~/.spack</code>
    directory takes highest precendence in the Spack configuration.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1651762796.0
justbennet/biospack:
  data_format: 2
  description: Setting up Spack to provide Bioinformatics packages
  filenames:
  - environments/arc/spack.yaml
  full_name: justbennet/biospack
  latest_release: null
  readme: '<h1>

    <a id="user-content-biospack" class="anchor" href="#biospack" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Biospack</h1>

    <p>These are files that provide local customization for the Spack installation

    that is used to provide Bioinformatics packages on the Great Lakes and

    Armis clusters.  This was all intended for use on Red Hat 8 systems, and

    the Spack installation will cohabit software installed in the traditional

    manner.  We are consciously restricting ourselves to non-MPI software,

    and only for Bioinformatics.</p>

    <p>The presumption is that these will be used to set up the test Spack for a

    new contributor, so all the settings in the configuration files presume a

    root directory of <code>/var/software/$USER</code>, and that all Spack created
    files

    (including temporary files) will be in directories beneath it.</p>

    <p>The files in the repository can be modified to create a Spack installation

    to provide the production installation intended for real users.  The targets

    should be</p>

    <p>Software:  <code>/sw/pkgs/bio</code>

    Modules:  <code>/sw/modules/bio/spack</code></p>

    <p>To set up a new installation, first run the <code>start_new_biospack</code>
    script.

    That will create the <code>/var/software/$USER/bio</code> directory, clone Spack

    itself into it, prompt you for the version of Spack to set, create the

    directories used for Spack temporary and cache files.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1650732078.0
key4hep/key4hep-spack:
  data_format: 2
  description: A Spack overlay repository of HEP software packaging.
  filenames:
  - environments/key4hep-nightlies-clang/spack.yaml
  - environments/key4hep-release-clang/spack.yaml
  - environments/key4hep-release/spack.yaml
  - environments/key4hep-nightlies/spack.yaml
  full_name: key4hep/key4hep-spack
  latest_release: '2021-10-29'
  readme: '<h1>

    <a id="user-content-spack-package-repo-for-key4hep-software-packaging" class="anchor"
    href="#spack-package-repo-for-key4hep-software-packaging" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://github.com/spack/spack">Spack</a>
    package repo for Key4HEP software packaging</h1>

    <p>This repository holds a set of Spack recipes for key4hep software. It grew
    out of <a href="https://github.com/HSF/hep-spack">https://github.com/HSF/hep-spack</a>,
    and many recipes habe been included in the upstream spack repostiory.</p>

    <p>Consult the <a href="https://spack.readthedocs.io/en/latest/" rel="nofollow">spack
    documentation</a> and the <a href="https://cern.ch/key4hep" rel="nofollow">key4hep
    documentation website</a> for more details.</p>

    <h3>

    <a id="user-content-repository-contents" class="anchor" href="#repository-contents"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Repository
    Contents</h3>

    <p>Apart from the recipes for key4hep packages in the folder <code>packages</code>,
    the repository contains some <code>scripts</code> used for publishing on cvmfs,
    and <code>config</code> files for spack.</p>

    <h3>

    <a id="user-content-central-installations" class="anchor" href="#central-installations"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Central
    Installations</h3>

    <p>Installations of the software stack can be found under <code>/cvmfs/sw.hsf.org/</code>,
    see:</p>

    <p><a href="https://key4hep.github.io/key4hep-doc/setup-and-getting-started/README.html"
    rel="nofollow">https://key4hep.github.io/key4hep-doc/setup-and-getting-started/README.html</a></p>

    '
  stargazers_count: 8
  subscribers_count: 9
  topics: []
  updated_at: 1652044946.0
laristra/ristra_spackages:
  data_format: 2
  description: 'A mirror of Ristra''s internal gitlab repository. '
  filenames:
  - env/broadwell/flecsi/spack.yaml
  - env/power9le/flecsalemm-deps/spack.yaml
  - .gitlab-ci/env/local-build/spack.yaml
  - env/x86_64/flecsalemm-deps/spack.yaml
  - .gitlab-ci/env/dry-run/spack.yaml
  - env/power9le/flecsi/spack.yaml
  - env/broadwell/flecsalemm-deps/spack.yaml
  - env/x86_64/flecsi/spack.yaml
  - .gitlab-ci/env/root-build/spack.yaml
  full_name: laristra/ristra_spackages
  latest_release: null
  readme: '<h1>

    <a id="user-content-ristra-spackages" class="anchor" href="#ristra-spackages"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Ristra
    Spackages</h1>

    <p>This repository contains the custom spackage files for the repos in laristra
    family.</p>

    <h2>

    <a id="user-content-basic-usage" class="anchor" href="#basic-usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Basic Usage</h2>

    <p>We assume the user wish to work in the home directory and already have a spack
    instance setup.  The minimum required version of spack is 0.15.2.</p>

    <p>To get the content of this repo</p>

    <pre><code>$ git clone git@gitlab.lanl.gov:laristra/ristra_spackages.git

    </code></pre>

    <p>To use the custom spackage files with your spack</p>

    <pre><code>$ spack repo add ristra_spackages/spack-repo

    ==&gt; Added repo with namespace ''lanl_ristra''.


    $ spack repo list

    ==&gt; 2 package repositories.

    lanl_ristra        /home/&lt;user&gt;/ristra_spackages/spack-repo

    builtin            /home/&lt;user&gt;/spack/var/spack/repos/builtin

    </code></pre>

    <p>[Optional]

    To ensure you have this custom repo in your spack all the time, move the <code>repos.yaml</code>
    into your spack config folder</p>

    <pre><code>$ mv /home/&lt;user&gt;/.spack/linux/repos.yaml /home/&lt;user&gt;/spack/etc/spack/

    </code></pre>

    <p>Please see the <a href="https://spack.readthedocs.io/en/latest/configuration.html"
    rel="nofollow">Spack documentation</a> for more detailed info.</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1649449003.0
lcompilers/lpython:
  data_format: 2
  description: Python compiler
  filenames:
  - spack.yaml
  full_name: lcompilers/lpython
  latest_release: null
  readme: '<h1>

    <a id="user-content-lpython" class="anchor" href="#lpython" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>LPython</h1>

    <p>LPython is a Python compiler. It is in heavy development, currently in

    pre-alpha stage. Some of the goals of LPython:</p>

    <ul>

    <li>The best possible performance for numerical array oriented code</li>

    <li>Run on all platforms</li>

    <li>Compile a subset of Python and be Python compatible</li>

    <li>Explore how to design it so that it can be eventually used with any Python

    code</li>

    <li>Fast compilation</li>

    <li>Excellent user friendly diagnostic messages: error, warnings, hints, notes,

    etc.</li>

    <li>Ahead of time compilation to binaries and interactive usage (Jupyter

    notebook)</li>

    <li>Able to transform the Python code to C++, Fortran and other languages</li>

    </ul>

    <p>And more.</p>

    <h1>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h1>

    <p>LPython works on Windows, macOS and Linux.</p>

    <h2>

    <a id="user-content-install-conda" class="anchor" href="#install-conda" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install Conda</h2>

    <p>If you do not have Conda already installed, please follow the instructions

    here to install Conda on your platform:</p>

    <p><a href="https://github.com/conda-forge/miniforge/#download">https://github.com/conda-forge/miniforge/#download</a></p>

    <h2>

    <a id="user-content-compile-lpython" class="anchor" href="#compile-lpython" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Compile LPython</h2>

    <p>Install required packages (Linux - 64 bit):</p>

    <div class="highlight highlight-source-shell"><pre>sudo apt install binutils-dev</pre></div>

    <p>Clone LPython</p>

    <div class="highlight highlight-source-shell"><pre>git clone https://github.com/lcompilers/lpython.git

    <span class="pl-c1">cd</span> lpython</pre></div>

    <p>Create a Conda environment using the preexisting environment.yml file:</p>

    <div class="highlight highlight-source-shell"><pre>conda env create -f environment.yml

    conda activate lp</pre></div>

    <p>Create autogenerated files (choose the command for your platform):</p>

    <div class="highlight highlight-source-shell"><pre>./build0.sh      <span class="pl-c"><span
    class="pl-c">#</span> macOS/Linux</span>

    call build0.bat  <span class="pl-c"><span class="pl-c">#</span> Windows</span></pre></div>

    <p>Compile LPython:</p>

    <div class="highlight highlight-source-shell"><pre>cmake -DCMAKE_BUILD_TYPE=Debug
    -DWITH_LLVM=yes -DWITH_STACKTRACE=yes -DWITH_LFORTRAN_BINARY_MODFILES=no <span
    class="pl-c1">.</span>

    cmake --build <span class="pl-c1">.</span> -j16</pre></div>

    <h2>

    <a id="user-content-tests" class="anchor" href="#tests" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Tests:</h2>

    <p>Run tests:</p>

    <div class="highlight highlight-source-shell"><pre>ctest

    ./run_tests.py</pre></div>

    <p>Also, run the integration tests:</p>

    <div class="highlight highlight-source-shell"><pre>./integration_tests/run_tests.py</pre></div>

    <h2>

    <a id="user-content-examples" class="anchor" href="#examples" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Examples</h2>

    <p>You can run the following examples by hand in a terminal:</p>

    <div class="highlight highlight-source-shell"><pre>./src/bin/lpython examples/expr2.py

    ./a.out

    ./src/bin/lpython --show-ast examples/expr2.py

    ./src/bin/lpython --show-asr examples/expr2.py

    ./src/bin/lpython --show-cpp examples/expr2.py

    ./src/bin/lpython --show-llvm examples/expr2.py</pre></div>

    <h2>

    <a id="user-content-contributing" class="anchor" href="#contributing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <p>We welcome contributions from anyone, even if you are new to open source. It

    might sound daunting to contribute to a compiler at first, but please do, it is

    not complicated. We will help you with any technical issues and help improve

    your contribution so that it can be merged.</p>

    <p>To contribute, submit a Pull Request (PR) against our repository at:</p>

    <p><a href="https://github.com/lcompilers/lpython">https://github.com/lcompilers/lpython</a></p>

    <p>Please report any bugs you may find at our issue tracker: <a href="https://github.com/lcompilers/lpython/issues">https://github.com/lcompilers/lpython/issues</a>.

    Or, even better, fork the repository on GitHub and create a PR. We welcome all
    changes, big or small, and we will help you make a PR if you are new to git.</p>

    <p>If you have any questions or need help, please ask us at Zulip (<a href="https://lfortran.zulipchat.com/"
    rel="nofollow"><img src="https://camo.githubusercontent.com/11e6556bfe778e7cf7331cac9c44bd0616062722036cc0d9bb0b7909aaae8779/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7a756c69702d6a6f696e5f636861742d627269676874677265656e2e737667"
    alt="project chat" data-canonical-src="https://img.shields.io/badge/zulip-join_chat-brightgreen.svg"
    style="max-width:100%;"></a>) or our

    <a href="https://groups.io/g/lfortran" rel="nofollow">mailinglist</a>.</p>

    <p>See the <a href="CONTRIBUTING.md">CONTRIBUTING</a> document for more information.</p>

    '
  stargazers_count: 38
  subscribers_count: 6
  topics: []
  updated_at: 1653544254.0
lzhang714/SIRIUS-7.2.6:
  data_format: 2
  description: null
  filenames:
  - dockerfile/spack.yaml
  full_name: lzhang714/SIRIUS-7.2.6
  latest_release: null
  readme: "<p align=\"center\">\n<a href=\"doc/images/sirius_logo.png\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img src=\"doc/images/sirius_logo.png\" width=\"\
    500\" style=\"max-width:100%;\"></a>\n</p>\n<p><a href=\"https://github.com/electronic-structure/SIRIUS/releases\"\
    ><img src=\"https://camo.githubusercontent.com/c73d057bc0ce51b550eae93560992046fe3d43509c00b3276aa1ec03a899bdc0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f656c656374726f6e69632d7374727563747572652f7369726975732e737667\"\
    \ alt=\"GitHub Releases\" data-canonical-src=\"https://img.shields.io/github/release/electronic-structure/sirius.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://electronic-structure.github.io/SIRIUS-doc\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/baa66788db0de398c9b2722c3d7063864d0f32fb76a004c4149f1da7f0c48939/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d646f787967656e2d626c75652e737667\"\
    \ alt=\"Documentation\" data-canonical-src=\"https://img.shields.io/badge/docs-doxygen-blue.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://raw.githubusercontent.com/electronic-structure/SIRIUS/master/LICENSE\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/44c92aa855b3a4b0b5c6f84818afb96ab66b53a102115f5167a396a3f0ff8f3a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4253442d626c75652e737667\"\
    \ alt=\"Licence\" data-canonical-src=\"https://img.shields.io/badge/license-BSD-blue.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/electronic-structure/SIRIUS/actions\"\
    ><img src=\"https://github.com/electronic-structure/SIRIUS/workflows/Build/badge.svg?branch=master\"\
    \ alt=\"Build\" style=\"max-width:100%;\"></a>\n<a href=\"https://gitlab.com/cscs-ci/electronic-structure/SIRIUS/-/commits/master\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/fb638ee73d7ef0f107eee37c02a3785fc83287cb88e7417bfc650631ff047d92/68747470733a2f2f6769746c61622e636f6d2f637363732d63692f656c656374726f6e69632d7374727563747572652f5349524955532f6261646765732f6d61737465722f706970656c696e652e7376673f6b65795f746578743d6d6173746572\"\
    \ alt=\"Verification tests master\" data-canonical-src=\"https://gitlab.com/cscs-ci/electronic-structure/SIRIUS/badges/master/pipeline.svg?key_text=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://gitlab.com/cscs-ci/electronic-structure/SIRIUS/-/commits/develop\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/632a3c9a8846d5d83954d9f31cc544fffccb9f40e1b3012ae790ccfd0d5e9ff1/68747470733a2f2f6769746c61622e636f6d2f637363732d63692f656c656374726f6e69632d7374727563747572652f5349524955532f6261646765732f646576656c6f702f706970656c696e652e7376673f6b65795f746578743d646576656c6f70\"\
    \ alt=\"Verification tests develop\" data-canonical-src=\"https://gitlab.com/cscs-ci/electronic-structure/SIRIUS/badges/develop/pipeline.svg?key_text=develop\"\
    \ style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-table-of-contents\"\
    \ class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Table of contents</h2>\n<ul>\n\
    <li><a href=\"#introduction\">Introduction</a></li>\n<li>\n<a href=\"#installation\"\
    >Installation</a>\n<ul>\n<li><a href=\"#configuring-sirius\">Configuring SIRIUS</a></li>\n\
    <li><a href=\"#developing-and-debugging-sirius\">Developing and debugging SIRIUS</a></li>\n\
    <li><a href=\"#manual-installation\">Manual installation</a></li>\n<li><a href=\"\
    #archlinux\">Archlinux</a></li>\n<li><a href=\"#installation-on-piz-daint\">Installation\
    \ on Piz Daint</a></li>\n</ul>\n</li>\n<li>\n<a href=\"#accelerating-dft-codes\"\
    >Accelerating DFT codes</a>\n<ul>\n<li><a href=\"#quantum-espresso\">Quantum ESPRESSO</a></li>\n\
    <li><a href=\"#cp2k\">CP2K</a></li>\n</ul>\n</li>\n<li><a href=\"#contacts\">Contacts</a></li>\n\
    <li><a href=\"#acknowledgements\">Acknowledgements</a></li>\n</ul>\n<h2>\n<a id=\"\
    user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Introduction</h2>\n\
    <p>SIRIUS is a domain specific library for electronic structure calculations.\
    \ It implements pseudopotential plane wave (PP-PW)\nand full potential linearized\
    \ augmented plane wave (FP-LAPW) methods and is designed for GPU acceleration\
    \ of popular community\ncodes such as Exciting, Elk and Quantum ESPRESSO. SIRIUS\
    \ is written in C++14 with MPI, OpenMP and CUDA/ROCm programming models.\nSIRIUS\
    \ is organised as a collection of classes that abstract away the different building\
    \ blocks of DFT self-consistency cycle.</p>\n<p>The following functionality is\
    \ currently implemented in SIRIUS:</p>\n<ul>\n<li>(PP-PW) Norm-conserving, ultrasoft\
    \ and PAW pseudopotentials</li>\n<li>(PP-PW) Spin-orbit coupling</li>\n<li>(PP-PW)\
    \ Stress tensor</li>\n<li>(PP-PW, FP-LAPW) Atomic forces</li>\n<li>(PP-PW, FP-LAPW)\
    \ Collinear and non-collinear magnetism</li>\n<li>(FP-LAPW) APW and LAPW basis\
    \ sets with arbitrary number of local orbitals</li>\n<li>(FP-LAPW) ZORA and IORA\
    \ approximations for valence states; full relativistic Dirac equation for core\
    \ states</li>\n<li>Symmetrization of lattice-periodic functions and on-site matrices</li>\n\
    <li>Generation of irreducible k-meshes</li>\n<li>Python frontend</li>\n</ul>\n\
    <h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<p>It is recommended to install SIRIUS through\
    \ <a href=\"https://spack.readthedocs.io/en/latest/getting_started.html\" rel=\"\
    nofollow\">Spack</a>. To set it up, use</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>git clone https://github.com/spack/spack.git\n<span class=\"pl-c1\">.</span>\
    \ spack/share/spack/setup-env.sh\nspack install sirius</pre></div>\n<h3>\n<a id=\"\
    user-content-configuring-sirius\" class=\"anchor\" href=\"#configuring-sirius\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Configuring SIRIUS</h3>\n<p>SIRIUS has many different configurations\
    \ to enable specific hardware and library support. Some common setups include:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> Use default BLAS, LAPACK, MPI and FFTW3 implementations,\
    \ without GPU support, using the latest GCC 9.x</span>\n$ spack install sirius\
    \ %gcc@:9\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Explicitly use\
    \ the latest 3.x release of MPICH for MPI, OpenBLAS for BLAS and LAPACK, FFTW\
    \ for FFTW3, without GPU support</span>\n$ spack install sirius ^mpich@:3 ^fftw\
    \ ^openblas\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Enable distributed\
    \ linear algebra, and use Intel MKL for BLAS, ScaLAPACK and FFTW3, without GPU\
    \ support</span>\n$ spack install sirius +scalapack ^intel-mkl\n\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> Build with CUDA support for NVIDIA GPUs</span>\n\
    $ spack install sirius +cuda cuda_arch=75\n\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> Build with ROCm support for AMD GPUs</span>\n$ spack install sirius\
    \ +rocm amdgpu_target=gfx906\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ Build with MAGMA</span>\n$ spack install sirius +cuda +magma\n\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> Build with ELPA</span>\n$ spack install sirius\
    \ +scalapack +elpa</pre></div>\n<p>Language interop with Fortran and Python can\
    \ be enabled with <code>+fortran</code> and <code>+python</code> respectively.</p>\n\
    <p>See <code>spack info sirius</code> for the full list of support variants.</p>\n\
    <h3>\n<a id=\"user-content-developing-and-debugging-sirius\" class=\"anchor\"\
    \ href=\"#developing-and-debugging-sirius\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Developing and debugging SIRIUS</h3>\n\
    <p>The recommended way to install the latest development version of SIRIUS is\
    \ through <code>spack dev-build</code>.</p>\n<p>As an example, the following builds\
    \ SIRIUS with CUDA support in debug mode:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ git clone --recursive -b develop https://github.com/electronic-structure/SIRIUS.git\n\
    $ <span class=\"pl-c1\">cd</span> SIRIUS\n$ spack dev-build sirius@develop build_type=Debug\
    \ +cuda</pre></div>\n<p>When more control over the build commands is necessary,\
    \ use <code>spack build-env [spec] -- [command]</code>:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>$ mkdir SIRIUS/build <span class=\"pl-k\">&amp;&amp;</span>\
    \ <span class=\"pl-c1\">cd</span> SIRIUS/build\n$ <span class=\"pl-k\">export</span>\
    \ SPEC=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>sirius@develop build_type=Debug\
    \ +cuda<span class=\"pl-pds\">\"</span></span>\n$ spack install --only=dependencies\
    \ <span class=\"pl-smi\">$SPEC</span>\n$ spack build-env <span class=\"pl-smi\"\
    >$SPEC</span> -- cmake ..\n$ spack build-env <span class=\"pl-smi\">$SPEC</span>\
    \ -- make -j<span class=\"pl-s\"><span class=\"pl-pds\">$(</span>nproc<span class=\"\
    pl-pds\">)</span></span></pre></div>\n<h3>\n<a id=\"user-content-manual-installation\"\
    \ class=\"anchor\" href=\"#manual-installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Manual installation</h3>\n<p>When\
    \ installing SIRIUS without Spack, make sure to install the required dependencies\
    \ first:</p>\n<ul>\n<li>CMake \u2265 3.14</li>\n<li>C++ compiler with C++14 support</li>\n\
    <li>MPI (OpenMPI or MPICH)</li>\n<li>BLAS/LAPACK (OpenBLAS or Intel MKL)</li>\n\
    <li>\n<a href=\"https://www.gnu.org/software/gsl/\" rel=\"nofollow\">GSL</a> -\
    \ GNU scientific library</li>\n<li>\n<a href=\"https://www.tddft.org/programs/libxc/\"\
    \ rel=\"nofollow\">LibXC</a> - library of exchange-correlation potentials</li>\n\
    <li><a href=\"https://www.hdfgroup.org/solutions/hdf5/\" rel=\"nofollow\">HDF5</a></li>\n\
    <li>\n<a href=\"https://atztogo.github.io/spglib/\" rel=\"nofollow\">spglib</a>\
    \ - library for finding and handling crystal symmetries</li>\n<li>\n<a href=\"\
    https://github.com/eth-cscs/SpFFT\">SpFFT</a> - domain-specific FFT library</li>\n\
    <li>\n<a href=\"https://github.com/eth-cscs/spla\">SPLA</a> - domain-specific\
    \ distributed GEMM library</li>\n</ul>\n<p>and optionally any of the additional\
    \ libraries:</p>\n<ul>\n<li>ScaLAPACK (Intel MKL or netlib scalapack)</li>\n<li><a\
    \ href=\"https://elpa.mpcdf.mpg.de/software\" rel=\"nofollow\">ELPA</a></li>\n\
    <li><a href=\"https://icl.cs.utk.edu/magma/\" rel=\"nofollow\">MAGMA</a></li>\n\
    <li>CUDA/ROCm</li>\n<li>\n<a href=\"https://www.boost.org/doc/libs/1_73_0/libs/filesystem/doc/index.htm\"\
    \ rel=\"nofollow\">Boost Filesystem</a>*</li>\n</ul>\n<p>* Only required when\
    \ <code>BUILD_APPS=On</code> and your compiler does not support <code>std::filesystem</code>\
    \ or <code>std::experimental::filesystem</code>.</p>\n<p>Clone the repository\
    \ and build as follows:</p>\n<div class=\"highlight highlight-source-shell\"><pre>git\
    \ clone --recursive https://github.com/electronic-structure/SIRIUS.git\nmkdir\
    \ SIRIUS/build\n<span class=\"pl-c1\">cd</span> SIRIUS/build\n<span class=\"pl-k\"\
    >export</span> CXX=mpicxx CC=mpicc FC=mpif90\n<span class=\"pl-k\">export</span>\
    \ CMAKE_PREFIX_PATH=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>path/to/BLAS;path/to/GSL;path/to/LibXC;path/to/HDF5;...<span\
    \ class=\"pl-pds\">\"</span></span>\ncmake -DCMAKE_INSTALL_PREFIX=<span class=\"\
    pl-smi\">$PWD</span>/sirius\nmake -j install</pre></div>\n<p>where <code>CMAKE_PREFIX_PATH</code>\
    \ is a list of installation paths of dependencies installed in non-standard locations.</p>\n\
    <h4>\n<a id=\"user-content-adding-gpu-support\" class=\"anchor\" href=\"#adding-gpu-support\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Adding GPU support</h4>\n<p>To enable CUDA you need to pass the following\
    \ options to CMake: <code>-DUSE_CUDA=On -DCUDA_ARCH='60;70'</code>, where <code>CUDA_ARCH</code>\
    \ is\na list of NVIDIA architectures. Use <code>60</code>, <code>61</code>, <code>62</code>\
    \ for Pascal; <code>70</code>, <code>72</code> for Volta; <code>75</code> for\
    \ Turing; and <code>80</code> for Ampere.\nIf CUDA is installed in a non-standard\
    \ directory, you have to pass additional parameter to cmake <code>-DCUDA_TOOLKIT_ROOT_DIR=/path/to/cuda</code>.</p>\n\
    <p>To enable MAGMA (GPU implementation of LAPACK) use <code>-DUSE_MAGMA=On</code>.\
    \ Append MAGMA's installation directory to <code>CMAKE_PREFIX_PATH</code> if necessary.</p>\n\
    <h4>\n<a id=\"user-content-parallel-eigensolvers\" class=\"anchor\" href=\"#parallel-eigensolvers\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Parallel eigensolvers</h4>\n<p>To compile with ScaLAPACK use <code>-DUSE_SCALAPACK=On</code>.\
    \ To use ELPA, both <code>-DUSE_SCALAPACK=On</code> and <code>-DUSE_ELPA=On</code>\
    \ are\nrequired, as we need ScaLAPACK functionality to transform the generalized\
    \ eigenvalue problem to standard form,\nwhich can then be solved by ELPA. Append\
    \ ScaLAPACK's and ELPA's install directory to <code>CMAKE_PREFIX_PATH</code> if\
    \ necessary.</p>\n<h4>\n<a id=\"user-content-python-module\" class=\"anchor\"\
    \ href=\"#python-module\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Python module</h4>\n<p>Use <code>-DCREATE_PYTHON_MODULE=On</code>\
    \ to build the Python module. The SIRIUS Python module depends on <code>mpi4py</code>\
    \ and\n<code>pybind11</code>, which need to be installed on your system.</p>\n\
    <h4>\n<a id=\"user-content-additional-options\" class=\"anchor\" href=\"#additional-options\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Additional options</h4>\n<p>To link against Intel MKL use <code>-DUSE_MKL=On</code>.\
    \ For Cray libsci use <code>-DUSE_CRAY_LIBSCI=On</code>. Building tests requires\
    \ <code>-DBUILD_TESTING=On</code>.</p>\n<p>By default example applications are\
    \ built. This can be turned off via <code>-DBUILD_APPS=Off</code>, which is recommended\
    \ when just building Fortran bindings.</p>\n<h3>\n<a id=\"user-content-arch-linux\"\
    \ class=\"anchor\" href=\"#arch-linux\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Arch Linux</h3>\n<p>Arch Linux\
    \ users can find SIRIUS in the <a href=\"https://aur.archlinux.org/packages/sirius-git/\"\
    \ rel=\"nofollow\">AUR</a>.</p>\n<h3>\n<a id=\"user-content-installation-on-piz-daint\"\
    \ class=\"anchor\" href=\"#installation-on-piz-daint\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation\
    \ on Piz Daint</h3>\n<p>Please refer to the <a href=\"https://github.com/electronic-structure/SIRIUS/wiki/Build-on-Piz-Daint\"\
    >SIRIUS wiki page</a> and\n<a href=\"https://user.cscs.ch/computing/applications/sirius/\"\
    \ rel=\"nofollow\">CSCS User portal</a> for detailed instructions.</p>\n<h2>\n\
    <a id=\"user-content-accelerating-dft-codes\" class=\"anchor\" href=\"#accelerating-dft-codes\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Accelerating DFT codes</h2>\n<h3>\n<a id=\"user-content-quantum-espresso\"\
    \ class=\"anchor\" href=\"#quantum-espresso\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Quantum ESPRESSO</h3>\n<p><a\
    \ href=\"https://www.quantum-espresso.org/\" rel=\"nofollow\">Quantum ESPRESSO</a>\
    \ is a popular open source suite of computer codes for\nelectronic-structure calculations\
    \ and materials modeling at the nanoscale. It is based on DFT, plane waves, and\n\
    pseudopotentials. We maintain the GPU-accelerated version of\n<a href=\"https://github.com/electronic-structure/q-e-sirius\"\
    >Quantum ESPRESSO with SIRIUS bindings</a>.\nThis version is frequently synchronised\
    \ with the\n<code>develop</code> branch of the official <a href=\"https://gitlab.com/QEF/q-e\"\
    \ rel=\"nofollow\">QE repository</a>. A typical example of using SIRIUS\ninside\
    \ QE is listed below:</p>\n<div class=\"highlight highlight-source-fortran\"><pre><span\
    \ class=\"pl-k\">subroutine</span> <span class=\"pl-en\">get_band_energies_from_sirius</span>\n\
    \  !\n  use wvfct,    only : nbnd, et\n  use klist,    only : nkstot, nks\n  use\
    \ lsda_mod, only : nspin\n  use sirius\n  !\n  <span class=\"pl-k\">implicit none</span>\n\
    \  !\n  <span class=\"pl-k\">integer</span>, <span class=\"pl-k\">external</span>\
    \ <span class=\"pl-k\">::</span> global_kpoint_index\n  !\n  <span class=\"pl-k\"\
    >real</span>(<span class=\"pl-c1\">8</span>), allocatable <span class=\"pl-k\"\
    >::</span> band_e(:,:)\n  <span class=\"pl-k\">integer</span> <span class=\"pl-k\"\
    >::</span> ik, nk, nb, nfv\n\n  allocate(band_e(nbnd, nkstot))\n\n  ! get band\
    \ energies\n  <span class=\"pl-k\">if</span> (nspin<span class=\"pl-k\">.ne.</span><span\
    \ class=\"pl-c1\">2</span>) <span class=\"pl-k\">then</span>\n    ! non<span class=\"\
    pl-k\">-</span>magnetic or non<span class=\"pl-k\">-</span>collinear case\n  \
    \  <span class=\"pl-k\">do</span> ik <span class=\"pl-k\">=</span> <span class=\"\
    pl-c1\">1</span>, nkstot\n      <span class=\"pl-k\">call</span> sirius_get_band_energies(ks_handler,\
    \ ik, <span class=\"pl-c1\">0</span>, band_e(<span class=\"pl-c1\">1</span>, ik))\n\
    \    <span class=\"pl-k\">end do</span>\n  <span class=\"pl-k\">else</span>\n\
    \    ! collinear magnetic case\n    nk <span class=\"pl-k\">=</span> nkstot <span\
    \ class=\"pl-k\">/</span> <span class=\"pl-c1\">2</span>\n    ! get band energies\n\
    \    <span class=\"pl-k\">do</span> ik <span class=\"pl-k\">=</span> <span class=\"\
    pl-c1\">1</span>, nk\n      <span class=\"pl-k\">call</span> sirius_get_band_energies(ks_handler,\
    \ ik, <span class=\"pl-c1\">0</span>, band_e(<span class=\"pl-c1\">1</span>, ik))\n\
    \      <span class=\"pl-k\">call</span> sirius_get_band_energies(ks_handler, ik,\
    \ <span class=\"pl-c1\">1</span>, band_e(<span class=\"pl-c1\">1</span>, nk <span\
    \ class=\"pl-k\">+</span> ik))\n    <span class=\"pl-k\">end do</span>\n\n  <span\
    \ class=\"pl-k\">endif</span>\n\n  ! convert <span class=\"pl-k\">to</span> Ry\n\
    \  <span class=\"pl-k\">do</span> ik <span class=\"pl-k\">=</span> <span class=\"\
    pl-c1\">1</span>, nks\n    et(:, ik) <span class=\"pl-k\">=</span> <span class=\"\
    pl-c1\">2.d0</span> <span class=\"pl-k\">*</span> band_e(:, global_kpoint_index(nkstot,\
    \ ik))\n  <span class=\"pl-k\">enddo</span>\n\n  deallocate(band_e)\n\n<span class=\"\
    pl-k\">end</span> <span class=\"pl-k\">subroutine</span><span class=\"pl-en\"\
    > get_band_energies_from_sirius</span></pre></div>\n<p>To compile Quantum ESPRESSO\
    \ with SIRIUS it is easiest to use Spack. The following installs a CUDA enabled\
    \ version:</p>\n<div class=\"highlight highlight-source-shell\"><pre>spack install\
    \ q-e-sirius ^sirius +shared +scalapack +cuda <span class=\"pl-k\">~</span>apps\
    \ ^intel-mkl ^mpich</pre></div>\n<p>Now you can load <code>pw.x</code> and MPI\
    \ related executables:</p>\n<div class=\"highlight highlight-source-shell\"><pre>spack\
    \ load q-e-sirius</pre></div>\n<p>Run <code>pw.x</code> using the same parameters\
    \ and input files as you would with native QE. Note that you have to explicitly\n\
    enable SIRIUS through the command-line option <code>-sirius</code> in <code>pw.x</code>.\
    \ For instance:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> run in default mode</span>\npw.x\
    \ -i pw.in\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> run with SIRIUS\
    \ enabled</span>\npw.x -i pw.in -sirius</pre></div>\n<p>The SIRIUS library is\
    \ using OpenMP for node-level parallelization. To run QE/SIRIUS efficiently, follow\
    \ these simple rules:</p>\n<ul>\n<li>always prefer k-point pool parallelization\
    \ over band parallelization</li>\n<li>use as few MPI ranks as possible for band\
    \ parallelization</li>\n<li>by default, use one rank per node and many OMP threads;\
    \ if the calculated system is really small, try to saturate\nthe GPU card by using\
    \ more MPI ranks (e.g.: on a 12-core node, use 2-3-4 ranks with 6-4-3 OMP threads)</li>\n\
    </ul>\n<h4>\n<a id=\"user-content-benchmarks\" class=\"anchor\" href=\"#benchmarks\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Benchmarks</h4>\n<p>In the following examples we compare the performance\
    \ of native and SIRIUS-enabled versions of QE. CPU-only runs are executed\non\
    \ dual-socket multi-core nodes containing two 18-core Intel Broadwell CPUs. GPU\
    \ runs are executed on hybrid\nnodes containing a 12-core Intel Haswell CPU and\
    \ an NVIDIA Tesla P100 card:</p>\n<table>\n<thead>\n<tr>\n<th>Hybrid partition\
    \ (Cray XC50)</th>\n<th>Multicore partition (Cray XC40)</th>\n</tr>\n</thead>\n\
    <tbody>\n<tr>\n<td>Intel Xeon E5-2690 v3 @2.60GHz, 12 cores <br> NVIDIA Tesla\
    \ P100 16GB</td>\n<td>Two Intel Xeon E5-2695 v4 @2.10GHz (2 x 18 cores)</td>\n\
    </tr>\n</tbody>\n</table>\n<p>Ground state calculation (<a href=\"https://github.com/electronic-structure/benchmarks/tree/master/performance/Si511Ge\"\
    >input</a>)\nof Si511Ge.</p>\n<p align=\"center\">\n<a href=\"doc/images/Si511Ge_perf.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"doc/images/Si511Ge_perf.png\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<p>Another example is the variable cell\
    \ relaxation of B6Ni8 (<a href=\"https://github.com/electronic-structure/benchmarks/tree/master/performance/B6Ni8\"\
    >input</a>).\nThe Brillouin zone contains 204 irreducible k-points and only k-pool\
    \ parallelization is used.</p>\n<p align=\"center\">\n<a href=\"doc/images/B6Ni8_perf.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"doc/images/B6Ni8_perf.png\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<h3>\n<a id=\"user-content-cp2k\" class=\"\
    anchor\" href=\"#cp2k\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>CP2K</h3>\n<p><a href=\"https://www.cp2k.org/\"\
    \ rel=\"nofollow\">CP2K</a> uses the SIRIUS library to enable plane-wave functionality.\
    \ The detailed description of the input parameters\ncan be found <a href=\"https://manual.cp2k.org\"\
    \ rel=\"nofollow\">here</a> under the <code>/CP2K_INPUT/FORCE_EVAL/PW_DFT</code>\
    \ section.</p>\n<h2>\n<a id=\"user-content-contacts\" class=\"anchor\" href=\"\
    #contacts\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Contacts</h2>\n<p>If you have any questions, feel free to contact\
    \ us:</p>\n<ul>\n<li>Anton Kozhevnikov (<a href=\"mailto:anton.kozhevnikov@cscs.ch\"\
    >anton.kozhevnikov@cscs.ch</a>)</li>\n<li>Mathieu Taillefumier (<a href=\"mailto:mathieu.taillefumier@cscs.ch\"\
    >mathieu.taillefumier@cscs.ch</a>)</li>\n<li>Simon Pintarelli (<a href=\"mailto:simon.pintarelli@cscs.ch\"\
    >simon.pintarelli@cscs.ch</a>)</li>\n</ul>\n<h2>\n<a id=\"user-content-acknowledgements\"\
    \ class=\"anchor\" href=\"#acknowledgements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Acknowledgements</h2>\n<p>The\
    \ development of the SIRIUS library would not be possible without support of the\
    \ following organizations:</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Logo</th>\n\
    <th align=\"center\">Name</th>\n<th align=\"center\">URL</th>\n</tr>\n</thead>\n\
    <tbody>\n<tr>\n<td align=\"center\"><a href=\"doc/images/logo_ethz.png\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img src=\"doc/images/logo_ethz.png\" alt=\"\
    ethz\" style=\"max-width:100%;\"></a></td>\n<td align=\"center\">Swiss Federal\
    \ Institute of Technology in Z\xFCrich</td>\n<td align=\"center\"><a href=\"https://www.ethz.ch/\"\
    \ rel=\"nofollow\">https://www.ethz.ch/</a></td>\n</tr>\n<tr>\n<td align=\"center\"\
    ><a href=\"doc/images/logo_cscs.png\" target=\"_blank\" rel=\"noopener noreferrer\"\
    ><img src=\"doc/images/logo_cscs.png\" alt=\"cscs\" style=\"max-width:100%;\"\
    ></a></td>\n<td align=\"center\">Swiss National Supercomputing Centre</td>\n<td\
    \ align=\"center\"><a href=\"https://www.cscs.ch/\" rel=\"nofollow\">https://www.cscs.ch/</a></td>\n\
    </tr>\n<tr>\n<td align=\"center\"><a href=\"doc/images/logo_pasc.png\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img src=\"doc/images/logo_pasc.png\" alt=\"\
    pasc\" style=\"max-width:100%;\"></a></td>\n<td align=\"center\">Platform for\
    \ Advanced Scientific Computing</td>\n<td align=\"center\"><a href=\"https://www.pasc-ch.org/\"\
    \ rel=\"nofollow\">https://www.pasc-ch.org/</a></td>\n</tr>\n<tr>\n<td align=\"\
    center\"><a href=\"doc/images/logo_marvel.png\" target=\"_blank\" rel=\"noopener\
    \ noreferrer\"><img src=\"doc/images/logo_marvel.png\" alt=\"pasc\" style=\"max-width:100%;\"\
    ></a></td>\n<td align=\"center\">NCCR MARVEL <br> Centre on Computational Design\
    \ and Discovery of Novel Materials</td>\n<td align=\"center\"><a href=\"https://nccr-marvel.ch/\"\
    \ rel=\"nofollow\">https://nccr-marvel.ch/</a></td>\n</tr>\n<tr>\n<td align=\"\
    center\"><a href=\"doc/images/logo_max.png\" target=\"_blank\" rel=\"noopener\
    \ noreferrer\"><img src=\"doc/images/logo_max.png\" alt=\"pasc\" style=\"max-width:100%;\"\
    ></a></td>\n<td align=\"center\">MAX (MAterials design at the eXascale) <br> European\
    \ Centre of Excellence</td>\n<td align=\"center\"><a href=\"http://www.max-centre.eu/\"\
    \ rel=\"nofollow\">http://www.max-centre.eu/</a></td>\n</tr>\n<tr>\n<td align=\"\
    center\"><a href=\"doc/images/logo_prace.png\" target=\"_blank\" rel=\"noopener\
    \ noreferrer\"><img src=\"doc/images/logo_prace.png\" alt=\"pasc\" style=\"max-width:100%;\"\
    ></a></td>\n<td align=\"center\">Partnership for Advanced Computing in Europe</td>\n\
    <td align=\"center\"><a href=\"https://prace-ri.eu/\" rel=\"nofollow\">https://prace-ri.eu/</a></td>\n\
    </tr>\n</tbody>\n</table>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1640854445.0
mdorier/mobject:
  data_format: 2
  description: Mobject is a Mochi object store presenting an API similar to that of
    RADOS
  filenames:
  - spack.yaml
  full_name: mdorier/mobject
  latest_release: null
  readme: '<p>Your project "mobject" has been setup!

    Enjoy programming with Mochi!</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1652975191.0
mochi-hpc/flamestore:
  data_format: 2
  description: Storage system for Deep Learning models designed using the Mochi components.
  filenames:
  - spack.yaml
  full_name: mochi-hpc/flamestore
  latest_release: null
  readme: '<h1>

    <a id="user-content-what-is-flamestore" class="anchor" href="#what-is-flamestore"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What
    is FlameStore?</h1>

    <p>FlameStore is a Mochi component to access Keras deep learning models

    and store them in various backends (right now: in memory, on a local

    file system, or on a composition of SDSKV and BAKE providers).</p>

    <p>FlameStore is developped by Matthieu Dorier (<a href="mailto:mdorier@anl.gov">mdorier@anl.gov</a>).

    More information on how to install and use is available

    <a href="https://xgitlab.cels.anl.gov/sds/flamestore/wikis/home" rel="nofollow">here</a>.</p>

    '
  stargazers_count: 1
  subscribers_count: 4
  topics: []
  updated_at: 1633975412.0
mochi-hpc/mochi-bedrock:
  data_format: 2
  description: Mochi bootstrapping service.
  filenames:
  - spack.yaml
  full_name: mochi-hpc/mochi-bedrock
  latest_release: v0.4.1
  readme: '<h1>

    <a id="user-content-bedrock" class="anchor" href="#bedrock" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Bedrock</h1>

    <p>Bedrock is Mochi''s service bootstrapping mechanism.

    For documentations and tutorials, please see

    <a href="https://mochi.readthedocs.io/en/latest/bedrock.html" rel="nofollow">here</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1640527359.0
mochi-hpc/mochi-mona:
  data_format: 2
  description: Mochi messaging over NA
  filenames:
  - spack.yaml
  full_name: mochi-hpc/mochi-mona
  latest_release: v0.1.1
  readme: '<h1>

    <a id="user-content-mona---messaging-over-na" class="anchor" href="#mona---messaging-over-na"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>MoNA
    - Messaging over NA</h1>

    <p>MoNA is a Mochi library combining the NA layer of Mercury with

    the Argobots threading library, in a way similar to how Margo

    combines Mercury with Argobots. It provides a low-level messaging

    interface and hides the NA progress loop into Argobots ULTs.</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1633974549.0
mochi-hpc/mochi-yokan:
  data_format: 2
  description: Remote Key/Value storage service for Mochi
  filenames:
  - spack.yaml
  full_name: mochi-hpc/mochi-yokan
  latest_release: v0.2.5
  readme: '<h1>

    <a id="user-content-yokan---mochis-keyvalue-and-more-storage-service" class="anchor"
    href="#yokan---mochis-keyvalue-and-more-storage-service" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Yokan - Mochi''s Key/Value
    (and more) storage service</h1>

    <p><a href="https://github.com/mochi-hpc/mochi-yokan/actions/workflows/test.yml/badge.svg?branch=main"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/mochi-hpc/mochi-yokan/actions/workflows/test.yml/badge.svg?branch=main"
    alt="" style="max-width:100%;"></a>

    <a href="https://codecov.io/gh/mochi-hpc/mochi-yokan" rel="nofollow"><img src="https://camo.githubusercontent.com/fc95c801bafa29b49219f4727f651b97e7385800c8dc4a4757a1dccadefe6611/68747470733a2f2f636f6465636f762e696f2f67682f6d6f6368692d6870632f6d6f6368692d796f6b616e2f6272616e63682f6d61696e2f67726170682f62616467652e737667"
    alt="codecov" data-canonical-src="https://codecov.io/gh/mochi-hpc/mochi-yokan/branch/main/graph/badge.svg"
    style="max-width:100%;"></a></p>

    <p>Please see documentation <a href="https://mochi.readthedocs.io/en/latest/yokan.html"
    rel="nofollow">here</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1641326484.0
mpbelhorn/olcf-spack:
  data_format: 2
  description: Spack fork used on OLCF resources
  filenames:
  - share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/build_systems/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/radiuss/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml
  full_name: mpbelhorn/olcf-spack
  latest_release: null
  readme: "<h1>\n<a id=\"user-content--spack\" class=\"anchor\" href=\"#-spack\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a\
    \ href=\"https://camo.githubusercontent.com/a01512f4480c4615a82f2b929789547a60d78e1f68d26be1a56e33d9258735d4/68747470733a2f2f63646e2e7261776769742e636f6d2f737061636b2f737061636b2f646576656c6f702f73686172652f737061636b2f6c6f676f2f737061636b2d6c6f676f2e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a01512f4480c4615a82f2b929789547a60d78e1f68d26be1a56e33d9258735d4/68747470733a2f2f63646e2e7261776769742e636f6d2f737061636b2f737061636b2f646576656c6f702f73686172652f737061636b2f6c6f676f2f737061636b2d6c6f676f2e737667\"\
    \ width=\"64\" valign=\"middle\" alt=\"Spack\" data-canonical-src=\"https://cdn.rawgit.com/spack/spack/develop/share/spack/logo/spack-logo.svg\"\
    \ style=\"max-width:100%;\"></a> Spack</h1>\n<p><a href=\"https://github.com/spack/spack/actions\"\
    ><img src=\"https://github.com/spack/spack/workflows/linux%20tests/badge.svg\"\
    \ alt=\"Unit Tests\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/spack/spack/actions/workflows/bootstrap.yml\"\
    ><img src=\"https://github.com/spack/spack/actions/workflows/bootstrap.yml/badge.svg\"\
    \ alt=\"Bootstrapping\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/spack/spack/actions?query=workflow%3A%22macOS+builds+nightly%22\"\
    ><img src=\"https://github.com/spack/spack/workflows/macOS%20builds%20nightly/badge.svg?branch=develop\"\
    \ alt=\"macOS Builds (nightly)\" style=\"max-width:100%;\"></a>\n<a href=\"https://codecov.io/gh/spack/spack\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4e223725486ecdae463d02d6ebd2b814945366210a908fc6f04b044ada8a7820/68747470733a2f2f636f6465636f762e696f2f67682f737061636b2f737061636b2f6272616e63682f646576656c6f702f67726170682f62616467652e737667\"\
    \ alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/spack/spack/branch/develop/graph/badge.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://spack.readthedocs.io\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/523aba38ec3f8d2294b874493fe63feed5805b98460c385611397b02be14a51c/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f737061636b2f62616467652f3f76657273696f6e3d6c6174657374\"\
    \ alt=\"Read the Docs\" data-canonical-src=\"https://readthedocs.org/projects/spack/badge/?version=latest\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://slack.spack.io\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/4bbdc2b44561be6dfffe64e15730e1c5a2bed9c4efe6f9942638091a4ce3ede2/68747470733a2f2f736c61636b2e737061636b2e696f2f62616467652e737667\"\
    \ alt=\"Slack\" data-canonical-src=\"https://slack.spack.io/badge.svg\" style=\"\
    max-width:100%;\"></a></p>\n<p>Spack is a multi-platform package manager that\
    \ builds and installs\nmultiple versions and configurations of software. It works\
    \ on Linux,\nmacOS, and many supercomputers. Spack is non-destructive: installing\
    \ a\nnew version of a package does not break existing installations, so many\n\
    configurations of the same package can coexist.</p>\n<p>Spack offers a simple\
    \ \"spec\" syntax that allows users to specify versions\nand configuration options.\
    \ Package files are written in pure Python, and\nspecs allow package authors to\
    \ write a single script for many different\nbuilds of the same package.  With\
    \ Spack, you can build your software\n<em>all</em> the ways you want to.</p>\n\
    <p>See the\n<a href=\"https://spack.readthedocs.io/en/latest/features.html\" rel=\"\
    nofollow\">Feature Overview</a>\nfor examples and highlights.</p>\n<p>To install\
    \ spack and your first package, make sure you have Python.\nThen:</p>\n<pre><code>$\
    \ git clone -c feature.manyFiles=true https://github.com/spack/spack.git\n$ cd\
    \ spack/bin\n$ ./spack install zlib\n</code></pre>\n<h2>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h2>\n<p><a href=\"\
    https://spack.readthedocs.io/\" rel=\"nofollow\"><strong>Full documentation</strong></a>\
    \ is available, or\nrun <code>spack help</code> or <code>spack help --all</code>.</p>\n\
    <p>For a cheat sheet on Spack syntax, run <code>spack help --spec</code>.</p>\n\
    <h2>\n<a id=\"user-content-tutorial\" class=\"anchor\" href=\"#tutorial\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Tutorial</h2>\n\
    <p>We maintain a\n<a href=\"https://spack.readthedocs.io/en/latest/tutorial.html\"\
    \ rel=\"nofollow\"><strong>hands-on tutorial</strong></a>.\nIt covers basic to\
    \ advanced usage, packaging, developer features, and large HPC\ndeployments. \
    \ You can do all of the exercises on your own laptop using a\nDocker container.</p>\n\
    <p>Feel free to use these materials to teach users at your organization\nabout\
    \ Spack.</p>\n<h2>\n<a id=\"user-content-community\" class=\"anchor\" href=\"\
    #community\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Community</h2>\n<p>Spack is an open source project.  Questions, discussion,\
    \ and\ncontributions are welcome. Contributions can be anything from new\npackages\
    \ to bugfixes, documentation, or even new core features.</p>\n<p>Resources:</p>\n\
    <ul>\n<li>\n<strong>Slack workspace</strong>: <a href=\"https://spackpm.slack.com\"\
    \ rel=\"nofollow\">spackpm.slack.com</a>.\nTo get an invitation, visit <a href=\"\
    https://slack.spack.io\" rel=\"nofollow\">slack.spack.io</a>.</li>\n<li>\n<strong>Mailing\
    \ list</strong>: <a href=\"https://groups.google.com/d/forum/spack\" rel=\"nofollow\"\
    >groups.google.com/d/forum/spack</a>\n</li>\n<li>\n<strong>Twitter</strong>: <a\
    \ href=\"https://twitter.com/spackpm\" rel=\"nofollow\">@spackpm</a>. Be sure\
    \ to\n<code>@mention</code> us!</li>\n</ul>\n<h2>\n<a id=\"user-content-contributing\"\
    \ class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contributing</h2>\n<p>Contributing\
    \ to Spack is relatively easy.  Just send us a\n<a href=\"https://help.github.com/articles/using-pull-requests/\"\
    >pull request</a>.\nWhen you send your request, make <code>develop</code> the\
    \ destination branch on the\n<a href=\"https://github.com/spack/spack\">Spack\
    \ repository</a>.</p>\n<p>Your PR must pass Spack's unit tests and documentation\
    \ tests, and must be\n<a href=\"https://www.python.org/dev/peps/pep-0008/\" rel=\"\
    nofollow\">PEP 8</a> compliant.  We enforce\nthese guidelines with our CI process.\
    \ To run these tests locally, and for\nhelpful tips on git, see our\n<a href=\"\
    https://spack.readthedocs.io/en/latest/contribution_guide.html\" rel=\"nofollow\"\
    >Contribution Guide</a>.</p>\n<p>Spack's <code>develop</code> branch has the latest\
    \ contributions. Pull requests\nshould target <code>develop</code>, and users\
    \ who want the latest package versions,\nfeatures, etc. can use <code>develop</code>.</p>\n\
    <h2>\n<a id=\"user-content-releases\" class=\"anchor\" href=\"#releases\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Releases</h2>\n\
    <p>For multi-user site deployments or other use cases that need very stable\n\
    software installations, we recommend using Spack's\n<a href=\"https://github.com/spack/spack/releases\"\
    >stable releases</a>.</p>\n<p>Each Spack release series also has a corresponding\
    \ branch, e.g.\n<code>releases/v0.14</code> has <code>0.14.x</code> versions of\
    \ Spack, and <code>releases/v0.13</code> has\n<code>0.13.x</code> versions. We\
    \ backport important bug fixes to these branches but\nwe do not advance the package\
    \ versions or make other changes that would\nchange the way Spack concretizes\
    \ dependencies within a release branch.\nSo, you can base your Spack deployment\
    \ on a release branch and <code>git pull</code>\nto get fixes, without the package\
    \ churn that comes with <code>develop</code>.</p>\n<p>The latest release is always\
    \ available with the <code>releases/latest</code> tag.</p>\n<p>See the <a href=\"\
    https://spack.readthedocs.io/en/latest/developer_guide.html#releases\" rel=\"\
    nofollow\">docs on releases</a>\nfor more details.</p>\n<h2>\n<a id=\"user-content-code-of-conduct\"\
    \ class=\"anchor\" href=\"#code-of-conduct\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Code of Conduct</h2>\n<p>Please\
    \ note that Spack has a\n<a href=\".github/CODE_OF_CONDUCT.md\"><strong>Code of\
    \ Conduct</strong></a>. By participating in\nthe Spack community, you agree to\
    \ abide by its rules.</p>\n<h2>\n<a id=\"user-content-authors\" class=\"anchor\"\
    \ href=\"#authors\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Authors</h2>\n<p>Many thanks go to Spack's <a href=\"\
    https://github.com/spack/spack/graphs/contributors\">contributors</a>.</p>\n<p>Spack\
    \ was created by Todd Gamblin, <a href=\"mailto:tgamblin@llnl.gov\">tgamblin@llnl.gov</a>.</p>\n\
    <h3>\n<a id=\"user-content-citing-spack\" class=\"anchor\" href=\"#citing-spack\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Citing Spack</h3>\n<p>If you are referencing Spack in a publication,\
    \ please cite the following paper:</p>\n<ul>\n<li>Todd Gamblin, Matthew P. LeGendre,\
    \ Michael R. Collette, Gregory L. Lee,\nAdam Moody, Bronis R. de Supinski, and\
    \ W. Scott Futral.\n<a href=\"https://www.computer.org/csdl/proceedings/sc/2015/3723/00/2807623.pdf\"\
    \ rel=\"nofollow\"><strong>The Spack Package Manager: Bringing Order to HPC Software\
    \ Chaos</strong></a>.\nIn <em>Supercomputing 2015 (SC\u201915)</em>, Austin, Texas,\
    \ November 15-20 2015. LLNL-CONF-669890.</li>\n</ul>\n<h2>\n<a id=\"user-content-license\"\
    \ class=\"anchor\" href=\"#license\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>License</h2>\n<p>Spack is distributed\
    \ under the terms of both the MIT license and the\nApache License (Version 2.0).\
    \ Users may choose either license, at their\noption.</p>\n<p>All new contributions\
    \ must be made under both the MIT and Apache-2.0\nlicenses.</p>\n<p>See <a href=\"\
    https://github.com/spack/spack/blob/develop/LICENSE-MIT\">LICENSE-MIT</a>,\n<a\
    \ href=\"https://github.com/spack/spack/blob/develop/LICENSE-APACHE\">LICENSE-APACHE</a>,\n\
    <a href=\"https://github.com/spack/spack/blob/develop/COPYRIGHT\">COPYRIGHT</a>,\
    \ and\n<a href=\"https://github.com/spack/spack/blob/develop/NOTICE\">NOTICE</a>\
    \ for details.</p>\n<p>SPDX-License-Identifier: (Apache-2.0 OR MIT)</p>\n<p>LLNL-CODE-811652</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1580743748.0
mpbelhorn/olcf-spack-environments:
  data_format: 2
  description: Spack environments for OLCF resources.
  filenames:
  - hosts/cirrus/envs/base/spack.yaml
  - hosts/bones/envs/base/spack.yaml
  - hosts/borg/envs/base/spack.yaml
  - hosts/crusher/envs/base/spack.yaml
  - hosts/summit/envs/base/spack.yaml
  - hosts/spock/envs/base/spack.yaml
  - hosts/afw/envs/base/spack.yaml
  - hosts/frontier/envs/base/spack.yaml
  - hosts/peak/envs/base/spack.yaml
  - hosts/ascent/envs/base/spack.yaml
  - hosts/andes/envs/base/spack.yaml
  full_name: mpbelhorn/olcf-spack-environments
  latest_release: null
  readme: '<h1>

    <a id="user-content-olcf-spack-environments" class="anchor" href="#olcf-spack-environments"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>OLCF
    Spack Environments</h1>

    <p>This repo contains the infrastructure and environment definitions to deploy

    site-provided software on OLCF resources via Spack environments.</p>

    <h2>

    <a id="user-content-getting-started" class="anchor" href="#getting-started" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Getting Started</h2>

    <p>Clone this repo and it''s facility-modified spack fork somewhere on an OLCF

    filesystem:</p>

    <pre><code>git clone --recurse-submodules git@github.com:mpbelhorn/olcf-spack-environments.git

    </code></pre>

    <p>or</p>

    <pre><code>git clone --recurse-submodules https://github.com/mpbelhorn/olcf-spack-environments

    </code></pre>

    <p>Next, initialize spack and the build environment. This is done by calling</p>

    <pre><code>FACSPACK_MY_ENVS=/path/to/host-specific/private/envs FACSPACK_ENV_NAME=base
    . ./init-facility-spack.sh

    </code></pre>

    <p>This will configure the spack build- and run-time environment build and install

    the facility spack environment <code>FACSPACK_ENV_NAME</code> tracked by this
    repo for the

    current machine in a private location under <code>FACSPACK_MY_ENVS</code>. Both
    of these

    variables are optional. If omitted, each variable will take on their default

    values:</p>

    <pre><code>FACSPACK_MY_ENVS="/sw/${_THIS_HOST}/spack-envs"

    FACSPACK_ENV_NAME="base"

    </code></pre>

    <p>such that sourcing this script by itself</p>

    <pre><code>. ./init-facility-spack.sh

    </code></pre>

    <p>will setup the runtime shell environment to manipulate the production spack

    environment on the current system.</p>

    <p>This repo will always track at least one spack environment per machine named

    <code>base</code> which is the complete standard software environment used in
    production

    for that machine. Furthermore, only the user account with owner permissions on

    the production environment may be used to manipulate it in the default

    <code>FACSPACK_MY_ENVS</code>.  This is an intentional safety mechanism to prevent
    multiple

    users from concurrently modifying the production environment. Users may set an

    alternate <code>FACSPACK_MY_ENVS</code> under which they can run build tests using
    any

    tracked <code>hosts/${_THIS_HOST}/${FACSPACK_ENV_NAME}/spack.yaml</code> file
    in this repo.</p>

    <p>From these variables, a unique path per each environment name will be

    constructed:</p>

    <pre><code>FACSPACK_ENV="${FACSPACK_MY_ENVS}/${FACSPACK_ENV_NAME}"

    </code></pre>

    <p>The value of <code>${_THIS_HOST}</code> is determined automatically from the
    hostname on

    which the init script is being run. For each system and environment tracked in

    this repo that you wish to work on, ensure that the final expanded value of

    <code>FACSPACK_ENV</code> corresponds to an actual existing directory.</p>

    <p>Configuration paths in our <code>spack.yaml</code> environments that are not
    fixed to

    universal values are expressed in terms of relative paths to either the spack

    instance setup by <code>init-facility-spack</code> or the path to the <code>FACSPACK_MY_ENVS</code>.

    These paths are referenced in the <code>spack.yaml</code> files via environment
    variables

    set by <code>init-facility-spack</code>. This allows the <code>spack.yaml</code>
    environment files to

    define portable and relocatable spack environments which can be re-deployed in

    arbitrary private locations by any users without needing to modify the

    environment file.</p>

    <p>The following variables are exported in Spack''s runtime environment by

    <code>init-facility-spack</code> and can be referred to in the <code>spack.yaml</code>
    the enviornment

    files tracked in this repo.</p>

    <ul>

    <li>

    <code>${FACSPACK_ENV}</code>:

    Path to where spack environment will be installed. Contains subdirs <code>opt</code>

    and <code>modules</code>.</li>

    <li>

    <code>${FACSPACK_ENV_MODULEROOT}</code>:

    Shortcut to <code>${FACSPACK_ENV}/modules</code> under which static and

    spack-generated modules are generated. Contains subdirectories <code>spack</code>,

    <code>flat</code>, and <code>site</code> corresponding to lmod, tcl, and static
    modulefiles

    respectively.</li>

    <li>

    <code>${FACSPACK_CONF_COMMON}</code>:

    Path to facility-wide common configuration files under <code>${this_repo}/share</code>.</li>

    <li>

    <code>${FACSPACK_CONF_HOST}</code>:

    Path to host-specific configuration files under <code>${this_repo}/hosts/${_THIS_HOST}</code>

    </li>

    </ul>

    <p>There are (as of spack v0.15.0) a couple exceptional paths used in <code>spack.yaml</code>

    files which cannot de-reference environment variables. These affect</p>

    <ul>

    <li>Mirrors</li>

    <li>Extensions</li>

    </ul>

    <p>Spack does not internally expand environment variables in the configuration
    of

    these items so they must be expressed as hard-coded full path strings. The

    default values in this repo should point to permanent world-readable paths on

    the OLCF filesystem populated with OLCF-maintained extensions and mirrors.</p>

    <h2>

    <a id="user-content-spack-fork" class="anchor" href="#spack-fork" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Spack Fork</h2>

    <p>The upstream development branch of spack is not used directly. Instead, the
    OLCF

    has implemented some customizations that are tracked in the "olcf-X.Y.Z"

    branches of a <a href="https://github.com/mpbelhorn/olcf-spack/tree/olcf-0.15.0">facility
    fork of spack</a>

    where <code>X.Y.Z</code> refers to the tagged release of upstream spack from which
    the

    OLCF-modified branch is forked.</p>

    '
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1645668126.0
ndevelder/cmb:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: ndevelder/cmb
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1647450577.0
qcif-training/singularity-containers:
  data_format: 2
  description: Introduction to Containers
  filenames:
  - demos/spack_blast/spack.yaml
  full_name: qcif-training/singularity-containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-readme" class="anchor" href="#readme" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Readme</h1>

    '
  stargazers_count: 3
  subscribers_count: 0
  topics:
  - containers
  - hpc
  - reproducibility
  updated_at: 1653096562.0
range3/chfs-containers:
  data_format: 2
  description: null
  filenames:
  - spack/envs/chfs/spack.yaml
  full_name: range3/chfs-containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-chfs-containers" class="anchor" href="#chfs-containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>chfs-containers</h1>

    <h2>

    <a id="user-content-example" class="anchor" href="#example" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>example</h2>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    explicitly pull the latest chfs image </span>

    docker pull range3/chfs:master


    git clone https://github.com/range3/chfs-containers

    <span class="pl-c1">cd</span> chfs-containers


    <span class="pl-c"><span class="pl-c">#</span> start servers</span>

    docker-compose up -d


    <span class="pl-c"><span class="pl-c">#</span> start another container for client</span>

    docker run -it --rm --network chfs_net --privileged range3/chfs:master bash

    <span class="pl-c"><span class="pl-c">#</span> set CHFS_SERVER env</span>

    <span class="pl-k">export</span> CHFS_SERVER=<span class="pl-s"><span class="pl-pds">$(</span>chlist
    -c -s ofi+sockets://172.30.0.3:50000<span class="pl-pds">)</span></span>


    <span class="pl-c"><span class="pl-c">#</span> list chfs servers</span>

    chlist


    <span class="pl-c"><span class="pl-c">#</span> mount chfs via FUSE</span>

    mkdir /tmp/m

    chmkdir /tmp/m

    chfuse -o direct_io,modules=subdir,subdir=<span class="pl-s"><span class="pl-pds">"</span>/tmp/m<span
    class="pl-pds">"</span></span> /tmp/m


    <span class="pl-c"><span class="pl-c">#</span> &lt;ctrl-D&gt;</span>

    <span class="pl-c"><span class="pl-c">#</span> the client container is removed</span>


    <span class="pl-c"><span class="pl-c">#</span> stop and remove server containers</span>

    docker-compose down</pre></div>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1652094301.0
range3/spack-playground:
  data_format: 2
  description: null
  filenames:
  - spack/envs/dev/spack.yaml
  full_name: range3/spack-playground
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-spack-playground\" class=\"anchor\" href=\"\
    #spack-playground\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>spack-playground</h1>\n<h2>\n<a id=\"user-content-development\"\
    \ class=\"anchor\" href=\"#development\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Development</h2>\n<div class=\"\
    highlight highlight-source-shell\"><pre><span class=\"pl-c1\">cd</span> /workspaces/spack-playground\n\
    spack env activate -d spack/envs/dev\nspack install --keep-stage</pre></div>\n\
    <h2>\n<a id=\"user-content-activate-intellisense-provided-by-clangd\" class=\"\
    anchor\" href=\"#activate-intellisense-provided-by-clangd\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>activate\
    \ IntelliSense provided by clangd</h2>\n<ul>\n<li>the vsode extensions are already\
    \ installed in the dev container.</li>\n<li>open vscode command palette\n<ul>\n\
    <li><code>&gt; clangd: Download language server</code></li>\n<li><code>&gt; Developper:\
    \ Reload Window</code></li>\n</ul>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-create-new-spack-env-if-you-want\"\
    \ class=\"anchor\" href=\"#create-new-spack-env-if-you-want\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Create new\
    \ spack env if you want</h2>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> /workspaces/spack-playground\nspack env\
    \ create -d spack/envs/dev2\nspack env activate -d spack/envs/dev2\nspack compiler\
    \ find\nspack external find\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ edit spack/envs/dev2/spack.yaml</span>\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span># suggestion: remove openssl and python from external packages</span>\n\
    spack concretize -f\nspack install --keep-stage</pre></div>\n<h2>\n<a id=\"user-content-3rd-party-library-license\"\
    \ class=\"anchor\" href=\"#3rd-party-library-license\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>3rd Party Library\
    \ License</h2>\n<h3>\n<a id=\"user-content-akka-httpsgithubcomakkaakka\" class=\"\
    anchor\" href=\"#akka-httpsgithubcomakkaakka\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Akka (<a href=\"https://github.com/akka/akka\"\
    >https://github.com/akka/akka</a>)</h3>\n<details><summary>Apache 2 license</summary>\n\
    <pre><code>This software is licensed under the Apache 2 license, quoted below.\n\
    \nCopyright 2009-2018 Lightbend Inc. &lt;https://www.lightbend.com&gt;\n\nLicensed\
    \ under the Apache License, Version 2.0 (the \"License\"); you may not\nuse this\
    \ file except in compliance with the License. You may obtain a copy of\nthe License\
    \ at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable\
    \ law or agreed to in writing, software\ndistributed under the License is distributed\
    \ on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either\
    \ express or implied. See the\nLicense for the specific language governing permissions\
    \ and limitations under\nthe License.\n</code></pre>\n</details>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1639559107.0
robertu94/libpressio-sperr:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: robertu94/libpressio-sperr
  latest_release: null
  readme: '<h1>

    <a id="user-content-libpressio-sperr" class="anchor" href="#libpressio-sperr"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>LibPressio-SPERR</h1>

    <p>A LibPressio compressor plugin for SPERR. Packaged seperately because of GPL
    Licensing</p>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>Via Spack</p>

    <pre><code>git clone https://github.com/robertu94/spack_packages robertu94_packages

    spack repo add ./robertu94_packages


    spack install libpressio-sperr

    </code></pre>

    <p>Manually Via CMake</p>

    <pre><code># install cmake, sperr, libpressio and dependencies first


    cmake -S . -B build -DCMAKE_INSTALL_PREFIX

    cmake --build build

    cmake --install

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1653608149.0
robertu94/libpressio_adios2:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: robertu94/libpressio_adios2
  latest_release: null
  readme: '<h1>

    <a id="user-content-libpressio-adios2-io-plugin" class="anchor" href="#libpressio-adios2-io-plugin"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>LibPressio
    ADIOS2 IO Plugin</h1>

    <p>An experimental plugin to read ADIOS2 files in LibPressio use at your own risk</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1652738176.0
robertu94/sz-zfp-zchecker:
  data_format: 2
  description: container for the ISC/SC compression tutorial
  filenames:
  - spack.yaml
  full_name: robertu94/sz-zfp-zchecker
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1652997619.0
salotz/scopes-chipmunk2d:
  data_format: 2
  description: Scopes language wrapper of Chipmunk2D
  filenames:
  - spack.yaml
  full_name: salotz/scopes-chipmunk2d
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-scopes-chipmunk2d\" class=\"anchor\" href=\"\
    #scopes-chipmunk2d\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>scopes-chipmunk2d</h1>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<p>The module\
    \ is under <code>src/chipmunk2d</code>. You can copy this subtree into your\n\
    project and then add it to the <code>package.path</code> in your Scopes\n<code>_project.sc</code>\
    \ file.</p>\n<h3>\n<a id=\"user-content-with-spack\" class=\"anchor\" href=\"\
    #with-spack\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>With Spack</h3>\n<p>This module is available as the\
    \ <code>scopes-chipmunk2d</code> package in the\n<a href=\"https://github.com/salotz/snailpacks\"\
    >snailpacks</a> repository. This will pull in the necessary dependencies\nincluding\
    \ Scopes.</p>\n<div class=\"highlight highlight-source-shell\"><pre>  spack install\
    \ scopes-chipmunk2d</pre></div>\n<p>See the <a href=\"https://github.com/salotz/snailpacks\"\
    >snailpacks</a> documentation for more best practices of installing.</p>\n<h2>\n\
    <a id=\"user-content-development-environment\" class=\"anchor\" href=\"#development-environment\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Development Environment</h2>\n<p>We use <a href=\"https://spack.io/\"\
    \ rel=\"nofollow\">Spack</a> to install dependencies. First install Spack.</p>\n\
    <p>Then you'll need our custom repo of build recipes:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>  mkdir -p <span class=\"pl-s\"><span class=\"\
    pl-pds\">`</span>/.spack/repos</span>\n<span class=\"pl-s\">  git clone git@github.com:salotz/snailpacks.git\
    \ <span class=\"pl-pds\">`</span></span>/.spack/repos/snailpacks\n  spack repo\
    \ add <span class=\"pl-s\"><span class=\"pl-pds\">`</span>/resources/spack-repos/snailpacks</span></pre></div>\n\
    <p>Then you need to create an environment in this folder that will\ncontain the\
    \ headers and libraries etc.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>  spack env create -d <span class=\"pl-c1\">.</span></pre></div>\n<p>Activate\
    \ the environment (i.e. set the environment variables\nappropriately) and install\
    \ the packages:</p>\n<div class=\"highlight highlight-source-shell\"><pre>  spacktivate\
    \ <span class=\"pl-c1\">.</span>\n  spack install</pre></div>\n<p>To exit the\
    \ environment (i.e. unset the env variables):</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>  despacktivate</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - scopes-lang
  - chipmunk2d
  updated_at: 1648788744.0
salotz/scopes-lib_copier-template:
  data_format: 2
  description: Copier template for a Scopes library
  filenames:
  - template/spack.yaml
  full_name: salotz/scopes-lib_copier-template
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-project-template-for-a-scopes-lang-library\"\
    \ class=\"anchor\" href=\"#project-template-for-a-scopes-lang-library\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Project\
    \ Template for a Scopes Lang Library</h1>\n<p>This is a project template generator\
    \ and updater using the\n<a href=\"https://github.com/copier-org/copier/\">copier</a>\
    \ tool for creating libraries for the <a href=\"http://scopes.rocks\" rel=\"nofollow\"\
    >Scopes</a> programming language.</p>\n<p>Please install from the latest copier\
    \ for this to work, not the latest\nstable release. Currently I am using\n<a href=\"\
    https://github.com/pypa/pipx\">pipx</a>:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>pipx install git+https://github.com/copier-org/copier.git@e98314063246993532048ba2ecf80a049154d2f6</pre></div>\n\
    <h2>\n<a id=\"user-content-generating-a-project\" class=\"anchor\" href=\"#generating-a-project\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Generating a Project</h2>\n<p>Then you can generate your project:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>copier <span class=\"pl-s\"\
    ><span class=\"pl-pds\">'</span>gh:salotz/scopes-lib_copier-template<span class=\"\
    pl-pds\">'</span></span> ./</pre></div>\n<p>This should generate the following\
    \ (<code>repo_name = my-lib</code>):</p>\n<pre><code>my-lib\n\u251C\u2500\u2500\
    \ __env.sc\n\u251C\u2500\u2500 Makefile\n\u251C\u2500\u2500 README.md\n\u251C\u2500\
    \u2500 spack.yaml\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 my-lib\n   \
    \     \u251C\u2500\u2500 init.sc\n        \u2514\u2500\u2500 sanity.sc\n</code></pre>\n\
    <h2>\n<a id=\"user-content-development-environment\" class=\"anchor\" href=\"\
    #development-environment\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Development Environment</h2>\n<p>Create a <a\
    \ href=\"https://spack.io/\" rel=\"nofollow\">Spack</a> environment and install\n\
    dependencies</p>\n<pre><code>cd my-lib\nspack env create -d .\nspacktivate .\n\
    spack install\n</code></pre>\n<p>If you need more you can add them to <code>spack.yaml</code>.</p>\n\
    <p>Then you should be able to run the sanity check:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>scopes -e -m my-lib.sanity</pre></div>\n<p>Start\
    \ coding!</p>\n<h2>\n<a id=\"user-content-libraries-using-this-template\" class=\"\
    anchor\" href=\"#libraries-using-this-template\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Libraries Using this Template</h2>\n\
    <ul>\n<li><a href=\"https://github.com/salotz/raylib-scopes\">scopes-raylib</a></li>\n\
    <li><a href=\"https://github.com/salotz/scopes-chipmunk2d\">scopes-chipmunk2d</a></li>\n\
    </ul>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - copier-template
  - scopes-lang
  updated_at: 1648781021.0
salotz/snailpacks:
  data_format: 2
  description: Spack repo for multimedia development
  filenames:
  - examples/scopes/spack.yaml
  full_name: salotz/snailpacks
  latest_release: null
  stargazers_count: 1
  subscribers_count: 1
  topics:
  - spack
  - spack-repo
  - scopes-lang
  - multimedia
  - game-development
  - package-manager
  - development-environment
  updated_at: 1648089720.0
scs-lab/ChronoLog:
  data_format: 2
  description: 'ChronoLog: A High-Performance Storage Infrastructure for Activity
    and Log Workloads'
  filenames:
  - CI/enviroment/spack.yaml
  full_name: scs-lab/ChronoLog
  latest_release: null
  readme: '<h1>

    <a id="user-content-chronolog" class="anchor" href="#chronolog" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>ChronoLog</h1>

    <p>ChronoLog: A High-Performance Storage Infrastructure for Activity and Log Workloads
    (NSF CSSI 2104013)</p>

    <h2>

    <a id="user-content-chronolog-project-synopsis" class="anchor" href="#chronolog-project-synopsis"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>ChronoLog
    Project Synopsis</h2>

    <p>This project will design and implement ChronoLog, a distributed and tiered
    shared log storage ecosystem. ChronoLog uses physical time to distribute log entries
    while providing total log ordering. It also utilizes multiple storage tiers to
    elastically scale the log capacity (i.e., auto-tiering). ChronoLog will serve
    as a foundation for developing scalable new plugins, including a SQL-like query
    engine for log data, a streaming processor leveraging the time-based data distribution,
    a log-based key-value store, and a log-based TensorFlow module.</p>

    <h2>

    <a id="user-content-workloads-and-applications" class="anchor" href="#workloads-and-applications"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Workloads
    and Applications</h2>

    <p>Modern applications spanning from Edge to High Performance Computing (HPC)
    systems, produce and process log data and create a plethora of workload characteristics
    that rely on a common storage model: <strong>the distributed shared log</strong>.</p>

    <p><a href="/doc/images/log_centric_paradigm.svg" target="_blank" rel="noopener
    noreferrer"><img src="/doc/images/log_centric_paradigm.svg" alt="Log centric paradigm"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-features" class="anchor" href="#features" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Features</h2>

    <p><a href="/doc/images/feature-matrix.png" target="_blank" rel="noopener noreferrer"><img
    src="/doc/images/feature-matrix.png" alt="Feature matrix" style="max-width:100%;"></a></p>

    <hr>

    <h1>

    <a id="user-content-coming-soon-" class="anchor" href="#coming-soon-" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Coming soon ...</h1>

    <p>For more details about the ChronoLog project, please visit our website <a href="http://www.cs.iit.edu/~scs/assets/projects/ChronoLog/ChronoLog.html"
    rel="nofollow">http://www.cs.iit.edu/~scs/assets/projects/ChronoLog/ChronoLog.html</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1646325735.0
simonpintarelli/nlcglib:
  data_format: 2
  description: Nonlinear CG methods for wave-function optimization in DFT
  filenames:
  - spack-envs/q-e-sirius-cuda/spack.yaml
  - spack-envs/q-e-sirius-cpu-only/spack.yaml
  full_name: simonpintarelli/nlcglib
  latest_release: null
  stargazers_count: 4
  subscribers_count: 2
  topics: []
  updated_at: 1646750517.0
smutch/regrider:
  data_format: 2
  description: Downsample gbpTrees and VELOCIraptor cartesian grids using FFTW
  filenames:
  - spack.yaml
  full_name: smutch/regrider
  latest_release: null
  readme: '<h1>

    <a id="user-content-regrider" class="anchor" href="#regrider" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Regrider</h1>

    <p>Downsample 3D cartesian grids using FFTW.</p>

    <p>Natively handles gbpTrees and VELOCIraptor files.</p>

    <h2>

    <a id="user-content-todo" class="anchor" href="#todo" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>TODO</h2>

    <ul>

    <li>[X] Malloc orig using FFTW (with inplace padding)</li>

    <li>[X] Do an inplace FFT</li>

    <li>[X] Convolution</li>

    <li>[X] Inverse FFTW</li>

    <li>[X] Reshuffle data inplace</li>

    <li>[X] Write the data back out</li>

    <li>[X] Get rid of choice of grid and convert all grids</li>

    <li>[X] Documentation</li>

    <li>[X] Do an implementation for VELOCIraptor</li>

    <li>[ ] Proper tests for known solutions</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1627973220.0
srini009/serviz:
  data_format: 2
  description: Ascent visualization microservice built using the Mochi software stack
  filenames:
  - spack.yaml
  full_name: srini009/serviz
  latest_release: null
  readme: '<h1>

    <a id="user-content-serviz-a-shared-in-situ-visualization-service" class="anchor"
    href="#serviz-a-shared-in-situ-visualization-service" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>SERVIZ: A Shared In
    Situ Visualization Service</h1>

    <p>This is an experimental repo implementing a distributed Ascent visualization
    microservice.</p>

    <p>Inline and in transit visualization have arisen as popular models of in situ
    visualization for high performance computing (HPC) applications. Inline visualization
    is invoked through a library call on the HPC application (simulation code), while
    in transit methods involve the invocation of the visualization module on in transit
    resources. Compared to inline methods, in transit methods have the flexibility
    to run at a lower level of concurrency than the simulation code, allowing them
    to offer better efficiency for the visualization operation. The state-of-the-art
    in transit schemes are limited to employing a dedicated in transit resource for
    every HPC application.

    This results in significant idle time on the in transit resource and severely
    limits the cost savings that can be achieved over the inline model.

    This research proposes that a single, in transit visualization service be shared
    amongst multiple HPC applications to make efficient use of the in transit resources
    by reducing the idle time. We realize this idea through SERVIZ, a shared in transit
    visualization service. SERVIZ achieves cost savings of up to 40% over inline (at
    scale) and up to 4x reduction in idle time compared to a dedicated in transit
    implementation.

    In all, the results from this work identify that a shared in transit resource
    is an attractive approach for cost efficiency.</p>

    <p>SERVIZ is a hybrid MPI + RPC visualization program that can be partitioned
    into multiple "instances" that are each capable

    of serving multiple clients simultaneously. RPC is used to transfer simulation
    data to the SERVIZ instance, and MPI is subsequently used to

    parallelize the visualization operation.

    <a href="SERVIZ.svg" target="_blank" rel="noopener noreferrer"><img src="SERVIZ.svg"
    alt="SERVIZ" style="max-width:100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1649894816.0
srini009/serviz-installation-instructions:
  data_format: 2
  description: Repository containing the installation instructions and customization
    scripts
  filenames:
  - spack_environment_recipe/spack.yaml
  full_name: srini009/serviz-installation-instructions
  latest_release: null
  readme: '<p>Repository containing the installation instructions and customization
    scripts for SC 2022 paper427: "SERVIZ: A Shared In Situ Visualization Service"</p>

    <h2>

    <a id="user-content-note-it-is-important-to-follow-these-steps-in-the-exact-order-specified-to-correctly-set-up-the-software-components-for-reproducibility"
    class="anchor" href="#note-it-is-important-to-follow-these-steps-in-the-exact-order-specified-to-correctly-set-up-the-software-components-for-reproducibility"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Note:
    It is important to follow these steps in the exact order specified to correctly
    set up the software components for reproducibility.</h2>

    <h2>

    <a id="user-content-note-the-instructions-here-assume-that-you-are-logged-in-to-the-theta-cluster-at-alcf"
    class="anchor" href="#note-the-instructions-here-assume-that-you-are-logged-in-to-the-theta-cluster-at-alcf"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Note:
    The instructions here assume that you are logged in to the Theta cluster at ALCF</h2>

    <h2>

    <a id="user-content-note-it-is-assumed-that-the-directory-structure-looks-like-the-following"
    class="anchor" href="#note-it-is-assumed-that-the-directory-structure-looks-like-the-following"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Note:
    It is assumed that the directory structure looks like the following:</h2>

    <ul>

    <li>$HOME/serviz-installation-instructions/</li>

    <li>$HOME/amr-wind/</li>

    <li>$HOME/serviz/</li>

    <li>$HOME/amr-wind-experiments/</li>

    <li>$HOME/mochi-spack-packages/</li>

    </ul>

    <h3>

    <a id="user-content-step-1-installation-of-radical-pilot-components" class="anchor"
    href="#step-1-installation-of-radical-pilot-components" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Step 1: Installation
    of radical-pilot components:</h3>

    <p>First, go over the installation instructions: <a href="https://radicalpilot.readthedocs.io/en/stable/installation.html"
    rel="nofollow">https://radicalpilot.readthedocs.io/en/stable/installation.html</a>.
    The following instructions assume that you have already installed and have a working
    MongoDB installation, Python, Conda,

    and other requirements for radical-pilot setup and working. The instructions that
    follow are only for customizing radical-pilot for SERVIZ.</p>

    <ol>

    <li>Install radical-saga@1.12.0 using the command: <code>pip install radical.saga==1.12.0</code>

    </li>

    <li>Install radical-utils@1.12.0 using the command: <code>pip install radical.utils==1.12.0</code>

    </li>

    <li>The third component, radical-pilot would require a custom installation. For
    this:

    <ul>

    <li>First download the radical.pilot github repo locally using git: <code>git
    clone https://github.com/radical-cybertools/radical.pilot.git@v1.12.0</code>

    </li>

    <li>Run: <code>cd radical-pilot &amp;&amp; cp ../serviz-installation-instructions/radical-pilot-customization/aprun.py
    ./src/radical/pilot/agent/launch_method/aprun.py</code>

    </li>

    <li>Run: <code>vi ./src/radical/pilot/agent/launch_method/aprun.py</code>, and
    look for the line that says "REPLACEME". You would need to create a protection
    domain on Theta and use that protection domain name here.</li>

    <li>Assuming you are still in $HOME/radical-pilot, run: <code>pip install .</code>

    </li>

    <li>That should install the "modified" radical-pilot stack. Check that the entire
    radical-stack has installed correctly at version 1.12.0 by running <code>radical-stack</code>

    </li>

    <li>Copy the Theta resource JSON config: <code>cp ../serviz-installation-instructions/radical-pilot-customization/resource_anl.json
    ~/.radical/pilot/configs/resource_anl.json</code>. This would override the default
    JSON configuration file to tell radical-pilot to only use 60 out of the total
    64 cores on each Theta KNL node.</li>

    <li>At this point, run a small test program to ensure that you are able to use
    radical-pilot along with the MongoDB installation to submit and run a batch job
    ensemble on Theta.</li>

    </ul>

    </li>

    </ol>

    <h3>

    <a id="user-content-step-2-installation-of-custom-spack-and-mochi-spack-packages"
    class="anchor" href="#step-2-installation-of-custom-spack-and-mochi-spack-packages"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step
    2: Installation of custom spack and mochi-spack-packages:</h3>

    <ol>

    <li>Download and install spack: <a href="https://spack.io/" rel="nofollow">https://spack.io/</a>

    </li>

    <li>Assuming that spack is download at $HOME/spack, <code>cd $HOME/spack</code>.</li>

    <li>Copy the spack packages file into your local spack directory: ```cp  $HOME/serviz-installation-instructions/spack_builtin_repo_customization/packages.yaml
    ~/.spack/cray/packages.yaml</li>

    <li>What we need to do next is to customize some spack built-in packages. The
    recipe for the spack built-in packages are found in <code>$HOME/spack/var/spack/repos/builtin/packages/*</code>

    </li>

    <li>For each of the three packages (ascent, conduit, and vtk-h) in <code>../serviz-installation-instructions/spack_builtin_repo_customization/</code>,
    copy-paste the <code>package.py</code> inside each of them to the corresponding
    spack built-in package directories in <code>$HOME/spack/var/spack/repos/builtin/packages/*</code>

    </li>

    <li>Add the custom mochi-spack-packages repo: <code>cd ../mochi-spack-packages
    &amp;&amp; git checkout experimental &amp;&amp; spack repo add .</code>

    </li>

    <li>Note that the <code>experimental</code> branch for this repo needs to be used.
    Verify that the spack repo got added successfully by running: <code>spack info
    mochi-symbiomon</code>. If you see some valid output, you are good to go!</li>

    </ol>

    <h3>

    <a id="user-content-step-3-installation-of-serviz-microservice-and-its-dependencies"
    class="anchor" href="#step-3-installation-of-serviz-microservice-and-its-dependencies"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step
    3: Installation of SERVIZ microservice and its dependencies:</h3>

    <h4>

    <a id="user-content-note-at-this-point-make-sure-your-environment-has-the-right-compilers-gcc930-conda-programming-environments-and-spack-environments-correctly-loaded-to-look-at-a-reference-file-see-homeserviz-installation-instructionsspack_environment_recipetheta_sourcemesh"
    class="anchor" href="#note-at-this-point-make-sure-your-environment-has-the-right-compilers-gcc930-conda-programming-environments-and-spack-environments-correctly-loaded-to-look-at-a-reference-file-see-homeserviz-installation-instructionsspack_environment_recipetheta_sourcemesh"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Note:
    At this point, make sure your environment has the right compilers (gcc@9.3.0),
    Conda programming environments, and spack environments correctly loaded. To look
    at a reference file, see <code>$HOME/serviz-installation-instructions/spack_environment_recipe/theta_sourceme.sh</code>

    </h4>

    <ol>

    <li>Go to the SERVIZ github directory: <code>cd ../serviz</code>

    </li>

    <li>Create a spack environment using the already-provided spack.yaml file: <code>spack
    env create serviz spack.yaml</code>

    </li>

    <li>Install the environment using: ```spack install``</li>

    <li>Install the SERVIZ microservice using: <code>mkdir build &amp;&amp; cd build
    &amp;&amp; cmake .. -DENABLE_TESTS=OFF -DENABLE_EXAMPLES=ON -DENABLE_BEDROCK=OFF
    -DCMAKE_INSTALL_PREFIX=`pwd` -DCMAKE_C_COMPILER=cc -DCMAKE_CXX_COMPILER=CC</code>

    </li>

    <li>Login to the cmake shell: <code>cd build &amp;&amp; ccmake .</code>

    </li>

    <li>We would need to add the spack-installed <code>include</code> and <code>library</code>
    directories to <code>CMAKE_CXX_FLAGS</code>, <code>CMAKE_C_FLAGS</code>, <code>CMAKE_EXE_LINKER_FLAGS</code>,
    and <code>CMAKE_SHARED_LINKER_FLAGS</code> respectively. To see an example of
    what content to add, look inside <code>$HOME/serviz-installation-instructions/spack_environment_recipe/theta.cmake_options</code>

    </li>

    <li>Once this is done, run: <code>make -j20 &amp;&amp; make install</code>.</li>

    </ol>

    <h3>

    <a id="user-content-step-4-installation-of-custom-amr-wind" class="anchor" href="#step-4-installation-of-custom-amr-wind"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step
    4: Installation of custom AMR-WIND:</h3>

    <ol>

    <li>Download the custom amr-wind repository: <code>git clone --recursive https://github.com/srini009/amr-wind.git</code>

    </li>

    <li><code>mkdir -p $HOME/AMR_WIND_INSTALL</code></li>

    <li><code>cd $HOME/amr-wind</code></li>

    <li><code>mkdir build &amp;&amp; cd build</code></li>

    <li><code>cmake .. -DAMR_WIND_ENABLE_TESTS:BOOL=ON -DAMR_WIND_ENABLE_ASCENT:BOOL=ON
    -DAscent_DIR:PATH="/spack/path/to/ascent/install/lib/cmake/ascent" -DConduit_DIR:PATH="/spack/path/to/conduit/install"
    -DCMAKE_INSTALL_PREFIX=$HOME/AMR_WIND_INSTALL -DAMR_WIND_ENABLE_MPI:BOOL=ON -DAMR_WIND_ENABLE_MPI:BOOL=ON</code></li>

    <li><code>make -j20 &amp;&amp; make install</code></li>

    </ol>

    <h3>

    <a id="user-content-step-5-run-amr-wind-experiments-using-serviz-and-radical-pilot"
    class="anchor" href="#step-5-run-amr-wind-experiments-using-serviz-and-radical-pilot"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step
    5: Run AMR-WIND experiments using SERVIZ and RADICAL-PILOT</h3>

    <ol>

    <li>At this point you should have all the software components successfully installed
    and ready to run.</li>

    <li>Go to the amr-wind-experiments repo: <code>cd $HOME/amr-wind-experiments/</code>
    and start running the experiments by making adjustments to the  Python run scripts
    as necessary. These scripts are numbered based on the configurations that they
    represent.</li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1649904535.0
srini009/soma:
  data_format: 2
  description: 'SPINCER: A Shared Performance Analysis and Monitoring Microservice'
  filenames:
  - spack.yaml
  full_name: srini009/soma
  latest_release: null
  readme: '<h1>

    <a id="user-content-soma-a-service-based-observability-monitoring-and-analytics-framework-for-hpc"
    class="anchor" href="#soma-a-service-based-observability-monitoring-and-analytics-framework-for-hpc"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SOMA:
    A Service-based Observability, Monitoring, and Analytics Framework for HPC</h1>

    <p>This is an experimental repo containing a performance monitoring and analysis
    microservice

    for the TAU performance system. SOMA is designed using the Mochi software stack.</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1653524006.0
supercontainers/isc-tutorial:
  data_format: 2
  description: ISC 2022 -- Getting Started with Containers on HPC
  filenames:
  - exercises/spack_contenerize/spack.yaml
  - files/spack_contenerize/spack.yaml
  full_name: supercontainers/isc-tutorial
  latest_release: null
  readme: '<h1>

    <a id="user-content-getting-started-with-containers-on-hpc" class="anchor" href="#getting-started-with-containers-on-hpc"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Getting
    Started with Containers on HPC</h1>

    <p>View this on the <a href="https://supercontainers.github.io/isc-tutorial/"
    rel="nofollow">Tutorial Homepage</a>.</p>

    <h2>

    <a id="user-content-ecp-supercontainers-tutorial-session" class="anchor" href="#ecp-supercontainers-tutorial-session"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>ECP
    Supercontainers Tutorial Session</h2>

    <p><a href="fig/ecp.jpg" target="_blank" rel="noopener noreferrer"><img src="fig/ecp.jpg"
    width="250" style="max-width:100%;"></a><a href="fig/pawsey.jpeg" target="_blank"
    rel="noopener noreferrer"><img src="fig/pawsey.jpeg" width="250" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-details" class="anchor" href="#details" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Details</h2>

    <p>Half-day Tutorial Session</p>

    <p>Venue: International Supercomputing Conference (ISC 2022)</p>

    <p>Date: 29 May 2022 2:00pm - 6:00pm, Central European Summer Time CEST (GMT+2)</p>

    <p>Location: Hamburg, Germany</p>

    <p>Link: <a href="https://app.swapcard.com/widget/event/isc-high-performance-2022/planning/UGxhbm5pbmdfODYxMTU3"
    rel="nofollow">ISC 2022 Schedule</a></p>

    <p>Keywords: Containerized HPC, System Software and Runtime Systems, Scientific
    Software Development, DevOps</p>

    <h2>

    <a id="user-content-ec2-login" class="anchor" href="#ec2-login" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>EC2 Login</h2>

    <p>These will be provided the day of the tutorial.</p>

    <h2>

    <a id="user-content-abstract" class="anchor" href="#abstract" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Abstract</h2>

    <p>Container computing has revolutionized the way applications are developed and
    delivered.  It offers opportunities that never existed before for significantly
    improving efficiency of scientific workflows and easily moving these workflows
    from the laptop to the supercomputer.  Tools like Docker, Shifter, Singularity,
    Charliecloud and Podman enable a new paradigm for scientific and technical computing.  However,
    to fully unlock its potential, users and administrators need to understand how
    to utilize these new approaches.  This tutorial will introduce attendees to the
    basics of creating container images, explain best practices, and cover more advanced
    topics such as creating images to be run on HPC platforms using various container
    runtimes.  The tutorial will also explain how research scientists can utilize
    container-based computing to accelerate their research and how these tools can
    boost the impact of their research by enabling better reproducibility and sharing
    of their scientific process without compromising security.</p>

    <p>This is an updated version of the highly successful tutorial presented at SC16-21
    and ISC19-21.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>This is a hands-on tutorial.  Participants should bring a laptop and load or
    pre-install a terminal and/or ssh client in advance to make best use of time during
    the tutorial.  We will be providing training user accounts to both pre-configured
    EC2 instances.</p>

    <div><a href="fig/AWS_logo.png" target="_blank" rel="noopener noreferrer"><img
    src="fig/AWS_logo.png" width="250" style="max-width:100%;"></a></div>

    <p>This tutorial is supported by the Amazon AWS Machine Learning Research Awards.  EC2
    images and temporary login credentials will be distributed onsite at the tutorial.</p>

    <p>After the tutorial, you can boot our tutorial image yourself on Amazon EC2
    to run through the tutorial again. We recommend you use your own EC2 key and change
    the password.</p>

    <p>US-West-Oregon: ami-0fe12765123c6a840</p>

    <h3>

    <a id="user-content-optional-prerequisites" class="anchor" href="#optional-prerequisites"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Optional
    Prerequisites</h3>

    <p>Users can also install Docker and Singularity prior to attending the tutorial
    session.  Here, it may be beneficial to create Docker and Sylabs (Singularity)
    accounts in advance at <a href="https://cloud.docker.com/" rel="nofollow">https://cloud.docker.com/</a>
    and <a href="https://cloud.sylabs.io/" rel="nofollow">https://cloud.sylabs.io/</a>.  These
    accounts will be needed to create images on Docker Cloud/Dockerhub and Sylabs
    Cloud.</p>

    <p><a href="https://sylabs.io/guides/3.7/user-guide/" rel="nofollow">Install Singularity
    on Linux</a></p>

    <p><a href="https://repo.sylabs.io/desktop/" rel="nofollow">Install Singularity
    on Mac</a> (Alpha)</p>

    <p><a href="https://www.docker.com/products/docker-desktop" rel="nofollow">Install
    Docker for Desktop</a></p>

    <h2>

    <a id="user-content-questions" class="anchor" href="#questions" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Questions</h2>

    <p>You can ask questions verbally or with this <a href="https://docs.google.com/document/d/11gMZ-T7iA5XiRWPLYIqX7Gqv7RMb-NF9kzGYHrnOi04/edit?usp=sharing"
    rel="nofollow">Google Doc</a>.

    Please append your question below the others in the document.</p>

    <p>We have also created a Slack Team for this.  The invitation link is <a href="https://join.slack.com/t/hpc-containers/shared_invite/enQtODI3NzY1NDU4OTk5LTUxOTgyOWJmYjIwOWI5YWU2MzBhZDI3Zjc1YmZmMjAxZjgzYzk4ZWEwNmFlNzlkOWI0MGNlZDNlMTBhYTBlOWY"
    rel="nofollow">here</a>.</p>

    <h2>

    <a id="user-content-schedule-see-the-git-pages-site-for-the-autogenerated-version"
    class="anchor" href="#schedule-see-the-git-pages-site-for-the-autogenerated-version"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Schedule
    (See the git pages site for the autogenerated version)</h2>

    <p>14:00 - 14:15 Introduction to containers in HPC (Shane)<br>

    Including defining jargon (containers, images, registries/repos,..)</p>

    <p>14:15 - 14:55 Build and run your first container (Eduardo)<br>

    Basic of containers and understanding the OCI Image Spec</p>

    <p>14:55 - 15:30 Deploy containers on a supercomputer (Alexis)</p>

    <p>15:30 - 16:00 High-performance containers (Alexis)</p>

    <p>16:00 - 16:30 BREAK</p>

    <p>16:30 - 17:05 Best practices (Shane)</p>

    <p>17:05 - 17:35 E4S containers initiative (Sameer)</p>

    <p>17:35 - 17:55 Advanced container builds (Eduardo)</p>

    <p>17:55 - 18:00 Wrap-up and final Q&amp;A</p>

    '
  stargazers_count: 6
  subscribers_count: 8
  topics:
  - hpc
  - containers
  - singularity-container
  - singularity
  - shifter
  - docker
  - tutorial
  - supercomputer
  updated_at: 1653332288.0
sxs-collaboration/spectre:
  data_format: 2
  description: SpECTRE is a code for multi-scale, multi-physics problems in astrophysics
    and gravitational physics.
  filenames:
  - support/DevEnvironments/spack.yaml
  full_name: sxs-collaboration/spectre
  latest_release: v2022.05.05
  readme: "<p><a href=\"https://github.com/sxs-collaboration/spectre/blob/develop/LICENSE.txt\"\
    ><img src=\"https://camo.githubusercontent.com/83d3746e5881c1867665223424263d8e604df233d0a11aae0813e0414d433943/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667\"\
    \ alt=\"license\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-blue.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://en.wikipedia.org/wiki/C%2B%2B#Standardization\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a3dbfd7a9a0364af5f02772460bf69fce89f741e10fb7c8e9aa3f26a0d96cfe7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f632532422532422d31372d626c75652e737667\"\
    \ alt=\"Standard\" data-canonical-src=\"https://img.shields.io/badge/c%2B%2B-17-blue.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/sxs-collaboration/spectre/actions\"\
    ><img src=\"https://github.com/sxs-collaboration/spectre/workflows/Tests/badge.svg?branch=develop\"\
    \ alt=\"Build Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://coveralls.io/github/sxs-collaboration/spectre?branch=develop\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/e909837c9640462fc3f2587028319e5f5dc6198453d97af6b12696ddeb34930b/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f7378732d636f6c6c61626f726174696f6e2f737065637472652f62616467652e7376673f6272616e63683d646576656c6f70\"\
    \ alt=\"Coverage Status\" data-canonical-src=\"https://coveralls.io/repos/github/sxs-collaboration/spectre/badge.svg?branch=develop\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://codecov.io/gh/sxs-collaboration/spectre\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2b0d1a9c279878e18a98627f608e86ad2f22a730eebd7713b9ba9b173a16cdbb/68747470733a2f2f636f6465636f762e696f2f67682f7378732d636f6c6c61626f726174696f6e2f737065637472652f6272616e63682f646576656c6f702f67726170682f62616467652e737667\"\
    \ alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/sxs-collaboration/spectre/branch/develop/graph/badge.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/sxs-collaboration/spectre/releases/tag/v2022.05.05\"\
    ><img src=\"https://camo.githubusercontent.com/02238bb82fe21cf1cef1ef1d641896a7094be1287c9f09032bf436525caeefff/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72656c656173652d76323032322e30352e30352d696e666f726d6174696f6e616c\"\
    \ alt=\"release\" data-canonical-src=\"https://img.shields.io/badge/release-v2022.05.05-informational\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://doi.org/10.5281/zenodo.6519635\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4280ee563670b5925614c4fc8d7c227267100d4521c951d12c295250763f79ff/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f646f692f31302e353238312f7a656e6f646f2e363531393633352e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/doi/10.5281/zenodo.6519635.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-what-is-spectre\"\
    \ class=\"anchor\" href=\"#what-is-spectre\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>What is SpECTRE?</h2>\n<p>SpECTRE\
    \ is an open-source code for multi-scale, multi-physics problems\nin astrophysics\
    \ and gravitational physics. In the future, we hope that\nit can be applied to\
    \ problems across discipline boundaries in fluid\ndynamics, geoscience, plasma\
    \ physics, nuclear physics, and\nengineering. It runs at petascale and is designed\
    \ for future exascale\ncomputers.</p>\n<p>SpECTRE is being developed in support\
    \ of our collaborative Simulating\neXtreme Spacetimes (SXS) research program into\
    \ the multi-messenger\nastrophysics of neutron star mergers, core-collapse supernovae,\
    \ and\ngamma-ray bursts.</p>\n<h2>\n<a id=\"user-content-citing-spectre\" class=\"\
    anchor\" href=\"#citing-spectre\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Citing SpECTRE</h2>\n<p>Please cite\
    \ SpECTRE in any publications that make use of its code or data. Cite\nthe latest\
    \ version that you use in your publication. The DOI for this version\nis:</p>\n\
    <ul>\n<li>DOI: <a href=\"https://doi.org/10.5281/zenodo.6519635\" rel=\"nofollow\"\
    >10.5281/zenodo.6519635</a>\n</li>\n</ul>\n<p>You can cite this BibTeX entry in\
    \ your publication:</p>\n\n\n<div class=\"highlight highlight-text-bibtex\"><pre><span\
    \ class=\"pl-k\">@software</span>{<span class=\"pl-en\">spectrecode</span>,\n\
    \    <span class=\"pl-s\">author</span> = <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>Deppe, Nils and Throwe, William and Kidder, Lawrence E. and\
    \ Vu,</span>\n<span class=\"pl-s\">Nils L. and H\\'ebert, Fran\\c{c}ois and Moxon,\
    \ Jordan and Armaza, Crist\\'obal and</span>\n<span class=\"pl-s\">Bonilla, Gabriel\
    \ S. and Kim, Yoonsoo and Kumar, Prayush and Lovelace, Geoffrey</span>\n<span\
    \ class=\"pl-s\">and Macedo, Alexandra and Nelli, Kyle C. and O'Shea, Eamonn and\
    \ Pfeiffer, Harald</span>\n<span class=\"pl-s\">P. and Scheel, Mark A. and Teukolsky,\
    \ Saul A. and Wittek, Nikolas A. and</span>\n<span class=\"pl-s\">others<span\
    \ class=\"pl-pds\">\"</span></span>,\n    <span class=\"pl-s\">title</span> =\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>\\texttt{SpECTRE v2022.05.05}<span\
    \ class=\"pl-pds\">\"</span></span>,\n    <span class=\"pl-s\">version</span>\
    \ = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2022.05.05<span class=\"\
    pl-pds\">\"</span></span>,\n    <span class=\"pl-s\">publisher</span> = <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>Zenodo<span class=\"pl-pds\"\
    >\"</span></span>,\n    <span class=\"pl-s\">doi</span> = <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>10.5281/zenodo.6519635<span class=\"pl-pds\"\
    >\"</span></span>,\n    <span class=\"pl-s\">url</span> = <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>https://spectre-code.org<span class=\"pl-pds\"\
    >\"</span></span>,\n    <span class=\"pl-s\">howpublished</span> =\n<span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>\\href{https://doi.org/10.5281/zenodo.6519635}{10.5281/zenodo.6519635}<span\
    \ class=\"pl-pds\">\"</span></span>,\n    <span class=\"pl-s\">license</span>\
    \ = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>MIT<span class=\"pl-pds\"\
    >\"</span></span>,\n    <span class=\"pl-s\">year</span> = <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>2022<span class=\"pl-pds\">\"</span></span>,\n\
    \    <span class=\"pl-s\">month</span> = <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>5<span class=\"pl-pds\">\"</span></span>\n}</pre></div>\n\n<p>To aid\
    \ reproducibility of your scientific results with SpECTRE, we recommend you\n\
    keep track of the version(s) you used and report this information in your\npublication.\
    \ We also recommend you supply the YAML input files and, if\nappropriate, any\
    \ additional C++ code you wrote to compile SpECTRE executables as\nsupplemental\
    \ material to the publication.</p>\n<p>See our <a href=\"https://spectre-code.org/publication_policies.html\"\
    \ rel=\"nofollow\">publication policy</a>\nfor more information.</p>\n<h2>\n<a\
    \ id=\"user-content-viewing-documentation\" class=\"anchor\" href=\"#viewing-documentation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Viewing Documentation</h2>\n<p>The documentation can be viewed at\
    \ <a href=\"https://spectre-code.org/\" rel=\"nofollow\">https://spectre-code.org/</a>.</p>\n"
  stargazers_count: 110
  subscribers_count: 14
  topics: []
  updated_at: 1653090843.0
ukri-excalibur/excalibur-tests:
  data_format: 2
  description: Performance benchmarks and regression tests for the ExCALIBUR project
  filenames:
  - spack-environments/cosma8/compute-node/spack.yaml
  - spack-environments/dial3/compute-node/spack.yaml
  - spack-environments/github-actions/default/spack.yaml
  - spack-environments/tursa/cpu/spack.yaml
  full_name: ukri-excalibur/excalibur-tests
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-excalibur-tests\" class=\"anchor\" href=\"#excalibur-tests\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>ExCALIBUR tests</h1>\n<p>Performance benchmarks and regression tests\
    \ for the ExCALIBUR project.</p>\n<p>These benchmarks are based on a similar project\
    \ by\n<a href=\"https://github.com/stackhpc/hpc-tests\">StackHPC</a>.</p>\n<p><em><strong>Note</strong>:\
    \ at the moment the ExCALIBUR benchmarks are a work-in-progress.</em></p>\n<h2>\n\
    <a id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Requirements</h2>\n\
    <h3>\n<a id=\"user-content-spack\" class=\"anchor\" href=\"#spack\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Spack</h3>\n\
    <p><em><strong>Note</strong>: in some HPC facilities there may be already a central\
    \ Spack\ninstallation available.  In principle you should be able to use that\
    \ one (you\nonly need to set the <code>SPACK_ROOT</code> environment variable),\
    \ but you may need an\nup-to-date version of Spack in order to install some packages.\
    \  Instructions\nbelow show you how to install Spack locally.</em></p>\n<p><a\
    \ href=\"https://spack.io/\" rel=\"nofollow\">Spack</a> is a package manager specifically\
    \ designed for HPC\nfacilities.  Follow the <a href=\"https://spack.readthedocs.io/en/latest/getting_started.html\"\
    \ rel=\"nofollow\">official\ninstructions</a> to\ninstall the latest version of\
    \ Spack.</p>\n<p>In order to use Spack in ReFrame, the framework we use to run\
    \ the benchmarks\n(see below), the directory where the <code>spack</code> program\
    \ is installed needs to be in\nthe <code>PATH</code> environment variable.  This\
    \ can be achieved for instance by running\nthe commands to get shell support described\
    \ in Spack documentation, which you\ncan also add to your shell init script to\
    \ do it automatically in every session.\nFor example, if you use a shell of the\
    \ family bash/zsh/sh you can add to your\ninit script:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre><span class=\"pl-k\">export</span> SPACK_ROOT=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>/path/to/spack<span class=\"\
    pl-pds\">\"</span></span>\n<span class=\"pl-k\">if</span> [ <span class=\"pl-k\"\
    >-f</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"\
    pl-smi\">${SPACK_ROOT}</span>/share/spack/setup-env.sh<span class=\"pl-pds\">\"\
    </span></span> ]<span class=\"pl-k\">;</span> <span class=\"pl-k\">then</span>\n\
    \    <span class=\"pl-c1\">source</span> <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span><span class=\"pl-smi\">${SPACK_ROOT}</span>/share/spack/setup-env.sh<span\
    \ class=\"pl-pds\">\"</span></span>\n<span class=\"pl-k\">fi</span></pre></div>\n\
    <p>replacing <code>/path/to/spack</code> with the actual path to your Spack installation.</p>\n\
    <p>ReFrame requires a <a href=\"https://spack.readthedocs.io/en/latest/environments.html\"\
    \ rel=\"nofollow\">Spack\nEnvironment</a>.  We\nprovide Spack environments for\
    \ some of the systems that are part of the\nExCALIBUR project.  If you want to\
    \ use a different Spack environment, set the\nenvironment variable <code>EXCALIBUR_SPACK_ENV</code>\
    \ to the path of the directory where\nthe environment is.  If this is not set,\
    \ ReFrame will try to use the environment\nfor the current system, if known, otherwise\
    \ it will automatically create a very\nbasic environment.</p>\n<h3>\n<a id=\"\
    user-content-reframe\" class=\"anchor\" href=\"#reframe\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>ReFrame</h3>\n\
    <p><a href=\"https://reframe-hpc.readthedocs.io/en/stable/\" rel=\"nofollow\"\
    >ReFrame</a> is a high-level\nframework for writing regression tests for HPC systems.\
    \  For our tests we\nrequire ReFrame 3.8.0.  Follow the <a href=\"https://reframe-hpc.readthedocs.io/en/stable/started.html\"\
    \ rel=\"nofollow\">official\ninstructions</a> to\ninstall this package.  Note\
    \ that ReFrame requires Python 3.6: in your HPC system\nyou may need to load a\
    \ specific module to have this version of Python available.</p>\n<p>We provide\
    \ a ReFrame configuration file with the settings of some systems that\nare part\
    \ of the ExCALIBUR project.  You can point ReFrame to this file by\nsetting the\n\
    <a href=\"https://reframe-hpc.readthedocs.io/en/stable/manpage.html#envvar-RFM_CONFIG_FILE\"\
    \ rel=\"nofollow\"><code>RFM_CONFIG_FILE</code></a>\nenvironment variable:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span>\
    \ RFM_CONFIG_FILE=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"\
    pl-smi\">${PWD}</span>/reframe_config.py<span class=\"pl-pds\">\"</span></span></pre></div>\n\
    <p>If you want to use a different ReFrame configuration file, for example because\n\
    you use a different system, you can set this environment variable to the path\
    \ of\nthat file.</p>\n<p><strong>Note</strong>: in order to use the Spack build\
    \ system in ReFrame, the <code>spack</code>\nexecutable must be in the <code>PATH</code>,\
    \ also on the computing nodes of a cluster, if\nyou want to run your benchmarks\
    \ on them.  Note that by default ReFrame uses</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-k\">!</span><span class=\"pl-c\"><span class=\"pl-c\">#</span>/bin/bash</span></pre></div>\n\
    <p>as <a href=\"https://en.wikipedia.org/wiki/Shebang_(Unix)\" rel=\"nofollow\"\
    >shebang</a>, which would not load\nthe user's init script.  If you have added\
    \ Spack to your <code>PATH</code> within your init\nscript, you may want to set\
    \ the\n<a href=\"https://reframe-hpc.readthedocs.io/en/stable/manpage.html#envvar-RFM_USE_LOGIN_SHELL\"\
    \ rel=\"nofollow\"><code>RFM_USE_LOGIN_SHELL</code></a>\nenvironment variable\
    \ in order to make ReFrame use</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-k\">!</span><span class=\"pl-c\"><span class=\"pl-c\">#</span>/bin/bash\
    \ -l</span></pre></div>\n<p>as shebang line, instead.</p>\n<h3>\n<a id=\"user-content-extra-python-modules\"\
    \ class=\"anchor\" href=\"#extra-python-modules\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Extra Python modules</h3>\n<p>The\
    \ benchmarks in this suite will additionally need the following Python modules:</p>\n\
    <ul>\n<li><a href=\"https://matplotlib.org/\" rel=\"nofollow\"><code>matplotlib</code></a></li>\n\
    <li><a href=\"https://pandas.pydata.org/\" rel=\"nofollow\"><code>pandas</code></a></li>\n\
    </ul>\n<p>Check the recommended way to install Python modules in your system,\
    \ it may be\nfor example by using <code>pip</code>, or creating environments with\
    \ <code>pyenv</code> or\nConda/Anaconda. For example, see <a href=\"https://docs.hpc.cam.ac.uk/hpc/software-tools/python.html\"\
    \ rel=\"nofollow\">the guide for CSD3</a>.</p>\n<h2>\n<a id=\"user-content-usage\"\
    \ class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Usage</h2>\n<p>Once you have set up\
    \ Spack and ReFrame, you can execute a benchmark with</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>reframe -c apps/BENCH_NAME -r --performance-report</pre></div>\n\
    <p>where <code>apps/BENCH_NAME</code> is the directory where the benchmark is.\
    \  The command\nabove supposes you have the program <code>reframe</code> in your\
    \ PATH, if it is not the\ncase you can also call <code>reframe</code> with its\
    \ relative or absolute path.  For\nexample, to run the Sombrero benchmark in the\
    \ <code>apps/sombrero</code> directory you can\nuse</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>reframe -c apps/sombrero -r --performance-report</pre></div>\n\
    <p>For benchmark using the Spack build system, the tests define a default Spack\
    \ specification\nto be installed in the environment, but users can change it when\
    \ invoking ReFrame on the\ncommand line with the\n<a href=\"https://reframe-hpc.readthedocs.io/en/stable/manpage.html#cmdoption-S\"\
    \ rel=\"nofollow\"><code>-S</code></a> option to set\nthe <code>spack_spec</code>\
    \ variable:</p>\n<pre><code>reframe -c apps/sombrero -r --performance-report -S\
    \ spack_spec='sombrero@2021-08-16%intel'\n</code></pre>\n<h3>\n<a id=\"user-content-selecting-system-and-queue-access-options\"\
    \ class=\"anchor\" href=\"#selecting-system-and-queue-access-options\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Selecting\
    \ system and queue access options</h3>\n<p>The provided ReFrame configuration\
    \ file contains the settings for multiple systems.  If you\nuse it, the automatic\
    \ detection of the system may fail, as some systems may use clashing\nhostnames.\
    \  You can always use the flag <a href=\"https://reframe-hpc.readthedocs.io/en/stable/manpage.html#cmdoption-system\"\
    \ rel=\"nofollow\"><code>--system NAME:PARTITION</code></a>\nto specify the system\
    \ (and optionally the partition) to use.</p>\n<p>Additionally, if submitting jobs\
    \ to the compute nodes requires additional options, like for\nexample the resource\
    \ group you belong to (for example <code>--account=...</code> for Slurm), you\
    \ have\nto pass the command line flag\n<a href=\"https://reframe-hpc.readthedocs.io/en/stable/manpage.html#cmdoption-J\"\
    \ rel=\"nofollow\"><code>--job-option=...</code></a>\nto <code>reframe</code>\
    \ (e.g., <code>--job-option='--account=...'</code>).</p>\n<h3>\n<a id=\"user-content-unsupported-systems\"\
    \ class=\"anchor\" href=\"#unsupported-systems\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Unsupported systems</h3>\n<p>The\
    \ configuration provided in <a href=\"./reframe_config.py\"><code>reframe_config.py</code></a>\
    \ lets you run the\nbenchmarks on systems for which the configuration has been\
    \ already contributed.  However you\ncan still use this framework on any system\
    \ by choosing the \"generic\" system with <code>--system generic</code>, or using\
    \ your own ReFrame configuration.  Note, however, that if you use the\n\"generic\"\
    \ system, ReFrame will not know anything about the queue manager of your system,\
    \ if\nany, or the MPI launcher.  For the benchmarks using the Spack build system,\
    \ if you choose\nthe \"generic\" system, a new empty Spack environment will be\
    \ automatically created in\n<code>spack-environments/generic</code>.  In any case,\
    \ you can always make the benchmarks use a\ndifferent Spack environment by setting\
    \ the environment variable <code>EXCALIBUR_SPACK_ENV</code>\ndescribed above.</p>\n\
    <h2>\n<a id=\"user-content-contributing-new-systems-or-benchmarks\" class=\"anchor\"\
    \ href=\"#contributing-new-systems-or-benchmarks\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contributing\
    \ new systems or benchmarks</h2>\n<p>Feel free to add new benchmark apps or support\
    \ new systems that are part of the\nExCALIBUR benchmarking collaboration.  Read\n\
    <a href=\"./CONTRIBUTING.md\"><code>CONTRIBUTING.md</code></a> for more details.</p>\n"
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1638371749.0
veit/jupyter-tutorial:
  data_format: 2
  description: 'Training materials for setting up and using a research infrastructure
    based on Jupyter notebooks: https://cusy.io/en/seminars'
  filenames:
  - spackenvs/python-38/spack.yaml
  full_name: veit/jupyter-tutorial
  latest_release: 0.8.0
  stargazers_count: 10
  subscribers_count: 5
  topics:
  - jupyter
  - jupyter-notebooks
  - jupyter-kernels
  - ipython
  - ipywidgets
  - ipython-widget
  - spack
  - pipenv
  - dvc
  - data-science
  - pandas
  updated_at: 1641756482.0
wr-hamburg/eurosys2022-cheops-compression:
  data_format: 2
  description: Data-Aware Compression for HPC using Machine Learning
  filenames:
  - library/spack.yaml
  full_name: wr-hamburg/eurosys2022-cheops-compression
  latest_release: null
  readme: '<h1>

    <a id="user-content-readme" class="anchor" href="#readme" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Readme</h1>

    <h2>

    <a id="user-content-library-for-tracing-and-inferencing-library" class="anchor"
    href="#library-for-tracing-and-inferencing-library" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Library for tracing and inferencing (/library)</h2>

    <h3>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h3>

    <p><code>spack env activate .</code></p>

    <p><code>spack install</code></p>

    <h3>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h3>

    <p><code>meson bld &amp;&amp; ninja -C bld</code></p>

    <h3>

    <a id="user-content-configuration" class="anchor" href="#configuration" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Configuration</h3>

    <table>

    <thead>

    <tr>

    <th>Option</th>

    <th>Description</th>

    <th align="center">Mode</th>

    <th align="center">Inferencing mode</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td></td>

    <td></td>

    <td align="center">Sampling</td>

    <td align="center">Inferencing</td>

    </tr>

    <tr>

    <td>-m, --min-size=9</td>

    <td>Min size of chunks to analyze in bytes</td>

    <td align="center">X</td>

    <td align="center">X</td>

    </tr>

    <tr>

    <td>-r, --repeat=3</td>

    <td>Number of times to repeat measurements</td>

    <td align="center">X</td>

    <td align="center"></td>

    </tr>

    <tr>

    <td>-p, --meta-path=/tmp/meta.h5</td>

    <td>Path for metadata storage</td>

    <td align="center">X</td>

    <td align="center">X</td>

    </tr>

    <tr>

    <td>-t, --tracing</td>

    <td>Activates tracing of MPI-Calls</td>

    <td align="center">X</td>

    <td align="center"></td>

    </tr>

    <tr>

    <td>-s, --store-chunks</td>

    <td>Activates chunk storage</td>

    <td align="center">X</td>

    <td align="center"></td>

    </tr>

    <tr>

    <td>-c, --chunk-path=/tmp/chunks/</td>

    <td>Storage path of chunks for later analysis</td>

    <td align="center">X</td>

    <td align="center"></td>

    </tr>

    <tr>

    <td>-e, --test-compression</td>

    <td>Activates compression tests according to metrics</td>

    <td align="center">X</td>

    <td align="center"></td>

    </tr>

    <tr>

    <td>-x, --model-path</td>

    <td>Path to exported ONNX model</td>

    <td align="center"></td>

    <td align="center">X</td>

    </tr>

    <tr>

    <td>-o, --settings-path</td>

    <td>Path to exported ONNX settings</td>

    <td align="center"></td>

    <td align="center">X</td>

    </tr>

    <tr>

    <td>-i, --inferencing</td>

    <td>Run inferencing</td>

    <td align="center"></td>

    <td align="center">X</td>

    </tr>

    <tr>

    <td>-d, --decompression</td>

    <td>Measure decompression</td>

    <td align="center">X</td>

    <td align="center"></td>

    </tr>

    </tbody>

    </table>

    <h3>

    <a id="user-content-usage-example" class="anchor" href="#usage-example" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage example</h3>

    <p><code>export IOA_OPTIONS="--repeat=3 --tracing --decompression --test-compression
    --meta-path=meta.h5 --chunk-path=chunks/</code>

    <code>G_MESSAGES_DEBUG=all LD_PRELOAD=bld/libmpi-preload.so mpiexec -np 2 application</code></p>

    <h3>

    <a id="user-content-usage-inferencing" class="anchor" href="#usage-inferencing"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Usage
    inferencing</h3>

    <p>Specify model and model settings files used in training step</p>

    <p><code>export IOA_OPTIONS="--min-size=9 --meta-path=evaluation.h5 --inferencing
    --model-path=compression-CR.onnx --settings-path=compression-CR-settings.txt</code></p>

    <h1>

    <a id="user-content-training-and-evaluation-compressionml-pytorch" class="anchor"
    href="#training-and-evaluation-compressionml-pytorch" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Training and evaluation
    (/CompressionML-PyTorch)</h1>

    <h2>

    <a id="user-content-dependencies-1" class="anchor" href="#dependencies-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h2>

    <ul>

    <li>Uses <a href="https://python-poetry.org/docs/basic-usage/" rel="nofollow">Poetry</a>
    for dependency management</li>

    </ul>

    <p><code>poetry shell &amp;&amp; poetry install</code></p>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <p>Allows for hyperparameter tuning, as well as final model creation with the
    discovered parameters.</p>

    <h3>

    <a id="user-content-tuningpy" class="anchor" href="#tuningpy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>tuning.py</code>

    </h3>

    <ul>

    <li>Specify meta.h5 and metric within file</li>

    <li>Run file and discover parameters, e.g by using <a href="https://www.tensorflow.org/tensorboard"
    rel="nofollow">Tensorboard</a>

    </li>

    </ul>

    <h3>

    <a id="user-content-trainingipynb" class="anchor" href="#trainingipynb" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>training.ipynb</code>

    </h3>

    <ul>

    <li>Use discovered parameters from previous step and train final model</li>

    </ul>

    <h2>

    <a id="user-content-evaluate" class="anchor" href="#evaluate" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Evaluate</h2>

    <h3>

    <a id="user-content-confusionipynb" class="anchor" href="#confusionipynb" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>confusion.ipynb</code>

    </h3>

    <ul>

    <li>Set meta path to <em>evaluation.h5</em> output path specified in <code>IOA_OPTIONS</code>
    when inferencing</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1649004657.0
xsdk-project/installxSDK:
  data_format: 2
  description: Bash shell script for installing xSDK and other IDEAS packages
  filenames:
  - platformFiles/lassen/spack.yaml
  - platformFiles/spock/spack.yaml
  full_name: xsdk-project/installxSDK
  latest_release: v0.1.1
  readme: '<h1>

    <a id="user-content-useful-supplementary-materials-for-installing-the-xsdk" class="anchor"
    href="#useful-supplementary-materials-for-installing-the-xsdk" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Useful supplementary
    materials for installing the xSDK</h1>

    <p>See <a href="https://xsdk.info/download/" rel="nofollow">https://xsdk.info/download/</a>
    for full directions on obtaining the xSDK.</p>

    <p>The primary content of this repository includes packages.yaml and

    compilers.yaml files, as well as other files useful for configuring builds of

    the xSDK through Spack for various platforms.</p>

    <p>The files are arranged generally as follows:</p>

    <p>installxSDK/platformFiles/&lt;platform description&gt;/&lt;files for platfom&gt;</p>

    <p>This repository is to be tagged for each major and minor release of the xSDK.</p>

    '
  stargazers_count: 4
  subscribers_count: 8
  topics: []
  updated_at: 1637108321.0
