Alpine-DAV/spack_configs:
  data_format: 2
  description: spack envs
  filenames:
  - _experimental/envs/alpinedav/ubuntu_18_cuda_10.1_devel/spack.yaml
  - _experimental/envs/llnl/pascal-cuda/spack.yaml
  full_name: Alpine-DAV/spack_configs
  latest_release: null
  readme: '<h1><a id="user-content-spack_configs" class="anchor" aria-hidden="true"
    href="#spack_configs"><span aria-hidden="true" class="octicon octicon-link"></span></a>spack_configs</h1>

    <p>shared spack configs repo</p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1639176281.0
ArjunaCluster/spack:
  data_format: 2
  description: Spack Repos and Configuration Files
  filenames:
  - environments/common/spack.yaml
  - environments/slurm/spack.yaml
  full_name: ArjunaCluster/spack
  latest_release: null
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1637623732.0
CUP-ECS/cajitafluids:
  data_format: 2
  description: A simple poisson finite difference fluid solver in Cajita/Kokkos for
    testing MPI communication abstractions and their performance
  filenames:
  - configs/tutorial-uao-cuda/spack.yaml
  - configs/github/spack.yaml
  - configs/generic/spack-cuda-wrapper.yaml
  - configs/llnl-lassen/spack.yaml
  - configs/generic/spack-cuda-clang.yaml
  full_name: CUP-ECS/cajitafluids
  latest_release: null
  readme: '<h1><a id="user-content-poisson-mpi-benchmark" class="anchor" aria-hidden="true"
    href="#poisson-mpi-benchmark"><span aria-hidden="true" class="octicon octicon-link"></span></a>Poisson
    MPI Benchmark</h1>

    <p>This directory contains code for a relatively simple finite difference

    fluid advection solver for exploring communication issues on modern architectures

    (particularly GPUs). The main goal is to look at different neighbor collective

    and GPU communication approaches.</p>

    <p>Computationally, the benchmark advects a material feature (that doesn''t otherwise
    effect

    fluid flow, e.g. by changing pressite) using the incompressible Euler fluid flow
    equations.

    Consider, for example, something like a dye being carried through a tank of water
    or a

    fragrance wafting across a room.</p>

    <p>The main elements of the benchmark are:</p>

    <ul>

    <li>Solution of the pressure gradient at each timestep to maintain

    incompressibility. The benchmark has two initial implementations:

    (1) Calling a matrix-free solver in HYPRE to solve the problem or (2)

    running a local matrix-free preconditioned CG solver, in which different

    MPI approaches for the halo exchange are explored.</li>

    <li>Interpolation (either cubic splines or linear) for semi-Lagrangian

    advection of the material being advected across timesteps.</li>

    <li>3rd-order Runge Kutta for time integration</li>

    </ul>

    <p>Sources:</p>

    <ul>

    <li>Fluid Simulation for Comptuer Graphics by Bridson</li>

    <li>Incremental Fluids in Kokkos (<a href="mailto:git@github.com">git@github.com</a>:pkestene/incremental-fluids-kokkos.git)</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 11
  topics: []
  updated_at: 1654983605.0
CivetWang/HPCHarryW:
  data_format: 2
  description: null
  filenames:
  - assignment/spack.yaml
  full_name: CivetWang/HPCHarryW
  latest_release: null
  readme: '<h1><a id="user-content-hpcharryw" class="anchor" aria-hidden="true" href="#hpcharryw"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HPCHarryW</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1653448958.0
E4S-Project/e4s:
  data_format: 2
  description: E4S for Spack
  filenames:
  - environments/22.05/oneapi.spack.yaml
  - environments/22.08/cuda-aarch64.spack.yaml
  - environments/22.05/cuda-ppc64le.spack.yaml
  - environments/22.05/cuda-x86_64.spack.yaml
  - environments/22.08/oneapi.spack.yaml
  - environments/22.05/rocm.spack.yaml
  - environments/22.08/rocm.spack.yaml
  - environments/22.08/cuda-ppc64le.spack.yaml
  - environments/22.08/cuda-x86_64.spack.yaml
  full_name: E4S-Project/e4s
  latest_release: null
  readme: "<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/E4S-Project/e4s/blob/master/logos/E4S-dark-green.png\"\
    ><img src=\"https://github.com/E4S-Project/e4s/raw/master/logos/E4S-dark-green.png\"\
    \ width=\"200\" alt=\"E4S\" style=\"max-width: 100%;\"></a></p> \n<p><a target=\"\
    _blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/58e7ffdceb32cd7a8facd6b6cd3920a56c15e0e2ef1d3398158ef4ec0d6ec886/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f4534532d50726f6a6563742f653473\"\
    ><img src=\"https://camo.githubusercontent.com/58e7ffdceb32cd7a8facd6b6cd3920a56c15e0e2ef1d3398158ef4ec0d6ec886/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f4534532d50726f6a6563742f653473\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/E4S-Project/e4s\"\
    \ style=\"max-width: 100%;\"></a>\n<a target=\"_blank\" rel=\"noopener noreferrer\
    \ nofollow\" href=\"https://camo.githubusercontent.com/94cdab1cd9efc5521be1590b3d0b5dc5f707838e0d20015c14f23921ef2b7326/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6534732f62616467652f3f76657273696f6e3d6c6174657374\"\
    ><img src=\"https://camo.githubusercontent.com/94cdab1cd9efc5521be1590b3d0b5dc5f707838e0d20015c14f23921ef2b7326/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6534732f62616467652f3f76657273696f6e3d6c6174657374\"\
    \ alt=\"Documentation\" data-canonical-src=\"https://readthedocs.org/projects/e4s/badge/?version=latest\"\
    \ style=\"max-width: 100%;\"></a>\n<a target=\"_blank\" rel=\"noopener noreferrer\
    \ nofollow\" href=\"https://camo.githubusercontent.com/c2e45205070ba0928aece78cf95f8658ef1cf69f7c113dc56f7d05e29e68755e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f4534532d50726f6a6563742f6534732e737667\"\
    ><img src=\"https://camo.githubusercontent.com/c2e45205070ba0928aece78cf95f8658ef1cf69f7c113dc56f7d05e29e68755e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f4534532d50726f6a6563742f6534732e737667\"\
    \ alt=\"GitHub Issues\" data-canonical-src=\"https://img.shields.io/github/issues/E4S-Project/e4s.svg\"\
    \ style=\"max-width: 100%;\"></a>\n<a target=\"_blank\" rel=\"noopener noreferrer\
    \ nofollow\" href=\"https://camo.githubusercontent.com/011fb27187d5b878949948752f73e86dea6828febca879ee69a5a5d52ce651ca/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f4534532d50726f6a6563742f653473\"\
    ><img src=\"https://camo.githubusercontent.com/011fb27187d5b878949948752f73e86dea6828febca879ee69a5a5d52ce651ca/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f4534532d50726f6a6563742f653473\"\
    \ alt=\"GitHub pull requests\" data-canonical-src=\"https://img.shields.io/github/issues-pr/E4S-Project/e4s\"\
    \ style=\"max-width: 100%;\"></a></p>\n<h1><a id=\"user-content-e4s\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#e4s\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>E4S</h1>\n<p>The <a href=\"https://e4s-project.github.io/\"\
    \ rel=\"nofollow\">Extreme-scale Scientific Software Stack (E4S)</a> is a community\
    \ effort to provide open source\nsoftware packages for developing, deploying and\
    \ running scientific applications on high-performance\ncomputing (HPC) platforms.\
    \ E4S provides from-source builds and containers of a\n<a href=\"https://e4s-project.github.io/Resources/ProductInfo.html\"\
    \ rel=\"nofollow\">broad collection of HPC software packages</a>.</p>\n<p>E4S\
    \ is available to download in the following formats:</p>\n<ul>\n<li>\n<p>Containers:\
    \ Docker, Singularity, CharlieCloud, OVA</p>\n</li>\n<li>\n<p>Spack manifest (<code>spack.yaml</code>)\
    \ to install from source. These can be found in <a href=\"https://github.com/E4S-Project/e4s/tree/master/environments\"\
    >environments</a> directory.</p>\n</li>\n<li>\n<p><a href=\"http://aws.amazon.com/\"\
    \ rel=\"nofollow\">AWS EC2 image</a> with image name <code>ami-0db9d49091db1c25f</code>\
    \ in <strong>US-West-2 (Oregon)</strong></p>\n</li>\n<li>\n<p><a href=\"https://oaciss.uoregon.edu/e4s/inventory.html\"\
    \ rel=\"nofollow\">E4S Build Cache</a></p>\n</li>\n</ul>\n<p>Please see <a href=\"\
    https://github.com/E4S-Project/e4s/blob/master/E4S_Products.md\">E4S Product Dictionary</a>\
    \ for complete list of E4S products.</p>\n<h2><a id=\"user-content-useful-links\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#useful-links\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Useful Links</h2>\n<ul>\n<li>User\
    \ Documentation: <a href=\"https://e4s.readthedocs.io\" rel=\"nofollow\">https://e4s.readthedocs.io</a>\n\
    </li>\n<li>Main Page: <a href=\"https://e4s-project.github.io/\" rel=\"nofollow\"\
    >https://e4s-project.github.io/</a>\n</li>\n<li>E4S GitHub: <a href=\"https://github.com/E4S-Project/\"\
    >https://github.com/E4S-Project/</a>\n</li>\n<li>Slack Channel: <a href=\"https://e4s-project.slack.com\"\
    \ rel=\"nofollow\">https://e4s-project.slack.com</a>\n</li>\n<li>E4S Dashboard:\
    \ <a href=\"https://dashboard.e4s.io/\" rel=\"nofollow\">https://dashboard.e4s.io/</a>\n\
    </li>\n</ul>\n<h2><a id=\"user-content-related-projects\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#related-projects\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Related Projects</h2>\n<ul>\n<li>\n<p><a href=\"https://github.com/E4S-Project/E4S-Project.github.io\"\
    >E4S-Project/E4S-Project.github.io</a> - E4S Documentation repo that is hosted\
    \ on <a href=\"https://e4s-project.github.io/\" rel=\"nofollow\">https://e4s-project.github.io/</a></p>\n\
    </li>\n<li>\n<p><a href=\"https://github.com/E4S-Project/testsuite\">E4S-Project/testsuite</a>\
    \ - E4S Testsuite with collection of validation tests that can be run post-install.</p>\n\
    </li>\n<li>\n<p><a href=\"https://github.com/E4S-Project/e4s-cl\">E4S-Project/e4s-cl</a>\
    \ - E4S Container Launcher is a tool to easily run MPI applications in E4S containers.</p>\n\
    </li>\n<li>\n<p><a href=\"https://github.com/E4S-Project/e4s-ci-badges\">E4S-Project/e4s-ci-badges</a>\
    \ - Display CI badges for E4S products that are available from <a href=\"https://shields.io/\"\
    \ rel=\"nofollow\">shields.io</a></p>\n</li>\n</ul>\n<h2><a id=\"user-content-license\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#license\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>License</h2>\n<p>E4S is released\
    \ as MIT license for more details see <a href=\"https://github.com/E4S-Project/e4s/blob/master/LICENSE\"\
    >LICENSE</a> file</p>\n<h2><a id=\"user-content-contact\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#contact\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Contact</h2>\n<ul>\n<li>Mike Heroux (<a href=\"mailto:maherou@sandia.gov\"\
    >maherou@sandia.gov</a>)</li>\n<li>Sameer Shende (<a href=\"mailto:sameer@cs.uoregon.edu\"\
    >sameer@cs.uoregon.edu</a>)</li>\n</ul>\n"
  stargazers_count: 15
  subscribers_count: 9
  topics: []
  updated_at: 1662151271.0
ECP-WarpX/WarpX:
  data_format: 2
  description: WarpX is an advanced electromagnetic Particle-In-Cell code.
  filenames:
  - Tools/machines/desktop/spack-ubuntu-cuda.yaml
  - Tools/machines/desktop/spack-debian-openmp.yaml
  - Tools/machines/desktop/spack-debian-cuda.yaml
  - Tools/machines/desktop/spack-ubuntu-openmp.yaml
  - Tools/machines/desktop/spack-ubuntu-rocm.yaml
  - Tools/machines/desktop/spack-debian-rocm.yaml
  full_name: ECP-WarpX/WarpX
  latest_release: '22.09'
  readme: '<h1><a id="user-content-warpx" class="anchor" aria-hidden="true" href="#warpx"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>WarpX</h1>

    <p><a href="https://dev.azure.com/ECP-WarpX/WarpX/_build/latest?definitionId=1&amp;branchName=development"
    rel="nofollow"><img src="https://camo.githubusercontent.com/992695de99c1381c366d74cf333cc4b4a29d53878b4808d643fb673748a73c5b/68747470733a2f2f6465762e617a7572652e636f6d2f4543502d57617270582f57617270582f5f617069732f6275696c642f7374617475732f4543502d57617270582e57617270583f6272616e63684e616d653d646576656c6f706d656e74"
    alt="Code Status development" data-canonical-src="https://dev.azure.com/ECP-WarpX/WarpX/_apis/build/status/ECP-WarpX.WarpX?branchName=development"
    style="max-width: 100%;"></a>

    <a href="https://dev.azure.com/ECP-WarpX/WarpX/_build?definitionId=2" rel="nofollow"><img
    src="https://camo.githubusercontent.com/f95e83fed565d15a3d259197389c3fbb68e0e9d529df7da1f73702528739c16f/68747470733a2f2f6465762e617a7572652e636f6d2f4543502d57617270582f57617270582f5f617069732f6275696c642f7374617475732f4543502d57617270582e4e696768746c793f6272616e63684e616d653d6e696768746c79266c6162656c3d6e696768746c792532307061636b61676573"
    alt="Nightly Installation Tests" data-canonical-src="https://dev.azure.com/ECP-WarpX/WarpX/_apis/build/status/ECP-WarpX.Nightly?branchName=nightly&amp;label=nightly%20packages"
    style="max-width: 100%;"></a>

    <a href="https://warpx.readthedocs.io/en/latest/?badge=latest" rel="nofollow"><img
    src="https://camo.githubusercontent.com/2fe6e4a1201d33edf1bdd9968c6c0446da41d44fef1b7a1e532cddc4fbd4c2ae/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f77617270782f62616467652f3f76657273696f6e3d6c6174657374"
    alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/warpx/badge/?version=latest"
    style="max-width: 100%;"></a>

    <a href="https://spack.readthedocs.io/en/latest/package_list.html#warpx" rel="nofollow"><img
    src="https://camo.githubusercontent.com/86cd172f84e0a3b89161faaeb17eb247cdb10062ed0e65f9f291db3011697416/68747470733a2f2f696d672e736869656c64732e696f2f737061636b2f762f7761727078"
    alt="Spack Version" data-canonical-src="https://img.shields.io/spack/v/warpx"
    style="max-width: 100%;"></a>

    <a href="https://anaconda.org/conda-forge/warpx" rel="nofollow"><img src="https://camo.githubusercontent.com/4434c608221acef026a4af7cb491f491413ab135a781e3537087938592334617/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f766e2f636f6e64612d666f7267652f7761727078"
    alt="Conda Version" data-canonical-src="https://img.shields.io/conda/vn/conda-forge/warpx"
    style="max-width: 100%;"></a>

    <a href="https://gitter.im/ECP-WarpX/community?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge"
    rel="nofollow"><img src="https://camo.githubusercontent.com/c1799ddb014bc7c83ab051f3668c05f25049c81bbf1ae3c4e4dd6a61c68314aa/68747470733a2f2f6261646765732e6769747465722e696d2f4543502d57617270582f636f6d6d756e6974792e737667"
    alt="Gitter" data-canonical-src="https://badges.gitter.im/ECP-WarpX/community.svg"
    style="max-width: 100%;"></a><br>

    <a href="https://warpx.readthedocs.io/en/latest/install/users.html" rel="nofollow"><img
    src="https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565"
    alt="Supported Platforms" data-canonical-src="https://img.shields.io/badge/platforms-linux%20%7C%20osx%20%7C%20win-blue"
    style="max-width: 100%;"></a>

    <a href="https://github.com/ECP-WarpX/WarpX/compare/development"><img src="https://camo.githubusercontent.com/4cb34757a4ca0098f7c5b01c1d559e13991f5cb4e6add106761194c2967a7911/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d6974732d73696e63652f4543502d57617270582f57617270582f6c61746573742f646576656c6f706d656e742e737667"
    alt="GitHub commits since last release" data-canonical-src="https://img.shields.io/github/commits-since/ECP-WarpX/WarpX/latest/development.svg"
    style="max-width: 100%;"></a>

    <a href="https://www.exascaleproject.org/research/" rel="nofollow"><img src="https://camo.githubusercontent.com/fe7a996983f2a22d3a469de3af6e13b7062bca7f02ffad7974bb724b27c2b218/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f737570706f7274656425323062792d4543502d6f72616e6765"
    alt="Exascale Computing Project" data-canonical-src="https://img.shields.io/badge/supported%20by-ECP-orange"
    style="max-width: 100%;"></a>

    <a href="https://isocpp.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/5d59fff46d59a1783cc24942cb4eb374014513db99f991164bd051bcd94aa598/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d4325324225324231372d6f72616e67652e737667"
    alt="Language: C++17" data-canonical-src="https://img.shields.io/badge/language-C%2B%2B17-orange.svg"
    style="max-width: 100%;"></a>

    <a href="https://python.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/9cf7ec75b074af6953db1304db75950ab917ecd8a1aecb41f0d1191d10872298/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d507974686f6e2d6f72616e67652e737667"
    alt="Language: Python" data-canonical-src="https://img.shields.io/badge/language-Python-orange.svg"
    style="max-width: 100%;"></a><br>

    <a href="https://spdx.org/licenses/BSD-3-Clause-LBNL.html" rel="nofollow"><img
    src="https://camo.githubusercontent.com/c468c77da60663856e2be1cdd66db538d4bca1b2a3bdf34a76a7f3953e58fc26/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4253442d2d332d2d436c617573652d2d4c424e4c2d626c75652e737667"
    alt="License WarpX" data-canonical-src="https://img.shields.io/badge/license-BSD--3--Clause--LBNL-blue.svg"
    style="max-width: 100%;"></a>

    <a href="https://doi.org/10.5281/zenodo.4571577" rel="nofollow"><img src="https://camo.githubusercontent.com/a80e066c199b28e39d95c8a3cd8a7061bb4c190c717db8b58ff8cea2de0952be/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f444f4925323028736f75726365292d31302e353238312f7a656e6f646f2e343537313537372d626c75652e737667"
    alt="DOI (source)" data-canonical-src="https://img.shields.io/badge/DOI%20(source)-10.5281/zenodo.4571577-blue.svg"
    style="max-width: 100%;"></a>

    <a href="https://doi.org/10.1016/j.parco.2021.102833" rel="nofollow"><img src="https://camo.githubusercontent.com/1f6ca17eba9f0dbca214c58a50e39d5e4d2c5513476e963147c57c7b9f40f378/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f444f49253230287061706572292d31302e313031362f6a2e706172636f2e323032312e3130323833332d626c75652e737667"
    alt="DOI (paper)" data-canonical-src="https://img.shields.io/badge/DOI%20(paper)-10.1016/j.parco.2021.102833-blue.svg"
    style="max-width: 100%;"></a></p>

    <h2><a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h2>

    <p>WarpX is an advanced electromagnetic Particle-In-Cell code.

    It supports many features including Perfectly-Matched Layers (PML), mesh refinement,
    and the boosted-frame technique.</p>

    <h2><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h2>

    <p><a href="https://picmi-standard.github.io" rel="nofollow"><img src="https://camo.githubusercontent.com/343c1eefa7d19641daf3e00da21e54db3a6211fe5f692c3004f2836a185668d8/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d253232776f726b7325323077697468253232266d6573736167653d2532325049434d4925323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="PICMI" data-canonical-src="https://img.shields.io/static/v1?label=%22works%20with%22&amp;message=%22PICMI%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="https://www.openPMD.org" rel="nofollow"><img src="https://camo.githubusercontent.com/062e5330b80f6eca55b1df50d6d154214f5a2033b7a87344ef2a580fd7a616dc/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d253232776f726b7325323077697468253232266d6573736167653d2532326f70656e504d4425323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="openPMD" data-canonical-src="https://img.shields.io/static/v1?label=%22works%20with%22&amp;message=%22openPMD%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="https://yt-project.org" rel="nofollow"><img src="https://camo.githubusercontent.com/9e6cacd2df0d5a581d8afad30a57807b71f4b67c58e74faa4080dad7d81c6184/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d253232776f726b7325323077697468253232266d6573736167653d253232797425323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="yt-project" data-canonical-src="https://img.shields.io/static/v1?label=%22works%20with%22&amp;message=%22yt%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a></p>

    <p>In order to learn how to install and run the code, please see the online documentation:

    <a href="https://warpx.readthedocs.io" rel="nofollow">https://warpx.readthedocs.io</a></p>

    <p>To contact the developers, feel free to open an issue on this repo, or visit
    our Gitter room at <a href="https://gitter.im/ECP-WarpX/community" rel="nofollow">https://gitter.im/ECP-WarpX/community</a></p>

    <h2><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <p><a href="https://amrex-codes.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/7053679f4412132d376afadf481432a9d435336f8127e7c8650808bc66d019b2/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d25323272756e732532306f6e253232266d6573736167653d253232414d52655825323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="AMReX" data-canonical-src="https://img.shields.io/static/v1?label=%22runs%20on%22&amp;message=%22AMReX%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="https://picsar.net" rel="nofollow"><img src="https://camo.githubusercontent.com/793037a9842c5343f4942ce7475c7c7696e69b44621531f666492ac87b5e80b8/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d25323272756e732532306f6e253232266d6573736167653d25323250494353415225323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="PICSAR" data-canonical-src="https://img.shields.io/static/v1?label=%22runs%20on%22&amp;message=%22PICSAR%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="https://openpmd-api.readthedocs.io" rel="nofollow"><img src="https://camo.githubusercontent.com/b7108e47d5ad6b76b60f07a4e04173ba260c5eef9bb244680f65ff91d8a319f8/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d25323272756e732532306f6e253232266d6573736167653d2532326f70656e504d442d61706925323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="openPMD-api" data-canonical-src="https://img.shields.io/static/v1?label=%22runs%20on%22&amp;message=%22openPMD-api%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="https://csmd.ornl.gov/adios" rel="nofollow"><img src="https://camo.githubusercontent.com/d525e37817dc6dfc5f173eb31f4e9fd52947e668793967565910166b335ced93/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d25323272756e732532306f6e253232266d6573736167653d2532324144494f5325323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="ADIOS" data-canonical-src="https://img.shields.io/static/v1?label=%22runs%20on%22&amp;message=%22ADIOS%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="https://www.hdfgroup.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/005f778c667adb78e4302d47579b9bedc5ec0f59f88c13552f6b4bb399f93438/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d25323272756e732532306f6e253232266d6573736167653d2532324844463525323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="HDF5" data-canonical-src="https://img.shields.io/static/v1?label=%22runs%20on%22&amp;message=%22HDF5%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="http://www.ascent-dav.org" rel="nofollow"><img src="https://camo.githubusercontent.com/204f53a0d216a0a2fce9a367e3ba3a1957ac2285ed89026cb80321df6a125fc4/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d25323272756e732532306f6e253232266d6573736167653d253232417363656e7425323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="Ascent" data-canonical-src="https://img.shields.io/static/v1?label=%22runs%20on%22&amp;message=%22Ascent%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a>

    <a href="https://sensei-insitu.org" rel="nofollow"><img src="https://camo.githubusercontent.com/6f870e29c1d57a4e4209ec97a00fbe4f73c8fd6fb589bf4c12f3feef9d3aaaeb/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d25323272756e732532306f6e253232266d6573736167653d25323253454e53454925323226636f6c6f723d253232626c756576696f6c6574253232"
    alt="SENSEI" data-canonical-src="https://img.shields.io/static/v1?label=%22runs%20on%22&amp;message=%22SENSEI%22&amp;color=%22blueviolet%22"
    style="max-width: 100%;"></a></p>

    <p>Our workflow is described in <a href="CONTRIBUTING.rst">CONTRIBUTING.rst</a>.</p>

    <h2><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>WarpX Copyright (c) 2018-2022, The Regents of the University of California,

    through Lawrence Berkeley National Laboratory (subject to receipt of any

    required approvals from the U.S. Dept. of Energy).  All rights reserved.</p>

    <p>If you have questions about your rights to use or distribute this software,

    please contact Berkeley Lab''s Innovation &amp; Partnerships Office at

    <a href="mailto:IPO@lbl.gov">IPO@lbl.gov</a>.</p>

    <p>NOTICE.  This Software was developed under funding from the U.S. Department

    of Energy and the U.S. Government consequently retains certain rights. As

    such, the U.S. Government has been granted for itself and others acting on

    its behalf a paid-up, nonexclusive, irrevocable, worldwide license in the

    Software to reproduce, distribute copies to the public, prepare derivative

    works, and perform publicly and display publicly, and to permit other to do

    so.</p>

    <p>License for WarpX can be found at <a href="LICENSE.txt">LICENSE.txt</a>.</p>

    '
  stargazers_count: 143
  subscribers_count: 15
  topics:
  - laser
  - plasma
  - physics
  - gpu
  - simulation
  - particle-in-cell
  - pic
  - research
  updated_at: 1661154442.0
ECP-WarpX/impactx:
  data_format: 2
  description: 'ImpactX: the next generation of the IMPACT-Z beam dynamics code'
  filenames:
  - docs/spack.yaml
  full_name: ECP-WarpX/impactx
  latest_release: '22.08'
  readme: "<h1><a id=\"user-content-impactx\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#impactx\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>ImpactX</h1>\n<p><a href=\"https://github.com/ECP-WarpX/impactx/actions/workflows/ubuntu.yml\"\
    ><img src=\"https://github.com/ECP-WarpX/impactx/actions/workflows/ubuntu.yml/badge.svg\"\
    \ alt=\"CI Status\" style=\"max-width: 100%;\"></a>\n<a href=\"https://impactx.readthedocs.io\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/1090ab96071a0b6311590a818911f8b10c5d65e31760367fbaed373f8d727e03/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f696d70616374782f62616467652f3f76657273696f6e3d6c6174657374\"\
    \ alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/impactx/badge/?version=latest\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://spdx.org/licenses/BSD-3-Clause-LBNL.html\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c468c77da60663856e2be1cdd66db538d4bca1b2a3bdf34a76a7f3953e58fc26/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4253442d2d332d2d436c617573652d2d4c424e4c2d626c75652e737667\"\
    \ alt=\"License ImpactX\" data-canonical-src=\"https://img.shields.io/badge/license-BSD--3--Clause--LBNL-blue.svg\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://impactx.readthedocs.io/en/latest/install/users.html\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\"\
    \ alt=\"Supported Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20%7C%20win-blue\"\
    \ style=\"max-width: 100%;\"></a><br>\n<a href=\"https://doi.org/10.5281/zenodo.6954922\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/baf88cee0be27d736412a9f20b5bbbcf3474dd6522e2c3aed8acb112ef750bd8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f444f4925323028736f75726365292d31302e353238312f7a656e6f646f2e363935343932322d626c75652e737667\"\
    \ alt=\"DOI (source)\" data-canonical-src=\"https://img.shields.io/badge/DOI%20(source)-10.5281/zenodo.6954922-blue.svg\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://doi.org/10.48550/arXiv.2208.02382\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0ce1f03bff8ee943abcb746f0129abb1a359bb5e010f3a5d95f5bda86b7fff59/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f444f49253230287061706572292d31302e34383535302f61725869762e323230382e30323338322d626c75652e737667\"\
    \ alt=\"DOI (paper)\" data-canonical-src=\"https://img.shields.io/badge/DOI%20(paper)-10.48550/arXiv.2208.02382-blue.svg\"\
    \ style=\"max-width: 100%;\"></a><br>\n<a href=\"https://en.wikipedia.org/wiki/Software_release_life_cycle\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/af248585e9a4fd6569aaa164ca9557ef2bb80c6659884fc684186c5ecce68c6c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646576656c6f706d656e742532307374617475732d616c7068612d6f72616e67652e737667\"\
    \ alt=\"Development Status\" data-canonical-src=\"https://img.shields.io/badge/development%20status-alpha-orange.svg\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://isocpp.org/\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/5d59fff46d59a1783cc24942cb4eb374014513db99f991164bd051bcd94aa598/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d4325324225324231372d6f72616e67652e737667\"\
    \ alt=\"Language: C++17\" data-canonical-src=\"https://img.shields.io/badge/language-C%2B%2B17-orange.svg\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://python.org/\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/9cf7ec75b074af6953db1304db75950ab917ecd8a1aecb41f0d1191d10872298/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d507974686f6e2d6f72616e67652e737667\"\
    \ alt=\"Language: Python\" data-canonical-src=\"https://img.shields.io/badge/language-Python-orange.svg\"\
    \ style=\"max-width: 100%;\"></a></p>\n<p>ImpactX: the next generation of the\
    \ <a href=\"https://github.com/impact-lbl/IMPACT-Z\">IMPACT-Z</a> code</p>\n<h2><a\
    \ id=\"user-content-documentation\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #documentation\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Documentation</h2>\n\
    <p>In order to learn how to install and run the code, please see the online documentation:\n\
    <a href=\"https://impactx.readthedocs.io\" rel=\"nofollow\">https://impactx.readthedocs.io</a></p>\n\
    <ul>\n<li>ImpactX Doxygen: <a href=\"https://impactx.readthedocs.io/en/latest/_static/doxyhtml\"\
    \ rel=\"nofollow\">https://impactx.readthedocs.io/en/latest/_static/doxyhtml</a>\n\
    </li>\n<li>AMReX Doxygen: <a href=\"https://amrex-codes.github.io/amrex/doxygen\"\
    \ rel=\"nofollow\">https://amrex-codes.github.io/amrex/doxygen</a>\n</li>\n<li>WarpX\
    \ Doxygen: <a href=\"https://warpx.readthedocs.io/en/latest/_static/doxyhtml\"\
    \ rel=\"nofollow\">https://warpx.readthedocs.io/en/latest/_static/doxyhtml</a>\n\
    </li>\n</ul>\n<h2><a id=\"user-content-contributing\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#contributing\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Contributing</h2>\n<p><a href=\"https://amrex-codes.github.io/\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/7053679f4412132d376afadf481432a9d435336f8127e7c8650808bc66d019b2/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d25323272756e732532306f6e253232266d6573736167653d253232414d52655825323226636f6c6f723d253232626c756576696f6c6574253232\"\
    \ alt=\"AMReX\" data-canonical-src=\"https://img.shields.io/static/v1?label=%22runs%20on%22&amp;message=%22AMReX%22&amp;color=%22blueviolet%22\"\
    \ style=\"max-width: 100%;\"></a></p>\n<p>Our workflow is described in <a href=\"\
    CONTRIBUTING.rst\">CONTRIBUTING.rst</a>.</p>\n<h2><a id=\"user-content-developer-environment\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#developer-environment\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Developer Environment</h2>\n\
    <p>Please prepare you local development environment as follows.\nPick <em>one</em>\
    \ of the methods below:</p>\n<h3><a id=\"user-content-perlmutter-nersc\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#perlmutter-nersc\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Perlmutter (NERSC)</h3>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>ssh perlmutter-p1.nersc.gov</pre></div>\n\
    <p>Now <code>cd</code> to your ImpactX source directory.</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>module load cmake/3.22.0\nmodule swap PrgEnv-nvidia\
    \ PrgEnv-gnu\nmodule load cudatoolkit\nmodule load cray-hdf5-parallel/1.12.1.1\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Python</span>\nmodule load\
    \ cray-python/3.9.7.1\n<span class=\"pl-k\">if</span> [ <span class=\"pl-k\">-d</span>\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\"\
    >$HOME</span>/sw/perlmutter/venvs/impactx<span class=\"pl-pds\">\"</span></span>\
    \ ]\n<span class=\"pl-k\">then</span>\n  <span class=\"pl-c1\">source</span> <span\
    \ class=\"pl-smi\">$HOME</span>/sw/perlmutter/venvs/impactx/bin/activate\n<span\
    \ class=\"pl-k\">else</span>\n  python3 -m pip install --user --upgrade pip\n\
    \  python3 -m pip install --user virtualenv\n  python3 -m venv <span class=\"\
    pl-smi\">$HOME</span>/sw/perlmutter/venvs/impactx\n  <span class=\"pl-c1\">source</span>\
    \ <span class=\"pl-smi\">$HOME</span>/sw/perlmutter/venvs/impactx/bin/activate\n\
    \n  python3 -m pip install --upgrade pip\n  MPICC=<span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>cc -target-accel=nvidia80 -shared<span class=\"pl-pds\">\"</span></span>\
    \ python3 -m pip install -U --no-cache-dir -v mpi4py\n  python3 -m pip install\
    \ -r requirements.txt\n<span class=\"pl-k\">fi</span>\n\n<span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> GPU-aware MPI</span>\n<span class=\"pl-k\">export</span>\
    \ MPICH_GPU_SUPPORT_ENABLED=1\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ necessary to use CUDA-Aware MPI and run a job</span>\n<span class=\"pl-k\">export</span>\
    \ CRAY_ACCEL_TARGET=nvidia80\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ optimize CUDA compilation for A100</span>\n<span class=\"pl-k\">export</span>\
    \ AMREX_CUDA_ARCH=8.0\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> compiler\
    \ environment hints</span>\n<span class=\"pl-k\">export</span> CC=cc\n<span class=\"\
    pl-k\">export</span> CXX=CC\n<span class=\"pl-k\">export</span> FC=ftn\n<span\
    \ class=\"pl-k\">export</span> CUDACXX=<span class=\"pl-s\"><span class=\"pl-pds\"\
    >$(</span>which nvcc<span class=\"pl-pds\">)</span></span>\n<span class=\"pl-k\"\
    >export</span> CUDAHOSTCXX=CC</pre></div>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> configure</span>\ncmake\
    \ -S <span class=\"pl-c1\">.</span> -B build_perlmutter -DImpactX_COMPUTE=CUDA\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> compile</span>\ncmake --build\
    \ build_perlmutter -j 10\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ test</span>\nsrun -N 1 --ntasks-per-node=4 -t 0:10:00 -C gpu -c 32 -G 4 --qos=debug\
    \ -A m3906_g ctest --test-dir build_perlmutter --output-on-failure\n\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> run</span>\n<span class=\"pl-c1\">cd</span>\
    \ build_perlmutter/bin\nsrun -N 1 --ntasks-per-node=4 -t 0:10:00 -C gpu -c 32\
    \ -G 4 --qos=debug -A m3906_g ./impactx ../../examples/fodo/input_fodo.in</pre></div>\n\
    <h3><a id=\"user-content-cori-knl-nersc\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#cori-knl-nersc\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Cori KNL (NERSC)</h3>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>ssh cori.nersc.gov</pre></div>\n<p>Now <code>cd</code> to your ImpactX source\
    \ directory.</p>\n<div class=\"highlight highlight-source-shell\"><pre>module\
    \ swap craype-haswell craype-mic-knl\nmodule swap PrgEnv-intel PrgEnv-gnu\nmodule\
    \ load cmake/3.22.1\nmodule load cray-hdf5-parallel/1.10.5.2\nmodule load cray-fftw/3.3.8.10\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Python</span>\nmodule load\
    \ cray-python/3.9.7.1\n<span class=\"pl-k\">if</span> [ <span class=\"pl-k\">-d</span>\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\"\
    >$HOME</span>/sw/knl/venvs/impactx<span class=\"pl-pds\">\"</span></span> ]\n\
    <span class=\"pl-k\">then</span>\n  <span class=\"pl-c1\">source</span> <span\
    \ class=\"pl-smi\">$HOME</span>/sw/knl/venvs/impactx/bin/activate\n<span class=\"\
    pl-k\">else</span>\n  python3 -m pip install --user --upgrade pip\n  python3 -m\
    \ pip install --user virtualenv\n  python3 -m venv <span class=\"pl-smi\">$HOME</span>/sw/knl/venvs/impactx\n\
    \  <span class=\"pl-c1\">source</span> <span class=\"pl-smi\">$HOME</span>/sw/knl/venvs/impactx/bin/activate\n\
    \n  python3 -m pip install --upgrade pip\n  MPICC=<span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>cc -shared<span class=\"pl-pds\">\"</span></span> python3 -m\
    \ pip install -U --no-cache-dir -v mpi4py\n  python3 -m pip install -r requirements.txt\n\
    <span class=\"pl-k\">fi</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ tune exactly for KNL sub-architecture</span>\n<span class=\"pl-k\">export</span>\
    \ CXXFLAGS=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>-march=knl<span\
    \ class=\"pl-pds\">\"</span></span>\n<span class=\"pl-k\">export</span> CFLAGS=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>-march=knl<span class=\"pl-pds\"\
    >\"</span></span></pre></div>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> configure</span>\ncmake\
    \ -S <span class=\"pl-c1\">.</span> -B build_knl\n\n<span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> compile</span>\ncmake --build build_knl -j 8\n\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> test</span>\nsrun -C knl -N 1 -t\
    \ 30 -q debug ctest --test-dir build_knl --output-on-failure\n\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> run</span>\n<span class=\"pl-c1\">cd</span>\
    \ build_knl/bin\nsrun -C knl -N 1 -t 30 -q debug ./impactx ../../examples/fodo/input_fodo.in</pre></div>\n\
    <h3><a id=\"user-content-homebrew-macos\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#homebrew-macos\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Homebrew (macOS)</h3>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>brew update\nbrew install adios2      <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> for openPMD</span>\nbrew install ccache\nbrew install cmake\n\
    brew install fftw\nbrew install git\nbrew install hdf5-mpi    <span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> for openPMD</span>\nbrew install libomp      <span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> for OpenMP</span>\nbrew install\
    \ pkg-config  <span class=\"pl-c\"><span class=\"pl-c\">#</span> for fftw</span>\n\
    brew install open-mpi</pre></div>\n<h3><a id=\"user-content-apt-debianubuntu\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#apt-debianubuntu\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Apt (Debian/Ubuntu)</h3>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>sudo apt update\nsudo apt install\
    \ build-essential ccache cmake g++ git libfftw3-mpi-dev libfftw3-dev libhdf5-openmpi-dev\
    \ libopenmpi-dev pkg-config python3 python3-matplotlib python3-numpy python3-scipy</pre></div>\n\
    <h3><a id=\"user-content-spack-linux\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #spack-linux\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Spack\
    \ (Linux)</h3>\n<div class=\"highlight highlight-source-shell\"><pre>spack env\
    \ create impactx-dev\nspack env activate impactx-dev\nspack add adios2       \
    \ <span class=\"pl-c\"><span class=\"pl-c\">#</span> for openPMD</span>\nspack\
    \ add ccache\nspack add cmake\nspack add fftw\nspack add hdf5          <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> for openPMD</span>\nspack add mpi\nspack\
    \ add pkgconfig     <span class=\"pl-c\"><span class=\"pl-c\">#</span> for fftw</span>\n\
    spack add python\nspack add py-pip\nspack add py-setuptools\nspack add py-wheel\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> OpenMP support on macOS</span>\n\
    [[ <span class=\"pl-smi\">$OSTYPE</span> <span class=\"pl-k\">==</span> <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">'</span>darwin<span class=\"pl-pds\">'</span></span><span\
    \ class=\"pl-k\">*</span> ]] <span class=\"pl-k\">&amp;&amp;</span> spack add\
    \ llvm-openmp\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> optional:\
    \ Linux only</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>spack add\
    \ cuda</span>\n\nspack install\npython3 -m pip install matplotlib numpy openpmd-api\
    \ pandas pytest scipy</pre></div>\n<p>In new terminals, re-activate the environment\
    \ with <code>spack env activate impactx-dev</code> again.</p>\n<h3><a id=\"user-content-conda-linuxmacoswindows\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#conda-linuxmacoswindows\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Conda (Linux/macOS/Windows)</h3>\n\
    <div class=\"highlight highlight-source-shell\"><pre>conda create -n impactx-dev\
    \ -c conda-forge adios2 ccache cmake compilers git hdf5 fftw matplotlib ninja\
    \ numpy pandas pytest scipy\nconda activate impactx-dev\n\n<span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> compile with -DImpactX_MPI=OFF</span></pre></div>\n\
    <h2><a id=\"user-content-get-the-source-code\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#get-the-source-code\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Get the Source Code</h2>\n<p>Before you start, you\
    \ will need a copy of the ImpactX source code:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>git clone git@github.com:ECP-WarpX/impactx.git\n<span class=\"pl-c1\">cd</span>\
    \ impactx</pre></div>\n<h2><a id=\"user-content-compile\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#compile\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Compile</h2>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> find dependencies &amp; configure</span>\n\
    cmake -S <span class=\"pl-c1\">.</span> -B build\n\n<span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> compile</span>\ncmake --build build -j 4</pre></div>\n\
    <p>That's all!\nImpactX binaries are now in <code>build/bin/</code>.\nMost people\
    \ execute these binaries directly or copy them out.</p>\n<p>You can inspect and\
    \ modify build options after running <code>cmake -S . -B</code> build with either</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>ccmake build</pre></div>\n\
    <p>or by adding arguments with <code>-D&lt;OPTION&gt;=&lt;VALUE&gt;</code> to\
    \ the first CMake call, e.g.:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>cmake -S <span class=\"pl-c1\">.</span> -B build -DImpactX_COMPUTE=CUDA\
    \ -DImpactX_MPI=OFF</pre></div>\n<h3><a id=\"user-content-python-compile\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#python-compile\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Python Compile</h3>\n<div class=\"\
    highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> find dependencies &amp; configure</span>\ncmake -S <span class=\"pl-c1\"\
    >.</span> -B build -DImpactX_PYTHON=ON\n\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> compile &amp; install</span>\ncmake --build build -j 4 --target\
    \ pip_install</pre></div>\n<h2><a id=\"user-content-run\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#run\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Run</h2>\n<p>An executable ImpactX binary with the current compile-time\
    \ options encoded in its file name will be created in <code>build/bin/</code>.</p>\n\
    <p>Additionally, a symbolic link named <code>impactx</code> can be found in that\
    \ directory, which points to the last built ImpactX executable.</p>\n<p>The command-line\
    \ syntax for this executable is:</p>\n<div class=\"highlight highlight-text-shell-session\"\
    ><pre><span class=\"pl-c1\">Usage: impactx &lt;inputs-file&gt; [some.overwritten.option=value]...</span>\n\
    \n<span class=\"pl-c1\">Mandatory arguments (remove the &lt;&gt;):</span>\n<span\
    \ class=\"pl-c1\">  inputs-file     the path to an input file; can be relative\
    \ to the current</span>\n<span class=\"pl-c1\">                  working directory\
    \ or absolute.</span>\n<span class=\"pl-c1\">                  Example: input_fodo.in</span>\n\
    \n<span class=\"pl-c1\">Optional arguments (remove the []):</span>\n<span class=\"\
    pl-c1\">  options         this can overwrite any line in an inputs-file</span>\n\
    <span class=\"pl-c1\">                  Example: quad1.ds=0.5 sbend1.rc=1.5</span>\n\
    \n<span class=\"pl-c1\">Examples:</span>\n<span class=\"pl-c1\">  In the current\
    \ working directory, there is a file \"input_fodo.in\" and the</span>\n<span class=\"\
    pl-c1\">  \"impactx\" executable.</span>\n<span class=\"pl-c1\">  The line to\
    \ execute would look like this:</span>\n<span class=\"pl-c1\">    ./impactx input_fodo.in</span>\n\
    \n<span class=\"pl-c1\">  In the current working directory, there is a file \"\
    input_fodo.in\" and the</span>\n<span class=\"pl-c1\">  executable \"impactx\"\
    \ is in a directory that is listed in the \"PATH\"</span>\n<span class=\"pl-c1\"\
    >  environment variable.</span>\n<span class=\"pl-c1\">  The line to execute would\
    \ look like this:</span>\n<span class=\"pl-c1\">    impactx input_fodo.in</span>\n\
    \n<span class=\"pl-c1\">  In the current working directory, there is a file \"\
    input_fodo.in\" and the</span>\n<span class=\"pl-c1\">  \"impactx\" executable.\
    \ We want to voerwrite the segment length of the beamline</span>\n<span class=\"\
    pl-c1\">  element \"quad1\" that is already defined in it. We also want to change\
    \ the</span>\n<span class=\"pl-c1\">  radius of curvature of the bending magnet\
    \ \"sbend1\" to a different value than</span>\n<span class=\"pl-c1\">  in the\
    \ file \"input_fodo.in\".</span>\n<span class=\"pl-c1\">  The line to execute\
    \ would look like this:</span>\n<span class=\"pl-c1\">    ./impactx input_fodo.in\
    \ quad1.ds=0.5 sbend1.rc=1.5</span></pre></div>\n<h2><a id=\"user-content-test\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#test\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Test</h2>\n<p>In order to run our\
    \ tests, you need to have a few Python packages installed:</p>\n<div class=\"\
    highlight highlight-text-shell-session\"><pre><span class=\"pl-c1\">python3 -m\
    \ pip install -U pip setuptools wheel pytest</span>\n<span class=\"pl-c1\">python3\
    \ -m pip install -r examples/requirements.txt</span></pre></div>\n<p>You can run\
    \ all our tests with:</p>\n<div class=\"highlight highlight-text-shell-session\"\
    ><pre><span class=\"pl-c1\">ctest --test-dir build --output-on-failure</span></pre></div>\n\
    <p>Further options:</p>\n<ul>\n<li>help: <code>ctest --test-dir build --help</code>\n\
    </li>\n<li>list all tests: <code>ctest --test-dir build -N</code>\n</li>\n<li>only\
    \ run tests that have \"FODO\" in their name: <code>ctest --test-dir build -R\
    \ FODO</code>\n</li>\n</ul>\n<h2><a id=\"user-content-acknowledgements\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#acknowledgements\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Acknowledgements</h2>\n<p>This\
    \ work was supported by the Laboratory Directed Research and Development Program\
    \ of Lawrence Berkeley National Laboratory under U.S. Department of Energy Contract\
    \ No. DE-AC02-05CH11231.</p>\n<h2><a id=\"user-content-copyright-notice\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#copyright-notice\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Copyright Notice</h2>\n<p>Copyright\
    \ (c) 2022, The Regents of the University of California, through Lawrence Berkeley\
    \ National Laboratory (subject to receipt of any required approvals from the U.S.\
    \ Dept. of Energy).\nAll rights reserved.</p>\n<p>If you have questions about\
    \ your rights to use or distribute this software, please contact Berkeley Lab's\
    \ Intellectual Property Office at <a href=\"mailto:IPO@lbl.gov\">IPO@lbl.gov</a>.</p>\n\
    <p>NOTICE. This Software was developed under funding from the U.S. Department\
    \ of Energy and the U.S. Government consequently retains certain rights.  As such,\
    \ the U.S. Government has been granted for itself and others acting on its behalf\
    \ a paid-up, nonexclusive, irrevocable, worldwide license in the Software to reproduce,\
    \ distribute copies to the public, prepare derivative works, and perform publicly\
    \ and display publicly, and to permit others to do so.</p>\n<p>Please see the\
    \ full license agreement in <a href=\"LICENSE.txt\">LICENSE.txt</a>, which is\
    \ the <code>BSD-3-Clause-LBNL</code> license.</p>\n"
  stargazers_count: 9
  subscribers_count: 7
  topics:
  - simulation
  - beam-dynamics
  - particle-in-cell
  - gpu
  - physics
  - pic
  - particle
  - accelerator
  - research
  updated_at: 1661473023.0
FTHPC/Correlation_Compressibility:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: FTHPC/Correlation_Compressibility
  latest_release: v0.1
  readme: "<h1><a id=\"user-content-compressibility-analysis-correlation_compressibility\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#compressibility-analysis-correlation_compressibility\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Compressibility\
    \ Analysis (Correlation_Compressibility)</h1>\n<h2><a id=\"user-content-statement-of-purpose\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#statement-of-purpose\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Statement of Purpose</h2>\n<p>This\
    \ repo contains scripts to perform compressibility analysis on several leading\
    \ lossy compressors.\nThe compressibility analysis relies on deriving statistics\
    \ on scientific data and explore their relationships to their compression ratios\
    \ from various lossy compressors (based on various compression scheme).\nThe extracted\
    \ relationships between compression ratios and statistical predictors are modeled\
    \ via regression models, which provide a statistical framework to predict compression\
    \ ratios for the different studied lossy compressors.</p>\n<p>This repo contains\
    \ an automatic framework of scripts that perform the compression of scientific\
    \ datasets from 8 compressors (SZ2, ZFP, MGARD, FPZIP, Digit Rounding and Bit\
    \ Grooming), the derivation of the statistical predictors of compression ratios\
    \ (SVD, standard deviation, quantized entropy), and scripts to perform the training\
    \ of the regression models (linear and spline regressions) as well as the validation\
    \ of the regression predictions.\nA runtime analysis is also performed and associated\
    \ codes are provided.</p>\n<h3><a id=\"user-content-main-code-structures\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#main-code-structures\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Main code structures</h3>\n<p>Compression\
    \ metrics, including compression ratios, and derivation of statistical predictors\
    \ (SVD, standard deviation, quantized entropy) codes are found in <code>compress_package</code>\
    \ and are run via <code>scripts/run.sh</code> as described in the section \"How\
    \ to compute statistical predictors and compression analysis on datasets\".\n\
    Linear and spline regressions training and validation (functions <code>cr_regression_linreg</code>\
    \ and <code>cr_regression_gam</code> from the script <code>replicate_figures/functions_paper.R</code>).\n\
    Codes for the different runtime analysis are found in the folder <code>runtime_analysis</code>\
    \ and are automated with the script <code>runtime.sh</code>, the study includes\
    \ compression time for SZ2, ZFP, MAGRD, FPZIP, data quantization, SVD, local (tiled)\
    \ variogram and local (tiled) variogram, and runtime for training and prediction\
    \ of the regressions.<br>\nFinally, the script <code>replicate_figures/graphs_paper_container.R</code>\
    \ replicates and saves all the figures from the paper ad as well as numbers from\
    \ the tables.</p>\n<p>For each dataset in the <code>dataset</code> folder, slicing\
    \ is performed for each variable field (e.g. density in Miranda), each slice is\
    \ stored in a class. The class is updated as compressions with the 8 compressors\
    \ is performed and updated as the statistical predictors are derived. Results\
    \ of each class are stored in a .csv file (example of csv files can be found at\
    \ <code>replicate_figures/generated_data/</code>).\nAll the datasets stored in\
    \ the <code>dataset</code> folder can be analyzed with the given set of codes,\
    \ one needs to source <code>scripts/config.json</code> with the appropriate dataset\
    \ name as described in the below section \"How to compute statistical predictors\
    \ and compression analysis on datasets\".\nThe regression analysis and its prediction\
    \ is then performed on R dataframes based on the aforementioned .csv files.</p>\n\
    <h2><a id=\"user-content-system-information\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#system-information\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>System Information</h2>\n<p>The hardware and software\
    \ versions used for the performance evaluations can be found in the table below.\
    \ These nodes come from Clemson University's Palmetto Cluster.</p>\n<p>These nodes\
    \ have:</p>\n<table>\n<thead>\n<tr>\n<th>component</th>\n<th>version</th>\n<th>component</th>\n\
    <th>version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>CPU</td>\n<td>Intel Xeon\
    \ 6148G (40 cores)</td>\n<td>sz2</td>\n<td>2.1.12.2</td>\n</tr>\n<tr>\n<td>GPU</td>\n\
    <td>2 Nvidia v100</td>\n<td>sz3</td>\n<td>3.1.3.1</td>\n</tr>\n<tr>\n<td>Memory</td>\n\
    <td>372GB</td>\n<td>zfp</td>\n<td>0.5.5</td>\n</tr>\n<tr>\n<td>Network</td>\n\
    <td>2 Mellanox MT27710 (HDR)</td>\n<td>mgard</td>\n<td>1.0.0</td>\n</tr>\n<tr>\n\
    <td>FileSystem</td>\n<td>BeeGFS 7.2.3 (24 targets)</td>\n<td>bit grooming</td>\n\
    <td>2.1.9</td>\n</tr>\n<tr>\n<td>Compiler</td>\n<td>GCC 8.4.1</td>\n<td>digit\
    \ rounding</td>\n<td>2.1.9</td>\n</tr>\n<tr>\n<td>OS</td>\n<td>CentOS 8.2.2004</td>\n\
    <td>R</td>\n<td>4.1.3</td>\n</tr>\n<tr>\n<td>MPI</td>\n<td>OpenMPI 4.0.5</td>\n\
    <td>Python</td>\n<td>3.9.12</td>\n</tr>\n<tr>\n<td>LibPressio</td>\n<td>0.83.4</td>\n\
    <td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h2><a id=\"user-content-first-time-setup\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#first-time-setup\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>First time setup</h2>\n<h3><a\
    \ id=\"user-content-container-installation-for-ease-of-setup\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#container-installation-for-ease-of-setup\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Container Installation\
    \ (for ease of setup)</h3>\n<p>We provide a container for <code>x86_64</code>\
    \ image for ease of installation.</p>\n<p>This container differs from our experimental\
    \ setup slightly. The production build used <code>-march=native -mtune=native</code>\
    \ for architecture optimized builds where as the container does not use these\
    \ flags to maximize compatibility across <code>x86_64</code> hardware.</p>\n<p>NOTE\
    \ this file is &gt;= 11 GB , download with caution.</p>\n<h4><a id=\"user-content-docker\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#docker\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Docker</h4>\n<p>Many other systems\
    \ can use podman or docker.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>docker pull ghcr.io/fthpc/correlation_compressibility:latest\n\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span>most systems</span>\ndocker run -it --rm ghcr.io/fthpc/correlation_compressibility:latest\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> if running on a SeLinux enforcing\
    \ system</span>\ndocker run -it --rm --security-opt label=disable ghcr.io/fthpc/correlation_compressibility:latest</pre></div>\n\
    <h3><a id=\"user-content-building-the-container\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#building-the-container\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Building the Container</h3>\n<p>You can build the\
    \ container yourself as follows:\nNOTE this process takes 3+ hours on a modern\
    \ laptop, and most clusters do not\nprovide sufficient permissions to run container\
    \ builds on the cluster.</p>\n<p>Additionally compiling MGRAD -- one of the compressors\
    \ we use takes &gt;= 4GB RAM per core, be cautious\nwith systems with low RAM.\
    \  You may be able compensate by using fewer cores by changing the spack install\n\
    instruction in the Dockerfile to have a <code>-j N</code> where <code>N</code>\
    \ is the number of cores you wish to use</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> install/module load git-lfs,\
    \ needed to download example_data for building the container</span>\nsudo dnf\
    \ install git-lfs <span class=\"pl-c\"><span class=\"pl-c\">#</span>Fedora/CentOS\
    \ Stream 8</span>\nsudo apt-get install git-lfs <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> Ubuntu</span>\nspack install git-lfs<span class=\"pl-k\">;</span>\
    \ spack load git-lfs <span class=\"pl-c\"><span class=\"pl-c\">#</span> using\
    \ spack</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> clone this\
    \ repository</span>\ngit clone --recursive https://github.com/FTHPC/Correlation_Compressibility\n\
    <span class=\"pl-c1\">cd</span> Correlation_Compressibility\ndocker build <span\
    \ class=\"pl-c1\">.</span> -t correlation_compressibility</pre></div>\n<h3><a\
    \ id=\"user-content-manual-installation\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#manual-installation\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Manual Installation</h3>\n<p>By default, it is recommended to follow\
    \ the install locations that are indicated on the top of <code>scripts/run.sh</code>\n\
    and the top of <code>config.json</code>. These two files provide the configuration\
    \ options to get the program running.</p>\n<p>Spack should be installed in the\
    \ following location: <code>$HOME/spack/</code></p>\n<p>This Github repo should\
    \ be cloned in the following location: <code>$HOME/Correlation_Compressibility/</code>\n\
    This location is also referenced as the <code>COMPRESS_HOME</code> environment\
    \ variable.</p>\n<p>A dataset folder called 'datasets' should be in the following\
    \ location: <code>$HOME/Correlation_Compressibility/datasets/</code>.</p>\n<p>Clone\
    \ the repo but make sure to install or load <code>git-lfs</code> first.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> install/module load git-lfs, needed to download example_data\
    \ for building the container</span>\nsudo dnf install git-lfs <span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span>Fedora/CentOS Stream 8</span>\nsudo apt-get install\
    \ git-lfs <span class=\"pl-c\"><span class=\"pl-c\">#</span> Ubuntu</span>\nspack\
    \ install git-lfs<span class=\"pl-k\">;</span> spack load git-lfs <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> using spack</span>\n\n<span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> clone this repository</span>\ngit clone https://github.com/FTHPC/Correlation_Compressibility\
    \ <span class=\"pl-smi\">$HOME</span>/Correlation_Compressibility\n<span class=\"\
    pl-c1\">cd</span> <span class=\"pl-smi\">$HOME</span>/Correlation_Compressibility</pre></div>\n\
    <p>If you forgot to install <code>git-lfs</code> before and have an empty file\
    \ in the  <code>datasets</code> folder, you should install <code>git-lfs</code>\n\
    and then run the following:</p>\n<pre><code>git lfs fetch\ngit lfs checkout\n\
    </code></pre>\n<p>Once Spack is installed, there is a <code>spack.yaml</code>\
    \ configuration file containing the Spack environment necessary to run the program.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c1\">cd</span>\
    \ <span class=\"pl-smi\">$HOME</span>\ngit clone --depth=1 https://github.com/spack/spack\n\
    git clone --depth=1 https://github.com/robertu94/spack_packages \n<span class=\"\
    pl-c1\">source</span> ./spack/share/spack/setup-env.sh \nspack compiler find\n\
    spack external find \nspack repo add --scope=site ./spack_packages \n<span class=\"\
    pl-c1\">cd</span> <span class=\"pl-smi\">$HOME</span>/Correlation_Compressibility\
    \ \nspack env activate <span class=\"pl-c1\">.</span>\nspack install\n<span class=\"\
    pl-k\">export</span> COMPRESS_HOME=<span class=\"pl-smi\">$HOME</span>/Correlation_Compressibility\
    \ </pre></div>\n<p>These commands will install the environment. The environment\
    \ only needs to be installed once.\nIf you are using an older &lt; gcc11, then\
    \ you will need to add the following to the <code>spack.yaml</code> file:</p>\n\
    <pre><code>^libstdcompat+boost\n</code></pre>\n<p>after <code>^mgard@robertu94+cuda</code>\
    \ but before the <code>,</code>.</p>\n<h2><a id=\"user-content-replication-of-results\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#replication-of-results\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Replication of\
    \ Results</h2>\n<h3><a id=\"user-content-how-to-compute-statistical-predictors-and-compression-metrics-on-datasets\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#how-to-compute-statistical-predictors-and-compression-metrics-on-datasets\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>How to compute\
    \ statistical predictors and compression metrics on datasets</h3>\n<p>In order\
    \ to run the statistical analysis that computes the statistical predictors (SVD,\
    \ standard deviation, quantized entropy) of compression ratios, a dataset and\
    \ a configuration file must be specified.\nTEST is a dataset that is specified\
    \ within the <code>config.json</code> file.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sh scripts/run.sh -c config.json -d TEST -n 2</pre></div>\n<p>The command\
    \ above performs the computation of statistical predictors and writes output to\
    \ the output file specified in the configuration file.\nThis will use local hardware\
    \ without a scheduler. Use <code>-n</code> to specify the MPI processes on your\
    \ local system. Default value is 32.\nIt is recommended that this value matches\
    \ your CPU core count.</p>\n<p>If one has the PBS scheduler and runs outside of\
    \ the container, feel free to use flags <code>-p</code> or <code>-s</code> for\
    \ job execution.\n<code>-p</code> will schedule multiple jobs based on the quantized\
    \ error bounds and error bound types for a specified dataset.\n<code>-s</code>\
    \ will schedule a single job grouping all the analysis for a specified dataset.</p>\n\
    <p>See <code>-h</code> for more options or help with syntax.</p>\n<p>If a dataset\
    \ is wanted to run, the <code>config.json</code> file provides options to add\
    \ datasets.\nThe following options must be added when adding another dataset in\
    \ the configuration file:</p>\n<div class=\"highlight highlight-source-json\"\
    ><pre><span class=\"pl-ent\">\"_comment\"</span> : \n{\n    <span class=\"pl-ent\"\
    >\"folder\"</span>            : <span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span>folder containing h5 or binary files<span class=\"pl-pds\">\"</span></span>,\n\
    \    <span class=\"pl-ent\">\"data_dimensions\"</span>   : <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>dimensions of the datasets within dataset_folder.\
    \ Either 1x2 or 1x3. EX: '1028, 1028'<span class=\"pl-pds\">\"</span></span>,\n\
    \    <span class=\"pl-ent\">\"slice_dimensions\"</span>  : <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>list of the dimensions wanted: EX: 'None' or\
    \ 'X, Y, Z'<span class=\"pl-pds\">\"</span></span>,\n    <span class=\"pl-ent\"\
    >\"output\"</span>            : <span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span>name of the output csv file: EX: 'test.csv'<span class=\"pl-pds\">\"</span></span>,\n\
    \    <span class=\"pl-ent\">\"dtype\"</span>             : <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>data type. can be 'float32' or 'float64'<span\
    \ class=\"pl-pds\">\"</span></span>,\n    <span class=\"pl-ent\">\"parse_info\"\
    </span>        : <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type of\
    \ parsing needed: 'None', 'slice', 'gaussian', 'gaussian_multi', 'spatialweight',\
    \ or 'scalarweight'<span class=\"pl-pds\">\"</span></span>,\n    <span class=\"\
    pl-ent\">\"dataset_name\"</span>      : <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>necessary accessing 2D HDF5 files: 'standard' if not custom. custom\
    \ EX: 'Z'<span class=\"pl-pds\">\"</span></span>\n} </pre></div>\n<p>From this\
    \ section, .csv files are generated for each dataset and contain all the statistical\
    \ predictors described in the paper as well as compression metrcis including compresison\
    \ ratios for the 8 lossy compressors and 4 error bounds.</p>\n<h3><a id=\"user-content-to-run-the-training-and-prediction-timing-analysis-demonstration\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#to-run-the-training-and-prediction-timing-analysis-demonstration\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To run the\
    \ training and prediction timing analysis demonstration</h3>\n<p>In order to run\
    \ the timing analysis, a dataset must be specified.\nThere are two datasets setup\
    \ within this demonstration.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sh runtime_analysis/runtime.sh -d [DATASET]</pre></div>\n<p>[DATASET] can\
    \ be either [NYX] or [SCALE]</p>\n<p>After running the above script, an *.RData\
    \ file(s) will be produced giving the approprirate timing information of\nthe\
    \ training and prediction for the regression models.</p>\n<p>Note: A quicker and\
    \ more efficient quantized entropy method is demonstrated in <code>qentropy.cc</code></p>\n\
    <h4><a id=\"user-content-the-following-below-runs-qentropycc\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#the-following-below-runs-qentropycc\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>The following below runs <code>qentropy.cc</code>\n\
    </h4>\n<div class=\"highlight highlight-source-shell\"><pre>g++ -std=c++2a -O3\
    \ qentropy.cc -o qentropy -march=native -mtune=native\n./qentropy</pre></div>\n\
    <p>Note: Please run the runtime analysis for both datasets before running the\
    \ following section.</p>\n<h3><a id=\"user-content-replication-of-figures-how-to-run-statistical-prediction-of-compression-ratios-and-the-prediction-validation\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#replication-of-figures-how-to-run-statistical-prediction-of-compression-ratios-and-the-prediction-validation\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Replication\
    \ of figures: how to run statistical prediction of compression ratios and the\
    \ prediction validation</h3>\n<p>The script <code>graphs_paper_container.R</code>\
    \  saves the graphs presented in the paper and provides associated validation\
    \ metrics (correlation and median absolute error percentage).</p>\n<p>The script\
    \ <code>graphs_paper_container.R</code> will source the scripts  <code>load_dataset_paper.R</code>\
    \ and <code>functions_paper.R</code> that respectively load the dataset of interest\
    \ and perform the regression analysis (training and prediction in cross-validation).\n\
    As a consequence the scripts  <code>load_dataset_paper.R</code> and <code>functions_paper.R</code>\
    \ do not need to be run by the user.</p>\n<p>The script <code>graphs_paper_container.R</code>\
    \  is run via the command:\n<code>bash sh replicate.sh</code></p>\n<p>From running\
    \ the script once, it will save all Figures 1, 3, 4 and 5 into .png files from\
    \ the paper as well as corresponding validation metrics.\nFigure 2 is not saved\
    \ as it provides a simple vizualization of slices of the datasets.\nSlices of\
    \ the datasets are generated in the Section \"How to compute statistical predictors\
    \ and compression metrics\" and can be stored, however we do not save them here\
    \ to save space in the container.\nNumbers for Tables 2, 3 and 5 are printed in\
    \ the R console.\nAll printed validation metrics are save into a file named <code>figure_replication.log</code>.\n\
    Figures and the log-file are saved in the same folder as the one where R script\
    \ is run and the filename structure is <code>figY_*.png</code> with Y is the figure\
    \ number reference in the paper and <code>*</code> provides additional informnation\
    \ about the data and the compressor.<br>\nNumbers for Table 4 are saved in the\
    \ last section in .txt files <code>statistic_benchmark_runtime_X.txt</code> with\
    \ X the studied dataset (NYX or SCALE).</p>\n<p>In order to limit the container\
    \ size to aid reproducibility, we only added a restricted number of scientific\
    \ datasets in the container and we rely on csv files from our production runs\
    \ (saved as described above in the Section \"How to compute statistical predictors\
    \ on datasets\").\nMore datasets are available on <a href=\"https://sdrbench.github.io\"\
    \ rel=\"nofollow\">SDRBench</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1648227729.0
HEPonHPC/hepnos_eventselection:
  data_format: 2
  description: null
  filenames:
  - docker/hepnos/spack.yaml
  full_name: HEPonHPC/hepnos_eventselection
  latest_release: null
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1658856345.0
LLNL/UnifyFS:
  data_format: 2
  description: 'UnifyFS: A file system for burst buffers'
  filenames:
  - .spack-env/unifyfs-lsf-gcc8_3_1/spack.yaml
  - .spack-env/unifyfs-slurm-gcc10_2_1/spack.yaml
  - .spack-env/unifyfs-lsf-gcc4_9_3/spack.yaml
  - .spack-env/unifyfs-slurm-gcc4_9_3/spack.yaml
  full_name: LLNL/UnifyFS
  latest_release: v1.0
  readme: '<h1><a id="user-content-unifyfs-a-distributed-burst-buffer-file-system"
    class="anchor" aria-hidden="true" href="#unifyfs-a-distributed-burst-buffer-file-system"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>UnifyFS: A Distributed
    Burst Buffer File System</h1>

    <p>Node-local burst buffers are becoming an indispensable hardware resource on

    large-scale supercomputers to buffer the bursty I/O from scientific

    applications. However, there is a lack of software support for burst buffers to

    be efficiently shared by applications within a batch-submitted job and recycled

    across different batch jobs. In addition, burst buffers need to cope with a

    variety of challenging I/O patterns from data-intensive scientific

    applications.</p>

    <p>UnifyFS is a user-level burst buffer file system under active development.

    UnifyFS supports scalable and efficient aggregation of I/O bandwidth from burst

    buffers while having the same life cycle as a batch-submitted job. While UnifyFS

    is designed for N-N write/read, UnifyFS compliments its functionality with the

    support for N-1 write/read. It efficiently accelerates scientific I/O based on

    scalable metadata indexing, co-located I/O delegation, and server-side read

    clustering and pipelining.</p>

    <h2><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h2>

    <p>UnifyFS documentation is at <a href="https://unifyfs.readthedocs.io" rel="nofollow">https://unifyfs.readthedocs.io</a>.</p>

    <p>For instructions on how to build and install UnifyFS,

    see <a href="http://unifyfs.readthedocs.io/en/dev/build.html" rel="nofollow">Build
    UnifyFS</a>.</p>

    <h2><a id="user-content-build-status" class="anchor" aria-hidden="true" href="#build-status"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build Status</h2>

    <p>Status of UnifyFS development branch (dev):</p>

    <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/LLNL/UnifyFS/actions/workflows/build-and-test.yml/badge.svg?branch=dev"><img
    src="https://github.com/LLNL/UnifyFS/actions/workflows/build-and-test.yml/badge.svg?branch=dev"
    alt="Build Status" style="max-width: 100%;"></a></p>

    <p><a href="https://unifyfs.readthedocs.io" rel="nofollow"><img src="https://camo.githubusercontent.com/e83e6f0dfc2d353a5c6d482643646205f8fcc8e0b3327cb32dc9b27292e16823/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f756e69667966732f62616467652f3f76657273696f6e3d646576"
    alt="Read the Docs" data-canonical-src="https://readthedocs.org/projects/unifyfs/badge/?version=dev"
    style="max-width: 100%;"></a></p>

    <h2><a id="user-content-contribute-and-develop" class="anchor" aria-hidden="true"
    href="#contribute-and-develop"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contribute
    and Develop</h2>

    <p>If you would like to help, please see our <a href="https://unifyfs.readthedocs.io/en/dev/contribute-ways.html"
    rel="nofollow">contributing guidelines</a>.</p>

    '
  stargazers_count: 79
  subscribers_count: 17
  topics:
  - system-software
  - burst-buffers
  - file-system
  updated_at: 1657609635.0
LLNL/hiop:
  data_format: 2
  description: HPC solver for nonlinear optimization problems
  filenames:
  - scripts/platforms/marianas/spack.yaml
  - scripts/platforms/newell/spack.yaml
  full_name: LLNL/hiop
  latest_release: v0.6.2
  readme: "<h1><a id=\"user-content-hiop---hpc-solver-for-optimization\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#hiop---hpc-solver-for-optimization\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>HiOp - HPC solver\
    \ for optimization</h1>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"\
    https://github.com/LLNL/hiop/workflows/tests/badge.svg\"><img src=\"https://github.com/LLNL/hiop/workflows/tests/badge.svg\"\
    \ alt=\"tests\" style=\"max-width: 100%;\"></a></p>\n<p>HiOp is an optimization\
    \ solver for solving certain mathematical optimization problems expressed as nonlinear\
    \ programming problems. HiOp is a lightweight HPC solver that leverages application's\
    \ existing data parallelism to parallelize the optimization iterations by using\
    \ specialized parallel linear algebra kernels.</p>\n<p>Please cite the user manual\
    \ whenever HiOp is used:</p>\n<pre><code>@TECHREPORT{hiop_techrep,\n  title={{HiOp}\
    \ -- {U}ser {G}uide},\n  author={Petra, Cosmin G. and Chiang, NaiYuan and Jingyi\
    \ Wang},\n  year={2018},\n  institution = {Center for Applied Scientific Computing,\
    \ Lawrence Livermore National Laboratory},\n  number = {LLNL-SM-743591}\n}\n</code></pre>\n\
    <p>In addition, when using the quasi-Newton solver please cite:</p>\n<pre><code>@ARTICLE{Petra_18_hiopdecomp,\n\
    title = {A memory-distributed quasi-Newton solver for nonlinear programming problems\
    \ with a small number of general constraints},\njournal = {Journal of Parallel\
    \ and Distributed Computing},\nvolume = {133},\npages = {337-348},\nyear = {2019},\n\
    issn = {0743-7315},\ndoi = {https://doi.org/10.1016/j.jpdc.2018.10.009},\nurl\
    \ = {https://www.sciencedirect.com/science/article/pii/S0743731518307731},\nauthor\
    \ = {Cosmin G. Petra},\n}\n</code></pre>\n<p>and when using the the PriDec solver\
    \ please cite:</p>\n<pre><code>@article{wang2022,\n  archivePrefix = {arXiv},\n\
    \  eprint = {arXiv:2204.09631},\n  author = {J. Wang and C. G. Petra},\n  title\
    \ = {An optimization algorithm for nonsmooth nonconvex problems with upper-$C^2$\
    \ objective},\n  publisher = {arXiv},\n  year = {2022},\n  journal={ (submitted)\
    \ },\n}\n@INPROCEEDINGS{wang2021,\n  author={Wang, Jingyi and Chiang, Nai-Yuan\
    \ and Petra, Cosmin G.},\n  booktitle={2021 20th International Symposium on Parallel\
    \ and Distributed Computing (ISPDC)}, \n  title={An asynchronous distributed-memory\
    \ optimization solver for two-stage stochastic programming problems}, \n  year={2021},\n\
    \  volume={},\n  number={},\n  pages={33-40},\n  doi={10.1109/ISPDC52870.2021.9521613}\n\
    }\n</code></pre>\n<h2><a id=\"user-content-buildinstall-instructions\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#buildinstall-instructions\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Build/install instructions</h2>\n\
    <p>HiOp uses a CMake-based build system. A standard build can be done by invoking\
    \ in the 'build' directory the following</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$<span class=\"pl-k\">&gt;</span> cmake ..\n$<span class=\"pl-k\">&gt;</span>\
    \ make \n$<span class=\"pl-k\">&gt;</span> make <span class=\"pl-c1\">test</span>\n\
    $<span class=\"pl-k\">&gt;</span> make install</pre></div>\n<p>This sequence will\
    \ build HiOp, run integrity and correctness tests, and install the headers and\
    \ the library in the directory '_dist-default-build' in HiOp's root directory.</p>\n\
    <p>Command <code>make test</code> runs extensive tests of the various modules\
    \ of HiOp to check integrity and correctness. The tests suite range from unit\
    \ testing to solving concrete optimization problems and checking the performance\
    \ of HiOp solvers on these problems against known solutions. By default <code>make\
    \ test</code> runs <code>mpirun</code> locally, which may not work on some HPC\
    \ machines. For these HiOp allows using <code>bsub</code> to schedule <code>make\
    \ test</code> on the compute nodes; to enable this, the use should use <em>-DHIOP_TEST_WITH_BSUB=ON</em>\
    \ with cmake when building and run <code>make test</code> in a bsub shell session,\
    \ for example,</p>\n<pre><code>bsub -P your_proj_name -nnodes 1 -W 30\nmake test\n\
    CTRL+D\n</code></pre>\n<p>The installation can be customized using the standard\
    \ CMake options. For example, one can provide an alternative installation directory\
    \ for HiOp by using</p>\n<div class=\"highlight highlight-source-shell\"><pre>$<span\
    \ class=\"pl-k\">&gt;</span> cmake -DCMAKE_INSTALL_PREFIX=/usr/lib/hiop ..<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">'</span></span></pre></div>\n<h3><a id=\"\
    user-content-selected-hiop-specific-build-options\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#selected-hiop-specific-build-options\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Selected HiOp-specific build options</h3>\n\
    <ul>\n<li>Enable/disable MPI: <em>-DHIOP_USE_MPI=[ON/OFF]</em> (by default ON)</li>\n\
    <li>GPU support: <em>-DHIOP_USE_GPU=ON</em>. MPI can be either off or on. For\
    \ more build system options related to GPUs, see \"Dependencies\" section below.</li>\n\
    <li>Enable/disable \"developer mode\" build that enforces more restrictive compiler\
    \ rules and guidelines: <em>-DHIOP_DEVELOPER_MODE=ON</em>. This option is by default\
    \ off.</li>\n<li>Additional checks and self-diagnostics inside HiOp meant to detect\
    \ abnormalities and help to detect bugs and/or troubleshoot problematic instances:\
    \ <em>-DHIOP_DEEPCHECKS=[ON/OFF]</em> (by default ON). Disabling HIOP_DEEPCHECKS\
    \ usually provides 30-40% execution speedup in HiOp. For full strength, it is\
    \ recommended to use HIOP_DEEPCHECKS with debug builds. With non-debug builds,\
    \ in particular the ones that disable the assert macro, HIOP_DEEPCHECKS does not\
    \ perform all checks and, thus, may overlook potential issues.</li>\n</ul>\n<p>For\
    \ example:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$<span class=\"\
    pl-k\">&gt;</span> cmake -DHIOP_USE_MPI=ON -DHIOP_DEEPCHECKS=ON ..\n$<span class=\"\
    pl-k\">&gt;</span> make \n$<span class=\"pl-k\">&gt;</span> make <span class=\"\
    pl-c1\">test</span>\n$<span class=\"pl-k\">&gt;</span> make install</pre></div>\n\
    <h3><a id=\"user-content-other-useful-options-to-use-with-cmake\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#other-useful-options-to-use-with-cmake\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Other useful\
    \ options to use with CMake</h3>\n<ul>\n<li>\n<em>-DCMAKE_BUILD_TYPE=Release</em>\
    \ will build the code with the optimization flags on</li>\n<li>\n<em>-DCMAKE_CXX_FLAGS=\"\
    -O3\"</em> will enable a high level of compiler code optimization</li>\n</ul>\n\
    <h3><a id=\"user-content-dependencies\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#dependencies\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Dependencies</h3>\n<p>HiOp requires LAPACK and BLAS. These dependencies\
    \ are automatically detected by the build system. MPI is optional and by default\
    \ enabled. To disable use cmake option '-DHIOP_USE_MPI=OFF'.</p>\n<p>HiOp has\
    \ some support for NVIDIA <strong>GPU-based computations</strong> via CUDA and\
    \ Magma. To enable the use of GPUs, use cmake with '-DHIOP_USE_GPU=ON'. The build\
    \ system will automatically search for CUDA Toolkit. For non-standard CUDA Toolkit\
    \ installations, use '-DHIOP_CUDA_LIB_DIR=/path' and '-DHIOP_CUDA_INCLUDE_DIR=/path'.\
    \ For \"very\" non-standard CUDA Toolkit installations, one can specify the directory\
    \ of cuBlas libraries as well with '-DHIOP_CUBLAS_LIB_DIR=/path'.</p>\n<h3><a\
    \ id=\"user-content-using-raja-and-umpire-portability-libraries\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#using-raja-and-umpire-portability-libraries\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using RAJA and\
    \ Umpire portability libraries</h3>\n<p>Portability libraries allow running HiOp's\
    \ linear algebra either on host (CPU) or a device (GPU). RAJA and Umpire are disabled\
    \ by default. You can turn them on together by passing <code>-DHIOP_USE_RAJA=ON</code>\
    \ to CMake. If the two libraries are not automatically found, specify their installation\
    \ directories like this:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$<span class=\"pl-k\">&gt;</span> cmake -DHIOP_USE_RAJA=ON -DRAJA_DIR=/path/to/raja/dir\
    \ -Dumpire_DIR=/path/to/umpire/dir</pre></div>\n<p>If the GPU support is enabled,\
    \ RAJA will run all HiOp linear algebra kernels on GPU, otherwise RAJA will run\
    \ the kernels on CPU using an OpenMP execution policy.</p>\n<h3><a id=\"user-content-support-for-gpu-computations\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#support-for-gpu-computations\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Support\
    \ for GPU computations</h3>\n<p>When GPU support is on, HiOp requires Magma linear\
    \ solver library and CUDA Toolkit. Both are detected automatically in most cases.\
    \ The typical cmake command to enable GPU support in HiOp is</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>$<span class=\"pl-k\">&gt;</span> cmake\
    \ -DHIOP_USE_GPU=ON ..</pre></div>\n<p>When Magma is not detected, one can specify\
    \ its location by passing <code>-DHIOP_MAGMA_DIR=/path/to/magma/dir</code> to\
    \ cmake.</p>\n<p>For custom CUDA Toolkit installations, the locations to the (missing/not\
    \ found) CUDA libraries can be specified to cmake via <code>-DNAME=/path/cuda/directory/lib</code>,\
    \ where <code>NAME</code> can be any of</p>\n<pre><code>CUDA_cublas_LIBRARY\n\
    CUDA_CUDART_LIBRARY\nCUDA_cudadevrt_LIBRARY\nCUDA_cusparse_LIBRARY\nCUDA_cublasLt_LIBRARY\n\
    CUDA_nvblas_LIBRARY\nCUDA_culibos_LIBRARY\n</code></pre>\n<p>Below is an example\
    \ for specifiying <code>cuBlas</code>, <code>cuBlasLt</code>, and <code>nvblas</code>\
    \ libraries, which were <code>NOT_FOUND</code> because of a non-standard CUDA\
    \ Toolkit instalation:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$<span\
    \ class=\"pl-k\">&gt;</span> cmake -DHIOP_USE_GPU=ON -DCUDA_cublas_LIBRARY=/usr/local/cuda-10.2/targets/x86_64-linux/lib/lib64\
    \ -DCUDA_cublasLt_LIBRARY=/export/home/petra1/work/installs/cuda10.2.89/targets/x86_64-linux/lib/\
    \ -DCUDA_nvblas_LIBRARY=/export/home/petra1/work/installs/cuda10.2.89/targets/x86_64-linux/lib/\
    \ .. <span class=\"pl-k\">&amp;&amp;</span> make -j <span class=\"pl-k\">&amp;&amp;</span>\
    \ make install</pre></div>\n<p>A detailed example on how to compile HiOp straight\
    \ of the box on <code>summit.olcf.ornl.gov</code> is available <a href=\"README_summit.md\"\
    >here</a>.</p>\n<p>RAJA and UMPIRE dependencies are usually detected by HiOp's\
    \ cmake build system.</p>\n<h3><a id=\"user-content-kron-reduction\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#kron-reduction\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Kron reduction</h3>\n<p>Kron reduction\
    \ functionality of HiOp is disabled by default. One can enable it by using</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>$<span class=\"pl-k\">&gt;</span>\
    \ rm -rf <span class=\"pl-k\">*</span><span class=\"pl-k\">;</span> cmake -DHIOP_WITH_KRON_REDUCTION=ON\
    \ -DUMFPACK_DIR=/Users/petra1/work/installs/SuiteSparse-5.7.1 -DMETIS_DIR=/Users/petra1/work/installs/metis-4.0.3\
    \ .. <span class=\"pl-k\">&amp;&amp;</span> make -j <span class=\"pl-k\">&amp;&amp;</span>\
    \ make install</pre></div>\n<p>Metis is usually detected automatically and needs\
    \ not be specified under normal circumstances.</p>\n<p>UMFPACK (part of SuiteSparse)\
    \ and METIS need to be provided as shown above.</p>\n<h1><a id=\"user-content-interfacing-with-hiop\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#interfacing-with-hiop\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Interfacing with\
    \ HiOp</h1>\n<p>HiOp supports three types of optimization problems, each with\
    \ a separate input formats in the form of the C++ interfaces <code>hiopInterfaceDenseConstraints</code>,<code>hiopInterfaceSparse</code>\
    \ and <code>hiopInterfaceMDS</code>. These interfaces are specified in <a href=\"\
    src/Interface/hiopInterface.hpp\">hiopInterface.hpp</a> and documented and discussed\
    \ as well in the <a href=\"doc/hiop_usermanual.pdf\">user manual</a>.</p>\n<p><em><code>hiopInterfaceDenseConstraints</code>\
    \ interface</em> supports NLPs with <strong>billions</strong> of variables with\
    \ and without bounds but only limited number (&lt;100) of general, equality and\
    \ inequality constraints. The underlying algorithm is a limited-memory quasi-Newton\
    \ interior-point method and generally scales well computationally (but it may\
    \ not algorithmically) on thousands of cores. This interface uses MPI for parallelization</p>\n\
    <p><em><code>hiopInterfaceSparse</code> interface</em> supports general sparse\
    \ and large-scale NLPs. This functionality is similar to that of the state-of-the-art\
    \ <a href=\"https://github.com/coin-or/Ipopt\">Ipopt</a> (without being as robust\
    \ and flexible as Ipopt is). Acceleration for this class of problems can be achieved\
    \ via OpenMP or CUDA, however, this is work in progress and you are encouraged\
    \ to contact HiOp's developers for up-to-date information.</p>\n<p><em><code>hiopInterfaceMDS</code>\
    \ interface</em> supports mixed dense-sparse NLPs and achives parallelization\
    \ using GPUs and RAJA portability abstraction layer.</p>\n<p>More information\
    \ on the HiOp interfaces are <a href=\"src/Interface/README.md\">here</a>.</p>\n\
    <h2><a id=\"user-content-running-hiop-tests-and-applications\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#running-hiop-tests-and-applications\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running HiOp tests and applications</h2>\n\
    <p>HiOp is using NVBlas library when built with CUDA support. If you don't specify\n\
    location of the <code>nvblas.conf</code> configuration file, you may get an annoying\n\
    warnings. HiOp provides default <code>nvblas.conf</code> file and installs it\
    \ at the same\nlocation as HiOp libraries. To use it, set environment variable\
    \ as</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ <span class=\"\
    pl-k\">export</span> NVBLAS_CONFIG_FILE=<span class=\"pl-k\">&lt;</span>hiop install\
    \ dir<span class=\"pl-k\">&gt;</span>/lib/nvblas.conf</pre></div>\n<p>or, if you\
    \ are using C-shell, as</p>\n<div class=\"highlight highlight-source-shell\"><pre>$\
    \ setenv NVBLAS_CONFIG_FILE <span class=\"pl-k\">&lt;</span>hiop install dir<span\
    \ class=\"pl-k\">&gt;</span>/lib/nvblas.conf</pre></div>\n<h2><a id=\"user-content-existing-issues\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#existing-issues\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Existing issues</h2>\n<p>Users\
    \ are highly encouraged to report any issues they found from using HiOp.\nOne\
    \ known issue is that there is some minor inconsistence between HiOp and linear\
    \ package STRUMPACK.\nWhen STRUMPACK is compiled with MPI (and Scalapack), user\
    \ must set flag <code>HIOP_USE_MPI</code> to <code>ON</code> when compiling HiOp.\n\
    Otherwise HiOp won't load MPI module and will return an error when links to STRUMPACK,\
    \ since the later one requires a valid MPI module.\nSimilarly, if both Magma and\
    \ STRUMPACK are linked to HiOp, user must guarantee the all the packages are compiled\
    \ by the same CUDA compiler.\nUser can check other issues and their existing status\
    \ from <a href=\"https://github.com/LLNL/hiop\">https://github.com/LLNL/hiop</a></p>\n\
    <h2><a id=\"user-content-acknowledgments\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#acknowledgments\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Acknowledgments</h2>\n<p>HiOp has been developed under the financial\
    \ support of:</p>\n<ul>\n<li>Department of Energy, Office of Advanced Scientific\
    \ Computing Research (ASCR): Exascale Computing Program (ECP) and Applied Math\
    \ Program.</li>\n<li>Department of Energy, Advanced Research Projects Agency-Energy\
    \ (ARPA\u2011E)</li>\n<li>Lawrence Livermore National Laboratory Institutional\
    \ Scientific Capability Portfolio (ISCP)</li>\n<li>Lawrence Livermore National\
    \ Laboratory, through the LDRD program</li>\n</ul>\n<h1><a id=\"user-content-contributors\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#contributors\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contributors</h1>\n<p>HiOp is\
    \ written by Cosmin G. Petra (<a href=\"mailto:petra1@llnl.gov\">petra1@llnl.gov</a>),\
    \ Nai-Yuan Chiang (<a href=\"mailto:chiang7@llnl.gov\">chiang7@llnl.gov</a>),\
    \ and Jingyi \"Frank\" Wang (<a href=\"mailto:wang125@llnl.gov\">wang125@llnl.gov</a>)\
    \ from LLNL and has received important contributions from Asher Mancinelli (PNNL),\
    \ Slaven Peles (ORNL), Cameron Rutherford (PNNL), Jake K. Ryan (PNNL), and Michel\
    \ Schanen (ANL).</p>\n<h1><a id=\"user-content-copyright\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#copyright\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Copyright</h1>\n<p>Copyright (c) 2017-2021, Lawrence Livermore National\
    \ Security, LLC. All rights reserved. Produced at the Lawrence Livermore National\
    \ Laboratory. LLNL-CODE-742473. HiOp is free software; you can modify it and/or\
    \ redistribute it under the terms of the BSD 3-clause license. See <a href=\"\
    /COPYRIGHT\">COPYRIGHT</a> and <a href=\"/LICENSE\">LICENSE</a> for complete copyright\
    \ and license information.</p>\n"
  stargazers_count: 157
  subscribers_count: 14
  topics:
  - hpc
  - nonlinear-optimization
  - nonlinear-programming
  - nonlinear-optimization-algorithms
  - nonlinear-programming-algorithms
  - interior-point-method
  - parallel-programming
  - mpi
  - bfgs
  - quasi-newton
  - constrained-optimization
  - solver
  - optimization
  - acopf
  - gpu-support
  - cuda
  - math-physics
  - radiuss
  updated_at: 1662192123.0
LLNL/sundials:
  data_format: 2
  description: Official development repository for SUNDIALS - a SUite of Nonlinear
    and DIfferential/ALgebraic equation Solvers. Pull requests are welcome for bug
    fixes and minor changes.
  filenames:
  - docker/sundials-ci/spack-nightly/int32-double/spack.yaml
  - docker/sundials-ci/e4s-quarterly/int64-single/spack.yaml
  - docker/sundials-ci/e4s-quarterly/int64-extended/spack.yaml
  - docker/sundials-ci/e4s-quarterly/int32-double/spack.yaml
  - docker/sundials-ci/e4s-quarterly/int32-single/spack.yaml
  - docker/sundials-ci/e4s-quarterly/int32-extended/spack.yaml
  - docker/sundials-ci/spack-nightly/int64-double/spack.yaml
  - docker/sundials-ci/e4s-quarterly/int64-double/spack.yaml
  full_name: LLNL/sundials
  latest_release: v6.3.0
  readme: '<h1><a id="user-content-sundials-suite-of-nonlinear-and-differentialalgebraic-equation-solvers"
    class="anchor" aria-hidden="true" href="#sundials-suite-of-nonlinear-and-differentialalgebraic-equation-solvers"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>SUNDIALS: SUite of
    Nonlinear and DIfferential/ALgebraic equation Solvers</h1>

    <h3><a id="user-content-version-630-aug-2022" class="anchor" aria-hidden="true"
    href="#version-630-aug-2022"><span aria-hidden="true" class="octicon octicon-link"></span></a>Version
    6.3.0 (Aug 2022)</h3>

    <p><strong>Center for Applied Scientific Computing, Lawrence Livermore National
    Laboratory</strong></p>

    <p>SUNDIALS is a family of software packages providing robust and efficient time

    integrators and nonlinear solvers that can easily be incorporated into existing

    simulation codes. The packages are designed to require minimal information from

    the user, allow users to supply their own data structures underneath the

    packages, and enable interfacing with user-supplied or third-party algebraic

    solvers and preconditioners.</p>

    <p>The SUNDIALS suite consists of the following packages for ordinary differential

    equation (ODE) systems, differential-algebraic equation (DAE) systems, and

    nonlinear algebraic systems:</p>

    <ul>

    <li>

    <p>ARKODE - for integrating stiff, nonstiff, and multirate ODEs of the form</p>

    <p>$$ M(t) \, y'' = f_1(t,y) + f_2(t,y), \quad y(t_0) = y_0 $$</p>

    </li>

    <li>

    <p>CVODE - for integrating stiff and nonstiff ODEs of the form</p>

    <p>$$ y'' = f(t,y), \quad y(t_0) = y_0 $$</p>

    </li>

    <li>

    <p>CVODES - for integrating and sensitivity analysis (forward and adjoint) of

    ODEs of the form</p>

    <p>$$ y'' = f(t,y,p), \quad y(t_0) = y_0(p) $$</p>

    </li>

    <li>

    <p>IDA - for integrating DAEs of the form</p>

    <p>$$ F(t,y,y'') = 0, \quad y(t_0) = y_0, \quad y''(t_0) = y_0'' $$</p>

    </li>

    <li>

    <p>IDAS - for integrating and sensitivity analysis (forward and adjoint) of DAEs

    of the form</p>

    <p>$$ F(t,y,y'',p) = 0, \quad y(t_0) = y_0(p), \quad y''(t_0) = y_0''(p) $$</p>

    </li>

    <li>

    <p>KINSOL - for solving nonlinear algebraic systems of the form</p>

    <p>$$ F(u) = 0 \quad \text{or} \quad G(u) = u $$</p>

    </li>

    </ul>

    <h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>For installation directions see the <a href="https://sundials.readthedocs.io/en/latest/Install_link.html"
    rel="nofollow">online install guide</a>,

    the installation chapter in any of the package user guides, or INSTALL_GUIDE.pdf.</p>

    <p>Warning to users who receive more than one of the individual packages at

    different times: Mixing old and new versions of SUNDIALS may fail. To avoid

    such failures, obtain all desired package at the same time.</p>

    <h2><a id="user-content-support" class="anchor" aria-hidden="true" href="#support"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Support</h2>

    <p>Full user guides for all of the SUNDIALS packages are available <a href="https://sundials.readthedocs.io"
    rel="nofollow">online</a>

    and in the <a href="./doc">doc</a> directory. Additionally, the <a href="./doc">doc</a>
    directory

    contains documentation for the package example programs.</p>

    <p>For information on recent changes to SUNDIALS see the <a href="./CHANGELOG.md">CHANGELOG</a>

    or the introduction chapter of any package user guide.</p>

    <p>A list of Frequently Asked Questions on build and installation procedures as

    well as common usage issues is available on the SUNDIALS <a href="https://computing.llnl.gov/projects/sundials/faq"
    rel="nofollow">FAQ</a>.

    For dealing with systems with unphysical solutions or discontinuities see the

    SUNDIALS <a href="https://computing.llnl.gov/projects/sundials/usage-notes" rel="nofollow">usage
    notes</a>.</p>

    <p>If you have a question not covered in the FAQ or usage notes, please submit

    your question to the SUNDIALS <a href="https://computing.llnl.gov/projects/sundials/mailing-list"
    rel="nofollow">mailing list</a>.</p>

    <h2><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <p>Bug fixes or minor changes are preferred via a pull request to the

    <a href="https://github.com/LLNL/sundials">SUNDIALS GitHub repository</a>. For
    more

    information on contributing see the <a href="./CONTRIBUTING.md">CONTRIBUTING</a>
    file.</p>

    <h2><a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Citing</h2>

    <p>See the <a href="https://sundials.readthedocs.io/en/latest/index.html#citing"
    rel="nofollow">online documentation</a>

    or <a href="./CITATIONS.md">CITATIONS</a> file for information on how to cite
    SUNDIALS in

    any publications reporting work done using SUNDIALS packages.</p>

    <h2><a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Authors</h2>

    <p>The SUNDIALS library has been developed over many years by a number of

    contributors. The current SUNDIALS team consists of Cody J. Balos,

    David J. Gardner, Alan C. Hindmarsh, Daniel R. Reynolds, and Carol S. Woodward.

    We thank Radu Serban for significant and critical past contributions.</p>

    <p>Other contributors to SUNDIALS include: James Almgren-Bell, Lawrence E. Banks,

    Peter N. Brown, George Byrne, Rujeko Chinomona, Scott D. Cohen, Aaron Collier,

    Keith E. Grant, Steven L. Lee, Shelby L. Lockhart, John Loffeld, Daniel McGreer,

    Slaven Peles, Cosmin Petra, H. Hunter Schwartz, Jean M. Sexton,

    Dan Shumaker, Steve G. Smith, Allan G. Taylor, Hilari C. Tiedeman, Chris White,

    Ting Yan, and Ulrike M. Yang.</p>

    <h2><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>SUNDIALS is released under the BSD 3-clause license. See the <a href="./LICENSE">LICENSE</a>

    and <a href="./NOTICE">NOTICE</a> files for details. All new contributions must
    be made

    under the BSD 3-clause license.</p>

    <p><strong>Please Note</strong> If you are using SUNDIALS with any third party
    libraries linked

    in (e.g., LAPACK, KLU, SuperLU_MT, PETSc, or <em>hypre</em>), be sure to review
    the

    respective license of the package as that license may have more restrictive

    terms than the SUNDIALS license.</p>

    <pre><code>SPDX-License-Identifier: BSD-3-Clause


    LLNL-CODE-667205  (ARKODE)

    UCRL-CODE-155951  (CVODE)

    UCRL-CODE-155950  (CVODES)

    UCRL-CODE-155952  (IDA)

    UCRL-CODE-237203  (IDAS)

    LLNL-CODE-665877  (KINSOL)

    </code></pre>

    '
  stargazers_count: 284
  subscribers_count: 36
  topics:
  - ode-solver
  - dae-solver
  - nonlinear-equation-solver
  - sensitivity-analysis
  - time-integration
  - scientific-computing
  - parallel-computing
  - hpc
  - math-physics
  - radiuss
  - solver
  - high-performance-computing
  updated_at: 1661969890.0
Lumi-supercomputer/lumi-spack-settings:
  data_format: 2
  description: Spack configuration files for LUMI
  filenames:
  - 22.08/0.18.1/spack.yaml
  full_name: Lumi-supercomputer/lumi-spack-settings
  latest_release: null
  readme: '<h1><a id="user-content-spack-configuration-files-for-lumi" class="anchor"
    aria-hidden="true" href="#spack-configuration-files-for-lumi"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Spack configuration files for LUMI</h1>

    <p>Repository containing configuration files for the Spack instances installed
    in <code>/appl/lumi/spack</code> on LUMI for public use. The files in this repository
    can be found in <code>/appl/lumi/spack/etc/</code> on LUMI. The folder hierarchy
    is determined by the Cray Programming Environment (CPE) version and Spack release
    version. For example, the directory</p>

    <pre><code>22.08/0.18.1/

    22.08/0.18.1-user/

    </code></pre>

    <p>contains the configuration files for Spack version 0.18.1 configured to use
    CPE 22.08. The first instance <code>0.18.1</code> is the upstream instance, which
    is maintained by the LUMI Support Team. The second instance <code>0.18.1-user</code>
    is a separate instance configured to install packages in a user-defined directory
    in e.g. <code>/project/</code>. It is chained to the upstream instance, so that
    already installed packages can be reused.</p>

    <p>If you are user of LUMI, and want to set up your own instance, you can copy
    the <code>compilers.yaml</code>and  <code>packages.yaml</code> files to your instance.
    The <code>config.yaml</code> needs to be modified if you want to use that one.</p>

    '
  stargazers_count: 0
  subscribers_count: 12
  topics: []
  updated_at: 1661775740.0
NCAR/spack-gust:
  data_format: 2
  description: Spack production user software stack on the Gust test system
  filenames:
  - spack.yaml
  full_name: NCAR/spack-gust
  latest_release: null
  readme: '<h1><a id="user-content-ncar-spack-deployment" class="anchor" aria-hidden="true"
    href="#ncar-spack-deployment"><span aria-hidden="true" class="octicon octicon-link"></span></a>NCAR
    Spack Deployment</h1>

    <p>This branch tracks the <strong>production</strong> deployment of Spack for
    the following configuration:</p>

    <table>

    <thead>

    <tr>

    <th></th>

    <th>gust</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Creation date</td>

    <td>Mon Aug 29 19:25:34 MDT 2022</td>

    </tr>

    <tr>

    <td>ncar-spack commit</td>

    <td>a9bd54c8de45dfa98b93fb8830a3f3e97d1cafd6</td>

    </tr>

    <tr>

    <td>Host version</td>

    <td>22.08b</td>

    </tr>

    <tr>

    <td>Spack version</td>

    <td>51244abee9f849c0ad6437f47f9b20da26671a49</td>

    </tr>

    <tr>

    <td>Deployment path</td>

    <td>/glade/u/apps/gust/22.08b</td>

    </tr>

    <tr>

    <td>Environments path</td>

    <td>/glade/work/csgteam/spack-deployments/gust/22.08b/envs</td>

    </tr>

    </tbody>

    </table>

    <p>This repository should <em>only</em> be updated via the <code>publish</code>
    script contained in the build environment. Any manual changes to this branch will
    cause headaches when you or another consultant attempt to publish new packages!</p>

    '
  stargazers_count: 0
  subscribers_count: 9
  topics: []
  updated_at: 1661357750.0
NERSC/spack-infrastructure:
  data_format: 2
  description: null
  filenames:
  - spack-configs/perlmutter-e4s-22.05/ci/spack.yaml
  - docs/spack.yaml
  - spack-configs/cori-spack-develop/spack.yaml
  - spack-configs/perlmutter-e4s-22.05/spack.yaml
  - spack-configs/perlmutter-e4s-22.05-mvapich2/ci/spack.yaml
  - spack-configs/perlmutter-spack-develop/spack.yaml
  full_name: NERSC/spack-infrastructure
  latest_release: null
  readme: "<h1><a id=\"user-content-spack-infrastructure\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#spack-infrastructure\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Spack Infrastructure</h1>\n<p>The spack infrastructure\
    \ repository contains spack configuration in the form of <code>spack.yaml</code>\
    \ required to build spack stacks on Cori and Perlmutter system. We leverage gitlab\
    \ to automate software stack deployment which is configured using the <a href=\"\
    https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/.gitlab-ci.yml\"\
    \ rel=\"nofollow\">.gitlab-ci.yml</a> file. The documentation is available at\
    \ <a href=\"https://nersc-spack-infrastructure.rtfd.io/\" rel=\"nofollow\">https://nersc-spack-infrastructure.rtfd.io/</a></p>\n\
    <h2><a id=\"user-content-spack-configuration\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#spack-configuration\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Spack Configuration</h2>\n<p>The spack configuration\
    \ can be found in <a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/tree/main/spack-configs\"\
    \ rel=\"nofollow\">spack-configs</a> directory with subdirectory for each deployment.\n\
    Each pipeline can be run if one sets the variable <code>PIPELINE_NAME</code> to\
    \ a unique value in order to run a pipeline. You can check the <a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/.gitlab-ci.yml\"\
    \ rel=\"nofollow\">.gitlab-ci.yml</a> for the gitlab configuration. The pipeline\
    \ can be run via <a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/pipelines/new\"\
    \ rel=\"nofollow\">web interface</a>, if you chose this route, you must set <code>PIPELINE_NAME</code>\
    \ to the appropriate value.</p>\n<p>If you want to trigger pipeline via <a href=\"\
    https://software.nersc.gov/NERSC/spack-infrastructure/-/pipelines/new\" rel=\"\
    nofollow\">web-interface</a> you will need to define PIPELINE_NAME variable to\
    \ trigger the appropriate pipeline.</p>\n<table>\n<thead>\n<tr>\n<th>system</th>\n\
    <th>status</th>\n<th>PIPELINE_NAME</th>\n<th>description</th>\n<th>spack.yaml</th>\n\
    </tr>\n</thead>\n<tbody>\n<tr>\n<td>Perlmutter</td>\n<td><strong>IN-PROGRESS</strong></td>\n\
    <td><code>PERLMUTTER_SPACK_DEVELOP</code></td>\n<td>This spack configuration is\
    \ based on <code>spack@develop</code> branch to see what packages can be built.\
    \ We expect this pipeline will fail and we are not expected to fix build failures.\
    \ The main purpose of this project is to build as many packages across all the\
    \ compilers, mpi, and blas providers of interest to see what works. Since we don't\
    \ know which package works during deployment, we will leverage data from this\
    \ pipeline to make informed decision what packages should be picked with given\
    \ compilers. This pipeline is our development and we should use this to experiment\
    \ new compilers. Note that we won't hardcode versions for packages since we want\
    \ to build with latest release. However we will hardcode external packages depending\
    \ on how the system is configured.</td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-spack-develop/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-spack-develop/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Cori</td>\n<td><strong>IN-PROGRESS</strong></td>\n<td><code>CORI_SPACK_DEVELOP</code></td>\n\
    <td>This spack configuration will build E4S stack using spack <code>develop</code>\
    \ branch on Cori.</td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-spack-develop/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-spack-develop/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Perlmutter</td>\n<td><strong>IN-PROGRESS</strong></td>\n<td><code>PERLMUTTER_E4S_22.05</code></td>\n\
    <td>This spack configuration will build E4S 22.05 on Perlmutter on scheduled pipeline</td>\n\
    <td><a href=\"https://software.nersc.gov/-/ide/project/NERSC/spack-infrastructure/tree/main/-/spack-configs/perlmutter-e4s-22.05/ci/spack.yaml/\"\
    \ rel=\"nofollow\">https://software.nersc.gov/-/ide/project/NERSC/spack-infrastructure/tree/main/-/spack-configs/perlmutter-e4s-22.05/ci/spack.yaml/</a></td>\n\
    </tr>\n<tr>\n<td>Muller</td>\n<td><strong>IN-PROGRESS</strong></td>\n<td><code>MULLER_E4S_22.05</code></td>\n\
    <td>This spack configuration will build E4S 22.05 on Muller on scheduled pipeline</td>\n\
    <td><a href=\"https://software.nersc.gov/-/ide/project/NERSC/spack-infrastructure/tree/main/-/spack-configs/perlmutter-e4s-22.05/ci/spack.yaml/\"\
    \ rel=\"nofollow\">https://software.nersc.gov/-/ide/project/NERSC/spack-infrastructure/tree/main/-/spack-configs/perlmutter-e4s-22.05/ci/spack.yaml/</a></td>\n\
    </tr>\n<tr>\n<td>Cori</td>\n<td><strong>COMPLETE</strong></td>\n<td><code>CORI_E4S_22.02</code></td>\n\
    <td>This spack configuration will build E4S/22.02 on Cori using a scheduled pipeline.</td>\n\
    <td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-22.02/ci/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-22.02/ci/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Gerty</td>\n<td><strong>COMPLETE</strong></td>\n<td><code>GERTY_E4S_22.02</code></td>\n\
    <td>This spack configuration will build E4S/22.02 on gerty using a scheduled pipeline.</td>\n\
    <td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-22.02/ci/gerty/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-22.02/ci/gerty/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Perlmutter</td>\n<td><strong>COMPLETE</strong></td>\n<td><code>PERLMUTTER_E4S_21.11_DEPLOY</code></td>\n\
    <td>This spack configuration is deployment configuration for E4S/21.11. For more\
    \ details on this stack see  <a href=\"https://docs.nersc.gov/applications/e4s/perlmutter/21.11/\"\
    \ rel=\"nofollow\">https://docs.nersc.gov/applications/e4s/perlmutter/21.11/</a>\n\
    </td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-e4s-21.11/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-e4s-21.11/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Perlmutter</td>\n<td><strong>COMPLETE</strong></td>\n<td><code>PERLMUTTER_E4S_21.11</code></td>\n\
    <td>This spack configuration is used for development for building E4S/21.11 using\
    \ scheduled pipeline.</td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-e4s-21.11/ci/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-e4s-21.11/ci/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Muller</td>\n<td><strong>COMPLETE</strong></td>\n<td><code>MULLER_E4S_21.11</code></td>\n\
    <td>This spack configuration was used to build E4S/21.11 on Muller using scheduled\
    \ pipeline. Once e4s/21.11 was built on Muller we followed up with building the\
    \ same spack configuration on Perlmutter.</td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-e4s-21.11/ci/muller/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/perlmutter-e4s-21.11/ci/muller/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Cori</td>\n<td><strong>COMPLETE</strong></td>\n<td></td>\n<td>E4S/21.05\
    \ spack stack based on <a href=\"https://github.com/spack/spack/tree/e4s-21.05\"\
    >e4s-21.05</a> branch of spack. This stack can be accessed via <code>module load\
    \ e4s/21.05</code>.</td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-21.05/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-21.05/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Cori</td>\n<td><strong>COMPLETE</strong></td>\n<td></td>\n<td>E4S/21.02\
    \ spack configuration used for deployment purposes, this can be accessed via <code>module\
    \ load e4s/21.02</code> on Cori. For more details see <a href=\"https://docs.nersc.gov/applications/e4s/cori/21.02/\"\
    \ rel=\"nofollow\">https://docs.nersc.gov/applications/e4s/cori/21.02/</a>\n</td>\n\
    <td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/tree/main/spack-configs/cori-e4s-21.02/prod/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/tree/main/spack-configs/cori-e4s-21.02/prod/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Cori</td>\n<td><strong>COMPLETE</strong></td>\n<td></td>\n<td>E4S/21.02\
    \ spack configuration that push to buildcache.</td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-21.02/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-21.02/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Cori</td>\n<td><strong>COMPLETE</strong></td>\n<td></td>\n<td>E4S/20.10\
    \ spack configuration that push to build cache using <code>spack ci</code>.  This\
    \ project lives in <a href=\"https://software.nersc.gov/NERSC/e4s-2010\" rel=\"\
    nofollow\">https://software.nersc.gov/NERSC/e4s-2010</a> and configuration was\
    \ copied over here.</td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-20.10/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-20.10/spack.yaml</a></td>\n\
    </tr>\n<tr>\n<td>Cori</td>\n<td><strong>COMPLETE</strong></td>\n<td></td>\n<td>E4S/20.10\
    \ spack configuration for Cori used for deployment purpose. This stack can be\
    \ accessed via <code>module load e4s/20.10</code>. This is documented at <a href=\"\
    https://docs.nersc.gov/applications/e4s/cori/20.10/\" rel=\"nofollow\">https://docs.nersc.gov/applications/e4s/cori/20.10/</a>\n\
    </td>\n<td><a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-20.10/prod/spack.yaml\"\
    \ rel=\"nofollow\">https://software.nersc.gov/NERSC/spack-infrastructure/-/blob/main/spack-configs/cori-e4s-20.10/prod/spack.yaml</a></td>\n\
    </tr>\n</tbody>\n</table>\n<h2><a id=\"user-content-running-ci-pipelines\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#running-ci-pipelines\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running CI Pipelines</h2>\n<p>This\
    \ project is configured with several <a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/pipeline_schedules\"\
    \ rel=\"nofollow\">scheduled pipelines</a> that will run at different times.</p>\n\
    <p>Currently, we have a shell runner installed on Perlmutter using <code>e4s</code>\
    \ account which is configured with following settings. You can find list of runners\
    \ and their runner status under <a href=\"https://software.nersc.gov/NERSC/spack-infrastructure/-/settings/ci_cd\"\
    \ rel=\"nofollow\">Settings &gt; CI/CD &gt; Runners</a>. Please make sure you\
    \ login to the appropriate hostname when starting the gitlab runner.</p>\n<table>\n\
    <thead>\n<tr>\n<th>System</th>\n<th>Runner Name</th>\n<th>Hostname</th>\n</tr>\n\
    </thead>\n<tbody>\n<tr>\n<td>perlmutter</td>\n<td><code>perlmutter-e4s</code></td>\n\
    <td><code>login27</code></td>\n</tr>\n<tr>\n<td>cori</td>\n<td><code>cori-e4s</code></td>\n\
    <td><code>cori02</code></td>\n</tr>\n<tr>\n<td>muller</td>\n<td><code>muller-e4s</code></td>\n\
    <td><code>login02</code></td>\n</tr>\n<tr>\n<td>gerty</td>\n<td><code>gerty-e4s</code></td>\n\
    <td><code>gert01</code></td>\n</tr>\n</tbody>\n</table>\n<p>The runner configuration\
    \ files are located in <code>~/.gitlab-runner</code> for user <strong>e4s</strong>.</p>\n\
    <p>The production pipelines are triggered via web-interface which requires approval\
    \ from a project maintainer. Production pipelines should be run when we need to\
    \ do full redeployment of stack.</p>\n<h2><a id=\"user-content-current-challenges\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#current-challenges\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Current Challenges</h2>\n<p>There\
    \ are several challenges with building spack stack at NERSC which can be summarized\
    \ as follows</p>\n<ul>\n<li>\n<p><strong>System OS + Cray Programming Environment\
    \ (CPE) changes</strong>: A system upgrade such as change to <code>glibc</code>\
    \ or upgrades in CPE can lead to full software stack rebuild, especially if you\
    \ have external packages set for packages like <code>cray-mpich</code>, <code>cray-libsci</code>\
    \ which generally change between versions</p>\n</li>\n<li>\n<p><strong>Incompatibile\
    \ compilers</strong>: Some packages can't be built with certain compilers (<code>nvhpc</code>,\
    \ <code>aocc</code>) which could be due to several factors.</p>\n<ul>\n<li>An\
    \ application doesn't have support though it was be added in newer version but\
    \ you don't have it in your spack release used for deployment</li>\n<li>Lack of\
    \ support in spack package recipe or spack-core base including spack-cray detection.\
    \ This may require getting fix and cherry-pick commit or waiting for new version</li>\n\
    <li>Spack Cray detection is an important part in build errors including how one\
    \ specifies externals via <code>modules</code> vs <code>prefix</code> both could\
    \ be provided and it requires experimentation. An example of this is trying to\
    \ get <code>cray-mpich</code> external one could set something like this with\
    \ modules or prefix</li>\n</ul>\n<div class=\"highlight highlight-source-yaml\"\
    ><pre>  <span class=\"pl-ent\">cray-mpich</span>:\n    <span class=\"pl-ent\"\
    >buildable</span>: <span class=\"pl-c1\">false</span>\n    <span class=\"pl-ent\"\
    >externals</span>:\n    - <span class=\"pl-ent\">spec</span>: <span class=\"pl-s\"\
    >cray-mpich@8.1.11 %gcc@9.3.0</span>\n      <span class=\"pl-ent\">prefix</span>:\
    \ <span class=\"pl-s\">/opt/cray/pe/mpich/8.1.11/ofi/gnu/9.1</span>\n      <span\
    \ class=\"pl-ent\">modules</span>:\n      - <span class=\"pl-s\">cray-mpich/8.1.11</span>\n\
    \      - <span class=\"pl-s\">cudatoolkit/21.9_11.4</span></pre></div>\n<ul>\n\
    <li>\n<strong>Spack concretizer</strong> prevent one from chosing a build configration\
    \ for a spec. This requires a few troubleshooting step but usually boils down\
    \ to:\n<ul>\n<li>Read the spack package file <code>spack edit &lt;package&gt;</code>\
    \ for conflicts and try <code>spack spec</code> to see concretized spec.</li>\n\
    <li>Try different version, different compiler, different dependency. Some packages\
    \ have conflicting variant for instance one can't enable <code>+openmp</code>\
    \ and <code>+pthread</code> it is mutually exclusive.</li>\n</ul>\n</li>\n</ul>\n\
    </li>\n</ul>\n<p>There is a document <a href=\"https://docs.google.com/document/d/1jWrCcK8LgpNDMytXhLdBYpIusidkoowrZAH1zos7zIw/edit?usp=sharing\"\
    \ rel=\"nofollow\">Spack E4S Issues on Permlutter</a> outlining current issues\
    \ with spack. If you need access to document please contact <strong>Shahzeb Siddiqui</strong>.</p>\n\
    <h2><a id=\"user-content-contact\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #contact\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contact</h2>\n\
    <p>If you need elevated privledge or assistance with this project please contact\
    \ one of the maintainers:</p>\n<ul>\n<li><strong>Shahzeb Siddiqui (<a href=\"\
    mailto:shahzebsiddiqui@lbl.gov\">shahzebsiddiqui@lbl.gov</a>)</strong></li>\n\
    <li><strong>Erik Palmer (<a href=\"mailto:epalmer@lbl.gov\">epalmer@lbl.gov</a>)</strong></li>\n\
    <li><strong>Justin Cook (<a href=\"mailto:JSCook@lbl.gov\">JSCook@lbl.gov</a>)</strong></li>\n\
    <li>E4S Team: <strong>Sameer Shende (<a href=\"mailto:sameer@cs.uoregon.edu\"\
    >sameer@cs.uoregon.edu</a>)</strong>, <strong>Christopher Peyralans (<a href=\"\
    mailto:lpeyrala@uoregon.edu\">lpeyrala@uoregon.edu</a>)</strong>, <strong>Wyatt\
    \ Spear (<a href=\"mailto:wspear@cs.uoregon.edu\">wspear@cs.uoregon.edu</a>)</strong>,\
    \ <strong>Nicholas Chaimov (<a href=\"mailto:nchaimov@paratools.com\">nchaimov@paratools.com</a>)</strong>\n\
    </li>\n</ul>\n"
  stargazers_count: 4
  subscribers_count: 14
  topics: []
  updated_at: 1661448138.0
NOAA-EMC/GSI:
  data_format: 2
  description: Gridpoint Statistical Interpolation
  filenames:
  - ci/spack.yaml
  full_name: NOAA-EMC/GSI
  latest_release: gefs_v12.0.2
  stargazers_count: 39
  subscribers_count: 17
  topics: []
  updated_at: 1661183345.0
NOAA-EMC/GSI-Monitor:
  data_format: 2
  description: GSI Monitoring Tools
  filenames:
  - ci/spack.yaml
  full_name: NOAA-EMC/GSI-Monitor
  latest_release: null
  readme: "<h1><a id=\"user-content-gsi-monitor\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#gsi-monitor\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>GSI-Monitor</h1>\n<p>GSI Monitoring Tools</p>\n<p>These tools monitor\
    \ the Gridpoint Statsical Interpolation (GSI) package's data assimiliation, detecting\n\
    and reporting missing data sources, low obervational counts, and high penalty\
    \ values.</p>\n<p>Suite includes:</p>\n<pre><code>  ConMon   Conventional Monitor\
    \     \n  MinMon   GSI Minimization Monitor \n  OznMon   Ozone Monitor       \
    \     \n  RadMon   Radiance Monitor         \n</code></pre>\n<p>PoC:  <a href=\"\
    mailto:edward.safford@noaa.gov\">edward.safford@noaa.gov</a></p>\n"
  stargazers_count: 1
  subscribers_count: 0
  topics: []
  updated_at: 1660063618.0
NOAA-EMC/GSI-utils:
  data_format: 2
  description: GSI related utilities
  filenames:
  - ci/spack.yaml
  full_name: NOAA-EMC/GSI-utils
  latest_release: null
  readme: '<h1><a id="user-content-gsi-utils" class="anchor" aria-hidden="true" href="#gsi-utils"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>GSI-Utils</h1>

    <p>GSI Utility Tools</p>

    <p>These are GSI utilities for various functions.</p>

    <p>For installation instruction see <a href="./INSTALL.md">here</a></p>

    '
  stargazers_count: 1
  subscribers_count: 6
  topics: []
  updated_at: 1660063597.0
NOAA-EMC/UPP:
  data_format: 2
  description: null
  filenames:
  - ci/spack.yaml
  full_name: NOAA-EMC/UPP
  latest_release: upp_v8.2.0
  readme: '<h1><a id="user-content-unified-post-processing-upp" class="anchor" aria-hidden="true"
    href="#unified-post-processing-upp"><span aria-hidden="true" class="octicon octicon-link"></span></a>Unified
    Post-Processing (UPP)</h1>

    <p>The Unified Post Processor (UPP) software package is a software

    package designed to generate useful products from raw model

    output.</p>

    <p>The UPP is currently used in operations with the Global Forecast

    System (GFS), GFS Ensemble Forecast System (GEFS), North American

    Mesoscale (NAM), Rapid Refresh (RAP), High Resolution Rapid Refresh

    (HRRR), Short Range Ensemble Forecast (SREF), and Hurricane WRF (HWRF)

    applications. It is also used in the Unified Forecasting System (UFS),

    including the Rapid Refresh Forecast System (RRFS), Hurricane Application

    Forecasting System (HAFS), and the Medium Range Weather (MRW) and Short

    Range Weather (SRW) Applications.</p>

    <p>The UPP provides the capability to compute a variety of diagnostic

    fields and interpolate to pressure levels or other vertical

    coordinates.</p>

    <p>UPP also incorporates the Joint Center for Satellite Data Assimilation

    (JCSDA) Community Radiative Transfer Model (CRTM) to compute model

    derived brightness temperature (TB) for various instruments and

    channels. This additional feature enables the generation of a number

    of simulated satellite products including GOES products.</p>

    <p>Output from the UPP is in National Weather Service (NWS) and World

    Meteorological Organization (WMO) GRIB2 format and can be used

    directly by visualization, plotting, or verification packages, or for

    further downstream post-processing, e.g. statistical post-processing

    techniques.</p>

    <p>Examples of UPP products include:</p>

    <ul>

    <li>T, Z, humidity, wind, cloud water, cloud ice, rain, and snow on pressure levels</li>

    <li>SLP, shelter level T, humidity, and wind fields</li>

    <li>Precipitation-related fields</li>

    <li>PBL-related fields</li>

    <li>Severe weather products (e.g. CAPE, Vorticity, Wind shear)</li>

    <li>Radiative/Surface fluxes</li>

    <li>Cloud related fields</li>

    <li>Aviation products</li>

    <li>Radar reflectivity products</li>

    <li>Satellite look-alike products</li>

    </ul>

    <h2><a id="user-content-user-support" class="anchor" aria-hidden="true" href="#user-support"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>User Support</h2>

    <p>Support for the UFS UPP is provided through the <a href="https://forums.ufscommunity.org/"
    rel="nofollow">UFS Forum</a>

    by the Developmental Testbed Center (DTC).</p>

    <h2><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h2>

    <p>User Guide for latest public release: <a href="https://upp.readthedocs.io/en/latest/"
    rel="nofollow">https://upp.readthedocs.io/en/latest/</a>.</p>

    <p>Technical code-level documentation: <a href="https://noaa-emc.github.io/UPP/"
    rel="nofollow">https://noaa-emc.github.io/UPP/</a>.</p>

    <h2><a id="user-content-developer-information" class="anchor" aria-hidden="true"
    href="#developer-information"><span aria-hidden="true" class="octicon octicon-link"></span></a>Developer
    Information</h2>

    <p>Please see review the <a href="https://github.com/NOAA-EMC/UPP/wiki">wiki</a></p>

    <h2><a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Authors</h2>

    <p>NCEP/EMC Developers</p>

    <p>Code Managers: Wen Meng, Huiya Chuang, Kate Fossell</p>

    <h2><a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>The UPP requires certain NCEPLIB packages to be installed via

    the HPC-Stack project.</p>

    <ul>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-g2">NCEPLIBS-g2</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-g2tmpl">NCEPLIBS-g2tmpl</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-sp">NCEPLIBS-sp</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-ip">NCEPLIBS-ip</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-bacio">NCEPLIBS-bacio</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-w3emc">NCEPLIBS-w3emc</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-w3nco">NCEPLIBS-w3nco</a></li>

    <li><a href="https://github.com/noaa-emc/emc_crtm">CRTM</a></li>

    </ul>

    <p>Also required to build NCEPpost executable (cmake option

    BUILD_POSTEXEC):</p>

    <ul>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-sigio">NCEPLIBS-sigio</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-sfcio">NCEPLIBS-sfcio</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-nemsio">NCEPLIBS-nemsio</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-gfsio">NCEPLIBS-gfsio</a></li>

    </ul>

    <p>The <a href="https://github.com/NOAA-EMC/NCEPLIBS-wrf_io">NCEPLIBS-wrf_io</a>

    library is required to build with NCEPpost with WRF-IO library (cmake

    option BUILD_WITH_WRFIO).</p>

    <p>The following third-party libraries are required:</p>

    <ul>

    <li><a href="https://github.com/Unidata/netcdf-c">netcdf-c</a></li>

    <li><a href="https://github.com/Unidata/netcdf-fortran">netcdf-fortran</a></li>

    <li><a href="https://github.com/jasper-software/jasper">Jasper</a></li>

    <li><a href="http://www.libpng.org/pub/png/libpng.html" rel="nofollow">libpng</a></li>

    <li><a href="https://zlib.net/" rel="nofollow">libz</a></li>

    </ul>

    <h2><a id="user-content-building" class="anchor" aria-hidden="true" href="#building"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Building</h2>

    <p>Builds include:</p>

    <ul>

    <li>

    <p>Inline post (UPP library): Currently only supported for the GFS, RRFS,

    HAFS, and the UFS-MRW Application.</p>

    </li>

    <li>

    <p>Offline post (UPP executable): Supported for Regional applications

    including SRW, RRFS, HAFS, and standalone applications of UPP.</p>

    </li>

    </ul>

    <p>CMake is used to manage all builds of the UPP.

    The script <code>UPP/tests/compile_upp.sh</code> can be used to automatically

    build UPP on fully supported platforms where HPC-stack is supported.

    Details in this script can be used to build on new platforms.</p>

    <h2><a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Disclaimer</h2>

    <p>The United States Department of Commerce (DOC) GitHub project code is

    provided on an "as is" basis and the user assumes responsibility for

    its use. DOC has relinquished control of the information and no longer

    has responsibility to protect the integrity, confidentiality, or

    availability of the information. Any claims against the Department of

    Commerce stemming from the use of its GitHub project will be governed

    by all applicable Federal law. Any reference to specific commercial

    products, processes, or services by service mark, trademark,

    manufacturer, or otherwise, does not constitute or imply their

    endorsement, recommendation or favoring by the Department of

    Commerce. The Department of Commerce seal and logo, or the seal and

    logo of a DOC bureau, shall not be used in any manner to imply

    endorsement of any commercial product or activity by DOC or the United

    States Government.</p>

    '
  stargazers_count: 21
  subscribers_count: 17
  topics: []
  updated_at: 1661527776.0
NOAA-EMC/WW3:
  data_format: 2
  description: WAVEWATCH III
  filenames:
  - model/ci/spack.yaml
  full_name: NOAA-EMC/WW3
  latest_release: 6.07.1
  readme: "<h1><a id=\"user-content-the-wavewatch-iii-framework\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#the-wavewatch-iii-framework\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>The WAVEWATCH III Framework</h1>\n\
    <p>WAVEWATCH III<sup>\xAE</sup>  is a community wave modeling framework that includes\
    \ the\nlatest scientific advancements in the field of wind-wave modeling and dynamics.</p>\n\
    <h2><a id=\"user-content-general-features\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#general-features\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>General Features</h2>\n<p>WAVEWATCH III<sup>\xAE</sup> solves the\
    \ random phase spectral action density\nbalance equation for wavenumber-direction\
    \ spectra. The model includes options\nfor shallow-water (surf zone) applications,\
    \ as well as wetting and drying of\ngrid points. Propagation of a wave spectrum\
    \ can be solved using regular\n(rectilinear or curvilinear) and unstructured (triangular)\
    \ grids. See\n<a href=\"https://github.com/NOAA-EMC/WW3/wiki/About-WW3\">About\
    \ WW3</a> for a\ndetailed description of WAVEWATCH III<sup>\xAE</sup> .</p>\n\
    <h2><a id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#installation\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<p>The WAVEWATCH III<sup>\xAE</sup>  framework\
    \ package has two parts that need to be combined so\nall runs smoothly: the GitHub\
    \ repo itself, and a binary data file bundle that\nneeds to be obtained from our\
    \ ftp site. Steps to successfully acquire and install\nthe framework are outlined\
    \ in our <a href=\"https://github.com/NOAA-EMC/WW3/wiki/Quick-Start\">Quick Start</a>\n\
    guide.</p>\n<h2><a id=\"user-content-disclaimer\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#disclaimer\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Disclaimer</h2>\n<p>The United States Department of Commerce (DOC)\
    \ GitHub project code is provided\non an 'as is' basis and the user assumes responsibility\
    \ for its use. DOC has\nrelinquished control of the information and no longer\
    \ has responsibility to\nprotect the integrity, confidentiality, or availability\
    \ of the information. Any\nclaims against the Department of Commerce stemming\
    \ from the use of its GitHub\nproject will be governed by all applicable Federal\
    \ law. Any reference to\nspecific commercial products, processes, or services\
    \ by service mark,\ntrademark, manufacturer, or otherwise, does not constitute\
    \ or imply their\nendorsement, recommendation or favoring by the Department of\
    \ Commerce. The\nDepartment of Commerce seal and logo, or the seal and logo of\
    \ a DOC bureau,\nshall not be used in any manner to imply endorsement of any commercial\
    \ product\nor activity by DOC or the United States Government.</p>\n"
  stargazers_count: 180
  subscribers_count: 47
  topics: []
  updated_at: 1661797246.0
NOAA-EMC/spack-stack:
  data_format: 2
  description: null
  filenames:
  - configs/templates/gfs-v16.2/spack.yaml
  - configs/templates/ufs-srw-dev/spack.yaml
  - configs/templates/ufs-srw-public-v2/spack.yaml
  - configs/templates/hpc-dev-v1/spack.yaml
  - configs/templates/hpc-stack-dev/spack.yaml
  - configs/templates/jedi-ufs-all/spack.yaml
  - configs/templates/empty/spack.yaml
  - configs/templates/ufs-weather-model/spack.yaml
  - configs/templates/skylab-dev/spack.yaml
  full_name: NOAA-EMC/spack-stack
  latest_release: spack-stack-1.0.2
  readme: '<h1><a id="user-content-spack-stack" class="anchor" aria-hidden="true"
    href="#spack-stack"><span aria-hidden="true" class="octicon octicon-link"></span></a>spack-stack</h1>

    <p>Spack-stack enables the installation of software required

    for HPC system deployments of NOAA''s Unified Forecast System (UFS) and

    other weather and climate models, including components of the Joint

    Effort for Data assimilation Integration (JEDI).</p>

    <p>Spack-stack is a collaborative effort between:</p>

    <ul>

    <li><a href="https://www.emc.ncep.noaa.gov/emc_new.php" rel="nofollow">NOAA Environmental
    Modeling Center (EMC)</a></li>

    <li><a href="https://www.jcsda.org/" rel="nofollow">UCAR Joint Center for Satellite
    Data Assimilation (JCSDA)</a></li>

    <li>

    <a href="https://epic.noaa.gov/" rel="nofollow">Earth Prediction Innovation Center
    (EPIC)</a>.</li>

    </ul>

    <p>Spack-stack is a thin layer around a fork of the

    <a href="https://github.com/spack/spack">spack</a> repository. Spack is a

    community-supported, multi-platform, Python-based package manager

    originally developed by the Lawrence Livermore National Laboratory

    (LLNL). Spack is provided as a submodule to spack-stack so that a

    stable version can be referenced. For more information about spack see

    the <a href="https://computing.llnl.gov/projects/spack-hpc-package-manager" rel="nofollow">LLNL
    project page for

    spack</a>

    and the <a href="https://spack.readthedocs.io/en/latest/" rel="nofollow">spack

    documentation</a>.</p>

    <p>The stack can be installed on a range of platforms, from Linux and

    macOS laptops to HPC systems, and comes pre-configured for many

    systems. Users can install the necessary packages for a particular

    application and later add the missing packages for another application

    without having to rebuild the entire stack.</p>

    <p>spack-stack is mainly a collection of Spack configuration files, but

    provides a Spack extension to simplify the installation process:</p>

    <ul>

    <li>

    <p><code>spack stack create</code> is provided to copy common, site-specific,
    and

    application-specific configuration files into a coherent Spack

    environment and to create container recipes</p>

    </li>

    <li>

    <p><code>spack stack setup-meta-modules</code> creates compiler, MPI and Python

    meta-modules for a convenient setup of a user environment using

    modules (lua and tcl)</p>

    </li>

    </ul>

    <p>Documentation for installing and using spack-stack can be found here:

    <a href="https://spack-stack.readthedocs.io/en/latest/" rel="nofollow">https://spack-stack.readthedocs.io/en/latest/</a></p>

    <p>spack-stack is maintained by:</p>

    <ul>

    <li>

    <p><a href="https://www.github.com/kgerheiser">Kyle Gerheiser</a>, <a href="https://www.github.com/Hang-Lei-NOAA">Hang

    Lei</a>, <a href="https://www.github.com/edwardhartnett">Ed

    Hartnett</a> NOAA-EMC</p>

    </li>

    <li>

    <p><a href="https://www.github.com/climbfuji">Dom Heinzeller</a>, JCSDA</p>

    </li>

    </ul>

    <p>For more information about the organization of the spack-stack

    project, see the <a href="project_charter.md">Project Charter</a>.</p>

    <h2><a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Disclaimer</h2>

    <p>The United States Department of Commerce (DOC) GitHub project code is

    provided on an "as is" basis and the user assumes responsibility for

    its use. DOC has relinquished control of the information and no longer

    has responsibility to protect the integrity, confidentiality, or

    availability of the information. Any claims against the Department of

    Commerce stemming from the use of its GitHub project will be governed

    by all applicable Federal law. Any reference to specific commercial

    products, processes, or services by service mark, trademark,

    manufacturer, or otherwise, does not constitute or imply their

    endorsement, recommendation or favoring by the Department of

    Commerce. The Department of Commerce seal and logo, or the seal and

    logo of a DOC bureau, shall not be used in any manner to imply

    endorsement of any commercial product or activity by DOC or the United

    States Government.</p>

    '
  stargazers_count: 5
  subscribers_count: 8
  topics: []
  updated_at: 1660579618.0
ParaToolsInc/exago-crusher:
  data_format: 2
  description: Spack-based deployment of ROCm enabled ExaGO for OLCF Crusher
  filenames:
  - spack.yaml
  full_name: ParaToolsInc/exago-crusher
  latest_release: null
  readme: "<h1><a id=\"user-content-exago-on-olcf-crusher\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#exago-on-olcf-crusher\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>ExaGO on OLCF Crusher</h1>\n<p>ROCm-enabled ExaGO\
    \ on OLCF Crusher using Spack</p>\n<h2><a id=\"user-content-install-from-build-cache-example\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#install-from-build-cache-example\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Install\
    \ from Build Cache (Example)</h2>\n<ul>\n<li>View a demo video of these instructions\
    \ run at <a href=\"https://asciinema.org/a/508123\" rel=\"nofollow\">https://asciinema.org/a/508123</a>\n\
    </li>\n</ul>\n<pre><code>$crusher:~&gt; git clone https://github.com/ParaToolsInc/exago-crusher.git\n\
    $crusher:~&gt; cd exago-crusher\n\n$crusher:~/exago-crusher&gt; git clone https://github.com/spack/spack\n\
    $crusher:~/exago-crusher&gt; (cd spack &amp;&amp; git checkout dac31ef3c)\n\n\
    $crusher:~/exago-crusher&gt; export SPACK_DISABLE_LOCAL_CONFIG=1\n$crusher:~/exago-crusher&gt;\
    \ export SPACK_USER_CACHE_PATH=$(pwd)/_cache\n$crusher:~/exago-crusher&gt; . spack/share/spack/setup-env.sh\n\
    \n$crusher:~/exago-crusher&gt; spack mirror add paratools /gpfs/alpine/csc439/world-shared/E4S/ParaTools/exago\n\
    $crusher:~/exago-crusher&gt; spack buildcache keys -it\ngpg: key 4345F04B40005581:\
    \ public key \"University of Oregon - E4S\" imported\ngpg: Total number processed:\
    \ 1\ngpg:               imported: 1\ngpg: inserting ownertrust of 6\n\n$crusher:~/exago-crusher&gt;\
    \ time spack -e . concretize -f | tee concretize.log\n... output truncated for\
    \ brevity; see concretize.log in this repo for full output\nreal\t0m46.340s\n\
    user\t1m25.263s\nsys\t0m2.094s\n\n$crusher:~/exago-crusher&gt; time spack -e .\
    \ install --cache-only\n... output truncated for brevity\nreal\t7m40.626s\nuser\t\
    4m44.081s\nsys\t0m33.864s\n\n$crusher:~/exago-crusher&gt; spack find -lv exago\
    \ hiop ipopt coinhsl\n==&gt; 8 installed packages\n-- cray-sles15-zen3 / gcc@11.2.0\
    \ --------------------------------\nue74alo coinhsl@2015.06.23+blas\nmnelc7u exago@develop~cuda+hiop~ipo~ipopt+mpi+python+raja+rocm\
    \ amdgpu_target=gfx90a build_type=RelWithDebInfo\nanahf5s exago@develop~cuda+hiop~ipo~ipopt+mpi+python+raja+rocm\
    \ amdgpu_target=gfx90a build_type=RelWithDebInfo\n2qog6zw exago@develop~cuda+hiop~ipo+ipopt+mpi+python+raja+rocm\
    \ amdgpu_target=gfx90a build_type=RelWithDebInfo\ndr3jlyb exago@develop~cuda+hiop~ipo+ipopt+mpi+python+raja+rocm\
    \ amdgpu_target=gfx90a build_type=RelWithDebInfo\nwtqj2hu hiop@0.6.2~cuda~cusolver~deepchecking~ginkgo~ipo~jsrun~kron+mpi+raja+rocm~shared+sparse\
    \ amdgpu_target=gfx90a build_type=RelWithDebInfo\nrlw4qhu hiop@0.6.2~cuda~cusolver~deepchecking~ginkgo~ipo~jsrun+kron+mpi+raja+rocm~shared+sparse\
    \ amdgpu_target=gfx90a build_type=RelWithDebInfo\nufjh4v7 ipopt@3.14.5+coinhsl~debug+metis~mumps\n\
    </code></pre>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1657663349.0
PawseySC/pawsey-spack-config:
  data_format: 2
  description: Configuration files for Spack at Pawsey
  filenames:
  - setonix/environments/env_io_libs/spack.yaml
  - setonix/environments/env_devel/spack.yaml
  - setonix/environments/env_python/spack.yaml
  - setonix/environments/env_vis/spack.yaml
  - setonix/environments/env_roms/spack.yaml
  - setonix/environments/env_num_libs/spack.yaml
  - setonix/environments/env_s3_clients/spack.yaml
  - setonix/environments/env_langs/spack.yaml
  - setonix/environments/env_astro/spack.yaml
  - setonix/environments/env_apps/spack.yaml
  - setonix/environments/env_utils/spack.yaml
  - setonix/environments/env_bench/spack.yaml
  - setonix/environments/env_bio/spack.yaml
  - setonix/environments/env_wrf/spack.yaml
  full_name: PawseySC/pawsey-spack-config
  latest_release: null
  readme: '<h1><a id="user-content-pawsey-spack-config" class="anchor" aria-hidden="true"
    href="#pawsey-spack-config"><span aria-hidden="true" class="octicon octicon-link"></span></a>pawsey-spack-config</h1>

    <p>Configuration files for Spack at Pawsey.</p>

    <h2><a id="user-content-setonix-setup" class="anchor" aria-hidden="true" href="#setonix-setup"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Setonix setup</h2>

    <p>This can be found in the <code>setonix/</code> directory.<br>

    See <code>README.md</code> in there for further information.</p>

    <h2><a id="user-content-other-setups" class="anchor" aria-hidden="true" href="#other-setups"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Other setups</h2>

    <ul>

    <li>

    <code>joey/</code>: test deployment for the Setonix test system</li>

    <li>Current Pawsey systems

    <ul>

    <li><code>askapingest/</code></li>

    <li><code>garrawarla/</code></li>

    <li><code>magnus/</code></li>

    <li><code>topaz/</code></li>

    <li><code>zeus/</code></li>

    </ul>

    </li>

    <li>

    <code>examples/</code>: deployment examples and tests</li>

    <li>

    <code>deprecated/</code>: legacy deployments</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 10
  topics: []
  updated_at: 1641801068.0
SCOREC/rhel7-spack-config:
  data_format: 2
  description: rhel7 spack configuration and scripts
  filenames:
  - v0.15.4/spack.yaml
  full_name: SCOREC/rhel7-spack-config
  latest_release: null
  readme: "<h1><a id=\"user-content-setup-on-scorec\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#setup-on-scorec\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>setup on SCOREC</h1>\n<pre><code>cd /opt/scorec/spack/rhel7-spack-config/\n\
    source setupSpack.sh\n</code></pre>\n<h1><a id=\"user-content-rhel7-spack-config\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#rhel7-spack-config\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>rhel7-spack-config</h1>\n<p>rhel7\
    \ spack configuration and scripts</p>\n<p>The <code>install.sh</code> script maintained\
    \ in this repo is for documentation purposes (e.g., in case we had to reinstall\
    \ the entire stack from scratch) and should not be executed as it will not use\
    \ all of our existing package installs.  More discussion of package installation\
    \ is below.</p>\n<h2><a id=\"user-content-useful-commands\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#useful-commands\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>useful commands</h2>\n<p>regenerate lmod module tree:</p>\n<pre><code>spack\
    \ module lmod refresh\n</code></pre>\n<h2><a id=\"user-content-installing-new-packages\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#installing-new-packages\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>installing new\
    \ packages</h2>\n<p>Our spack repo is tracking the master spack branch.  Spack\
    \ package updates could result in additional installation of packages with little\
    \ or no package source code changes.  These additional installs can be avoided\
    \ when installing new packages by first examining the output of the <code>spack\
    \ spec -I</code> command.  If a utility/infrastructure level package, such as\
    \ cmake or mpich, is marked with a <code>[+]</code> symbol in the leftmost column\
    \ then it means that the existing install will be used.  If spack does not default\
    \ to using the existing install you can append the hash of the package to the\
    \ spec command.</p>\n<p>For example, lets see what happens when we ask for a pumi\
    \ install using gcc 7.3.0</p>\n<pre><code>$ spack spec -I pumi@develop%gcc@7.3.0\n\
    Input spec\n--------------------------------\n -   pumi@develop%gcc@7.3.0\n\n\
    Concretized\n--------------------------------\n -   pumi@develop%gcc@7.3.0 build_type=RelWithDebInfo\
    \ ~fortran~shared simmodsuite=none ~zoltan arch=linux-rhel7-x86_64 \n[+]     \
    \ ^cmake@3.13.1%gcc@7.3.0~doc+ncurses+openssl+ownlibs~qt arch=linux-rhel7-x86_64\
    \ \n[+]          ^ncurses@6.1%gcc@7.3.0~symlinks~termlib arch=linux-rhel7-x86_64\
    \ \n[+]              ^pkgconf@1.5.4%gcc@7.3.0 arch=linux-rhel7-x86_64 \n[+]  \
    \        ^openssl@1.1.1%gcc@7.3.0+systemcerts arch=linux-rhel7-x86_64 \n[+]  \
    \            ^perl@5.16.3%gcc@7.3.0+cpanm patches=0eac10ed90aeb0459ad8851f88081d439a4e41978e586ec743069e8b059370ac\
    \ +shared+threads arch=linux-rhel7-x86_64 \n[+]              ^zlib@1.2.11%gcc@7.3.0+optimize+pic+shared\
    \ arch=linux-rhel7-x86_64 \n -       ^mpich@3.3%gcc@7.3.0 device=ch3 +hydra netmod=tcp\
    \ +pmi+romio~verbs arch=linux-rhel7-x86_64 \n[+]          ^findutils@4.6.0%gcc@7.3.0\
    \ patches=84b916c0bf8c51b7e7b28417692f0ad3e7030d1f3c248ba77c42ede5c1c5d11e,bd9e4e5cc280f9753ae14956c4e4aa17fe7a210f55dd6c84aa60b12d106d47a2\
    \ arch=linux-rhel7-x86_64 \n[+]              ^autoconf@system%gcc@7.3.0 arch=linux-rhel7-x86_64\
    \ \n[+]              ^automake@system%gcc@7.3.0 arch=linux-rhel7-x86_64 \n[+]\
    \              ^libtool@system%gcc@7.3.0 arch=linux-rhel7-x86_64 \n[+]       \
    \       ^m4@1.4.16%gcc@7.3.0 patches=c0a408fbffb7255fcc75e26bd8edab116fc81d216bfd18b473668b7739a4158e\
    \ +sigsegv arch=linux-rhel7-x86_64 \n[+]              ^texinfo@6.5%gcc@7.3.0 arch=linux-rhel7-x86_64\n\
    </code></pre>\n<p>Spack wants to install mpich 3.3, but we don't want to change\
    \ to the new mpich version yet.  So, we will get the hash of the existing mpich\
    \ 3.2.1 install:</p>\n<pre><code>$ spack find -ldv mpich%gcc@7.3.0\n==&gt; 1 installed\
    \ package\n-- linux-rhel7-x86_64 / gcc@7.3.0 -------------------------------\n\
    niuhmad    mpich@3.2.1 device=ch3 +hydra netmod=tcp +pmi+romio~verbs\n</code></pre>\n\
    <p>then append the hash <code>niuhmad</code> to the spec for pumi using the <code>^</code>\
    \ syntax to specify it as a dependency:</p>\n<pre><code>$ spack spec -I pumi@develop%gcc@7.3.0\
    \ ^/niuhmad\nInput spec\n--------------------------------\n -   pumi@develop%gcc@7.3.0\n\
    [+]      ^mpich@3.2.1%gcc@7.3.0 device=ch3 +hydra netmod=tcp +pmi+romio~verbs\
    \ arch=linux-rhel7-x86_64 \n\nConcretized\n--------------------------------\n\
    \ -   pumi@develop%gcc@7.3.0 build_type=RelWithDebInfo ~fortran~shared simmodsuite=none\
    \ ~zoltan arch=linux-rhel7-x86_64 \n[+]      ^cmake@3.13.1%gcc@7.3.0~doc+ncurses+openssl+ownlibs~qt\
    \ arch=linux-rhel7-x86_64 \n[+]          ^ncurses@6.1%gcc@7.3.0~symlinks~termlib\
    \ arch=linux-rhel7-x86_64 \n[+]              ^pkgconf@1.5.4%gcc@7.3.0 arch=linux-rhel7-x86_64\
    \ \n[+]          ^openssl@1.1.1%gcc@7.3.0+systemcerts arch=linux-rhel7-x86_64\
    \ \n[+]              ^perl@5.16.3%gcc@7.3.0+cpanm patches=0eac10ed90aeb0459ad8851f88081d439a4e41978e586ec743069e8b059370ac\
    \ +shared+threads arch=linux-rhel7-x86_64 \n[+]              ^zlib@1.2.11%gcc@7.3.0+optimize+pic+shared\
    \ arch=linux-rhel7-x86_64 \n[+]      ^mpich@3.2.1%gcc@7.3.0 device=ch3 +hydra\
    \ netmod=tcp +pmi+romio~verbs arch=linux-rhel7-x86_64 \n</code></pre>\n<p>And\
    \ see that in the Concretized spec it is now using the existing mpich 3.2.1 install.</p>\n\
    <h2><a id=\"user-content-contents\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #contents\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>contents</h2>\n\
    <p>compilers.yaml - compiler list\nconfig.yaml - global config\ninstall.sh - package\
    \ installation commands\nmodules.yaml - hierarchical layout for lua modules\n\
    packages.yaml - system installed packages\nREADME.md - this file\nsetupSpack.sh\
    \ - env needed for executing spack commands</p>\n"
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1643231013.0
SouthernMethodistUniversity/mp_testing:
  data_format: 2
  description: null
  filenames:
  - mp/testing/05_openmm/spack.yaml
  - mp/testing/08_pytorch/spack.yaml
  - mp/testing/01_spack/spack_lammps.yaml
  - mp/testing/01_spack/spack_nvhpc.yaml
  full_name: SouthernMethodistUniversity/mp_testing
  latest_release: null
  readme: '<h1><a id="user-content-notes" class="anchor" aria-hidden="true" href="#notes"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Notes</h1>

    <p><strong>All code in this repo is for testing. The code may not work and may
    change. Pull requests and issues welcome.</strong></p>

    <p>See <a href="quick_start_notes.md">Quick Start Notes</a> for a short overview
    of MP usage.</p>

    <h2><a id="user-content-usage-example" class="anchor" aria-hidden="true" href="#usage-example"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage Example</h2>

    <div class="highlight highlight-source-shell"><pre>git clone https://github.com/SouthernMethodistUniversity/mp_testing.git

    <span class="pl-c1">cd</span> mp_testing/demos/00_nemo

    ./submit_jobs.sh</pre></div>

    <h2><a id="user-content-applications" class="anchor" aria-hidden="true" href="#applications"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Applications</h2>

    <ul>

    <li>LAMMPS (NGC)</li>

    <li>AMBER</li>

    <li>NAMD (NGC)</li>

    <li>OpenMM</li>

    <li>Gaussian</li>

    <li>VASP</li>

    <li>CRYSTAL</li>

    <li>Q-Chem</li>

    <li>Quantum Espresso</li>

    </ul>

    <h2><a id="user-content-analysis-tools" class="anchor" aria-hidden="true" href="#analysis-tools"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Analysis Tools</h2>

    <ul>

    <li>Memory profiling</li>

    <li>Performance profiling</li>

    </ul>

    <h2><a id="user-content-librariesapis" class="anchor" aria-hidden="true" href="#librariesapis"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Libraries/APIs</h2>

    <ul>

    <li>Raja</li>

    <li>Magma</li>

    <li>heFFTe</li>

    <li>Pandas</li>

    <li>NumPy</li>

    <li>TensorFlow</li>

    <li>PyTorch</li>

    <li>DALI</li>

    </ul>

    <h2><a id="user-content-languages" class="anchor" aria-hidden="true" href="#languages"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Languages</h2>

    <ul>

    <li>C</li>

    <li>C++</li>

    <li>Python</li>

    <li>Some custom layer in C++/CUDA</li>

    <li>Fortran</li>

    <li>CUDA Fortran</li>

    <li>Julia</li>

    </ul>

    <h2><a id="user-content-molecular-dynamics" class="anchor" aria-hidden="true"
    href="#molecular-dynamics"><span aria-hidden="true" class="octicon octicon-link"></span></a>Molecular
    Dynamics</h2>

    <ul>

    <li>OpenMM</li>

    <li>AMBER</li>

    <li>Desmond</li>

    <li>GROMACS</li>

    <li>Mentioned MIC modes?</li>

    <li>NGC for Keras/TF and Pytorch</li>

    </ul>

    <h2><a id="user-content-issues" class="anchor" aria-hidden="true" href="#issues"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Issues</h2>

    <ul>

    <li>Can''t run enroot images directly via <code>enroot start hello_world.sqsh</code>.
    The OS

    needs squashfuse and fuse-overlayfs installed. I installed these on Easley and

    it works.</li>

    <li>Custom build and final images for containerized Spack environments fails due

    to apparently assuming that Spack already exists. See: <code>01_spack/spack_nvhpc.yaml</code>.</li>

    <li>Spack-blessed NVIDIA container fails to build due to public key error. See:
    <code>01_spack/spack_lammps.yaml</code>.</li>

    <li>

    <code>export ENROOT_MOUNT_HOME=1</code> to bind $HOME.</li>

    <li>Default flags and <code>target=zen2</code> gave LAMMPS run times of 4:44,
    while <code>target=zen2 cppflags=-O3</code>

    </li>

    <li>Running containers or non-hpc-x MPI produces warnings about <code>Unknown
    interface name</code> /

    <code>An invalid value was given for btl_tcp_if_include</code>. It appears not
    to see the Mellanox / IB correctly?</li>

    </ul>

    <h2><a id="user-content-maybe-useful" class="anchor" aria-hidden="true" href="#maybe-useful"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Maybe Useful</h2>

    <ul>

    <li><a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/gpu-applications-catalog.pdf"
    rel="nofollow">https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/gpu-applications-catalog.pdf</a></li>

    <li><a href="https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html"
    rel="nofollow">https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html</a></li>

    <li><a href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/index.html"
    rel="nofollow">https://docs.nvidia.com/deeplearning/dali/user-guide/docs/index.html</a></li>

    <li><a href="https://secure.cci.rpi.edu/wiki/" rel="nofollow">https://secure.cci.rpi.edu/wiki/</a></li>

    </ul>

    <h2><a id="user-content-things-we-need-to-plan-for" class="anchor" aria-hidden="true"
    href="#things-we-need-to-plan-for"><span aria-hidden="true" class="octicon octicon-link"></span></a>Things
    we need to plan for</h2>

    <ul>

    <li>How and when do we decide we''re updating Nvidia Drivers / Cuda. I think we
    need to be very clear about this if we''re not going to maintain the latest and
    greatest. (we''re currently on 11.4, but 11.7 and associated drivers are available)</li>

    </ul>

    '
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1661535707.0
SouthernMethodistUniversity/msds_hpc:
  data_format: 2
  description: null
  filenames:
  - classes/06_2/spack_20.04_openblas.yaml
  - classes/06_2/spack_20.04_mkl.yaml
  - classes/04_2/spack_containers/spack.yaml
  full_name: SouthernMethodistUniversity/msds_hpc
  latest_release: null
  readme: '<h1><a id="user-content-ds-7347-high-performance-computing-hpc-and-data-science"
    class="anchor" aria-hidden="true" href="#ds-7347-high-performance-computing-hpc-and-data-science"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>DS 7347 High-Performance
    Computing (HPC) and Data Science</h1>

    <h2><a id="user-content-assignments" class="anchor" aria-hidden="true" href="#assignments"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Assignments</h2>

    <table>

    <thead>

    <tr>

    <th align="left">Key</th>

    <th align="left">Value</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td align="left">A</td>

    <td align="left">Assignment</td>

    </tr>

    <tr>

    <td align="left">L</td>

    <td align="left">Lab</td>

    </tr>

    <tr>

    <td align="left">P</td>

    <td align="left">Project</td>

    </tr>

    </tbody>

    </table>

    <table>

    <thead>

    <tr>

    <th align="left">Assignment</th>

    <th align="left">Issued</th>

    <th align="left">Due</th>

    <th align="left">Deliverable</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td align="left">A1</td>

    <td align="left">04-26</td>

    <td align="left">NA</td>

    <td align="left">Fork class repo.</td>

    </tr>

    <tr>

    <td align="left">A2</td>

    <td align="left">04-28</td>

    <td align="left">05-03</td>

    <td align="left"><code>assignments/assignment_02.md</code></td>

    </tr>

    <tr>

    <td align="left">A3</td>

    <td align="left">05-05</td>

    <td align="left">NA</td>

    <td align="left">Detail the CPU, GPU, memory, and hard drive for your own computer.</td>

    </tr>

    <tr>

    <td align="left">A4.1</td>

    <td align="left">05-10</td>

    <td align="left">05-17</td>

    <td align="left"><code>assignments/assignment_04.sh</code></td>

    </tr>

    <tr>

    <td align="left">L1</td>

    <td align="left">05-12</td>

    <td align="left">05-19</td>

    <td align="left"><code>assignments/lab_01.{yaml,md}</code></td>

    </tr>

    <tr>

    <td align="left">A4.2</td>

    <td align="left">05-17</td>

    <td align="left">05-24</td>

    <td align="left"><code>assignments/assignment_04.dockerfile</code></td>

    </tr>

    <tr>

    <td align="left">L2</td>

    <td align="left">05-19</td>

    <td align="left">05-26</td>

    <td align="left"><code>assignments/lab_02.{dockerfile,png,jpg}</code></td>

    </tr>

    <tr>

    <td align="left">A5.1</td>

    <td align="left">05-24</td>

    <td align="left">05-31</td>

    <td align="left"><code>assignments/assignment_05.out</code></td>

    </tr>

    <tr>

    <td align="left">P1</td>

    <td align="left">05-26</td>

    <td align="left">06-02</td>

    <td align="left"><code>project/proposal.md</code></td>

    </tr>

    <tr>

    <td align="left">L3</td>

    <td align="left">06-07</td>

    <td align="left">06-21</td>

    <td align="left"><code>assignments/lab_03.{yaml,sh,make or cmake}</code></td>

    </tr>

    <tr>

    <td align="left">P2</td>

    <td align="left">06-09</td>

    <td align="left">06-16</td>

    <td align="left">Create new GitHub repo from project template.</td>

    </tr>

    <tr>

    <td align="left">P3</td>

    <td align="left">06-16</td>

    <td align="left">06-21</td>

    <td align="left">Prototype of multi-job Slurm submit script.</td>

    </tr>

    <tr>

    <td align="left">A5.2</td>

    <td align="left">06-21</td>

    <td align="left">06-23</td>

    <td align="left"><code>assignments/assignment_05.txt</code></td>

    </tr>

    <tr>

    <td align="left">P4</td>

    <td align="left">06-23</td>

    <td align="left">06-28</td>

    <td align="left">Implement one subtask of your workflow using "easiest" installation
    path.</td>

    </tr>

    <tr>

    <td align="left">P5</td>

    <td align="left">06-30</td>

    <td align="left">NA</td>

    <td align="left">Explore various file formats for your data and compare performance.</td>

    </tr>

    <tr>

    <td align="left">P6</td>

    <td align="left">07-05</td>

    <td align="left">07-12</td>

    <td align="left">Complete non-optimized and basic workflow, reduce data or analysis
    complexity if needed.</td>

    </tr>

    <tr>

    <td align="left">P7</td>

    <td align="left">07-14</td>

    <td align="left">07-19</td>

    <td align="left">Report three targets for optimization and baseline performance</td>

    </tr>

    <tr>

    <td align="left">P8</td>

    <td align="left">07-19</td>

    <td align="left">07-28</td>

    <td align="left">Implement initial improvements for your three optimization targets</td>

    </tr>

    </tbody>

    </table>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1657068578.0
UO-OACISS/e4s:
  data_format: 2
  description: E4S Spack environments and container recipes
  filenames:
  - docker-recipes/minimal/ubuntu20.04-aarch64/spack.yaml
  - docker-recipes/runner/ubuntu22.04-x86_64/spack.yaml
  - docker-recipes/minimal/ubuntu20.04-x86_64/spack.yaml
  - docker-recipes/runner/ubuntu22.04-ppc64le/spack.yaml
  - docker-recipes/runner/ubuntu22.04-aarch64/spack.yaml
  - docker-recipes/minimal/ubuntu22.04-aarch64/spack.yaml
  - docker-recipes/minimal/ubuntu20.04-ppc64le/spack.yaml
  - docker-recipes/runner/rhel8-ppc64le/spack.yaml
  - docker-recipes/runner/rhel8-x86_64/spack.yaml
  - docker-recipes/runner/ubuntu18.04-ppc64le/spack.yaml
  - docker-recipes/runner/ubuntu20.04-x86_64-oneapi/spack.yaml
  - docker-recipes/minimal/ubuntu22.04-x86_64/spack.yaml
  - docker-recipes/minimal/ubuntu22.04-ppc64le/spack.yaml
  - docker-recipes/runner/rhel8-aarch64/spack.yaml
  - docker-recipes/runner/ubuntu20.04-x86_64/spack.yaml
  - docker-recipes/minimal/rhel8-x86_64/spack.yaml
  - docker-recipes/runner/ubuntu20.04-aarch64/spack.yaml
  - docker-recipes/minimal/rhel8-aarch64/spack.yaml
  - docker-recipes/runner/ubuntu20.04-ppc64le/spack.yaml
  - docker-recipes/minimal/rhel8-ppc64le/spack.yaml
  - docker-recipes/runner/ubuntu18.04-x86_64/spack.yaml
  full_name: UO-OACISS/e4s
  latest_release: null
  readme: '<p>This is a collection of configurations for building ECP SDK

    containers with combinations of packages, including the full

    E4S set.</p>

    <p>These are the set of stacks that are targeted for the first release:</p>

    <p><a target="_blank" rel="noopener noreferrer" href="figures/SDKdefinition1.png"><img
    src="figures/SDKdefinition1.png" alt="SDK definitions" style="max-width: 100%;"></a></p>

    <p>The configuration files for each container platform will be specified under
    each directory.  For example, the Docker configurations are under the "docker"
    subdirectory.  Each subdirectory will have a README.md file to explain how to
    build the container image for each stack.</p>

    '
  stargazers_count: 18
  subscribers_count: 6
  topics: []
  updated_at: 1661875839.0
adamqc/devcontainer:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: adamqc/devcontainer
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1646725990.0
alexpacheco/spackenv:
  data_format: 2
  description: 'Spack Environments '
  filenames:
  - cent8/envs/avx/lusoft/spack.yaml
  - cent8/envs/avx2/lusoft/spack.yaml
  - cent8/envs/avx512/lusoft/spack.yaml
  - cent8/envs/x86_64/spack.yaml
  - cent8/envs/solhawk/spack.yaml
  full_name: alexpacheco/spackenv
  latest_release: null
  readme: '<h1><a id="user-content-spack-environments" class="anchor" aria-hidden="true"
    href="#spack-environments"><span aria-hidden="true" class="octicon octicon-link"></span></a>SPACK
    Environments</h1>

    <p>This repo contains the environment definitions to deploy site-software on Lehigh
    University''s Research Computing resources via SPACK environments.</p>

    <h2><a id="user-content-software-deployment-for-centos-8x" class="anchor" aria-hidden="true"
    href="#software-deployment-for-centos-8x"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Software deployment for CentOS 8.x</h2>

    <p>Software is deployed using two Spack installations.</p>

    <ol>

    <li>For compilers and module environments</li>

    <li>Site software for general use</li>

    </ol>

    <h3><a id="user-content-compilers" class="anchor" aria-hidden="true" href="#compilers"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Compilers</h3>

    <p>This spack installation provides the gcc, nvhpc and cuda compilers, and lmod
    software for module management. In the future, this installation will also provide
    intel-oneapi compilers. For legacy reasons, intel@19.0.3 and intel@20.0.3 were
    installed in /share/Apps/intel with older intel compilers. This installation should
    not be used for deploying site software nor should the software provided be made
    available using the module environment.</p>

    <p>To reproduce installation</p>

    <pre><code>git clone https://github.com/alexpacheco/spackenv.git

    cd spackenv/compilers/envs/compilers

    spack env activate -d .

    spack concretize -f # optional

    spack install

    </code></pre>

    <p>The directory <code>etc/lmod</code> contains the LMOD configuration to switch
    between avx, avx2 and avx512 enabled <code>MODULEPATHS</code></p>

    <h3><a id="user-content-lu-software" class="anchor" aria-hidden="true" href="#lu-software"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>LU Software</h3>

    <p>This spack installation provides the deployed site-software on Sol and Hawk.</p>

    <p>To reproduce this installation, you need to first copy the site configuration
    files from <code>etc/spack</code> to your spack install tree. This assumes that
    SLURM and the compiler environment above is already installed. Edit the <code>packages.yaml</code>
    file to point to the location of slurm (/usr/local), rmda-core (/usr), gcc, intel,
    cuda, and nvhpc. The file <code>repo.yaml</code> is hardwired with  location of
    the lubio repository and should be changed to your location. The directory <code>templates</code>
    contains the template lua file for a few modules as defined in the <code>modules.yaml</code>
    file  and should be copied to the <code>etc</code> directory in your spack installation
    tree.</p>

    <p>On Sol, these files are available at <code>/share/Apps/lusoft/etc/spack</code>.</p>

    <h4><a id="user-content-available-environments" class="anchor" aria-hidden="true"
    href="#available-environments"><span aria-hidden="true" class="octicon octicon-link"></span></a>Available
    Environments</h4>

    <h5><a id="user-content-solhawk" class="anchor" aria-hidden="true" href="#solhawk"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>solhawk</h5>

    <p>This environment builds the entire software except the various python and r
    packages for ivybridge, haswell and skylake_avx512 architectures. This environment
    also builds the tcl environment modules that is not currently used. This should
    be build first and any new packages should be added to this environment.</p>

    <pre><code>cd spackenv/cent8/envs/solhawk

    spack env activate -d .

    spack concretize -f # optional

    spack install

    </code></pre>

    <h4><a id="user-content-avxavx2avx512" class="anchor" aria-hidden="true" href="#avxavx2avx512"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>avx/avx2/avx512</h4>

    <p>These environment builds the software stack except the various python and r
    packages for ivybridge/haswell/skylake_avx512 architectures. If software in the
    <code>solhawk</code> environment is already built, then these environments are
    only setting up the installation root for the LMOD module files <code>/share/Apps/lusoft/share/modules/lmod/{avx,avx2,avx512}</code>.
    The only reason these environments exist is due to SPACK''s inability to built
    a architecture based LMOD module tree similar to the TCL module tree.

    <em>Note</em>: If you change the path of the installation root, make sure that
    you change the corresponding path in <code>compilers/etc/SitePackage.lua</code>.</p>

    <pre><code>cd spackenv/cent8/envs/avx2/lusoft

    spack env activate -d .

    spack concretize -f # optional

    spack install

    </code></pre>

    <h4><a id="user-content-python-and-r-packages" class="anchor" aria-hidden="true"
    href="#python-and-r-packages"><span aria-hidden="true" class="octicon octicon-link"></span></a>Python
    and R packages</h4>

    <p>Rather than building module files for various python and r packages, a single
    module is created for a filesystem view of all python and r packages respectively.
    The path to the r filesystem is setup as <code>R_LIBS_SITE</code> so that any
    application such as <code>trinity</code> that requires many R packages only need
    to load the r module. If new packages added to the above environments require
    a dependent R package, then that dependency should be added to the rpoject environment
    and concretized. The python environment uses a <code>concretization: together</code>
    and may not provide the same python package as the above software environments.
    The filesystem views are hardwired as <code>/share/Apps/py_spack/3.8.6/{avx,avx2,avx512}</code>
    and <code>/share/Apps/r_spack/4.0.3/{avx,avx2,avx512}</code>.</p>

    <pre><code>cd spackenv/cent8/envs/avx/python

    spack env activate -d .

    spack concretize -f # optional

    spack install

    </code></pre>

    <pre><code>cd spackenv/cent8/envs/avx512/rproject

    spack env activate -d .

    spack concretize -f # optional

    spack install

    </code></pre>

    <h4><a id="user-content-x86_64" class="anchor" aria-hidden="true" href="#x86_64"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>x86_64</h4>

    <p>This environment builds unoptimized software such as anaconda python, gnu parallel,
    scree, tmux, etc for generic x86_64 processor.</p>

    <h2><a id="user-content-centos-7x-software" class="anchor" aria-hidden="true"
    href="#centos-7x-software"><span aria-hidden="true" class="octicon octicon-link"></span></a>CentOS
    7.x software</h2>

    <p>This just collects the various environments for building software before the
    CentOS 8.x upgrade.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1657632897.0
arcaneframework/containers:
  data_format: 2
  description: Containers with Arcane and Alien
  filenames:
  - spack/envs/alien/spack.yaml
  - spack/envs/all/spack.yaml
  full_name: arcaneframework/containers
  latest_release: null
  readme: '<h1><a id="user-content-containers" class="anchor" aria-hidden="true" href="#containers"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>containers</h1>

    <p>Containers with Arcane and Alien</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1637878998.0
ashermancinelli/vimconfig:
  data_format: 2
  description: null
  filenames:
  - spack/spack.yaml
  full_name: ashermancinelli/vimconfig
  latest_release: null
  readme: "<h1><a id=\"user-content-vimconfig\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#vimconfig\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>vimconfig</h1>\n<p>Lots and lots of different configurations for various\
    \ programs all wrapped up into one repo. Under heavy development so tread with\
    \ some caution :)</p>\n<h2><a id=\"user-content-how-to-use\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#how-to-use\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>How to use</h2>\n<p>The top directory has a\
    \ script to deal with installation - you should pretty much only interact with\
    \ the repo through that script.\nThe help message is quite descriptive:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>$ ./configure --h\n\n  Usage:\n\
    \n  -p <span class=\"pl-k\">&lt;</span>path<span class=\"pl-k\">&gt;</span>  \
    \         Sets install prefix. Default: /people/manc568/.local\n  -r <span class=\"\
    pl-k\">&lt;</span>path<span class=\"pl-k\">&gt;</span>           Path to RC file\
    \ <span class=\"pl-k\">for</span> given shell. Default: /qfs/people/manc568/.bashrc\n\
    \  -d                  Default installation. Installs ctags, vim, and bash\n \
    \ -s <span class=\"pl-k\">&lt;</span>pkg<span class=\"pl-k\">&gt;</span>     \
    \       Show installation script <span class=\"pl-k\">for</span> pacakge\n  -i\
    \                  One or more of the following list, separated by commas with\
    \ no spaces:\n\n       zsh\n       bash\n       ctags\n       vim\n       tmux\n\
    \       emacs\n       profiles\n       modules\n       rice\n       rice.sh\n\
    \       fresh</pre></div>\n<h2><a id=\"user-content-examples\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#examples\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Examples</h2>\n<p>For example, to just install\
    \ my vim configuration, you'd do:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ ./configure -i vim</pre></div>\n<p>Or to install configs for multiple\
    \ programs:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ ./configure\
    \ -i vim,ctags,tmux,emacs,bash</pre></div>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1641783693.0
bfovet/config:
  data_format: 2
  description: My personal configuration files
  filenames:
  - spack-environments/linux-neon20-skylake/spack.yaml
  full_name: bfovet/config
  latest_release: null
  readme: '<h1><a id="user-content-config" class="anchor" aria-hidden="true" href="#config"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>config</h1>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1658465316.0
bsurc/spack-configs:
  data_format: 2
  description: spack configuration settings used at BSU research computing
  filenames:
  - OLCF/spock/spack.yaml
  - NREL/configs/eagle/utilities/spack.yaml
  - NERSC/cori/e4s-stacks/x86/spack.yaml
  - BOISESTATE/falcon/environments/base/_spack.yaml
  - BOISESTATE/falcon/environments/libraries/hdf5/_spack.yaml
  - BOISESTATE/falcon/environments/applications/openfoam/_spack.yaml
  - NREL/configs/rhodes/utilities/spack.yaml
  - BOISESTATE/falcon/environments/applications/vacuumms/_spack.yaml
  - UOREGON/E4S-21.08-Facility-Examples/spack.yaml
  - ANL/JLSE/Arcticus/E4S-21.08/spack.yaml
  - UOREGON/E4S-Develop/spack-ubuntu18.04-ppc64le.yaml
  - UOREGON/E4S-21.05-Facility-Examples/NERSC-Cori/intel-spack.yaml
  - NERSC/cori/e4s-stacks/hsw/spack.yaml
  - NERSC/cori/e4s-21.02/spack.yaml
  - UOREGON/E4S-Develop/spack-ubuntu18.04-x86_64.yaml
  - UOREGON/E4S-21.05-Facility-Examples/Frank-Jupiter/spack.yaml
  - ANL/JLSE/Arcticus/E4S-21.11/spack.yaml
  - BOISESTATE/falcon/environments/applications/vasp/_spack.yaml
  - UOREGON/E4S-Develop/spack-ubuntu20.04-ppc64le.yaml
  - NERSC/cori/e4s-20.10/spack.yaml
  - ANL/JLSE/Arcticus/E4S-21.08/prod/spack.yaml
  - BOISESTATE/falcon/environments/applications/wps/_spack.yaml
  - NREL/configs/eagle/compilers/spack.yaml
  - NERSC/perlmutter/e4s-21.11/spack.yaml
  - BOISESTATE/falcon/environments/Core/_spack.yaml
  full_name: bsurc/spack-configs
  latest_release: null
  readme: '<h1><a id="user-content-spack-configs" class="anchor" aria-hidden="true"
    href="#spack-configs"><span aria-hidden="true" class="octicon octicon-link"></span></a>Spack
    Configs</h1>

    <p>This is a repository that sites can use to share their configuration

    files for Spack.  You can contribute your own configuration files, or

    browse around and look at what others have done.</p>

    <h2><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>Spack is distributed under the terms of both the MIT license and the

    Apache License (Version 2.0). Users may choose either license, at their

    option.</p>

    <p>All new contributions must be made under both the MIT and Apache-2.0

    licenses.</p>

    <p>See <a href="https://github.com/spack/spack-configs/blob/master/LICENSE-MIT">LICENSE-MIT</a>,

    <a href="https://github.com/spack/spack-configs/blob/master/LICENSE-APACHE">LICENSE-APACHE</a>,

    <a href="https://github.com/spack/spack-configs/blob/master/COPYRIGHT">COPYRIGHT</a>,
    and

    <a href="https://github.com/spack/spack-configs/blob/master/NOTICE">NOTICE</a>
    for details.</p>

    <p>SPDX-License-Identifier: (Apache-2.0 OR MIT)</p>

    <p>LLNL-CODE-811652</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1660257692.0
buildsi/splice-experiment:
  data_format: 2
  description: Preparing for the splice experiment (notes are currently here)
  filenames:
  - manual/ref/e4s/spack.yaml
  full_name: buildsi/splice-experiment
  latest_release: null
  readme: '<h1><a id="user-content-splice-experiment" class="anchor" aria-hidden="true"
    href="#splice-experiment"><span aria-hidden="true" class="octicon octicon-link"></span></a>Splice
    Experiment</h1>

    <p>This is planning for the <a href="https://github.com/buildsi/spliced">spliced</a>
    experiment

    that we plan to run for the BUILDSI project. We will use a container base to the
    largest extent possible.

    To see our original manual setup, you can look in <a href="manual">manual</a>,
    or to read the original

    experiment plan and design, see <a href="plan.md">plan.md</a>. Note that although
    the original plan was to run this on HPC, the file-system had significant issues
    and it was entirely run in GitHub actions. For the interested user,

    examples of running scripts are provided for Singularity, Podman, and Docker,
    in the case you want to do this manually. The actual running of experiments happened
    in <a href="https://github.com/buildsi/splice-experiment-runs">this repository</a>.</p>

    <h2><a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Setup</h2>

    <h3><a id="user-content-1-experiment-derivation" class="anchor" aria-hidden="true"
    href="#1-experiment-derivation"><span aria-hidden="true" class="octicon octicon-link"></span></a>1.
    Experiment Derivation</h3>

    <p><em>data from this step is provided here</em></p>

    <p>To see our first experiment attempt setup, see <a href="attempts.md">attemps.md</a>
    where we tried using tests in spack for a ground truth. Ultimately we decided
    this was not good enough and we would use a set of known libraries with ABI issues
    (manually defined) in <a href="diffs">diffs</a>.</p>

    <h3><a id="user-content-2-running-experiments" class="anchor" aria-hidden="true"
    href="#2-running-experiments"><span aria-hidden="true" class="octicon octicon-link"></span></a>2.
    Running Experiments</h3>

    <p>Running experiments is easy, and automated! We use the container build alongside
    this repostiory with a Github workflow in a separate repository and then can programatically
    get results. Simply:</p>

    <ol>

    <li>Ensure this repository is pushed (up to date), as the diffs/splices come from
    here.</li>

    <li>Go to <a href="https://github.com/buildsi/splice-experiment-runs/actions">buildsi/splice-experiment-runs
    Actions</a>

    </li>

    <li>Click on the "Spliced Analysis" workflow, and then "Run Workflow"</li>

    <li>The name in the box should correspond to the main package and dependency folder
    to run.</li>

    </ol>

    <p>When you are done, you can clone the <a href="https://github.com/buildsi/splice-experiment-artifacts">artifacts
    repository</a> to manually update artifacts, or just wait for it to update overnight.
    The full analysis (with the artifacts as a git submodule) is in <a href="https://github.com/buildsi/splice-experiment-results">buildsi/splice-experiment-results</a>.</p>

    <h2><a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Changelog:</h2>

    <ul>

    <li>version 0.0.1: original version with some tweaks</li>

    <li>version 0.0.11: updating cle from its master to resolve dependency install
    bugs <a href="https://github.com/vsoch/cle/commit/b631940d5598e457533866cbc7284123c2c08ef1">commit</a>

    </li>

    <li>version 0.0.12: refactor of spliced to include diff functionality</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1639367380.0
buildtesters/buildtest-nersc:
  data_format: 2
  description: null
  filenames:
  - buildspecs/apps/e4s/22.02/spack.yaml
  full_name: buildtesters/buildtest-nersc
  latest_release: null
  readme: '<h1><a id="user-content-buildtest-nersc" class="anchor" aria-hidden="true"
    href="#buildtest-nersc"><span aria-hidden="true" class="octicon octicon-link"></span></a>buildtest-nersc</h1>

    <p>This repository contains tests for Cori and Perlmutter using <a href="https://buildtest.readthedocs.io/en/devel/"
    rel="nofollow">buildtest</a> framework. A mirror of this repository is located
    on GitHub at <a href="https://github.com/buildtesters/buildtest-nersc">https://github.com/buildtesters/buildtest-nersc</a>
    that is public facing.</p>

    <h2><a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Setup</h2>

    <p>To get started, please <a href="https://docs.nersc.gov/connect/" rel="nofollow">connect
    to NERSC system</a> and clone this repo and buildtest:</p>

    <pre><code>git clone https://github.com/buildtesters/buildtest.git

    git clone https://software.nersc.gov/NERSC/buildtest-nersc.git

    </code></pre>

    <p>Note if you don''t have access to Gitlab server you may clone the mirror on
    Github:</p>

    <pre><code>git clone https://github.com/buildtesters/buildtest-nersc.git

    </code></pre>

    <p>You will need python 3.7 or higher to <a href="https://buildtest.readthedocs.io/en/devel/installing_buildtest.html"
    rel="nofollow">install buildtest</a>, on Cori/Perlmutter this can be done by loading
    <strong>python</strong>

    module and create a conda environment as shown below.</p>

    <pre><code>module load python

    conda create -n buildtest

    conda activate buildtest

    </code></pre>

    <p>Now let''s install buildtest, assuming you have cloned buildtest in $HOME directory
    source the setup script. For csh users you need to source <strong>setup.csh</strong></p>

    <pre><code>source ~/buildtest/setup.sh


    # csh users

    source ~/buildtest/setup.csh

    </code></pre>

    <p>Next, navigate to <code>buildtest-nersc</code> directory and set environment
    <code>BUILDTEST_CONFIGFILE</code> to point to <a href="https://software.nersc.gov/NERSC/buildtest-nersc/-/blob/devel/config.yml"
    rel="nofollow">config.yml</a> which is the configuration file for NERSC system.</p>

    <pre><code>cd buildtest-nersc

    export BUILDTEST_CONFIGFILE=$(pwd)/config.yml

    </code></pre>

    <p>Make sure the configuration is valid, this can be done by running the following.
    buildtest will validate the configuration file with the JSON schema :</p>

    <pre><code>buildtest config validate

    </code></pre>

    <p>Please make sure you are using tip of <a href="https://github.com/buildtesters/buildtest/tree/devel">devel</a>
    branch of buildtest when writing tests. You should sync your local devel branch
    with upstream

    fork, for more details see <a href="https://buildtest.readthedocs.io/en/devel/contributing/code_contribution_guide.html"
    rel="nofollow">contributing guide</a>.</p>

    <p>First time around you should discover all buildspecs this can be done via <code>buildtest
    buildspec find</code>.  The command below will find

    and validate all buildspecs in the <strong>buildtest-nersc</strong> repo and load
    them in buildspec cache. Note that one needs to specify <code>--root</code> to
    specify location where

    all buildspecs are located, we have not configured <a href="https://buildtest.readthedocs.io/en/devel/configuring_buildtest/overview.html#buildspec-roots"
    rel="nofollow">buildspec_root</a> in the configuration file since we don''t have
    a central location where this repo will reside.</p>

    <pre><code>cd buildtest-nersc

    buildtest buildspec find --root buildspecs --rebuild -q

    </code></pre>

    <p>The buildspecs are loaded in buildspec cache file (JSON) that is used by <code>buildtest
    buildspec find</code> for querying cache. Subsequent runs will

    read from cache.  For more details see <a href="https://buildtest.readthedocs.io/en/devel/gettingstarted/buildspecs_interface.html"
    rel="nofollow">buildspec interface</a>.</p>

    <h2><a id="user-content-building-tests" class="anchor" aria-hidden="true" href="#building-tests"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Building Tests</h2>

    <p><strong>Note: All tests are written in YAML using .yml extension</strong></p>

    <p>To build tests use <code>buildtest build</code> command for example we build
    all tests in <code>system</code> directory as follows</p>

    <pre><code>buildtest build -b system/

    </code></pre>

    <p>You can specify multiple buildspecs either files or directory via <code>-b</code>
    option</p>

    <pre><code>buildtest build -b slurm/partition.yml -b slurmutils/

    </code></pre>

    <p>You can exclude a buildspec via <code>-x</code> option this behaves same way
    as <code>-b</code> option so you can specify

    a directory or filepath which could be absolute path, or relative path. This is
    useful when

    you want to run multiple tests grouped in directory but exclude a few.</p>

    <pre><code>buildtest build -b slurm -x slurm/sinfo.yml

    </code></pre>

    <p>buildtest can run tests via tags which can be useful when grouping tests, to
    see a list of available tags you

    can run: <code>buildtest buildspec find --tags</code></p>

    <p>For instance if you want to run all <code>lustre</code> tests you can run the
    following:</p>

    <pre><code>buildtest build --tags lustre

    </code></pre>

    <p>For more details on buildtest test please see the <a href="https://buildtest.readthedocs.io/en/devel/getting_started.html"
    rel="nofollow">buildtest tutorial</a></p>

    <h2><a id="user-content-tags-breakdown" class="anchor" aria-hidden="true" href="#tags-breakdown"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Tags Breakdown</h2>

    <p>When you write buildspecs, please make sure you attach one or more <code>tags</code>
    to the test that way your test will get picked up during one of the CI checks.
    Shown

    below is a summary of tag description</p>

    <ul>

    <li>

    <strong>daily</strong> - this tag is used for running daily system checks using
    gitlab CI. Tests should run relatively quick</li>

    <li>

    <strong>system</strong> - this tag is used for classifying all system tests that
    may include: system configuration, servers, network, cray tests. This tag should
    be used</li>

    <li>

    <strong>slurm</strong> - this tag is used for slurm test that includes slurm utility
    check, slurm controller, etc... This tag <strong>shouldn''t</strong> be used for
    job submission that is managed by <strong>jobs</strong> tag. The <code>slurm</code>
    tag tests should be short running test that use a Local Executor.</li>

    <li>

    <strong>jobs</strong> - this tag is used for testing slurm policies by submitting
    jobs to scheduler.</li>

    <li>

    <strong>compile</strong> - this tag is used for compilation of application (OpenMP,
    MPI, OpenACC, CUDA, upc, bupc, etc...)</li>

    <li>

    <strong>e4s</strong> - this tag is used for running tests for E4S stack via <code>spack
    test</code> or <a href="https://github.com/E4S-Project/testsuite">E4S Testsuite</a>.</li>

    <li>

    <strong>module</strong> - this tag is used for testing module system</li>

    <li>

    <strong>benchmark</strong> - this tag is used for benchmark tests. This can be
    application benchmarks, mini-benchmarks, kernels, etc...</li>

    </ul>

    <p>You can see breakdown of tags and buildspec summary with the following commands</p>

    <pre><code>buildtest buildspec summary

    buildtest buildspec find --group-by-tags

    </code></pre>

    <h2><a id="user-content-querying-tests" class="anchor" aria-hidden="true" href="#querying-tests"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Querying Tests</h2>

    <p>You can use <code>buildtest report</code> and <code>buildtest inspect</code>
    to query tests. The commands differ slightly and data is

    represented differently. The <code>buildtest report</code> command will show output
    in tabular form and only show some of the metadata,

    if you want to access the entire test record use <code>buildtest inspect</code>
    command which displays the content in JSON format.

    For more details on querying tests see <a href="https://buildtest.readthedocs.io/en/devel/gettingstarted/query_test_report.html"
    rel="nofollow">https://buildtest.readthedocs.io/en/devel/gettingstarted/query_test_report.html</a></p>

    <h2><a id="user-content-ci-setup" class="anchor" aria-hidden="true" href="#ci-setup"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>CI Setup</h2>

    <p>Tests are run on schedule basis with one schedule corresponding to one gitlab
    job in <a href="https://software.nersc.gov/NERSC/buildtest-nersc/-/blob/devel/.gitlab-ci.yml"
    rel="nofollow">.gitlab-ci.yml</a>. The scheduled pipelines are configured in

    <a href="https://software.nersc.gov/NERSC/buildtest-nersc/-/pipeline_schedules"
    rel="nofollow">https://software.nersc.gov/NERSC/buildtest-nersc/-/pipeline_schedules</a>.
    Each schedule has a variable <code>TESTNAME</code> defined to control which pipeline

    is run since we have multiple gitlab jobs. In the <code>.gitlab-ci.yml</code>
    we make use of conditional rules using <a href="https://docs.gitlab.com/ee/ci/yaml/#onlyexcept-basic"
    rel="nofollow">only</a>.</p>

    <p>The scheduled jobs are run at different intervals (1x/day, 1x/week, etc...)
    at different times of day to avoid overloading the system. The gitlab jobs

    will run jobs based on tags, alternately some tests may be defined by running
    all tests in a directory (<code>buildtest build -b apps</code>). If you want to
    add a new

    scheduled job, please define a <a href="https://software.nersc.gov/NERSC/buildtest-nersc/-/pipeline_schedules/new"
    rel="nofollow">new schedule</a> with an appropriate time. The

    <code>target branch</code> should be <code>devel</code> and define a unique variable
    used to distinguish scheduled jobs. Next, create a job in <code>.gitlab-ci.yml</code>
    that references the scheduled job and define variable <code>TESTNAME</code> in
    the scheduled pipeline.</p>

    <h2><a id="user-content-integrations" class="anchor" aria-hidden="true" href="#integrations"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Integrations</h2>

    <p>This project has integration with Slack to notify CI builds to <a href="https://hpcbuildtest.slack.com"
    rel="nofollow">buildtest Slack</a> at <strong>#buildtest-nersc</strong> workspace.
    The integrations can be

    found at <a href="https://software.nersc.gov/NERSC/buildtest-nersc/-/settings/integrations"
    rel="nofollow">https://software.nersc.gov/NERSC/buildtest-nersc/-/settings/integrations</a>.</p>

    <p>This project has setup a push mirror to <a href="https://github.com/buildtesters/buildtest-nersc">https://github.com/buildtesters/buildtest-nersc</a>
    which can be seen at <a href="https://software.nersc.gov/NERSC/buildtest-nersc/-/settings/repository"
    rel="nofollow">https://software.nersc.gov/NERSC/buildtest-nersc/-/settings/repository</a>

    under <strong>Mirroring Repositories</strong>. If the push mirror is not setup,
    please add the mirror.</p>

    <h2><a id="user-content-cdash" class="anchor" aria-hidden="true" href="#cdash"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>CDASH</h2>

    <p>buildtest will push test results to <a href="https://www.kitware.com/cdash/project/about.html"
    rel="nofollow">CDASH</a> server

    at <a href="https://my.cdash.org/index.php?project=buildtest-nersc" rel="nofollow">https://my.cdash.org/index.php?project=buildtest-nersc</a>
    using <code>buildtest cdash upload</code> command.</p>

    <h2><a id="user-content-contributing-guide" class="anchor" aria-hidden="true"
    href="#contributing-guide"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contributing
    Guide</h2>

    <p>To contribute back you will want to make sure your buildspec is validated before
    you contribute back, this could be

    done by running test manually <code>buildtest build</code> or see if buildspec
    is valid via <code>buildtest buildspec find</code>. It

    would be good to run your test and make sure it is working as expected, you can
    view test detail using <code>buildtest inspect name &lt;testname&gt;</code> or
    <code>buildtest inspect query &lt;testname&gt;</code>. For more

    details on querying test please see <a href="https://buildtest.readthedocs.io/en/devel/gettingstarted/query_test_report.html"
    rel="nofollow">https://buildtest.readthedocs.io/en/devel/gettingstarted/query_test_report.html</a>.</p>

    <p>If you want to contribute your tests, please see <a href="https://software.nersc.gov/NERSC/buildtest-nersc/-/blob/devel/CONTRIBUTING.md"
    rel="nofollow">CONTRIBUTING.md</a></p>

    <h2><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>References</h2>

    <ul>

    <li>buildtest documentation: <a href="https://buildtest.readthedocs.io/en/devel/"
    rel="nofollow">https://buildtest.readthedocs.io/en/devel/</a>

    </li>

    <li>buildtest schema docs: <a href="https://buildtesters.github.io/buildtest/"
    rel="nofollow">https://buildtesters.github.io/buildtest/</a>

    </li>

    <li>Getting Started: <a href="https://buildtest.readthedocs.io/en/devel/getting_started.html"
    rel="nofollow">https://buildtest.readthedocs.io/en/devel/getting_started.html</a>

    </li>

    <li>Writing Buildspecs: <a href="https://buildtest.readthedocs.io/en/devel/buildspec_tutorial.html"
    rel="nofollow">https://buildtest.readthedocs.io/en/devel/buildspec_tutorial.html</a>

    </li>

    <li>Contributing Guide: <a href="https://buildtest.readthedocs.io/en/devel/contributing.html"
    rel="nofollow">https://buildtest.readthedocs.io/en/devel/contributing.html</a>

    </li>

    </ul>

    '
  stargazers_count: 4
  subscribers_count: 4
  topics:
  - buildtest
  updated_at: 1657142158.0
celeritas-project/celeritas:
  data_format: 2
  description: Celeritas is a new Monte Carlo transport code designed for high-performance
    simulation of high-energy physics detectors.
  filenames:
  - scripts/spack.yaml
  full_name: celeritas-project/celeritas
  latest_release: v0.1.1
  readme: "<h1><a id=\"user-content-celeritas\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#celeritas\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Celeritas</h1>\n<p>The Celeritas project implements HEP detector physics\
    \ on GPU accelerator\nhardware with the ultimate goal of supporting the massive\
    \ computational\nrequirements of LHC-HL upgrade.</p>\n<h1><a id=\"user-content-installation-and-development\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#installation-and-development\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation\
    \ and development</h1>\n<p>This project requires external dependencies to build\
    \ with full functionality.\nHowever, any combination of these requirements can\
    \ be omitted to enable\nlimited development on personal machines with fewer available\
    \ components.</p>\n<ul>\n<li>\n<a href=\"https://developer.nvidia.com/cuda-toolkit\"\
    \ rel=\"nofollow\">CUDA</a>: on-device computation</li>\n<li>an MPI implementation\
    \ (such as <a href=\"https://www.open-mpi.org\" rel=\"nofollow\">Open MPI</a>):\
    \ distributed-memory parallelism</li>\n<li>\n<a href=\"https://root.cern\" rel=\"\
    nofollow\">ROOT</a>: I/O</li>\n<li>\n<a href=\"https://github.com/nlohmann/json\"\
    >nljson</a>: simple text-based I/O for\ndiagnostics and program setup</li>\n<li>\n\
    <a href=\"https://gitlab.cern.ch/VecGeom/VecGeom\" rel=\"nofollow\">VecGeom</a>:\
    \ on-device navigation of GDML-defined detector geometry</li>\n<li>\n<a href=\"\
    https://geant4.web.cern.ch/support/download\" rel=\"nofollow\">Geant4</a>: preprocessing\
    \ physics data for a problem input</li>\n<li>\n<a href=\"https://geant4.web.cern.ch/support/download\"\
    \ rel=\"nofollow\">G4EMLOW</a>: EM physics model data</li>\n<li>\n<a href=\"http://hepmc.web.cern.ch/hepmc/\"\
    \ rel=\"nofollow\">HepMC3</a>: Event input</li>\n<li>\n<a href=\"http://swig.org\"\
    \ rel=\"nofollow\">SWIG</a>: limited set of Python wrappers for analyzing input\n\
    data</li>\n</ul>\n<p>Build/test dependencies are:</p>\n<ul>\n<li>\n<a href=\"\
    https://cmake.org\" rel=\"nofollow\">CMake</a>: build system</li>\n<li>\n<a href=\"\
    https://clang.llvm.org/docs/ClangFormat.html\" rel=\"nofollow\">clang-format</a>:\
    \ formatting enforcement</li>\n<li>\n<a href=\"https://github.com/google/googletest\"\
    >GoogleTest</a>: test harness</li>\n</ul>\n<h2><a id=\"user-content-installing-with-spack\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#installing-with-spack\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing with\
    \ Spack</h2>\n<p><a href=\"https://github.com/spack/spack\">Spack</a> is an HPC-oriented\
    \ package manager that\nincludes numerous scientific packages, including those\
    \ used in HEP. An included\nSpack \"environment\" (at <code>scripts/dev/env/celeritas-{platform}.yaml</code>)\
    \ defines\nthe required prerequisites for this project.</p>\n<p>The script at\
    \ <code>scripts/dev/install-spack.sh</code> provides a \"one-button solution\"\
    \nto installing and activating the Spack prerequisites for building Celeritas.\n\
    Alternatively, you can manually perform the following steps:</p>\n<ul>\n<li>Clone\
    \ Spack following its <a href=\"https://spack.readthedocs.io/en/latest/getting_started.html\"\
    \ rel=\"nofollow\">getting started instructions</a>\n</li>\n<li>Add CUDA to your\
    \ <code>$SPACK_ROOT/etc/spack/packages.yaml</code> file</li>\n<li>Run <code>spack\
    \ env create celeritas scripts/dev/env/celeritas-linux.yaml</code> (or\nreplace\
    \ <code>linux</code> with <code>darwin</code> if running on a mac); then <code>spack\
    \ -e celeritas concretize</code> and <code>spack -e celeritas install</code>\n\
    </li>\n<li>Run and add to your startup environment profile <code>spack env activate\
    \ celeritas</code>\n</li>\n<li>Configure Celeritas by creating a build directory\
    \ and running CMake (or\n<code>ccmake</code> for an interactive prompt for configuring\
    \ options).</li>\n</ul>\n<p>An example file for a <code>packages.yaml</code> that\
    \ defines an externally installed CUDA\non a system with an NVIDIA GPU that has\
    \ architecture capability 3.5 is thus:</p>\n<div class=\"highlight highlight-source-yaml\"\
    ><pre><span class=\"pl-ent\">packages</span>:\n  <span class=\"pl-ent\">cuda</span>:\n\
    \    <span class=\"pl-ent\">paths</span>:\n      <span class=\"pl-ent\">cuda@10.2</span>:\
    \ <span class=\"pl-s\">/usr/local/cuda-10.2</span>\n    <span class=\"pl-ent\"\
    >buildable</span>: <span class=\"pl-c1\">False</span>\n  <span class=\"pl-ent\"\
    >all</span>:\n    <span class=\"pl-ent\">variants</span>: <span class=\"pl-s\"\
    >cuda_arch=35</span></pre></div>\n<h2><a id=\"user-content-configuring-and-building-celeritas\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#configuring-and-building-celeritas\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Configuring\
    \ and building Celeritas</h2>\n<p>To configure Celeritas, assuming the dependencies\
    \ you want are located in the\n<code>CMAKE_PREFIX_PATH</code> search path, and\
    \ other environment variables such as <code>CXX</code>\nare set, you should be\
    \ able to just run CMake and build:</p>\n<div class=\"highlight highlight-text-shell-session\"\
    ><pre>$ <span class=\"pl-s1\">mkdir build</span>\n$ <span class=\"pl-s1\"><span\
    \ class=\"pl-c1\">cd</span> build <span class=\"pl-k\">&amp;&amp;</span> cmake\
    \ ..</span>\n$ <span class=\"pl-s1\">make</span></pre></div>\n<p>Ideally you will\
    \ build Celeritas with all dependencies to gain the full\nfunctionality of the\
    \ code, but there are circumstances in which you may not\nhave all the dependencies\
    \ or features available. By default, the CMake code in\nCeleritas queries available\
    \ packages and sets several <code>CELERITAS_USE_{package}</code>\noptions based\
    \ on what it finds, so you have a good chance of successfully\nconfiguring Celeritas\
    \ on the first go. Two optional components,\n<code>CELERITAS_BUILD_&lt;DEMOS|TESTS&gt;</code>,\
    \ will error in the configure if their required\ncomponents are missing, but they\
    \ will update the CMake cache variable so that\nthe next configure will succeed\
    \ (but with that component disabled).</p>\n<p>For a slightly more advanced but\
    \ potentially simpler setup, you can use the\nCMake presets provided by Celeritas\
    \ via the <code>CMakePresets.json</code> file for CMake\n3.21 and higher:</p>\n\
    <div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\"\
    >cmake --preset=default</span></pre></div>\n<p>The three main options are \"minimal\"\
    , \"default\", and \"full\", which all set\ndifferent expectations for available\
    \ dependencies.</p>\n<p>If you want to add your own set of custom options and\
    \ flags, create a\n<code>CMakeUserPresets.json</code> file or, if you are a developer,\
    \ create a preset at\n<code>scripts/cmake-presets/${HOSTNAME%%.*}.json</code>\
    \ and call <code>scripts/build.sh {preset}</code> to create the symlink, configure\
    \ the preset, build, and test. See\n<a href=\"scripts/README.md\">the scripts\
    \ readme</a> for more details.</p>\n<p>If your CMake version is too old, you may\
    \ get an unhelpful message:</p>\n<pre><code>CMake Error: Could not read presets\
    \ from celeritas: Unrecognized \"version\" field\n</code></pre>\n<p>which is just\
    \ a poor way of saying the version in the <code>CMakePresets.json</code> file\n\
    is newer than that version knows how to handle.</p>\n<h2><a id=\"user-content-commit-hooks\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#commit-hooks\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Commit hooks</h2>\n<p>Run <code>scripts/dev/install-commit-hooks.sh</code>\
    \ to install a git post-commit hook\nthat will amend each commit with clang-format\
    \ updates if necessary.</p>\n<h2><a id=\"user-content-contributing\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#contributing\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Contributing</h2>\n<p>See the <a href=\"\
    https://github.com/celeritas-project/celeritas/wiki/Development\">development\
    \ wiki\npage</a> for\nguidelines and best practices for code in the project.</p>\n\
    <p>The <a href=\"https://github.com/celeritas-project/celeritas/wiki/Code-design\"\
    >code design\npage</a> outlines\nthe basic physics design philosophy and classes,\
    \ and <a href=\"https://github.com/celeritas-project/celeritas-docs/tree/master/graphs\"\
    >the layout of some\nalgorithms and\nclasses</a>\nare available on the <code>celeritas-docs</code>\
    \ repo.</p>\n<p>All submissions to the Celeritas project are automatically licensed\
    \ under the\nterms of <a href=\"COPYRIGHT\">the project copyright</a> as formalized\
    \ by the <a href=\"https://docs.github.com/en/github/site-policy/github-terms-of-service#6-contributions-under-repository-license\"\
    >GitHub terms\nof service</a>.</p>\n"
  stargazers_count: 23
  subscribers_count: 9
  topics:
  - hep
  - cuda
  - computational-physics
  - monte-carlo
  updated_at: 1662131823.0
dbkinghorn/Puget-Labs-Containers:
  data_format: 2
  description: Puget Labs container build files
  filenames:
  - hmmer-amd/spack.yaml
  - namd-amd/spack.yaml
  - hpl-amd/spack.yaml
  - openfoam-amd/spack.yaml
  - quantum-espresso-amd/spack.yaml
  - wrf-amd/spack.yaml
  full_name: dbkinghorn/Puget-Labs-Containers
  latest_release: null
  readme: '<h1><a id="user-content-puget-labs-containers" class="anchor" aria-hidden="true"
    href="#puget-labs-containers"><span aria-hidden="true" class="octicon octicon-link"></span></a>Puget
    Labs Containers</h1>

    <p>This is a collection of container spec files used to build the images available
    on <a href="https://hub.docker.com/orgs/pugetsystems/repositories" rel="nofollow">https://hub.docker.com/orgs/pugetsystems/repositories</a></p>

    <p>Many of these images are based on performance optimized application builds
    for specific hardware targets i.e. AMD Zen3, Intel OneAPI, NVIDIA CUDA etc.</p>

    <p>These container images are the basis for some of our Scientific and Machine
    Learning benchmarks.</p>

    <p>Files included for each application include,</p>

    <ul>

    <li>Spack spec.yaml build specifications</li>

    <li>Dockerfiles (Multi-stage)</li>

    <li>Enroot container-bundle (self running) build scripts</li>

    <li>Benchmarks</li>

    <li>Usage notes</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1660670570.0
eflows4hpc/workflow-registry:
  data_format: 2
  description: Registry to store workflow descriptions
  filenames:
  - minimal_workflow/wordcount/spack.yaml
  - rom_pillar_I/reduce_order_model/spack.yaml
  full_name: eflows4hpc/workflow-registry
  latest_release: null
  readme: "<p>#Workflow Registry</p>\n<p>This is a repository to store the Workflow\
    \ descriptions using the eFlows4HPC methodology. This description consist of at\
    \ least the TOSCA description of the worklfow, the code of the their different\
    \ steps and their required software per step.</p>\n<h2><a id=\"user-content-repository-structure\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#repository-structure\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Repository structure</h2>\n<p>Workflow\
    \ descriptions have to be included inside this repository according to the following\
    \ structure.</p>\n<pre><code>workflow-registry\n  |- workflow_1\n  |    |- tosca\n\
    \  |    |    |- types.yml               TOSCA description of the different components\
    \ involved in the workflow\n  |    |       ... \n  |    |- step_1\n  |    |  \
    \  |- spack.yml               Sofware requirements for this workflow step as a\
    \ Spack environment specification \n  |    |    |- src                     PyCOMPSs\
    \ code of the workflow step\n  |    |       ...\n  |    |- step_2\n  |       \
    \  ....\n  |- workflow_2                                \n  |\t...\n\n</code></pre>\n\
    <h2><a id=\"user-content-including-new-workflows\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#including-new-workflows\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Including new Workflows</h2>\n<p>To include new workflows\
    \ in the repository, first create a new fork of the repository and  include a\
    \ new folder for the workflow with a subfolder for the TOSCA description and the\
    \ different workflow steps. Finally, create a pull request with the new workflow\
    \ description. This pull request will be reviewed and included in the repository.</p>\n"
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1647595412.0
eic/eic-spack-environments:
  data_format: 2
  description: Spack environments for the Electron Ion Collider
  filenames:
  - eic-shell/spack.yaml
  full_name: eic/eic-spack-environments
  latest_release: null
  readme: '<h1><a id="user-content-eic-spack-environments" class="anchor" aria-hidden="true"
    href="#eic-spack-environments"><span aria-hidden="true" class="octicon octicon-link"></span></a>EIC
    Spack Environments</h1>

    <p>This repository contains <a href="https://spack.readthedocs.io/en/latest/index.html"
    rel="nofollow">Spack</a> environments for the EIC.</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1661880440.0
eth-cscs/spack-stack:
  data_format: 2
  description: fast spack builds on slow filesystem
  filenames:
  - compilers/3-llvm/spack.yaml
  - packages/tools/spack.yaml
  - packages/gcc/spack.yaml
  - compilers/2-gcc/spack.yaml
  - packages/nvhpc/spack.yaml
  - packages/clang/spack.yaml
  - compilers/1-gcc/spack.yaml
  full_name: eth-cscs/spack-stack
  latest_release: null
  readme: '<p>Bootstrap GCC, LLVM and NVHPC, and build an HPC software stack based
    on

    OpenMPI, with a few unique features:</p>

    <ol>

    <li>parallel package builds with single jobserver for all builds;</li>

    <li>avoiding relocation issues by fixing the install path to a new directory <code>/some-dir</code>
    of choice (no root access required);</li>

    <li>fast, in-memory builds.</li>

    </ol>

    <p><strong>Requirements</strong>:</p>

    <ul>

    <li><code>spack</code></li>

    <li>

    <code>bwrap</code> (when not already building inside a sandbox)</li>

    </ul>

    <p><strong>Usage</strong>:</p>

    <ol>

    <li>Copy <code>Make.user.example</code> to <code>Make.user</code> and change some
    variables.</li>

    <li>Run <code>make -j$(nproc)</code> to bootstrap compilers and packages.</li>

    <li>Run <code>make store.squashfs</code> to bundle those in a squashfs file.</li>

    <li>Run <code>make build.tar.gz</code> to create a tarball of all concrete environments
    and

    generated config files for posterity. This excludes the actual software.</li>

    </ol>

    <p><strong>Variables</strong></p>

    <p>A few variables in <code>Make.user</code>:</p>

    <ul>

    <li>

    <code>STORE</code>: where to install packages;</li>

    <li>

    <code>SPACK</code>: what <code>spack</code> to use;</li>

    <li>

    <code>SPACK_SYSTEM_CONFIG_PATH</code>: path to spack config dir (e.g. <a href="config/hohgant">config/hohgant</a>).</li>

    <li>

    <code>SANDBOX</code>: run commands in a sandbox (e.g. bubblewrap), see <code>Make.user.example</code>
    for details.</li>

    <li>

    <code>SPACK_INSTALL_FLAGS</code>: specify more install flags, like <code>--verbose</code>.</li>

    </ul>

    <p><strong>Reproducibility</strong></p>

    <p>When building on a production system instead of in a sandbox, there''s a few
    things to

    do to improve reproducibility:</p>

    <ol>

    <li>Always run <code>make</code> inside a clean environment:

    <pre><code>env --ignore-environment PATH=/usr/bin:/bin make

    </code></pre>

    </li>

    <li>Update <code>Make.user</code> to hide your home folder so that no user config
    is picked up:

    <pre><code>SANDBOX := bwrap --tmpfs ~ ...

    </code></pre>

    </li>

    <li>Set <code>LC_ALL</code>, <code>TZ</code> and <code>SOURCE_DATE_EPOCH</code>
    to something fixed in <code>Make.user</code>.</li>

    </ol>

    <p><strong>Unprivileged mounts</strong></p>

    <p>The squashfs file can then be mounted using <a href="https://github.com/eth-cscs/squashfs-mount">squashfs-mount</a>
    or <code>squashfuse</code></p>

    '
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1661852790.0
eugeneswalker/exago-crusher:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: eugeneswalker/exago-crusher
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1657654737.0
eugeneswalker/facility-spack:
  data_format: 2
  description: null
  filenames:
  - crusher/22.08/PrgEnv-amd/spack.yaml
  - uo-containers/22.05/archives/spack-cuda-external.spack.yaml
  - oneapi/ubuntu20.04-runner-x86_64-oneapi/spack.yaml
  - oneapi/ubuntu20.04-runner-x86_64-oneapi/breakdowns/separately.w-failures.spack.yaml
  - uo-containers/22.08/_archive/prep/cuda-ppc64le-noex/spack.yaml
  - arcticus/22.05/failures/spack.yaml
  - uo-containers/22.05/components/e4s-cpu-builder-ppc64le.spack.yaml
  - uo-containers/22.05/archives/spack-cpu-ppc64le.spack.yaml
  - oneapi/ubuntu20.04-runner-x86_64-oneapi/breakdowns/together.wo-failures.spack.yaml
  - uo-containers/22.05/components/e4s-cpu-builder.spack.yaml
  - uo-containers/22.08/_archive/prep/cuda-ppc64le/spack.yaml
  - uo-containers/22.05/archives/spack-cuda-external-ppc64le.spack.yaml
  - perlmutter/22.08/PrgEnv-gnu/spack.yaml
  - uo-containers/22.08/release/e4s-22.08-oneapi.spack.yaml
  - uo-containers/22.05/components/e4s-cuda-builder.spack.yaml
  - uo-containers/22.05/production/e4s-22.05-oneapi.spack.yaml
  - uo-containers/22.08/release/_backup/e4s-22.08-cuda-noex.spack.yaml
  - uo-containers/22.08/_archive/prep/cuda-aarch64-noex/spack.yaml
  - perlmutter/22.08/mvapich2-3.0a/spack.yaml
  - uo-containers/22.05/production/e4s-22.05-cuda.spack.yaml
  - uo-containers/22.08/_archive/prep/oneapi/spack.yaml
  - frontera/22.05/spack.yaml
  - crusher/22.05/PrgEnv-cray/spack.yaml
  - crusher/22.08/PrgEnv-gnu/failures/spack.yaml
  - uo-containers/22.08/release/_backup/e4s-22.08-cuda-ppc64le-noex.spack.yaml
  - uo-containers/22.08/release/public/cuda.spack.yaml
  - uo-containers/22.08/_archive/prep/cuda/spack.yaml
  - uo-containers/22.08/release/public/oneapi.spack.yaml
  - uo-containers/22.08/_archive/prep/cpu-ppc64le/spack.yaml
  - uo-containers/22.08/release/_backup/e4s-22.08-cuda-aarch64-noex.spack.yaml
  - uo-containers/22.05/production/base/aarch64-cuda.spack.yaml
  - uo-containers/22.05/production/save/e4s-22.05-oneapi.spack.yaml
  - crusher/22.05/mvapich2/spack.yaml
  - oneapi/ubuntu20.04-runner-x86_64-oneapi/breakdowns/together.w-failures.spack.yaml
  - aws/paratools/parallelcluster-3.1.4/spack.yaml
  - crusher/22.05/PrgEnv-cray/failures/spack.yaml
  - aws/paratools/e4s-mvapich/spack.yaml
  - uo-containers/22.08/release/e4s-22.08-rocm.spack.yaml
  - uo-containers/22.05/archives/spack-rocm-external.spack.yaml
  - uo-containers/22.05/production/e4s-22.05-rocm.spack.yaml
  - oneapi/ubuntu20.04-runner-x86_64-oneapi/breakdowns/separately.wo-failures.spack.yaml
  - oci/mvapich2/spack.yaml
  - aws/paratools/parallelcluster-3.1.4/exawind-demo/spack.yaml
  - uo-containers/22.05/failed-oneapi/spack.yaml
  - uo-containers/22.08/release/public/rocm.spack.yaml
  - uo-containers/22.05/archives/spack-cuda.spack.yaml
  - crusher/22.08/PrgEnv-amd/failures/spack.yaml
  - uo-containers/22.05/components/save/e4s-oneapi-builder.spack.yaml
  - uo-containers/22.05/components/e4s-oneapi-builder.spack.yaml
  - uo-containers/22.08/release/public/cpu.spack.yaml
  - uo-containers/22.05/components/e4s-cuda-builder-ppc64le.spack.yaml
  - arcticus/22.05/spack.yaml
  - uo-containers/22.08/components/gpu-cuda-ppc64le/spack.yaml
  - uo-containers/22.08/release/e4s-22.08-cpu.spack.yaml
  - crusher/22.05/PrgEnv-amd/failures/spack.yaml
  - uo-containers/22.05/production/e4s-22.05-rocm-noextern.spack.yaml
  - perlmutter/22.05/mvapich2/spack.yaml
  - arcticus/experimental/failures/spack.yaml
  - uo-containers/22.08/components/cpu-plus-gpu-oneapi/spack.yaml
  - uo-containers/22.08/release/_backup/e4s-22.08-rocm-noex.spack.yaml
  - perlmutter/22.05/PrgEnv-nvhpc/spack.yaml
  - uo-containers/22.08/_archive/prep/rocm-noex/spack.yaml
  - crusher/22.08/PrgEnv-gnu/spack.yaml
  - perlmutter/22.05/mvapich2-2.3.7/spack.yaml
  - uo-containers/22.05/production/e4s-22.05-cuda-noextern.spack.yaml
  - uo-containers/22.05/production/e4s-22.05-cuda-ppc64le.spack.yaml
  - cori/21.05/PrgEnv-intel/spack.yaml
  - uo-containers/22.05/production/base/ppc64le-cuda.spack.yaml
  - uo-containers/22.08/components/gpu-rocm-noex/spack.yaml
  - bridges2/22.05/openmpi-4.1.1-gcc8.3.1/spack.yaml
  - uo-containers/22.05/production/base/amd64-oneapi.spack.yaml
  - crusher/22.08/PrgEnv-cray/spack.yaml
  - uo-containers/22.08/release/public/cuda-aarch64.spack.yaml
  - uo-containers/22.08/components/gpu-rocm/spack.yaml
  - uo-containers/22.05/components/e4s-rocm-builder-noextern.spack.yaml
  - uo-containers/22.05/production/e4s-22.05-cuda-ppc64le-noextern.spack.yaml
  - uo-containers/22.08/components/cpu/spack.yaml
  - perlmutter/22.05/PrgEnv-gnu/spack.yaml
  - crusher/22.08/PrgEnv-cray/failures/spack.yaml
  - uo-containers/22.05/production/base/amd64-rocm.spack.yaml
  - uo-containers/22.05/archives/spack-rocm.spack.yaml
  - uo-containers/22.08/components/gpu-cuda-noex/spack.yaml
  - crusher/22.05/PrgEnv-amd/spack.yaml
  - uo-containers/22.08/components/gpu-cuda-ppc64le-noex/spack.yaml
  - arcticus/develop/spack.yaml
  - uo-containers/22.08/release/e4s-22.08-cuda.spack.yaml
  - perlmutter/22.05/mvapich2-3.0a/spack.yaml
  - uo-containers/22.05/archives/spack-cpu.spack.yaml
  - uo-containers/22.08/_archive/prep/cuda-noex/spack.yaml
  - crusher/22.05/PrgEnv-gnu/failures/spack.yaml
  - arcticus/22.08/failures/spack.yaml
  - uo-containers/22.08/_archive/prep/rocm/spack.yaml
  - arcticus/22.08/spack.yaml
  - uo-containers/22.05/production/base/amd64-cuda.spack.yaml
  - crusher/22.05/PrgEnv-gnu/spack.yaml
  - uo-containers/ubuntu20-minimal/spack.yaml
  - uo-containers/22.08/components/gpu-cuda/spack.yaml
  - oci/adaptive-test-env/spack.yaml
  - uo-containers/22.08/release/e4s-22.08-cuda-ppc64le.spack.yaml
  - applications/exago/crusher/spack.yaml
  - uo-containers/22.05/components/e4s-cuda-builder-noextern.spack.yaml
  - uo-containers/22.05/components/e4s-rocm-builder.spack.yaml
  - uo-containers/22.05/archives/spack-cuda-ppc64le.spack.yaml
  - arcticus/experimental/spack.yaml
  - uo-containers/22.05/components/e4s-cuda-builder-ppc64le-noextern.spack.yaml
  - perlmutter/22.05/PrgEnv-gnu/failures/spack.yaml
  - uo-containers/22.05/archives/spack-full-clang.spack.yaml
  - uo-containers/22.08/_archive/prep/cpu/spack.yaml
  full_name: eugeneswalker/facility-spack
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1655395315.0
fnalacceleratormodeling/synergia2-containers:
  data_format: 2
  description: null
  filenames:
  - ubuntu-gcc/spack.yaml
  - ubuntu-clang/spack.yaml
  full_name: fnalacceleratormodeling/synergia2-containers
  latest_release: null
  readme: '<h1><a id="user-content-synergia2-containers" class="anchor" aria-hidden="true"
    href="#synergia2-containers"><span aria-hidden="true" class="octicon octicon-link"></span></a>synergia2-containers</h1>

    <p>This repository contains docker recipes for building containers that contain
    all dependencies for synergia2. These recipes are generated using <a href="https://spack.readthedocs.io/en/latest/environments.html"
    rel="nofollow">spack environments</a> via <a href="https://spack.readthedocs.io/en/latest/containers.html"
    rel="nofollow"><code>spack containerize</code></a>, with some minor modifications.
    GithubActions is used to build these containers for <code>x86_64_v2</code> ISA
    and these containers can be pulled from the github container registry. For instructions
    on how to pull a particular image, visit the page associated with it <a href="https://github.com/orgs/fnalacceleratormodeling/packages?repo_name=synergia2-containers">here</a>.</p>

    <p>These containers are used as test environments for testing synergia2 via GithubActions.</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1646758059.0
giltirn/mochi-margo:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: giltirn/mochi-margo
  latest_release: null
  readme: "<h1><a id=\"user-content-margo\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#margo\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Margo</h1>\n\
    <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/mochi-hpc/mochi-margo/actions/workflows/test.yml/badge.svg?branch=main\"\
    ><img src=\"https://github.com/mochi-hpc/mochi-margo/actions/workflows/test.yml/badge.svg?branch=main\"\
    \ alt=\"\" style=\"max-width: 100%;\"></a>\n<a href=\"https://codecov.io/gh/mochi-hpc/mochi-margo\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c64ae5809121f4158ced0cf46c628aca60e6db908b2639f6758b0595a6fdd779/68747470733a2f2f636f6465636f762e696f2f67682f6d6f6368692d6870632f6d6f6368692d6d6172676f2f6272616e63682f6d61696e2f67726170682f62616467652e737667\"\
    \ alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/mochi-hpc/mochi-margo/branch/main/graph/badge.svg\"\
    \ style=\"max-width: 100%;\"></a></p>\n<p>Margo provides Argobots-aware bindings\
    \ to the Mercury RPC library.</p>\n<p>Mercury (<a href=\"https://mercury-hpc.github.io/\"\
    \ rel=\"nofollow\">https://mercury-hpc.github.io/</a>) is a remote procedure call\n\
    library optimized for use in HPC environments.  Its native API presents a\ncallback-oriented\
    \ interface to manage asynchronous operation.  Argobots\n(<a href=\"https://www.argobots.org/\"\
    \ rel=\"nofollow\">https://www.argobots.org/</a>) is a user-level threading package.</p>\n\
    <p>Margo combines Mercury and Argobots to simplify development of distributed\n\
    services.  Mercury operations are presented as conventional blocking\noperations,\
    \ and RPC handlers are presented as sequential threads.  This\nconfiguration enables\
    \ high degree of concurrency while hiding the\ncomplexity associated with asynchronous\
    \ communication progress and callback\nmanagement.</p>\n<p>Internally, Margo suspends\
    \ callers after issuing a Mercury operation, and\nautomatically resumes them when\
    \ the operation completes.  This allows\nother concurrent user-level threads to\
    \ make progress while Mercury\noperations are in flight without consuming operating\
    \ system threads.\nThe goal of this design is to combine the performance advantages\
    \ of\nMercury's native event-driven execution model with the progamming\nsimplicity\
    \ of a multi-threaded execution model.</p>\n<p>A companion library called abt-io\
    \ provides similar wrappers for POSIX I/O\nfunctions: <a href=\"https://github.com/mochi-hpc/mochi-abt-io\"\
    >https://github.com/mochi-hpc/mochi-abt-io</a></p>\n<p>Note that Margo should\
    \ be compatible with any Mercury network\ntransport (NA plugin).  The documentation\
    \ assumes the use of\nthe NA SM (shared memory) plugin that is built into Mercury\
    \ for\nsimplicity.  This plugin is only valid for communication between\nprocesses\
    \ on a single node.  See <a href=\"##using-margo-with-other-mercury-na-plugins\"\
    >Using Margo with other Mercury NA\nplugins</a> for information\non other configuration\
    \ options.</p>\n<h2><a id=\"user-content-spack\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#spack\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Spack</h2>\n<p>The simplest way to install Margo is by installing\
    \ the \"mochi-margo\" package\nin spack (<a href=\"https://spack.io/\" rel=\"\
    nofollow\">https://spack.io/</a>).</p>\n<h2><a id=\"user-content-dependencies\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#dependencies\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Dependencies</h2>\n<ul>\n<li>mercury\
    \  (git clone --recurse-submodules <a href=\"https://github.com/mercury-hpc/mercury.git\"\
    >https://github.com/mercury-hpc/mercury.git</a>)</li>\n<li>argobots (git clone\
    \ <a href=\"https://github.com/pmodels/argobots.git\">https://github.com/pmodels/argobots.git</a>)</li>\n\
    </ul>\n<h3><a id=\"user-content-recommended-mercury-build-options\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#recommended-mercury-build-options\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Recommended Mercury build options</h3>\n\
    <ul>\n<li>Mercury must be compiled with -DMERCURY_USE_BOOST_PP:BOOL=ON to enable\
    \ the\nBoost preprocessor macros for encoding.</li>\n<li>Mercury should be compiled\
    \ with -DMERCURY_USE_SELF_FORWARD:BOOL=ON in order to enable\nfast execution path\
    \ for cases in which a Mercury service is linked into the same\nexecutable as\
    \ the client</li>\n</ul>\n<p>Example Mercury compilation:</p>\n<pre><code>mkdir\
    \ build\ncd build\ncmake -DMERCURY_USE_SELF_FORWARD:BOOL=ON \\\n -DBUILD_TESTING:BOOL=ON\
    \ -DMERCURY_USE_BOOST_PP:BOOL=ON \\\n -DCMAKE_INSTALL_PREFIX=/home/pcarns/working/install\
    \ \\\n -DBUILD_SHARED_LIBS:BOOL=ON -DCMAKE_BUILD_TYPE:STRING=Debug ../\n</code></pre>\n\
    <h2><a id=\"user-content-building\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #building\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building</h2>\n\
    <p>Example configuration:</p>\n<pre><code>../configure --prefix=/home/pcarns/working/install\
    \ \\\n    PKG_CONFIG_PATH=/home/pcarns/working/install/lib/pkgconfig \\\n    CFLAGS=\"\
    -g -Wall\"\n</code></pre>\n<h2><a id=\"user-content-running-examples\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#running-examples\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running examples</h2>\n<p>The\
    \ examples subdirectory contains:</p>\n<ul>\n<li>margo-example-client.c: an example\
    \ client</li>\n<li>margo-example-server.c: an example server</li>\n<li>my-rpc.[ch]:\
    \ an example RPC definition</li>\n</ul>\n<p>The following example shows how to\
    \ execute them.  Note that when the server starts it will display the address\
    \ that the client can use to connect to it.</p>\n<pre><code>$ examples/margo-example-server\
    \ na+sm://\n# accepting RPCs on address \"na+sm://13367/0\"\nGot RPC request with\
    \ input_val: 0\nGot RPC request with input_val: 1\nGot RPC request with input_val:\
    \ 2\nGot RPC request with input_val: 3\nGot RPC request to shutdown\n\n$ examples/margo-example-client\
    \ na+sm://13367/0\nULT [0] running.\nULT [1] running.\nULT [2] running.\nULT [3]\
    \ running.\nGot response ret: 0\nULT [0] done.\nGot response ret: 0\nULT [1] done.\n\
    Got response ret: 0\nULT [2] done.\nGot response ret: 0\nULT [3] done.\n</code></pre>\n\
    <p>The client will issue 4 concurrent RPCs to the server and wait for them to\n\
    complete.</p>\n<h2><a id=\"user-content-running-tests\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#running-tests\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Running tests</h2>\n<p><code>make check</code></p>\n<h2><a id=\"user-content-using-margo-with-the-other-na-plugins\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#using-margo-with-the-other-na-plugins\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using Margo\
    \ with the other NA plugins</h2>\n<p>See the <a href=\"http://mercury-hpc.github.io/documentation/\"\
    \ rel=\"nofollow\">Mercury\ndocumentation</a> for details.\nMargo is compatible\
    \ with any Mercury transport and uses the same address\nformat.</p>\n<h2><a id=\"\
    user-content-instrumentation\" class=\"anchor\" aria-hidden=\"true\" href=\"#instrumentation\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Instrumentation</h2>\n\
    <p>See the <a href=\"doc/instrumentation.md\">Instrumentation documentation</a>\
    \ for\ninformation on how to extract diagnostic instrumentation from Margo.</p>\n\
    <h2><a id=\"user-content-debugging\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #debugging\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Debugging</h2>\n\
    <p>See the <a href=\"doc/debugging.md\">Debugging documentation</a> for Margo\
    \ debugging\nfeatures and strategies.</p>\n<h2><a id=\"user-content-design-details\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#design-details\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Design details</h2>\n<p><a target=\"\
    _blank\" rel=\"noopener noreferrer\" href=\"doc/fig/margo-diagram.png\"><img src=\"\
    doc/fig/margo-diagram.png\" alt=\"Margo architecture\" style=\"max-width: 100%;\"\
    ></a></p>\n<p>Margo provides Argobots-aware wrappers to common Mercury library\
    \ functions\nlike HG_Forward(), HG_Addr_lookup(), and HG_Bulk_transfer().  The\
    \ wrappers\nhave the same arguments as their native Mercury counterparts except\
    \ that no\ncallback function is specified.  Each function blocks until the operation\n\
    is complete.  The above diagram illustrates a typical control flow.</p>\n<p>Margo\
    \ launches a long-running user-level thread internally to drive\nprogress on Mercury\
    \ and execute Mercury callback functions (labeled\n<code>__margo_progress()</code>\
    \ above).  This thread can be assigned to a\ndedicated Argobots execution stream\
    \ (i.e., an operating system thread)\nto drive network progress with a dedicated\
    \ core.  Otherwise it will be\nautomatically scheduled when the caller's execution\
    \ stream is blocked\nwaiting for network events as shown in the above diagram.</p>\n\
    <p>Argobots eventual constructs are used to suspend and resume user-level\nthreads\
    \ while Mercury operations are in flight.</p>\n<p>Margo allows several different\
    \ threading/multicore configurations:</p>\n<ul>\n<li>The progress loop can run\
    \ on a dedicated operating system thread or not</li>\n<li>Multiple Margo instances\
    \ (and thus progress loops) can be\nexecuted on different operating system threads</li>\n\
    <li>(for servers) a single Margo instance can launch RPC handlers\non different\
    \ operating system threads</li>\n</ul>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1654705049.0
giordano/julia-on-fugaku:
  data_format: 2
  description: null
  filenames:
  - benchmarks/blas-axpy/spack-env/spack.yaml
  full_name: giordano/julia-on-fugaku
  latest_release: null
  readme: "<h1><a id=\"user-content-julia-on-fugaku-2022-07-23\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#julia-on-fugaku-2022-07-23\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Julia on Fugaku (2022-07-23)</h1>\n\
    <p><em>Note: many links refer to internal documentation which is accessible only\
    \ to Fugaku users.</em></p>\n<h2><a id=\"user-content-storage\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#storage\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Storage</h2>\n<p>Before doing anything on Fugaku,\
    \ be aware that there are <a href=\"https://www.fugaku.r-ccs.riken.jp/en/operation/20220408_01\"\
    \ rel=\"nofollow\">tight\nlimits</a> on the size of (20 GiB)\nand the number of\
    \ inodes in (200k) your home directory.  If you use many Julia Pkg\nartifacts,\
    \ it's very likely you'll hit these limits.  You'll notice that you hit the limit\n\
    because any disk I/O operation will result in a <code>Disk quota exceeded</code>\
    \ error like this:</p>\n<div class=\"highlight highlight-text-shell-session\"\
    ><pre><span class=\"pl-e\">[user@fn01sv03 ~]</span>$ <span class=\"pl-s1\">touch\
    \ foo</span>\n<span class=\"pl-c1\">touch: cannot touch 'foo': Disk quota exceeded</span></pre></div>\n\
    <p>You can check the quota of your home directory with <code>accountd</code> for\
    \ the size, and <code>accountd -i</code> for the number of inodes.</p>\n<h3><a\
    \ id=\"user-content-using-the-data-directory\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#using-the-data-directory\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Using the data directory</h3>\n<p>In order to\
    \ avoid clogging up the home directory you may want to move the Julia depot to\
    \ the\ndata directory:</p>\n<div class=\"highlight highlight-source-shell\"><pre>DATADIR=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>/data/&lt;YOUR GROUP&gt;/<span\
    \ class=\"pl-smi\">${USER}</span><span class=\"pl-pds\">\"</span></span>\n<span\
    \ class=\"pl-k\">export</span> JULIA_DEPOT_PATH=<span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span><span class=\"pl-smi\">${DATADIR}</span>/julia-depot<span class=\"\
    pl-pds\">\"</span></span></pre></div>\n<h2><a id=\"user-content-interactive-usage\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#interactive-usage\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Interactive usage</h2>\n<p>The\
    \ login nodes you access via <code>login.fugaku.r-ccs.riken.jp</code> (<a href=\"\
    https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/use_latest/AccessToTheSystem/LoggingInToTheFugakuComputerWithLocalAccount.html\"\
    \ rel=\"nofollow\">connection\ninstructions</a>)\nhave Cascade Lake CPUs, so they\
    \ aren't much useful if you want to run an aarch64 Julia.</p>\n<p>You can <a href=\"\
    https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/use_latest/JobExecution/Overview.html\"\
    \ rel=\"nofollow\">submit jobs to the\nqueue</a>\nto run Julia code on the A64FX\
    \ compute nodes, but this can be cumbersone if you need quick\nfeedback during\
    \ development or debugging.  You can also request an <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/use_latest/JobExecution/InteractiveJob.html\"\
    \ rel=\"nofollow\">interactive\nnode</a>,\nfor example with:</p>\n<pre><code>pjsub\
    \ --interact -L \"node=1\" -L \"rscgrp=int\" -L \"elapse=30:00\" --sparam \"wait-time=600\"\
    \ --mpi \"max-proc-per-node=4\"\n</code></pre>\n<h2><a id=\"user-content-available-software\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#available-software\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Available software</h2>\n<p>Fugaku\
    \ uses the <a href=\"https://spack.io/\" rel=\"nofollow\">Spack package manager</a>.\
    \  For more information about how\nto use it, see the <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/FugakuSpackGuide/\"\
    \ rel=\"nofollow\">Fugaku Spack User\nGuide</a>.</p>\n<p>Note that Spack is installed\
    \ in <code>/vol0004</code>, this means that if your home directory isn't\nmounted\
    \ on this volume you will have to <a href=\"https://www.fugaku.r-ccs.riken.jp/en/operation/20211130_02\"\
    \ rel=\"nofollow\">explicitly request the\npartition</a> in your submission\n\
    job scripts or commands, for example by adding <code>-x PJM_LLIO_GFSCACHE=/vol0004</code>\
    \ to the\n<code>pjsub</code> command, or the line</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>PJM\
    \ -x PJM_LLIO_GFSCACHE=/vol0004</span></pre></div>\n<p>in a job script.</p>\n\
    <h2><a id=\"user-content-using-julia-on-the-compute-nodes\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#using-julia-on-the-compute-nodes\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Using Julia on the compute nodes</h2>\n<p>There\
    \ is a Julia module built with Spack <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/UsingOSS/oss_e.html#packages-installed-on-the-compute-nodes\"\
    \ rel=\"nofollow\">available on the compute\nnodes</a>,\nbut as of this writing\
    \ (2022-07-23) the version of Julia provided is 1.6.3, so you may want\nto download\
    \ a more recent version from the <a href=\"https://julialang.org/downloads/\"\
    \ rel=\"nofollow\">official\nwebsite</a>.  Use the <code>aarch64</code> builds\
    \ for Glibc Linux,\npreferably <a href=\"https://julialang.org/downloads/#current_stable_release\"\
    \ rel=\"nofollow\">latest stable</a> or even\nthe <a href=\"https://julialang.org/downloads/nightlies/\"\
    \ rel=\"nofollow\">nightly build</a> if you feel confident.</p>\n<p>To enable\
    \ full vectorisation you may need to set the environment variable\n<code>JULIA_LLVM_ARGS=\"\
    -aarch64-sve-vector-bits-min=512\"</code>.  Example:\n<a href=\"https://github.com/JuliaLang/julia/issues/40308#issuecomment-901478623\"\
    >https://github.com/JuliaLang/julia/issues/40308#issuecomment-901478623</a>. \
    \ However, note that\nare a couple of severe bugs when using 512-bit vectors:</p>\n\
    <ul>\n<li>\n<a href=\"https://github.com/JuliaLang/julia/issues/44401\">https://github.com/JuliaLang/julia/issues/44401</a>\
    \ (may be an upstream LLVM bug:\n<a href=\"https://github.com/llvm/llvm-project/issues/53331\"\
    >https://github.com/llvm/llvm-project/issues/53331</a>)</li>\n<li>\n<a href=\"\
    https://github.com/JuliaLang/julia/issues/44263\">https://github.com/JuliaLang/julia/issues/44263</a>\
    \ (only in Julia v1.8+)</li>\n</ul>\n<p><em><strong>Note</strong></em>: Julia\
    \ v1.9, which is based on <a href=\"https://community.arm.com/arm-community-blogs/b/tools-software-ides-blog/posts/llvm-14\"\
    \ rel=\"nofollow\">LLVM\n14</a>,\nis able to natively autovectorise code for A64FX\
    \ <em>without</em> having to set\n<code>JULIA_LLVM_ARGS</code>, side stepping\
    \ the issues above altogether.</p>\n<h2><a id=\"user-content-mpijl\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#mpijl\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>MPI.jl</h2>\n<p><a href=\"https://github.com/JuliaParallel/MPI.jl\"\
    ><code>MPI.jl</code></a> with default JLL-provided MPICH works\nout of the box!\
    \  In order to\n<a href=\"https://juliaparallel.github.io/MPI.jl/stable/configuration/\"\
    \ rel=\"nofollow\">configure</a> <code>MPI.jl</code> v0.19 to\nuse system-provided\
    \ Fujitsu MPI (based on OpenMPI) you have to specify the <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/lang_latest/FujitsuCompiler/CompileCommands.html\"\
    \ rel=\"nofollow\">MPI C\ncompiler</a>\nfor A64FX with</p>\n<pre><code>julia --project\
    \ -e 'ENV[\"JULIA_MPI_BINARY\"]=\"system\"; ENV[\"JULIA_MPICC\"]=\"mpifcc\"; using\
    \ Pkg; Pkg.build(\"MPI\"; verbose=true)'\n</code></pre>\n<p><em><strong>Note #1</strong></em>:\
    \ <code>mpifcc</code> is available only on the compute nodes.  On the login nodes\
    \ that would be\n<code>mpifccpx</code>, but this is the cross compiler running\
    \ on Intel architecture, it's unlikely\nyou'll run an <code>aarch64</code> Julia\
    \ on there.  <a href=\"https://github.com/JuliaParallel/MPI.jl/issues/539\">Preliminary\n\
    tests</a> show that <code>MPI.jl</code> should work\nmostly fine with Fujitsu\
    \ MPI, but custom error handlers may not be available (read: trying\nto use them\
    \ causes segmentation faults).</p>\n<p><em><strong>Note #2</strong></em>: in <code>MPI.jl</code>\
    \ v0.20 Fujitsu MPI is a known ABI (it's the same as OpenMPI) and\nthere is nothing\
    \ special to do to configure it apart from <a href=\"https://juliaparallel.org/MPI.jl/dev/configuration/#Configuration-2\"\
    \ rel=\"nofollow\">choosing the system\nbinaries</a>.</p>\n<p><em><strong>Note\
    \ #3</strong></em>: we recommend using <code>MPI.jl</code>'s wrapper of <code>mpiexec</code>\
    \ to run MPI applications\nwith Julia:\n<a href=\"https://juliaparallel.org/MPI.jl/stable/configuration/#Julia-wrapper-for-mpiexec\"\
    \ rel=\"nofollow\"><code>mpiexecjl</code></a>.</p>\n<h3><a id=\"user-content-file-system-latency\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#file-system-latency\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>File system latency</h3>\n<p>Fugaku\
    \ has an advanced system to handle <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/use_latest/LyeredStorageAndLLIO/index.html\"\
    \ rel=\"nofollow\">parallel file system\nlatency</a>.\nIn order.  In order to\
    \ speed up parallel applications run through MPI you may want to\ndistribute it\
    \ to the cache area of the second-layer storage on the first-layer storage using\n\
    <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/use_latest/LyeredStorageAndLLIO/TheSecondLayerStrage.html#common-file-distribution-function-llio-transfer\"\
    \ rel=\"nofollow\"><code>llio_transfer</code></a>.\nIn particular, if you're using\
    \ Julia, you likely want to distribute the <code>julia</code> executable\nitself\
    \ together with its installation bundle.</p>\n<p>For example, assuming that you\
    \ are using the official binaries from the website, instead of\nthe Julia module\
    \ provided by Spack, you can do the following:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Directory for log of\
    \ `llio_transfer` and its wrapper `dir_transfer`</span>\nLOGDIR=<span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${TMPDIR}</span>/log<span\
    \ class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> Create the log directory if necessary</span>\nmkdir -p <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${LOGDIR}</span><span\
    \ class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> Get directory where Julia is placed</span>\nJL_BUNDLE=<span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-s\"><span class=\"pl-pds\"\
    >$(</span>dirname <span class=\"pl-s\"><span class=\"pl-pds\">$(</span>julia --startup-file=no\
    \ -O0 --compile=min -e <span class=\"pl-s\"><span class=\"pl-pds\">'</span>print(Sys.BINDIR)<span\
    \ class=\"pl-pds\">'</span></span><span class=\"pl-pds\">)</span></span><span\
    \ class=\"pl-pds\">)</span></span><span class=\"pl-pds\">\"</span></span>\n\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Move Julia installation to\
    \ fast LLIO directory</span>\n/home/system/tool/dir_transfer -l <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${LOGDIR}</span><span\
    \ class=\"pl-pds\">\"</span></span> <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span><span class=\"pl-smi\">${JL_BUNDLE}</span><span class=\"pl-pds\">\"\
    </span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Do not write\
    \ empty stdout/stderr files for MPI processes.</span>\n<span class=\"pl-k\">export</span>\
    \ PLE_MPI_STD_EMPTYFILE=off\n\nmpiexecjl --project=. -np ... julia ...\n\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> Remove Julia installation directory\
    \ from the cache.</span>\n/home/system/tool/dir_transfer -p -l <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${LOGDIR}</span><span\
    \ class=\"pl-pds\">\"</span></span> <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span><span class=\"pl-smi\">${JL_BUNDLE}</span><span class=\"pl-pds\">\"\
    </span></span></pre></div>\n<h2><a id=\"user-content-reverse-engineering-fujitsu-compiler-using-llvm-output\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#reverse-engineering-fujitsu-compiler-using-llvm-output\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Reverse\
    \ engineering Fujitsu compiler using LLVM output</h2>\n<p>The Fujitsu compiler\
    \ has <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/lang_latest/FujitsuCompiler/C/modeTradAndClangC.html\"\
    \ rel=\"nofollow\">two operation\nmodes</a>:\n\"trad\" (for \"traditional\") and\
    \ \"clang\" (enabled by the flag <code>-Nclang</code>).  In clang mode it's\n\
    based on LLVM (version 7 at the moment).  This means you can get it to emit LLVM\
    \ IR with\n<code>-emit-llvm</code>.  For example, with</p>\n<div class=\"highlight\
    \ highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\"><span class=\"pl-c1\"\
    >echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>int main(){}<span\
    \ class=\"pl-pds\">'</span></span> <span class=\"pl-k\">|</span> fcc -Nclang -x\
    \ c - -S -emit-llvm -o -</span></pre></div>\n<p>you get</p>\n<div class=\"highlight\
    \ highlight-source-llvm\"><pre><span class=\"pl-c\">; ModuleID = '-'</span>\n\
    source_filename = <span class=\"pl-s\">\"-\"</span>\n<span class=\"pl-k\">target</span>\
    \ <span class=\"pl-k\">datalayout</span> = <span class=\"pl-s\">\"e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128\"\
    </span>\n<span class=\"pl-k\">target</span> <span class=\"pl-k\">triple</span>\
    \ = <span class=\"pl-s\">\"aarch64-unknown-linux-gnu\"</span>\n\n<span class=\"\
    pl-c\">; Function Attrs: norecurse nounwind readnone uwtable</span>\n<span class=\"\
    pl-k\">define</span> dso_local <span class=\"pl-k\">i32</span> <span class=\"\
    pl-c1\">@main</span>() <span class=\"pl-k\">local_unnamed_addr</span> #<span class=\"\
    pl-c1\">0</span> <span class=\"pl-v\">!dbg</span> <span class=\"pl-v\">!8</span>\
    \ {\n  <span class=\"pl-k\">ret</span> <span class=\"pl-k\">i32</span> <span class=\"\
    pl-c1\">0</span>, <span class=\"pl-v\">!dbg</span> <span class=\"pl-v\">!11</span>\n\
    }\n\n<span class=\"pl-k\">attributes</span> #<span class=\"pl-c1\">0</span> =\
    \ { <span class=\"pl-k\">norecurse</span> <span class=\"pl-k\">nounwind</span>\
    \ <span class=\"pl-k\">readnone</span> <span class=\"pl-k\">uwtable</span> <span\
    \ class=\"pl-s\">\"correctly-rounded-divide-sqrt-fp-math\"</span>=<span class=\"\
    pl-s\">\"false\"</span> <span class=\"pl-s\">\"disable-tail-calls\"</span>=<span\
    \ class=\"pl-s\">\"false\"</span> <span class=\"pl-s\">\"less-precise-fpmad\"\
    </span>=<span class=\"pl-s\">\"false\"</span> <span class=\"pl-s\">\"no-frame-pointer-elim\"\
    </span>=<span class=\"pl-s\">\"true\"</span> <span class=\"pl-s\">\"no-frame-pointer-elim-non-leaf\"\
    </span> <span class=\"pl-s\">\"no-infs-fp-math\"</span>=<span class=\"pl-s\">\"\
    false\"</span> <span class=\"pl-s\">\"no-jump-tables\"</span>=<span class=\"pl-s\"\
    >\"false\"</span> <span class=\"pl-s\">\"no-nans-fp-math\"</span>=<span class=\"\
    pl-s\">\"false\"</span> <span class=\"pl-s\">\"no-signed-zeros-fp-math\"</span>=<span\
    \ class=\"pl-s\">\"false\"</span> <span class=\"pl-s\">\"no-trapping-math\"</span>=<span\
    \ class=\"pl-s\">\"false\"</span> <span class=\"pl-s\">\"stack-protector-buffer-size\"\
    </span>=<span class=\"pl-s\">\"8\"</span> <span class=\"pl-s\">\"target-cpu\"\
    </span>=<span class=\"pl-s\">\"a64fx\"</span> <span class=\"pl-s\">\"target-features\"\
    </span>=<span class=\"pl-s\">\"+crc,+crypto,+fp-armv8,+lse,+neon,+ras,+rdm,+sve,+v8.2a\"\
    </span> <span class=\"pl-s\">\"unsafe-fp-math\"</span>=<span class=\"pl-s\">\"\
    false\"</span> <span class=\"pl-s\">\"use-soft-float\"</span>=<span class=\"pl-s\"\
    >\"false\"</span> }\n\n<span class=\"pl-v\">!llvm.dbg.cu</span> = !{<span class=\"\
    pl-v\">!0</span>}\n<span class=\"pl-v\">!llvm.module.flags</span> = !{<span class=\"\
    pl-v\">!3</span>, <span class=\"pl-v\">!4</span>, <span class=\"pl-v\">!5</span>}\n\
    <span class=\"pl-v\">!llvm.ident</span> = !{<span class=\"pl-v\">!6</span>}\n\
    <span class=\"pl-v\">!llvm.compinfo</span> = !{<span class=\"pl-v\">!7</span>}\n\
    \n<span class=\"pl-v\">!0</span> = distinct <span class=\"pl-v\">!DICompileUnit</span>(language:\
    \ DW_LANG_C99, file: <span class=\"pl-v\">!1</span>, producer: <span class=\"\
    pl-s\">\"clang: Fujitsu C/C++ Compiler 4.7.0 (Nov  4 2021 10:55:52) (based on\
    \ LLVM 7.1.0)\"</span>, isOptimized: <span class=\"pl-k\">true</span>, runtimeVersion:\
    \ <span class=\"pl-c1\">0</span>, emissionKind: LineTablesOnly, enums: <span class=\"\
    pl-v\">!2</span>)\n<span class=\"pl-v\">!1</span> = <span class=\"pl-v\">!DIFile</span>(filename:\
    \ <span class=\"pl-s\">\"-\"</span>, directory: <span class=\"pl-s\">\"/home/ra000019/a04463\"\
    </span>)\n<span class=\"pl-v\">!2</span> = !{}\n<span class=\"pl-v\">!3</span>\
    \ = !{<span class=\"pl-k\">i32</span> <span class=\"pl-c1\">2</span>, !<span class=\"\
    pl-s\">\"Dwarf Version\"</span>, <span class=\"pl-k\">i32</span> <span class=\"\
    pl-c1\">4</span>}\n<span class=\"pl-v\">!4</span> = !{<span class=\"pl-k\">i32</span>\
    \ <span class=\"pl-c1\">2</span>, !<span class=\"pl-s\">\"Debug Info Version\"\
    </span>, <span class=\"pl-k\">i32</span> <span class=\"pl-c1\">3</span>}\n<span\
    \ class=\"pl-v\">!5</span> = !{<span class=\"pl-k\">i32</span> <span class=\"\
    pl-c1\">1</span>, !<span class=\"pl-s\">\"wchar_size\"</span>, <span class=\"\
    pl-k\">i32</span> <span class=\"pl-c1\">4</span>}\n<span class=\"pl-v\">!6</span>\
    \ = !{!<span class=\"pl-s\">\"clang: Fujitsu C/C++ Compiler 4.7.0 (Nov  4 2021\
    \ 10:55:52) (based on LLVM 7.1.0)\"</span>}\n<span class=\"pl-v\">!7</span> =\
    \ !{!<span class=\"pl-s\">\"C::clang\"</span>}\n<span class=\"pl-v\">!8</span>\
    \ = distinct <span class=\"pl-v\">!DISubprogram</span>(name: <span class=\"pl-s\"\
    >\"main\"</span>, scope: <span class=\"pl-v\">!9</span>, file: <span class=\"\
    pl-v\">!9</span>, line: <span class=\"pl-c1\">1</span>, type: <span class=\"pl-v\"\
    >!10</span>, isLocal: <span class=\"pl-k\">false</span>, isDefinition: <span class=\"\
    pl-k\">true</span>, scopeLine: <span class=\"pl-c1\">1</span>, isOptimized: <span\
    \ class=\"pl-k\">true</span>, unit: <span class=\"pl-v\">!0</span>, retainedNodes:\
    \ <span class=\"pl-v\">!2</span>)\n<span class=\"pl-v\">!9</span> = <span class=\"\
    pl-v\">!DIFile</span>(filename: <span class=\"pl-s\">\"&lt;stdin&gt;\"</span>,\
    \ directory: <span class=\"pl-s\">\"/home/ra000019/a04463\"</span>)\n<span class=\"\
    pl-v\">!10</span> = <span class=\"pl-v\">!DISubroutineType</span>(types: <span\
    \ class=\"pl-v\">!2</span>)\n<span class=\"pl-v\">!11</span> = <span class=\"\
    pl-v\">!DILocation</span>(line: <span class=\"pl-c1\">1</span>, column: <span\
    \ class=\"pl-c1\">12</span>, scope: <span class=\"pl-v\">!8</span>)</pre></div>\n\
    <h2><a id=\"user-content-systembenchmarksjl\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#systembenchmarksjl\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>SystemBenchmarks.jl</h2>\n<p>I ran <a href=\"https://github.com/IanButterworth/SystemBenchmark.jl\"\
    ><code>SystemBenchmarks.jl</code></a> on a\ncompute node.  Here are the results:\n\
    <a href=\"https://github.com/IanButterworth/SystemBenchmark.jl/issues/8#issuecomment-1039775968\"\
    >https://github.com/IanButterworth/SystemBenchmark.jl/issues/8#issuecomment-1039775968</a>.</p>\n\
    <h2><a id=\"user-content-blas\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #blas\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>BLAS</h2>\n\
    <p>OpenBLAS seems to have poor performance:</p>\n<div class=\"highlight highlight-source-julia\"\
    ><pre>julia<span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">using</span>\
    \ LinearAlgebra\n\njulia<span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\"\
    >peakflops</span>()\n<span class=\"pl-c1\">2.589865257047898e10</span></pre></div>\n\
    <p>Up to v1.7, Julia uses OpenBLAS v0.3.17, which actually doesn't support A64FX\
    \ at all, so\nit's probably using the generic kernels.\n<a href=\"https://github.com/xianyi/OpenBLAS/releases/tag/v0.3.19\"\
    ><code>v0.3.19</code></a> and\n<a href=\"https://github.com/xianyi/OpenBLAS/releases/tag/v0.3.20\"\
    ><code>v0.3.20</code></a> improved support for\nthis chip, you can find a build\
    \ of 0.3.20 at\n<a href=\"https://github.com/JuliaBinaryWrappers/OpenBLAS_jll.jl/releases/download/OpenBLAS-v0.3.20%2B0/OpenBLAS.v0.3.20.aarch64-linux-gnu-libgfortran5.tar.gz\"\
    >https://github.com/JuliaBinaryWrappers/OpenBLAS_jll.jl/releases/download/OpenBLAS-v0.3.20%2B0/OpenBLAS.v0.3.20.aarch64-linux-gnu-libgfortran5.tar.gz</a>,\n\
    but sadly there isn't a great performance improvement:</p>\n<div class=\"highlight\
    \ highlight-source-julia\"><pre>julia<span class=\"pl-k\">&gt;</span> BLAS<span\
    \ class=\"pl-k\">.</span><span class=\"pl-c1\">lbt_forward</span>(<span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>lib/libopenblas64_.so<span class=\"pl-pds\"\
    >\"</span></span>)\n<span class=\"pl-c1\">4856</span>\n\njulia<span class=\"pl-k\"\
    >&gt;</span> <span class=\"pl-c1\">peakflops</span>()\n<span class=\"pl-c1\">2.6362952057793587e10</span></pre></div>\n\
    <p>There is an <a href=\"https://www.fugaku.r-ccs.riken.jp/doc_root/en/user_guides/lang_latest/Library/BLASLAPACKScaLAPACKLibrary.html#how-to-dynamically-load-and-use-blas-lapack-and-scalapack\"\
    \ rel=\"nofollow\">optimised\nBLAS</a>\nprovided by Fujitsu, with support for\
    \ SVE (with both LP64 and ILP64).  In order to use it,\ninstall <a href=\"https://github.com/giordano/FujitsuBLAS.jl\"\
    ><code>FujitsuBLAS.jl</code></a></p>\n<div class=\"highlight highlight-source-julia\"\
    ><pre>julia<span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">using</span>\
    \ FujitsuBLAS, LinearAlgebra\n\njulia<span class=\"pl-k\">&gt;</span> BLAS<span\
    \ class=\"pl-k\">.</span><span class=\"pl-c1\">get_config</span>()\nLinearAlgebra<span\
    \ class=\"pl-k\">.</span>BLAS<span class=\"pl-k\">.</span>LBTConfig\nLibraries<span\
    \ class=\"pl-k\">:</span> \n\u2514 [ILP64] libfjlapackexsve_ilp64<span class=\"\
    pl-k\">.</span>so\n\njulia<span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\"\
    >peakflops</span>()\n<span class=\"pl-c1\">4.801227630694119e10</span></pre></div>\n\
    <p>The package <a href=\"https://github.com/carstenbauer/BLISBLAS.jl\"><code>BLISBLAS.jl</code></a>\
    \ similarly forwards\nBLAS calls to the <a href=\"https://github.com/flame/blis\"\
    >blis</a> library, which has optimised kernels\nfor A64FX.</p>\n<h2><a id=\"user-content-building-julia-from-source\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#building-julia-from-source\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building Julia\
    \ from source</h2>\n<h3><a id=\"user-content-with-gcc\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#with-gcc\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>with GCC</h3>\n<p>Building Julia from source with GCC (which is the\
    \ default if you don't set <code>CC</code> and <code>CXX</code>)\nworks fine,\
    \ it's just <em>slow</em>:</p>\n<pre><code>[...]\n    JULIA usr/lib/julia/corecompiler.ji\n\
    Core.Compiler \u2500\u2500\u2500\u2500 903.661 seconds\n[...]\nBase  \u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500271.257337 seconds\n\
    ArgTools  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 50.348227 seconds\n\
    Artifacts  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  1.193792 seconds\n\
    Base64  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  1.057241\
    \ seconds\nCRC32c  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500  0.097865 seconds\nFileWatching  \u2500\u2500\u2500\u2500\u2500  1.169747\
    \ seconds\nLibdl  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500  0.026215 seconds\nLogging  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500  0.411966 seconds\nMmap  \u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500  0.972844 seconds\nNetworkOptions \
    \ \u2500\u2500\u2500  1.159094 seconds\nSHA  \u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  2.067851 seconds\nSerialization\
    \  \u2500\u2500\u2500\u2500  2.942512 seconds\nSockets  \u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500  3.568797 seconds\nUnicode  \u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500  0.814165 seconds\nDelimitedFiles \
    \ \u2500\u2500\u2500  1.121546 seconds\nLinearAlgebra  \u2500\u2500\u2500\u2500\
    109.560774 seconds\nMarkdown  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500  7.977584 seconds\nPrintf  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500  1.635409 seconds\nRandom  \u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500 13.843395 seconds\nTar  \u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  3.146368 seconds\n\
    Dates  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \ 16.694863 seconds\nDistributed  \u2500\u2500\u2500\u2500\u2500\u2500  8.163152\
    \ seconds\nFuture  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500  0.060472 seconds\nInteractiveUtils  \u2500  5.245523 seconds\nLibGit2\
    \  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 15.469061 seconds\n\
    Profile  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  5.399918\
    \ seconds\nSparseArrays  \u2500\u2500\u2500\u2500\u2500 42.660136 seconds\nUUIDs\
    \  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  0.165799\
    \ seconds\nREPL  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500 40.149298 seconds\nSharedArrays  \u2500\u2500\u2500\u2500\u2500 \
    \ 5.476926 seconds\nStatistics  \u2500\u2500\u2500\u2500\u2500\u2500\u2500  2.130843\
    \ seconds\nSuiteSparse  \u2500\u2500\u2500\u2500\u2500\u2500 16.849304 seconds\n\
    TOML  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500  0.714203 seconds\nTest  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500  3.538098 seconds\nLibCURL  \u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500  3.547585 seconds\nDownloads  \u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500  3.657012 seconds\nPkg  \u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 54.053634 seconds\n\
    LazyArtifacts  \u2500\u2500\u2500\u2500  0.019103 seconds\nStdlibs total  \u2500\
    \u2500\u2500\u2500427.178257 seconds\nSysimage built. Summary:\nTotal \u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500 698.447219 seconds \nBase: \u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500 271.257337 seconds 38.8372%\nStdlibs: \u2500\u2500\u2500\u2500\
    \ 427.178257 seconds 61.1611%\n[...]\nPrecompilation complete. Summary:\nTotal\
    \ \u2500\u2500\u2500\u2500\u2500\u2500\u2500 1274.714700 seconds\nGeneration \u2500\
    \u2500 886.445205 seconds 69.5407%\nExecution \u2500\u2500\u2500 388.269495 seconds\
    \ 30.4593%\n</code></pre>\n<h3><a id=\"user-content-with-fujitsu-compiler\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#with-fujitsu-compiler\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>With Fujitsu compiler</h3>\n\
    <p><em>For reference, the version used for the last build I attempted was\n<a\
    \ href=\"https://github.com/JuliaLang/julia/commit/1ad2396f05fa63a71e5842c814791cd7c7715100\"\
    ><code>1ad2396f</code></a></em></p>\n<p>Compiling Julia from source with the Fujitsu\
    \ compiler is complicated.  In particular, it's\nan absolute pain to use the Fujitsu\
    \ compiler in trad mode.  You can have some more luck with\nclang mode.</p>\n\
    <p>Preparation.  Create the <code>Make.user</code> file with this content (I'm\
    \ not sure this file is\nactually necessary when using Clang mode, but it definitely\
    \ is with trad mode):</p>\n<div class=\"highlight highlight-source-makefile\"\
    ><pre><span class=\"pl-k\">override</span> <span class=\"pl-smi\">ARCH</span>\
    \ := aarch64\n<span class=\"pl-k\">override</span> <span class=\"pl-smi\">BUILD_MACHINE</span>\
    \ := aarch64-unknown-linux-gnu</pre></div>\n<p>Then you can compile with (<code>-Nclang</code>\
    \ is to select clang mode)</p>\n<pre><code>make -j50 CC=\"fcc -Nclang\" CFLAGS=\"\
    -Kopenmp\" CXX=\"FCC -Nclang\" CXXFLAGS=\"-Kopenmp\"\n</code></pre>\n<p>The compiler\
    \ in trad mode doesn't define the macro <code>__SIZEOF_POINTER__</code>, so compilation\n\
    would fail in\n<a href=\"https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/support/platform.h#L114-L115\"\
    >https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/support/platform.h#L114-L115</a>.\n\
    The solution is to set the macro <code>-D__SIZEOF_POINTER__=8</code> in the <code>CFLAGS</code>\
    \ (or just not use\ntrad mode).  Then, you may get errors like</p>\n<pre><code>/vol0003/ra000019/a04463/repo/julia/src/jltypes.c:2000:13:\
    \ error: initializer element is not a compile-time constant\n            jl_typename_type,\n\
    \            ^~~~~~~~~~~~~~~~\n./julia_internal.h:437:41: note: expanded from\
    \ macro 'jl_svec'\n                n == sizeof((void *[]){ __VA_ARGS__ })/sizeof(void\
    \ *),        \\\n                                        ^~~~~~~~~~~\n/usr/include/sys/cdefs.h:439:53:\
    \ note: expanded from macro '_Static_assert'\n      [!!sizeof (struct { int __error_if_negative:\
    \ (expr) ? 2 : -1; })]\n                                                    ^~~~\n\
    /vol0003/ra000019/a04463/repo/julia/src/jltypes.c:2025:43: error: initializer\
    \ element is not a compile-time constant\n    jl_typename_type-&gt;types = jl_svec(13,\
    \ jl_symbol_type, jl_any_type /*jl_module_type*/,\n                          \
    \                ^~~~~~~~~~~~~~\n./julia_internal.h:437:41: note: expanded from\
    \ macro 'jl_svec'\n                n == sizeof((void *[]){ __VA_ARGS__ })/sizeof(void\
    \ *),        \\\n                                        ^~~~~~~~~~~\n/usr/include/sys/cdefs.h:439:53:\
    \ note: expanded from macro '_Static_assert'\n      [!!sizeof (struct { int __error_if_negative:\
    \ (expr) ? 2 : -1; })]\n                                                    ^~~~\n\
    </code></pre>\n<p>This is the compiler's fault, which is supposed to be able to\
    \ handle this, but you can just\ndelete the assertions at lines\n<a href=\"https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/julia_internal.h#L427-L429\"\
    >https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/julia_internal.h#L427-L429</a>,\n\
    <a href=\"https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/julia_internal.h#L436-L438\"\
    >https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/julia_internal.h#L436-L438</a>,\n\
    <a href=\"https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/julia_internal.h#L444-L446\"\
    >https://github.com/JuliaLang/julia/blob/1ad2396f05fa63a71e5842c814791cd7c7715100/src/julia_internal.h#L444-L446</a>.</p>\n\
    <p>If you're lucky enough, with all these changes, you may be able to build <code>usr/bin/julia</code>.\n\
    Unfortunately, last time I tried, run this executable causes a segmentation fault\
    \ in\n<code>dl_init</code>:</p>\n<pre><code>(gdb) run\nStarting program: /vol0003/ra000019/a04463/repo/julia/julia\
    \ \nMissing separate debuginfos, use: yum debuginfo-install glibc-2.28-151.el8.aarch64\n\
    [Thread debugging using libthread_db enabled]\nUsing host libthread_db library\
    \ \"/lib64/libthread_db.so.1\".\n\nProgram received signal SIGSEGV, Segmentation\
    \ fault.\n0x000040000000def4 in _dl_init () from /lib/ld-linux-aarch64.so.1\n\
    Missing separate debuginfos, use: yum debuginfo-install FJSVxoslibmpg-2.0.0-25.14.1.el8.aarch64\
    \ elfutils-libelf-0.182-3.el8.aarch64\n(gdb) bt\n#0  0x000040000000def4 in _dl_init\
    \ () from /lib/ld-linux-aarch64.so.1\n#1  0x000040000020adb0 in _dl_catch_exception\
    \ () from /lib64/libc.so.6\n#2  0x00004000000125e4 in dl_open_worker () from /lib/ld-linux-aarch64.so.1\n\
    #3  0x000040000020ad54 in _dl_catch_exception () from /lib64/libc.so.6\n#4  0x0000400000011aa8\
    \ in _dl_open () from /lib/ld-linux-aarch64.so.1\n#5  0x0000400000091094 in dlopen_doit\
    \ () from /lib64/libdl.so.2\n#6  0x000040000020ad54 in _dl_catch_exception ()\
    \ from /lib64/libc.so.6\n#7  0x000040000020ae20 in _dl_catch_error () from /lib64/libc.so.6\n\
    #8  0x00004000000917f0 in _dlerror_run () from /lib64/libdl.so.2\n#9  0x0000400000091134\
    \ in dlopen@@GLIBC_2.17 () from /lib64/libdl.so.2\n#10 0x0000400000291f34 in load_library\
    \ (rel_path=0x400001e900c6 &lt;dep_libs+30&gt; \"libjulia-internal.so.1\", src_dir=&lt;optimized\
    \ out&gt;, err=1) at /vol0003/ra000019/a04463/repo/julia/cli/loader_lib.c:65\n\
    #11 0x0000400000291c78 in jl_load_libjulia_internal () at /vol0003/ra000019/a04463/repo/julia/cli/loader_lib.c:200\n\
    #12 0x000040000000de04 in call_init.part () from /lib/ld-linux-aarch64.so.1\n\
    #13 0x000040000000df08 in _dl_init () from /lib/ld-linux-aarch64.so.1\n#14 0x0000400000001044\
    \ in _dl_start_user () from /lib/ld-linux-aarch64.so.1\nBacktrace stopped: previous\
    \ frame identical to this frame (corrupt stack?)\n</code></pre>\n"
  stargazers_count: 2
  subscribers_count: 2
  topics: []
  updated_at: 1651480020.0
gyselax/gyselalibxx:
  data_format: 2
  description: Gyselalib++ is a collection of C++ components for writing gyrokinetic
    semi-lagrangian codes and similar
  filenames:
  - spack.yaml
  full_name: gyselax/gyselalibxx
  latest_release: null
  readme: '<h1><a id="user-content-gyselalib" class="anchor" aria-hidden="true" href="#gyselalib"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Gyselalib++</h1>

    <p>Gyselalib++ is a collection of C++ components for writing gyrokinetic semi-lagrangian
    codes and

    similar as well as a collection of such codes.</p>

    <h2><a id="user-content-compilation" class="anchor" aria-hidden="true" href="#compilation"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Compilation</h2>

    <p>to compile voice++:</p>

    <pre><code>git clone --recurse-submodules git@gitlab.maisondelasimulation.fr:gysela-developpers/voicexx.git

    cd voicexx

    mkdir build

    cd build

    cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS="-Wall -Wno-sign-compare" ..

    make

    </code></pre>

    <h2><a id="user-content-execution" class="anchor" aria-hidden="true" href="#execution"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Execution</h2>

    <p>to run the tests:</p>

    <pre><code>ctest --output-on-failure

    </code></pre>

    <p>Then, just have a look at <code>tests/landau/growthrate_t0.0to45.0.png</code>:</p>

    <p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b767f6df1712f338f85a7b0b813f7452888524845e8a67bf6223a02a8ca6dc83/68747470733a2f2f6769746c61622e6d6169736f6e64656c6173696d756c6174696f6e2e66722f677973656c612d646576656c6f70706572732f766f69636578782f2d2f6a6f62732f6172746966616374732f6d61696e2f7261772f6275696c642f74657374732f6c616e6461752f6666742f67726f777468726174655f74302e30746f34352e302e706e673f6a6f623d636d616b655f74657374735f52656c65617365"><img
    src="https://camo.githubusercontent.com/b767f6df1712f338f85a7b0b813f7452888524845e8a67bf6223a02a8ca6dc83/68747470733a2f2f6769746c61622e6d6169736f6e64656c6173696d756c6174696f6e2e66722f677973656c612d646576656c6f70706572732f766f69636578782f2d2f6a6f62732f6172746966616374732f6d61696e2f7261772f6275696c642f74657374732f6c616e6461752f6666742f67726f777468726174655f74302e30746f34352e302e706e673f6a6f623d636d616b655f74657374735f52656c65617365"
    alt="tests/landau/fft/growthrate_t0.0to45.0.png" title="Landau damping rate" data-canonical-src="https://gitlab.maisondelasimulation.fr/gysela-developpers/voicexx/-/jobs/artifacts/main/raw/build/tests/landau/fft/growthrate_t0.0to45.0.png?job=cmake_tests_Release"
    style="max-width: 100%;"></a></p>

    <p>and <code>tests/landau/frequency_t0.0to45.0.png</code>:</p>

    <p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4aa67726f68146816ee08dc5994ec66f3ac47f1de0f4ef1693d513c69ff71aee/68747470733a2f2f6769746c61622e6d6169736f6e64656c6173696d756c6174696f6e2e66722f677973656c612d646576656c6f70706572732f766f69636578782f2d2f6a6f62732f6172746966616374732f6d61696e2f7261772f6275696c642f74657374732f6c616e6461752f6666742f6672657175656e63795f74302e30746f34352e302e706e673f6a6f623d636d616b655f74657374735f52656c65617365"><img
    src="https://camo.githubusercontent.com/4aa67726f68146816ee08dc5994ec66f3ac47f1de0f4ef1693d513c69ff71aee/68747470733a2f2f6769746c61622e6d6169736f6e64656c6173696d756c6174696f6e2e66722f677973656c612d646576656c6f70706572732f766f69636578782f2d2f6a6f62732f6172746966616374732f6d61696e2f7261772f6275696c642f74657374732f6c616e6461752f6666742f6672657175656e63795f74302e30746f34352e302e706e673f6a6f623d636d616b655f74657374735f52656c65617365"
    alt="tests/landau/fft/frequency_t0.0to45.0.png" title="Landau damping frequency"
    data-canonical-src="https://gitlab.maisondelasimulation.fr/gysela-developpers/voicexx/-/jobs/artifacts/main/raw/build/tests/landau/fft/frequency_t0.0to45.0.png?job=cmake_tests_Release"
    style="max-width: 100%;"></a></p>

    <h2><a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h2>

    <p>To install dependencies through spack, first follow the the 3 first steps of

    <a href="https://github.com/pdidev/spack">https://github.com/pdidev/spack</a></p>

    <p>Then execute the following:</p>

    <div class="highlight highlight-source-shell"><pre>spack env create voice spack.yaml

    spack env activate voice

    spack concretize --reuse

    spack install</pre></div>

    <p>For example, you can find a Dockerfile installing these dependencies on ubuntu
    in

    <code>voicexx_env/Dockerfile</code>.</p>

    '
  stargazers_count: 4
  subscribers_count: 1
  topics:
  - hpc
  - numerical-simulation
  - gyrokinetic
  - poisson-solver
  - vlasov-solver
  - plasma-physics
  - ddc
  updated_at: 1656331134.0
haampie/spack-pgo-lto-environment:
  data_format: 2
  description: Enable PGO and LTO in Spack software stacks
  filenames:
  - spack.yaml
  full_name: haampie/spack-pgo-lto-environment
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1653473825.0
hariharan-devarajan/brahma:
  data_format: 2
  description: Interceptor library for I/O calls using Gotcha
  filenames:
  - dependency/spack.yaml
  full_name: hariharan-devarajan/brahma
  latest_release: null
  readme: '<h1><a id="user-content-brahma" class="anchor" aria-hidden="true" href="#brahma"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Brahma</h1>

    <p>Interceptor library for I/O calls using Gotcha</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1659995517.0
hariharan-devarajan/tailorfs:
  data_format: 2
  description: null
  filenames:
  - dependency/spack.yaml
  full_name: hariharan-devarajan/tailorfs
  latest_release: null
  readme: ''
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1656016375.0
hepnos/HEPnOS:
  data_format: 2
  description: HEPnOS is a distributed object store for high energy physics applications,
    developed at Argonne National Laboratory.
  filenames:
  - spack.yaml
  full_name: hepnos/HEPnOS
  latest_release: v0.6.11
  readme: '<h1><a id="user-content-hepnos" class="anchor" aria-hidden="true" href="#hepnos"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HEPnOS</h1>

    <p>HEPnOS is the <em>High-Energy Physics''s new Object Store</em>, a distributed
    storage

    system specially designed for HEP experiments and workflows for the FermiLab.

    HEPnOS relies on libraries developed at Argonne National Laboratory within the

    context of the Mochi project (ANL, CMU, LANL, HDF Group).</p>

    <p>For information on copyright and licensing, see the COPYRIGHT file.

    For information on how to use, see the <a href="https://xgitlab.cels.anl.gov/sds/HEPnOS/wikis/home"
    rel="nofollow">wiki</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1641296454.0
hepnos/HEPnOS-ICARUS-Benchmark:
  data_format: 2
  description: A HEPnOS benchmark aimed at investigating performance issues with the
    ICARUS application access pattern.
  filenames:
  - spack.yaml
  full_name: hepnos/HEPnOS-ICARUS-Benchmark
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1659444215.0
hppritcha/spack_ompix:
  data_format: 2
  description: null
  filenames:
  - intel_release_x86_64/spack.yaml
  - intel_master_x86_64/spack.yaml
  full_name: hppritcha/spack_ompix
  latest_release: null
  readme: '<p>Project for using Gitlab CI to test spack builds of Open MPI master
    and release tarballs.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1640037910.0
jkbk2004/FV3-vis:
  data_format: 2
  description: null
  filenames:
  - upp/ci/spack.yaml
  full_name: jkbk2004/FV3-vis
  latest_release: null
  readme: '<h1><a id="user-content-fv3atm" class="anchor" aria-hidden="true" href="#fv3atm"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>fv3atm</h1>

    <p>This repository contains a driver and key subcomponents of the

    atmospheric component of the NOAA''s <a href="https://ufscommunity.org/" rel="nofollow">Unified
    Forecast System

    (UFS)</a> weather model.</p>

    <p>The subcomponents include:</p>

    <ul>

    <li>The Finite-Volume Cubed-Sphere (FV3) dynamical core, originally

    from the <a href="https://www.gfdl.noaa.gov/" rel="nofollow">Geophysical Fluid
    Dynamics

    Laboratory</a>.</li>

    <li>The Common Community Physics Package (CCPP) supported by the

    <a href="https://dtcenter.org/community-code/common-community-physics-package-ccpp"
    rel="nofollow">Developmental Testbed Center

    (DTC)</a>,

    including:

    <ul>

    <li>

    <a href="https://github.com/NCAR/ccpp-framework">CCPP Framework</a>.</li>

    <li><a href="https://github.com/NCAR/ccpp-physics">CCPP Physics</a></li>

    </ul>

    </li>

    <li>wrapper code to call <a href="https://stochastic-physics.readthedocs.io/en/latest/"
    rel="nofollow">UFS stochastic

    physics</a>

    </li>

    <li>The io code handles netCDF I/O.</li>

    <li>The cpl coupler code connects the different components and allows

    them to communicate.</li>

    </ul>

    <h2><a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>This package requires the following

    <a href="https://github.com/NOAA-EMC/NCEPLIBS">NCEPLIBS</a> packages:</p>

    <ul>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-w3emc">NCEPLIBS-w3emc</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-bacio">NCEPLIBS-bacio</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-nemsio">NCEPLIBS-nemsio</a></li>

    <li><a href="https://github.com/NOAA-EMC/NCEPLIBS-sp">NCEPLIBS-sp</a></li>

    </ul>

    <p>If the INLINE_POST cmake variable is set, the upp library will be

    needed:</p>

    <ul>

    <li><a href="https://github.com/NOAA-EMC/EMC_post">Unified Post Processing Library</a></li>

    </ul>

    <p>This package also requires the following external packages:</p>

    <ul>

    <li><a href="https://github.com/Unidata/netcdf-c">netcdf-c Library</a></li>

    <li><a href="https://github.com/Unidata/netcdf-fortran">netcdf-fortran Library</a></li>

    <li><a href="https://github.com/esmf-org/esmf">ESMF</a></li>

    <li><a href="https://github.com/NOAA-GFDL/FMS">GFDL''s Flexible Modeling System</a></li>

    </ul>

    <h2><a id="user-content-obtaining-fv3atm" class="anchor" aria-hidden="true" href="#obtaining-fv3atm"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Obtaining fv3atm</h2>

    <p>To obtain fv3atm, clone the git repository, and update the submodules:</p>

    <pre><code>git clone https://github.com/NOAA-EMC/fv3atm.git

    cd fv3atm

    git submodule update --init --recursive

    </code></pre>

    <h2><a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Disclaimer</h2>

    <p>The United States Department of Commerce (DOC) GitHub project code is

    provided on an "as is" basis and the user assumes responsibility for

    its use. DOC has relinquished control of the information and no longer

    has responsibility to protect the integrity, confidentiality, or

    availability of the information. Any claims against the Department of

    Commerce stemming from the use of its GitHub project will be governed

    by all applicable Federal law. Any reference to specific commercial

    products, processes, or services by service mark, trademark,

    manufacturer, or otherwise, does not constitute or imply their

    endorsement, recommendation or favoring by the Department of

    Commerce. The Department of Commerce seal and logo, or the seal and

    logo of a DOC bureau, shall not be used in any manner to imply

    endorsement of any commercial product or activity by DOC or the United

    States Government.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1660597003.0
jkbk2004/src:
  data_format: 2
  description: null
  filenames:
  - src/ufs-weather-model/WW3/model/ci/spack.yaml
  - src/UPP/ci/spack.yaml
  full_name: jkbk2004/src
  latest_release: null
  readme: "<h1><a id=\"user-content-ufs-short-range-weather-application\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#ufs-short-range-weather-application\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>UFS Short-Range\
    \ Weather Application</h1>\n<p>The Unified Forecast System (UFS) is a community-based,\
    \ coupled, comprehensive Earth modeling system. It is designed to be the source\
    \ system for NOAA\u2019s operational numerical weather prediction applications\
    \ while enabling research, development, and contribution opportunities for the\
    \ broader weather enterprise. For more information about the UFS, visit the UFS\
    \ Portal at <a href=\"https://ufscommunity.org/\" rel=\"nofollow\">https://ufscommunity.org/</a>.</p>\n\
    <p>The UFS includes multiple applications (see a complete list at <a href=\"https://ufscommunity.org/science/aboutapps/\"\
    \ rel=\"nofollow\">https://ufscommunity.org/science/aboutapps/</a>) that support\
    \ different forecast durations and spatial domains. This documentation describes\
    \ the development branch of the UFS Short-Range Weather (SRW) Application, which\
    \ targets predictions of atmospheric behavior on a limited spatial domain and\
    \ on time scales from minutes to several days. The development branch of the application\
    \ is continually evolving as the system undergoes open development. The latest\
    \ SRW App release (v2.0.0) represents a snapshot of this continuously evolving\
    \ system.</p>\n<p>The UFS SRW App User's Guide associated with the development\
    \ branch is at: <a href=\"https://ufs-srweather-app.readthedocs.io/en/develop/\"\
    \ rel=\"nofollow\">https://ufs-srweather-app.readthedocs.io/en/develop/</a>, while\
    \ the guide specific to the SRW App v2.0.0 release can be found at: <a href=\"\
    https://ufs-srweather-app.readthedocs.io/en/release-public-v2/\" rel=\"nofollow\"\
    >https://ufs-srweather-app.readthedocs.io/en/release-public-v2/</a>. The repository\
    \ is at: <a href=\"https://github.com/ufs-community/ufs-srweather-app\">https://github.com/ufs-community/ufs-srweather-app</a>.</p>\n\
    <p>For instructions on how to clone the repository, build the code, and run the\
    \ workflow, see:\n<a href=\"https://github.com/ufs-community/ufs-srweather-app/wiki/Getting-Started\"\
    >https://github.com/ufs-community/ufs-srweather-app/wiki/Getting-Started</a></p>\n\
    <p>UFS Development Team. (2022, June 23). Unified Forecast System (UFS) Short-Range\
    \ Weather (SRW) Application (Version v2.0.0). Zenodo. <a href=\"https://doi.org/10.5281/zenodo.6505854\"\
    \ rel=\"nofollow\">https://doi.org/10.5281/zenodo.6505854</a></p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1661692471.0
jkbk2004/ww3-vis:
  data_format: 2
  description: null
  filenames:
  - model/ci/spack.yaml
  full_name: jkbk2004/ww3-vis
  latest_release: null
  readme: "<h1><a id=\"user-content-the-wavewatch-iii-framework\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#the-wavewatch-iii-framework\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>The WAVEWATCH III Framework</h1>\n\
    <p>WAVEWATCH III<sup>\xAE</sup>  is a community wave modeling framework that includes\
    \ the\nlatest scientific advancements in the field of wind-wave modeling and dynamics.</p>\n\
    <h2><a id=\"user-content-general-features\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#general-features\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>General Features</h2>\n<p>WAVEWATCH III<sup>\xAE</sup> solves the\
    \ random phase spectral action density\nbalance equation for wavenumber-direction\
    \ spectra. The model includes options\nfor shallow-water (surf zone) applications,\
    \ as well as wetting and drying of\ngrid points. Propagation of a wave spectrum\
    \ can be solved using regular\n(rectilinear or curvilinear) and unstructured (triangular)\
    \ grids. See\n<a href=\"https://github.com/NOAA-EMC/WW3/wiki/About-WW3\">About\
    \ WW3</a> for a\ndetailed description of WAVEWATCH III<sup>\xAE</sup> .</p>\n\
    <h2><a id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#installation\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<p>The WAVEWATCH III<sup>\xAE</sup>  framework\
    \ package has two parts that need to be combined so\nall runs smoothly: the GitHub\
    \ repo itself, and a binary data file bundle that\nneeds to be obtained from our\
    \ ftp site. Steps to successfully acquire and install\nthe framework are outlined\
    \ in our <a href=\"https://github.com/NOAA-EMC/WW3/wiki/Quick-Start\">Quick Start</a>\n\
    guide.</p>\n<h2><a id=\"user-content-disclaimer\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#disclaimer\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Disclaimer</h2>\n<p>The United States Department of Commerce (DOC)\
    \ GitHub project code is provided\non an 'as is' basis and the user assumes responsibility\
    \ for its use. DOC has\nrelinquished control of the information and no longer\
    \ has responsibility to\nprotect the integrity, confidentiality, or availability\
    \ of the information. Any\nclaims against the Department of Commerce stemming\
    \ from the use of its GitHub\nproject will be governed by all applicable Federal\
    \ law. Any reference to\nspecific commercial products, processes, or services\
    \ by service mark,\ntrademark, manufacturer, or otherwise, does not constitute\
    \ or imply their\nendorsement, recommendation or favoring by the Department of\
    \ Commerce. The\nDepartment of Commerce seal and logo, or the seal and logo of\
    \ a DOC bureau,\nshall not be used in any manner to imply endorsement of any commercial\
    \ product\nor activity by DOC or the United States Government.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1661796370.0
jotsap/msds_hpc_project:
  data_format: 2
  description: null
  filenames:
  - bin/spack_r_final.yaml
  full_name: jotsap/msds_hpc_project
  latest_release: null
  readme: '<h1><a id="user-content-msds-hpc-and-ds-final-project" class="anchor" aria-hidden="true"
    href="#msds-hpc-and-ds-final-project"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>MSDS HPC and DS Final Project</h1>

    <p>The goal of semester project is to produce a single-submit, end-to-end,

    performant pipeline for a complex and computationally intensive data analysis

    workflow.</p>

    <h2><a id="user-content-semester-project-details" class="anchor" aria-hidden="true"
    href="#semester-project-details"><span aria-hidden="true" class="octicon octicon-link"></span></a>Semester
    Project Details</h2>

    <ul>

    <li>The analysis and dataset, possibly generative, needs to be

    sufficiently computationally intensive such that a reasonable

    performance analysis can be conducted.</li>

    <li>The specific dataset, analysis, and performance analysis will be

    agreed to at various stages during the semester.</li>

    <li>The pipeline should be single-submit, meaning that a single job is

    submitted to the queue system and then entire pipeline is run with

    each stage run on appropriate hardware with appropriately optimized

    software stacks.</li>

    <li>The deliverable will be a ready to present slide deck in your GitHub

    repo, <em>i.e.</em> a job will be submitted on an SMU HPC cluster and then,

    sometime later with zero human interaction, a PDF presentation will

    appear in your GitHub repo.</li>

    <li>The presentation should discuss both the dataset analysis and

    performance analysis.</li>

    <li>Specific compute resources will be reserved for final testing and

    the production run.</li>

    </ul>

    <h2><a id="user-content-repository-structure" class="anchor" aria-hidden="true"
    href="#repository-structure"><span aria-hidden="true" class="octicon octicon-link"></span></a>Repository
    Structure</h2>

    <ul>

    <li>

    <code>bin</code>, Executable scripts.</li>

    <li>

    <code>src</code>, Non-directly executable source code.</li>

    <li>

    <code>data</code>, Datasets, where appropriate, and parameter files</li>

    <li>

    <code>docs</code>, Workflow documentation and location of the final deliverable</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1659416600.0
key4hep/key4hep-spack:
  data_format: 2
  description: A Spack recipe repository of Key4hep software.
  filenames:
  - environments/key4hep-ci/spack.yaml
  - environments/key4hep-desy-release/spack.yaml
  - environments/key4hep-nightlies/spack.yaml
  - environments/key4hep-nightlies-rootmod/spack.yaml
  - environments/key4hep-nightlies-clang/spack.yaml
  - environments/contrib-compilers/spack.yaml
  - environments/geant4-data-share/spack.yaml
  - environments/key4hep-release/spack.yaml
  - environments/key4hep-release-user/spack.yaml
  full_name: key4hep/key4hep-spack
  latest_release: '2021-10-29'
  readme: '<h1><a id="user-content-spack-package-repo-for-key4hep-software-packaging"
    class="anchor" aria-hidden="true" href="#spack-package-repo-for-key4hep-software-packaging"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>

    <a href="https://github.com/spack/spack">Spack</a> package repo for Key4HEP software
    packaging</h1>

    <p>This repository holds a set of Spack recipes for key4hep software. It grew
    out of <a href="https://github.com/HSF/hep-spack">https://github.com/HSF/hep-spack</a>,
    and many recipes habe been included in the upstream spack repostiory.</p>

    <p>Consult the <a href="https://spack.readthedocs.io/en/latest/" rel="nofollow">spack
    documentation</a> and the <a href="https://cern.ch/key4hep" rel="nofollow">key4hep
    documentation website</a> for more details.</p>

    <h3><a id="user-content-repository-contents" class="anchor" aria-hidden="true"
    href="#repository-contents"><span aria-hidden="true" class="octicon octicon-link"></span></a>Repository
    Contents</h3>

    <p>Apart from the recipes for key4hep packages in the folder <code>packages</code>,
    the repository contains some <code>scripts</code> used for publishing on cvmfs,
    and <code>config</code> files for spack.</p>

    <h3><a id="user-content-central-installations" class="anchor" aria-hidden="true"
    href="#central-installations"><span aria-hidden="true" class="octicon octicon-link"></span></a>Central
    Installations</h3>

    <p>Installations of the software stack can be found under <code>/cvmfs/sw.hsf.org/</code>,
    see:</p>

    <p><a href="https://key4hep.github.io/key4hep-doc/setup-and-getting-started/README.html"
    rel="nofollow">https://key4hep.github.io/key4hep-doc/setup-and-getting-started/README.html</a></p>

    '
  stargazers_count: 9
  subscribers_count: 9
  topics: []
  updated_at: 1657179516.0
laristra/ristra_spackages:
  data_format: 2
  description: 'A mirror of Ristra''s internal gitlab repository. '
  filenames:
  - .gitlab-ci/env/root-build/spack.yaml
  - env/x86_64/flecsi/spack.yaml
  - .gitlab-ci/env/dry-run/spack.yaml
  - env/power9le/flecsi/spack.yaml
  - env/broadwell/flecsi/spack.yaml
  - .gitlab-ci/env/local-build/spack.yaml
  full_name: laristra/ristra_spackages
  latest_release: null
  readme: '<h1><a id="user-content-ristra-spackages" class="anchor" aria-hidden="true"
    href="#ristra-spackages"><span aria-hidden="true" class="octicon octicon-link"></span></a>Ristra
    Spackages</h1>

    <p>This repository contains the custom spackage files for the repos in laristra
    family.</p>

    <h2><a id="user-content-basic-usage" class="anchor" aria-hidden="true" href="#basic-usage"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Basic Usage</h2>

    <p>We assume the user wish to work in the home directory and already have a spack
    instance setup.  The minimum required version of spack is 0.15.2.</p>

    <p>To get the content of this repo</p>

    <pre><code>$ git clone git@gitlab.lanl.gov:laristra/ristra_spackages.git

    </code></pre>

    <p>To use the custom spackage files with your spack</p>

    <pre><code>$ spack repo add ristra_spackages/spack-repo

    ==&gt; Added repo with namespace ''lanl_ristra''.


    $ spack repo list

    ==&gt; 2 package repositories.

    lanl_ristra        /home/&lt;user&gt;/ristra_spackages/spack-repo

    builtin            /home/&lt;user&gt;/spack/var/spack/repos/builtin

    </code></pre>

    <p>[Optional]

    To ensure you have this custom repo in your spack all the time, move the <code>repos.yaml</code>
    into your spack config folder</p>

    <pre><code>$ mv /home/&lt;user&gt;/.spack/linux/repos.yaml /home/&lt;user&gt;/spack/etc/spack/

    </code></pre>

    <p>Please see the <a href="https://spack.readthedocs.io/en/latest/configuration.html"
    rel="nofollow">Spack documentation</a> for more detailed info.</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1649449003.0
lfortran/lfortran:
  data_format: 2
  description: Official main repository for LFortran
  filenames:
  - spack.yaml
  full_name: lfortran/lfortran
  latest_release: null
  readme: '<h1><a id="user-content-lfortran" class="anchor" aria-hidden="true" href="#lfortran"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>LFortran</h1>

    <p><a href="https://mybinder.org/v2/gl/lfortran%2Fweb%2Flfortran-binder/master?filepath=Demo.ipynb"
    rel="nofollow"><img src="https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667"
    alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:
    100%;"></a>

    <a href="https://lfortran.zulipchat.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/11e6556bfe778e7cf7331cac9c44bd0616062722036cc0d9bb0b7909aaae8779/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7a756c69702d6a6f696e5f636861742d627269676874677265656e2e737667"
    alt="project chat" data-canonical-src="https://img.shields.io/badge/zulip-join_chat-brightgreen.svg"
    style="max-width: 100%;"></a>

    <a href="https://gitlab.com/lfortran/lfortran/-/commits/master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/779e847f325091dfbcc9a392bdcb5a7718f2cffd076460ff9fc9e03d15666fca/68747470733a2f2f6769746c61622e636f6d2f6c666f727472616e2f6c666f727472616e2f6261646765732f6d61737465722f706970656c696e652e737667"
    alt="pipeline status" data-canonical-src="https://gitlab.com/lfortran/lfortran/badges/master/pipeline.svg"
    style="max-width: 100%;"></a></p>

    <p>LFortran is a modern open-source (BSD licensed) interactive Fortran compiler

    built on top of LLVM. It can execute user''s code interactively to allow

    exploratory work (much like Python, MATLAB or Julia) as well as compile to

    binaries with the goal to run user''s code on modern architectures such as

    multi-core CPUs and GPUs.</p>

    <p>Website: <a href="https://lfortran.org/" rel="nofollow">https://lfortran.org/</a></p>

    <h1><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h1>

    <p>All documentation, installation instructions, motivation, design, ... is

    available at:</p>

    <p><a href="https://docs.lfortran.org/" rel="nofollow">https://docs.lfortran.org/</a></p>

    <p>Which is generated using the files in the <code>doc</code> directory.</p>

    <h1><a id="user-content-development" class="anchor" aria-hidden="true" href="#development"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Development</h1>

    <p>We welcome all contributions.

    The main development repository is at GitHub:</p>

    <p><a href="https://github.com/lfortran/lfortran">https://github.com/lfortran/lfortran</a></p>

    <p>Please send Pull Requests (PRs) and open issues there.</p>

    <p>See the <a href="CONTRIBUTING.md">CONTRIBUTING</a> document for more information.</p>

    <p>Main mailinglist:</p>

    <p><a href="https://groups.io/g/lfortran" rel="nofollow">https://groups.io/g/lfortran</a></p>

    <p>You can also chat with us on Zulip (<a href="https://lfortran.zulipchat.com/"
    rel="nofollow"><img src="https://camo.githubusercontent.com/11e6556bfe778e7cf7331cac9c44bd0616062722036cc0d9bb0b7909aaae8779/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7a756c69702d6a6f696e5f636861742d627269676874677265656e2e737667"
    alt="project chat" data-canonical-src="https://img.shields.io/badge/zulip-join_chat-brightgreen.svg"
    style="max-width: 100%;"></a>).</p>

    <p>Note: We moved to the above GitHub repository from GitLab on July 18, 2022.</p>

    '
  stargazers_count: 376
  subscribers_count: 14
  topics:
  - fortran
  - interactive
  - compiler
  - library
  - repl
  - jupyter
  - jupyter-notebook
  - jupyter-kernels
  updated_at: 1662086400.0
mdorier/mobject:
  data_format: 2
  description: Mobject is a Mochi object store presenting an API similar to that of
    RADOS
  filenames:
  - spack.yaml
  full_name: mdorier/mobject
  latest_release: null
  readme: '<p>Your project "mobject" has been setup!

    Enjoy programming with Mochi!</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1652975191.0
mfem/mfem:
  data_format: 2
  description: Lightweight, general, scalable C++ library for finite element methods
  filenames:
  - config/docker/spack.yaml
  full_name: mfem/mfem
  latest_release: v4.4
  readme: "<pre><code>                Finite Element Discretization Library\n    \
    \                           __\n                   _ __ ___   / _|  ___  _ __\
    \ ___\n                  | '_ ` _ \\ | |_  / _ \\| '_ ` _ \\\n               \
    \   | | | | | ||  _||  __/| | | | | |\n                  |_| |_| |_||_|   \\___||_|\
    \ |_| |_|\n\n                           https://mfem.org\n</code></pre>\n<p><a\
    \ href=\"https://mfem.org\" rel=\"nofollow\">MFEM</a> is a modular parallel C++\
    \ library for finite element\nmethods. Its goal is to enable high-performance\
    \ scalable finite element\ndiscretization research and application development\
    \ on a wide variety of\nplatforms, ranging from laptops to supercomputers.</p>\n\
    <p>We welcome contributions and feedback from the community. Please see the file\n\
    <a href=\"CONTRIBUTING.md\">CONTRIBUTING.md</a> for additional details about our\
    \ development\nprocess.</p>\n<ul>\n<li>\n<p>For building instructions, see the\
    \ file <a href=\"INSTALL\">INSTALL</a>, or type \"make help\".</p>\n</li>\n<li>\n\
    <p>Copyright and licensing information can be found in files <a href=\"LICENSE\"\
    >LICENSE</a> and <a href=\"NOTICE\">NOTICE</a>.</p>\n</li>\n<li>\n<p>The best\
    \ starting point for new users interested in MFEM's features is to\nreview the\
    \ examples and miniapps at <a href=\"https://mfem.org/examples\" rel=\"nofollow\"\
    >https://mfem.org/examples</a>.</p>\n</li>\n<li>\n<p>Instructions for learning\
    \ with Docker are in <a href=\"config/docker\">config/docker</a>.</p>\n</li>\n\
    </ul>\n<p>Conceptually, MFEM can be viewed as a finite element toolbox that provides\
    \ the\nbuilding blocks for developing finite element algorithms in a manner similar\
    \ to\nthat of MATLAB for linear algebra methods. In particular, MFEM provides\
    \ support\nfor arbitrary high-order H1-conforming, discontinuous (L2), H(div)-conforming,\n\
    H(curl)-conforming and NURBS finite element spaces in 2D and 3D, as well as many\n\
    bilinear, linear and nonlinear forms defined on them. It enables the quick\nprototyping\
    \ of various finite element discretizations, including Galerkin\nmethods, mixed\
    \ finite elements, Discontinuous Galerkin (DG), isogeometric\nanalysis, hybridization\
    \ and Discontinuous Petrov-Galerkin (DPG) approaches.</p>\n<p>MFEM includes classes\
    \ for dealing with a wide range of mesh types: triangular,\nquadrilateral, tetrahedral\
    \ and hexahedral, as well as surface and topologically\nperiodical meshes. It\
    \ has general support for mesh refinement, including local\nconforming and non-conforming\
    \ (AMR) adaptive refinement. Arbitrary element\ntransformations, allowing for\
    \ high-order mesh elements with curved boundaries,\nare also supported.</p>\n\
    <p>When used as a \"finite element to linear algebra translator\", MFEM can take\
    \ a\nproblem described in terms of finite element-type objects, and produce the\n\
    corresponding linear algebra vectors and fully or partially assembled operators,\n\
    e.g. in the form of global sparse matrices or matrix-free operators. The library\n\
    includes simple smoothers and Krylov solvers, such as PCG, MINRES and GMRES, as\n\
    well as support for sequential sparse direct solvers from the SuiteSparse\nlibrary.\
    \ Nonlinear solvers (the Newton method), eigensolvers (LOBPCG), and\nseveral explicit\
    \ and implicit Runge-Kutta time integrators are also available.</p>\n<p>MFEM supports\
    \ MPI-based parallelism throughout the library, and can readily be\nused as a\
    \ scalable unstructured finite element problem generator. Starting with\nversion\
    \ 4.0, MFEM offers support for GPU acceleration, and programming models,\nsuch\
    \ as CUDA, HIP, OCCA, RAJA and OpenMP. MFEM-based applications require\nminimal\
    \ changes to switch from a serial to a highly-performant MPI-parallel\nversion\
    \ of the code, where they can take advantage of the integrated linear\nsolvers\
    \ from the hypre library. Comprehensive support for other external\npackages,\
    \ e.g. PETSc, SUNDIALS and libCEED is also included, giving access to\nadditional\
    \ linear and nonlinear solvers, preconditioners, time integrators, etc.</p>\n\
    <p>For examples of using MFEM, see the <a href=\"examples\">examples/</a> and\
    \ <a href=\"miniapps\">miniapps/</a>\ndirectories, as well as the OpenGL visualization\
    \ tool GLVis which is available\nat <a href=\"https://glvis.org\" rel=\"nofollow\"\
    >https://glvis.org</a>.</p>\n<h2><a id=\"user-content-license\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#license\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>License</h2>\n<p>MFEM is distributed under the terms\
    \ of the BSD-3 license. All new contributions\nmust be made under this license.\
    \ See <a href=\"LICENSE\">LICENSE</a> and <a href=\"NOTICE\">NOTICE</a> for\n\
    details.</p>\n<p>SPDX-License-Identifier: BSD-3-Clause <br>\nLLNL Release Number:\
    \ LLNL-CODE-806117 <br>\nDOI: 10.11578/dc.20171025.1248</p>\n"
  stargazers_count: 1001
  subscribers_count: 118
  topics:
  - finite-elements
  - high-order
  - high-performance-computing
  - parallel-computing
  - amr
  - computational-science
  - fem
  - scientific-computing
  - hpc
  - math-physics
  - radiuss
  updated_at: 1662293945.0
mochi-hpc-experiments/colza-experiments:
  data_format: 2
  description: Experiments using Colza for In Situ Analysis
  filenames:
  - ubuntu/overhead/spack.yaml
  - cori/vtk/spack.yaml
  - cori/collectives/spack.yaml
  - ubuntu/amr-wind/spack.yaml
  - ubuntu/vtk/spack.yaml
  - ubuntu/collectives/spack.yaml
  - theta/amr-wind/spack.yaml
  - ubuntu/resizing/spack.yaml
  full_name: mochi-hpc-experiments/colza-experiments
  latest_release: ipdps2022
  readme: '<h1><a id="user-content-colza-experiments" class="anchor" aria-hidden="true"
    href="#colza-experiments"><span aria-hidden="true" class="octicon octicon-link"></span></a>Colza
    Experiments</h1>

    <p>This repository contains scripts to reproduce experiments

    related to the Colza elastic in situ analysis framework.

    These experiments were run on the Cori supercomputer.</p>

    <p>Each subfolder contains a README file explaining what the

    experiment in the subfolder does, how to install its

    dependencies, and how to run it.</p>

    <p>The ubuntu folder contains scripts that allow reproducing

    the most experiments on a single Linux workstation or a

    cluster of Linux machines.</p>

    '
  stargazers_count: 1
  subscribers_count: 3
  topics: []
  updated_at: 1655200495.0
mochi-hpc-experiments/platform-configurations:
  data_format: 2
  description: This repository provides a set of configuration files and example scripts
    for running Mochi experiments on various platforms.
  filenames:
  - ANL/ThetaGPU/spack.yaml
  - ANL/Bebop/spack.yaml
  - NERSC/Cori/spack.yaml
  - NERSC/Perlmutter/ss11/spack.yaml
  - NERSC/Perlmutter/ss10/spack.yaml
  full_name: mochi-hpc-experiments/platform-configurations
  latest_release: null
  readme: '<h1><a id="user-content-platform-configurations-for-mochi" class="anchor"
    aria-hidden="true" href="#platform-configurations-for-mochi"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Platform configurations for Mochi</h1>

    <p>This repository provides Spack configuration files, example job scripts, and

    notes about building and running Mochi-based codes on various platforms.

    Please refer to the subdirectory for your platform of interest for more

    information.</p>

    <h2><a id="user-content-using-spackyaml-files" class="anchor" aria-hidden="true"
    href="#using-spackyaml-files"><span aria-hidden="true" class="octicon octicon-link"></span></a>Using
    spack.yaml files</h2>

    <p>Each platform subdirectory in this repository provides a <code>spack.yaml</code>
    file.

    A <code>spack.yaml</code> file fully describes a Spack environment, including

    system-provided packages and compilers. It does so independently of any

    <code>compilers.yaml</code> or <code>packages.yaml</code> files installed in <code>~/.spack</code>,
    thereby

    preventing interference with user-specific spack configurations as much as

    possible.</p>

    <p>You may use <code>spack.yaml</code> files to create a

    <a href="https://spack.readthedocs.io/en/latest/environments.html" rel="nofollow">Spack
    environment</a>

    in which Mochi packages will be installed.</p>

    <p>If you don''t have Spack installed on your platform, clone it and set it up

    as follows.</p>

    <pre><code>$ git clone https://github.com/spack/spack.git

    $ . spack/share/spack/setup-env.sh

    </code></pre>

    <p>Remember that the second line needs to be executed every time you open a new

    terminal; it may be helpful to create an alias in your bashrc file as a

    shortcut.</p>

    <p>You will then need to clone <code>mochi-spack-packages</code>, which contains
    the Mochi packages.</p>

    <pre><code>$ git clone https://github.com/mochi-hpc/mochi-spack-packages.git

    $ spack repo add mochi-spack-packages

    </code></pre>

    <p>Now clone the present repository and <code>cd</code> into the subdirectory
    relevant

    to your platform. For example:</p>

    <pre><code>$ git clone https://github.com/mochi-hpc-experiments/platform-configurations.git

    $ cd platform-configurations/ANL/Bebop

    </code></pre>

    <p>Edit the path to <code>mochi-spack-packages</code> in the <code>repos</code>
    field of the <code>spack.yaml</code> file to

    match your installation.</p>

    <p>Then, execute the following command

    (changing <em>myenv</em> into an appropriate name for your environment).</p>

    <pre><code>$ spack env create myenv spack.yaml

    </code></pre>

    <p>Change to a directory outside of the <code>platform-configurations</code> folders

    and activate the environment as follows.</p>

    <pre><code>$ spack env activate myenv

    </code></pre>

    <p>You may now add specs to your environment. For instance if you want

    to install Margo, execute the following command.</p>

    <pre><code>$ spack add mochi-margo

    </code></pre>

    <p>If the <code>spack.yaml</code> file provides multiple compilers and you want

    to use another than the default one, specify the compiler explicitely,

    for example:</p>

    <pre><code>$ spack add mochi-margo %gcc@8.2.0

    </code></pre>

    <p>Note that the <code>spack.yaml</code> file you used may already have a spec

    added as an example (usually <code>mochi-margo</code>). You can remove it as

    follows.</p>

    <pre><code>$ spack rm mochi-margo

    </code></pre>

    <p>Once you have added the specs you need in your environment, install

    everything by executing the following command.</p>

    <pre><code>$ spack install

    </code></pre>

    <p>You may add more specs later on. For more information on how to manage

    Spack environments, please refer to the Spack documentation.</p>

    <h2><a id="user-content-contributing-to-this-repository" class="anchor" aria-hidden="true"
    href="#contributing-to-this-repository"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>Contributing to this repository</h2>

    <p>Should you want to contribute a <code>spack.yaml</code> for a particular machine,

    please submit a merge request with it, and ensure the following.</p>

    <ul>

    <li>The <code>spack.yaml</code> file should contain the compiler(s) that have
    been tested

    and confirmed to work with Mochi packages.</li>

    <li>The <code>spack.yaml</code> file should try to list system-provided packages,

    in particular packages used for building (<code>cmake</code>, <code>autoconf</code>,
    etc.),

    and relevant system-provided MPI implementations.

    <ul>

    <li>Note that this must be done manually.  Spack provides a <code>spack external
    find</code> command that can be used to locate a subset of system packages,

    but it does not populate the <code>spack.yaml</code> file.</li>

    </ul>

    </li>

    <li>The <code>spack.yaml</code> file should contain the relevant variants for
    packages,

    in particular the transport methods to use with <code>libfabric</code>.</li>

    <li>The path to the <code>spack.yaml</code> file should be of the form

    <code>&lt;institution&gt;/&lt;platform&gt;/spack.yaml</code>.</li>

    <li>Please make sure that your <code>spack.yaml</code> is a reliable way to work
    with

    Mochi on the target platform, other people will rely on it!</li>

    </ul>

    <p>You can also contribute changes to existing <code>spack.yaml</code> files,
    in particular

    to add working compilers, system packages, etc. As always, please test that

    new setups work before creating a merge request.</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1641290694.0
mochi-hpc/mobject:
  data_format: 2
  description: Mobject is a prototype Mochi object storage system based on RADOS
  filenames:
  - spack.yaml
  full_name: mochi-hpc/mobject
  latest_release: v0.6.1
  readme: ''
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1640785210.0
mochi-hpc/mochi-bedrock:
  data_format: 2
  description: Mochi bootstrapping service.
  filenames:
  - spack.yaml
  full_name: mochi-hpc/mochi-bedrock
  latest_release: v0.5.2
  readme: '<h1><a id="user-content-bedrock" class="anchor" aria-hidden="true" href="#bedrock"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Bedrock</h1>

    <p>Bedrock is Mochi''s service bootstrapping mechanism.

    For documentations and tutorials, please see

    <a href="https://mochi.readthedocs.io/en/latest/bedrock.html" rel="nofollow">here</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1640527359.0
mochi-hpc/mochi-colza:
  data_format: 2
  description: Mochi-based staging service for in situ analysis and visualization
  filenames:
  - spack.yaml
  full_name: mochi-hpc/mochi-colza
  latest_release: v0.2.1
  readme: ''
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1640788783.0
mochi-hpc/mochi-doc:
  data_format: 2
  description: Documentations and tutorials for Margo, Thallium, Argobots, Mercury,
    and other Mochi libraries.
  filenames:
  - code/spack.yaml
  full_name: mochi-hpc/mochi-doc
  latest_release: null
  readme: '<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mochi-hpc/mochi-doc/actions/workflows/build.yml/badge.svg"><img
    src="https://github.com/mochi-hpc/mochi-doc/actions/workflows/build.yml/badge.svg"
    alt="build" style="max-width: 100%;"></a></p>

    <h1><a id="user-content-mochi-documentation" class="anchor" aria-hidden="true"
    href="#mochi-documentation"><span aria-hidden="true" class="octicon octicon-link"></span></a>Mochi
    documentation</h1>

    <p>This repository contains a Sphinx-based documentation

    for the Mochi libraries: Margo, Thallium, Argobots, Mercury,

    ABT-IO, and SSG, as well as corresponding code examples.</p>

    <h2><a id="user-content-building-the-documentation" class="anchor" aria-hidden="true"
    href="#building-the-documentation"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    the documentation</h2>

    <p>To build and/or contribute to this documentation, make sure

    that you have Sphinx installed as well as the ReadTheDoc theme.

    These can be installed as follows using Python''s <code>pip</code>.</p>

    <pre><code>pip install sphinx

    pip install sphinx_rtd_theme

    pip install sphinx_copybutton

    </code></pre>

    <p>Once you have these dependencies installed, clone this

    repository and cd into it. You can change the documentation

    by editing the files in the source subdirectory (these files

    use the .rst format). You can build the documentation

    using the following command.</p>

    <pre><code>cd docs

    make html

    </code></pre>

    <p>And check the result by opening the <code>build/index.html</code> page

    that has been created in the docs directory.</p>

    <h2><a id="user-content-building-the-code-examples" class="anchor" aria-hidden="true"
    href="#building-the-code-examples"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    the code examples</h2>

    <p>To build the code, you will need spack and the

    <a href="https://github.com/mochi-hpc/mochi-spack-packages">mochi repo</a> setup.</p>

    <pre><code>cd code

    spack env create mochi-doc-env spack.yaml

    spack env activate mochi-doc-env

    spack install

    mkdir build

    cd build

    cmake .. -DCMAKE_CXX_COMPILER=mpicxx -DCMAKE_C_COMPILER=mpicc

    make

    </code></pre>

    '
  stargazers_count: 3
  subscribers_count: 4
  topics: []
  updated_at: 1646801350.0
mochi-hpc/mochi-margo:
  data_format: 2
  description: Argobots bindings for the Mercury RPC library
  filenames:
  - spack.yaml
  full_name: mochi-hpc/mochi-margo
  latest_release: v0.9.10
  readme: "<h1><a id=\"user-content-margo\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#margo\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Margo</h1>\n\
    <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/mochi-hpc/mochi-margo/actions/workflows/test.yml/badge.svg?branch=main\"\
    ><img src=\"https://github.com/mochi-hpc/mochi-margo/actions/workflows/test.yml/badge.svg?branch=main\"\
    \ alt=\"\" style=\"max-width: 100%;\"></a>\n<a href=\"https://codecov.io/gh/mochi-hpc/mochi-margo\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c64ae5809121f4158ced0cf46c628aca60e6db908b2639f6758b0595a6fdd779/68747470733a2f2f636f6465636f762e696f2f67682f6d6f6368692d6870632f6d6f6368692d6d6172676f2f6272616e63682f6d61696e2f67726170682f62616467652e737667\"\
    \ alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/mochi-hpc/mochi-margo/branch/main/graph/badge.svg\"\
    \ style=\"max-width: 100%;\"></a></p>\n<p>Margo provides Argobots-aware bindings\
    \ to the Mercury RPC library.</p>\n<p>Mercury (<a href=\"https://mercury-hpc.github.io/\"\
    \ rel=\"nofollow\">https://mercury-hpc.github.io/</a>) is a remote procedure call\n\
    library optimized for use in HPC environments.  Its native API presents a\ncallback-oriented\
    \ interface to manage asynchronous operation.  Argobots\n(<a href=\"https://www.argobots.org/\"\
    \ rel=\"nofollow\">https://www.argobots.org/</a>) is a user-level threading package.</p>\n\
    <p>Margo combines Mercury and Argobots to simplify development of distributed\n\
    services.  Mercury operations are presented as conventional blocking\noperations,\
    \ and RPC handlers are presented as sequential threads.  This\nconfiguration enables\
    \ high degree of concurrency while hiding the\ncomplexity associated with asynchronous\
    \ communication progress and callback\nmanagement.</p>\n<p>Internally, Margo suspends\
    \ callers after issuing a Mercury operation, and\nautomatically resumes them when\
    \ the operation completes.  This allows\nother concurrent user-level threads to\
    \ make progress while Mercury\noperations are in flight without consuming operating\
    \ system threads.\nThe goal of this design is to combine the performance advantages\
    \ of\nMercury's native event-driven execution model with the progamming\nsimplicity\
    \ of a multi-threaded execution model.</p>\n<p>A companion library called abt-io\
    \ provides similar wrappers for POSIX I/O\nfunctions: <a href=\"https://github.com/mochi-hpc/mochi-abt-io\"\
    >https://github.com/mochi-hpc/mochi-abt-io</a></p>\n<p>Note that Margo should\
    \ be compatible with any Mercury network\ntransport (NA plugin).  The documentation\
    \ assumes the use of\nthe NA SM (shared memory) plugin that is built into Mercury\
    \ for\nsimplicity.  This plugin is only valid for communication between\nprocesses\
    \ on a single node.  See <a href=\"##using-margo-with-other-mercury-na-plugins\"\
    >Using Margo with other Mercury NA\nplugins</a> for information\non other configuration\
    \ options.</p>\n<h2><a id=\"user-content-spack\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#spack\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Spack</h2>\n<p>The simplest way to install Margo is by installing\
    \ the \"mochi-margo\" package\nin spack (<a href=\"https://spack.io/\" rel=\"\
    nofollow\">https://spack.io/</a>).</p>\n<h2><a id=\"user-content-dependencies\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#dependencies\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Dependencies</h2>\n<ul>\n<li>mercury\
    \  (git clone --recurse-submodules <a href=\"https://github.com/mercury-hpc/mercury.git\"\
    >https://github.com/mercury-hpc/mercury.git</a>)</li>\n<li>argobots (git clone\
    \ <a href=\"https://github.com/pmodels/argobots.git\">https://github.com/pmodels/argobots.git</a>)</li>\n\
    </ul>\n<h3><a id=\"user-content-recommended-mercury-build-options\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#recommended-mercury-build-options\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Recommended Mercury build options</h3>\n\
    <ul>\n<li>Mercury must be compiled with -DMERCURY_USE_BOOST_PP:BOOL=ON to enable\
    \ the\nBoost preprocessor macros for encoding.</li>\n<li>Mercury should be compiled\
    \ with -DMERCURY_USE_SELF_FORWARD:BOOL=ON in order to enable\nfast execution path\
    \ for cases in which a Mercury service is linked into the same\nexecutable as\
    \ the client</li>\n</ul>\n<p>Example Mercury compilation:</p>\n<pre><code>mkdir\
    \ build\ncd build\ncmake -DMERCURY_USE_SELF_FORWARD:BOOL=ON \\\n -DBUILD_TESTING:BOOL=ON\
    \ -DMERCURY_USE_BOOST_PP:BOOL=ON \\\n -DCMAKE_INSTALL_PREFIX=/home/pcarns/working/install\
    \ \\\n -DBUILD_SHARED_LIBS:BOOL=ON -DCMAKE_BUILD_TYPE:STRING=Debug ../\n</code></pre>\n\
    <h2><a id=\"user-content-building\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #building\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building</h2>\n\
    <p>Example configuration:</p>\n<pre><code>../configure --prefix=/home/pcarns/working/install\
    \ \\\n    PKG_CONFIG_PATH=/home/pcarns/working/install/lib/pkgconfig \\\n    CFLAGS=\"\
    -g -Wall\"\n</code></pre>\n<h2><a id=\"user-content-running-examples\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#running-examples\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running examples</h2>\n<p>The\
    \ examples subdirectory contains:</p>\n<ul>\n<li>margo-example-client.c: an example\
    \ client</li>\n<li>margo-example-server.c: an example server</li>\n<li>my-rpc.[ch]:\
    \ an example RPC definition</li>\n</ul>\n<p>The following example shows how to\
    \ execute them.  Note that when the server starts it will display the address\
    \ that the client can use to connect to it.</p>\n<pre><code>$ examples/margo-example-server\
    \ na+sm://\n# accepting RPCs on address \"na+sm://13367/0\"\nGot RPC request with\
    \ input_val: 0\nGot RPC request with input_val: 1\nGot RPC request with input_val:\
    \ 2\nGot RPC request with input_val: 3\nGot RPC request to shutdown\n\n$ examples/margo-example-client\
    \ na+sm://13367/0\nULT [0] running.\nULT [1] running.\nULT [2] running.\nULT [3]\
    \ running.\nGot response ret: 0\nULT [0] done.\nGot response ret: 0\nULT [1] done.\n\
    Got response ret: 0\nULT [2] done.\nGot response ret: 0\nULT [3] done.\n</code></pre>\n\
    <p>The client will issue 4 concurrent RPCs to the server and wait for them to\n\
    complete.</p>\n<h2><a id=\"user-content-running-tests\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#running-tests\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Running tests</h2>\n<p><code>make check</code></p>\n<h2><a id=\"user-content-using-margo-with-the-other-na-plugins\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#using-margo-with-the-other-na-plugins\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using Margo\
    \ with the other NA plugins</h2>\n<p>See the <a href=\"http://mercury-hpc.github.io/documentation/\"\
    \ rel=\"nofollow\">Mercury\ndocumentation</a> for details.\nMargo is compatible\
    \ with any Mercury transport and uses the same address\nformat.</p>\n<h2><a id=\"\
    user-content-instrumentation\" class=\"anchor\" aria-hidden=\"true\" href=\"#instrumentation\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Instrumentation</h2>\n\
    <p>See the <a href=\"doc/instrumentation.md\">Instrumentation documentation</a>\
    \ for\ninformation on how to extract diagnostic instrumentation from Margo.</p>\n\
    <h2><a id=\"user-content-debugging\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #debugging\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Debugging</h2>\n\
    <p>See the <a href=\"doc/debugging.md\">Debugging documentation</a> for Margo\
    \ debugging\nfeatures and strategies.</p>\n<h2><a id=\"user-content-design-details\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#design-details\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Design details</h2>\n<p><a target=\"\
    _blank\" rel=\"noopener noreferrer\" href=\"doc/fig/margo-diagram.png\"><img src=\"\
    doc/fig/margo-diagram.png\" alt=\"Margo architecture\" style=\"max-width: 100%;\"\
    ></a></p>\n<p>Margo provides Argobots-aware wrappers to common Mercury library\
    \ functions\nlike HG_Forward(), HG_Addr_lookup(), and HG_Bulk_transfer().  The\
    \ wrappers\nhave the same arguments as their native Mercury counterparts except\
    \ that no\ncallback function is specified.  Each function blocks until the operation\n\
    is complete.  The above diagram illustrates a typical control flow.</p>\n<p>Margo\
    \ launches a long-running user-level thread internally to drive\nprogress on Mercury\
    \ and execute Mercury callback functions (labeled\n<code>__margo_progress()</code>\
    \ above).  This thread can be assigned to a\ndedicated Argobots execution stream\
    \ (i.e., an operating system thread)\nto drive network progress with a dedicated\
    \ core.  Otherwise it will be\nautomatically scheduled when the caller's execution\
    \ stream is blocked\nwaiting for network events as shown in the above diagram.</p>\n\
    <p>Argobots eventual constructs are used to suspend and resume user-level\nthreads\
    \ while Mercury operations are in flight.</p>\n<p>Margo allows several different\
    \ threading/multicore configurations:</p>\n<ul>\n<li>The progress loop can run\
    \ on a dedicated operating system thread or not</li>\n<li>Multiple Margo instances\
    \ (and thus progress loops) can be\nexecuted on different operating system threads</li>\n\
    <li>(for servers) a single Margo instance can launch RPC handlers\non different\
    \ operating system threads</li>\n</ul>\n"
  stargazers_count: 14
  subscribers_count: 9
  topics: []
  updated_at: 1660707472.0
mochi-hpc/mochi-mona:
  data_format: 2
  description: Mochi messaging over NA
  filenames:
  - spack.yaml
  full_name: mochi-hpc/mochi-mona
  latest_release: v0.2.3
  readme: '<h1><a id="user-content-mona---messaging-over-na" class="anchor" aria-hidden="true"
    href="#mona---messaging-over-na"><span aria-hidden="true" class="octicon octicon-link"></span></a>MoNA
    - Messaging over NA</h1>

    <p>MoNA is a Mochi library combining the NA layer of Mercury with

    the Argobots threading library, in a way similar to how Margo

    combines Mercury with Argobots. It provides a low-level messaging

    interface and hides the NA progress loop into Argobots ULTs.</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1633974549.0
mochi-hpc/mochi-ssg:
  data_format: 2
  description: Scalable Service Groups (SSG), a group membership service for Mochi
  filenames:
  - spack.yaml
  full_name: mochi-hpc/mochi-ssg
  latest_release: v0.5.3
  readme: '<h1><a id="user-content-ssg-scalable-service-groups" class="anchor" aria-hidden="true"
    href="#ssg-scalable-service-groups"><span aria-hidden="true" class="octicon octicon-link"></span></a>SSG
    (Scalable Service Groups)</h1>

    <p>SSG is a group membership microservice based on the Mercury RPC system.

    It provides mechanisms for bootstrapping sets of Mercury processes into

    logical groups and for managing the membership of these process groups

    over time. At a high-level, each group collectively maintains a <em>group view</em>,

    which is just a mapping from group member identifiers to Mercury address

    information. The inital group membership view is specified completely

    when the group is bootstrapped (created). Currently, SSG offers the

    following group bootstrapping methods:</p>

    <ul>

    <li>MPI communicator-based bootstrap</li>

    <li>config file-based bootstrap</li>

    <li>generic bootstrap method using an array of Mercury address strings</li>

    </ul>

    <p>Process groups are referenced using unique group identifiers

    which encode Mercury address information that can be used to connect

    with a representative member of the group. These identifiers may be

    transmitted to other processes so they can join the group or attach to

    the group (<em>attachment</em> provides non-group members a way to access a

    group''s view).</p>

    <p>Optionally, SSG can be configured to use the <a href="http://www.cs.cornell.edu/~asdas/research/dsn02-SWIM.pdf"
    rel="nofollow">SWIM failure detection and

    group membership protocol</a>

    internally to detect and respond to group member failures. SWIM uses a

    randomized probing mechanism to detect faulty group members and uses an

    efficient gossip protocol to dissmeninate group membership changes to other

    group members. SSG propagates group membership view updates back to the SSG

    user using a callback interface.</p>

    <p><strong>NOTE</strong>: SSG does not currently support group members dynamically
    leaving

    or joining a group, though this should be supported in the near future.

    This means that, for now, SSG groups are immutable after creation.

    When using SWIM, this means members can be marked as faulty, but they

    cannot be fully evicted from the group view yet.</p>

    <p><strong>NOTE</strong>: SSG does not currently allow for user-specified group
    member

    identifiers, and instead assigns identifiers as dense ranks into the

    list of address strings specified at group creation time. That is,

    the group member with the first address string in the list is rank 0,

    and so on.</p>

    <h2><a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h2>

    <ul>

    <li>mercury (git clone --recurse-submodules <a href="https://github.com/mercury-hpc/mercury.git">https://github.com/mercury-hpc/mercury.git</a>)</li>

    <li>argobots (git clone <a href="https://github.com/pmodels/argobots.git">https://github.com/pmodels/argobots.git</a>)</li>

    <li>margo (git clone <a href="https://xgitlab.cels.anl.gov/sds/margo.git" rel="nofollow">https://xgitlab.cels.anl.gov/sds/margo.git</a>)</li>

    <li>libev (e.g libev-dev package on Ubuntu or Debian)</li>

    </ul>

    <h2><a id="user-content-building" class="anchor" aria-hidden="true" href="#building"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Building</h2>

    <p>(if configuring for the first time)

    ./prepare.sh</p>

    <p>./configure [standard options] PKG_CONFIG_PATH=/path/to/pkgconfig/files</p>

    <p>make</p>

    <p>make install</p>

    <p>MPI support is by default optionally included. If you wish to compile with
    MPI

    support, set CC=mpicc (or equivalent) in configure. If you wish to disable MPI

    entirely, use --disable-mpi (you can also force MPI inclusion through

    --enable-mpi).</p>

    '
  stargazers_count: 1
  subscribers_count: 6
  topics: []
  updated_at: 1641352657.0
mochi-hpc/mochi-yokan:
  data_format: 2
  description: Remote Key/Value storage service for Mochi
  filenames:
  - spack.yaml
  full_name: mochi-hpc/mochi-yokan
  latest_release: v0.2.8
  readme: '<h1><a id="user-content-yokan---mochis-keyvalue-and-more-storage-service"
    class="anchor" aria-hidden="true" href="#yokan---mochis-keyvalue-and-more-storage-service"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Yokan - Mochi''s Key/Value
    (and more) storage service</h1>

    <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mochi-hpc/mochi-yokan/actions/workflows/test.yml/badge.svg?branch=main"><img
    src="https://github.com/mochi-hpc/mochi-yokan/actions/workflows/test.yml/badge.svg?branch=main"
    alt="" style="max-width: 100%;"></a>

    <a href="https://codecov.io/gh/mochi-hpc/mochi-yokan" rel="nofollow"><img src="https://camo.githubusercontent.com/fc95c801bafa29b49219f4727f651b97e7385800c8dc4a4757a1dccadefe6611/68747470733a2f2f636f6465636f762e696f2f67682f6d6f6368692d6870632f6d6f6368692d796f6b616e2f6272616e63682f6d61696e2f67726170682f62616467652e737667"
    alt="codecov" data-canonical-src="https://codecov.io/gh/mochi-hpc/mochi-yokan/branch/main/graph/badge.svg"
    style="max-width: 100%;"></a></p>

    <p>Please see documentation <a href="https://mochi.readthedocs.io/en/latest/yokan.html"
    rel="nofollow">here</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1641326484.0
mochi-hpc/ycsb-cpp-interface:
  data_format: 2
  description: Mochi-based DB backends for the YCSB benchmark
  filenames:
  - spack.yaml
  full_name: mochi-hpc/ycsb-cpp-interface
  latest_release: null
  readme: "<h1><a id=\"user-content-ycsb-c-interface\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#ycsb-c-interface\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>YCSB C++ Interface</h1>\n<p><a href=\"https://github.com/brianfrankcooper/YCSB\"\
    >YCSB</a> is one of the most popular Cloud\nstorage benchmark. However it is written\
    \ in Java, forcing databases implemented\nin other languages to provide a Java\
    \ wrapper. While <a href=\"https://github.com/ls4154/YCSB-cpp\">YCSC-cpp</a>\n\
    provides a reimplementation of YCSB in C++, to date it only supports three backends,\
    \ as\nopposed to 45 for the original YCSB.</p>\n<p><a href=\"https://github.com/mochi-hpc/ycsb-cpp-interface\"\
    >ycsb-cpp-interface</a>\ntakes a different approach from YCSB-cpp, providing a\
    \ Java/C++ library\nthat enables the use of C++ to write DB backends for YCSB.</p>\n\
    <p>ycsb-cpp-inteface works in a modular way, dynamically loading your C++ database\n\
    implementation from a library using a factory pattern.</p>\n<h2><a id=\"user-content-installing\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#installing\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installing</h2>\n<h3><a id=\"\
    user-content-building-manually\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #building-manually\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Building manually</h3>\n<p>To build this repository from source, you\
    \ will first need to have\nits dependencies installed and findable by CMake. These\
    \ dependencies\ninclude:</p>\n<ul>\n<li>Java Development Kit (e.g., OpenJDK)</li>\n\
    <li>YCSB</li>\n<li>cmake</li>\n</ul>\n<p>Make sure to set the <code>JAVA_HOME</code>\
    \ environment variable\nto point to where your JDK is installed so that CMake\
    \ can find it.\nIt is recommended to install a distribution of YCSB, rather than\n\
    the source.</p>\n<p>You can then build the source contained in this repository\
    \ as follows.</p>\n<pre><code>$ mkdir build\n$ cd build\n$ cmake .. -DYCSB_ROOT=&lt;path/to/where/ycsb/is/installed&gt;\
    \ \\\n           -DCMAKE_INSTALL_PREFIX=&lt;install/prefix&gt;\n$ make\n</code></pre>\n\
    <h3><a id=\"user-content-installing-using-spack\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#installing-using-spack\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Installing using Spack</h3>\n<p>You can install this\
    \ library using <a href=\"https://spack.io/\" rel=\"nofollow\">Spack</a>.\nThe\
    \ <code>ycsb-cpp-interface</code> Spack package is available via the\n<a href=\"\
    https://github.com/mochi-hpc/mochi-spack-packages\">Mochi repository</a>,\nwhich\
    \ can be added to Spack as follows.</p>\n<pre><code>$ git clone https://github.com/mochi-hpc/mochi-spack-packages.git\n\
    $ spack repo add mochi-spack-packages\n</code></pre>\n<p>Once the <code>mochi-spack-packages</code>\
    \ repository has been made available to Spack,\nyou can install <code>ycsb-cpp-interface</code>\
    \ as follows.</p>\n<pre><code>$ spack install ycsb-cpp-interface\n</code></pre>\n\
    <h2><a id=\"user-content-testing\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #testing\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Testing</h2>\n\
    <p>If you have installed ycsb-cpp-interface with Spack, make sure that\nthe package\
    \ is loaded (<code>spack load ycsb-cpp-interface</code>), then you\ncan start\
    \ the CLI for testing, as follows.</p>\n<pre><code>ycsb-cpp-cli\n</code></pre>\n\
    <p>When building from source, the CLI is located in the <code>bin</code> subdirectory\n\
    of your build folder.</p>\n<p>You will end up in YCBS's CLI, with the YcsbDBClient\
    \ loaded as the\nDB backend, itself using a test implementation of an in-memory\
    \ database\nwith which you can interact (type <code>help</code> to see a list\
    \ of available commands).</p>\n<h2><a id=\"user-content-writing-your-own-c-db-backend\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#writing-your-own-c-db-backend\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Writing\
    \ your own C++ DB backend</h2>\n<p>ycsb-cpp-interface provides a header file,\
    \ <code>YCSBCppInterface.hpp</code>, with\na <code>ycsb::DB</code> abstract class.\
    \ To implement your own C++ backend database,\nyou simply need to implement a\
    \ child class of the <code>ycsb::DB</code> class that\nimplements the required\
    \ virtual functions. You may look at <a href=\"src/TestDB.cpp\"></a>\nas an example\
    \ of such an implementation. Note the use of the\n<code>YCSB_CPP_REGISTER_DB_TYPE</code>\
    \ macro after the class definition. This macro\nmust be called in a .cpp file\
    \ to associate the name of your backend\n(e.g. <code>myawesomedb</code>) with\
    \ the class name to use (e.g., <code>MyAwesomeDB</code>).</p>\n<p>Once your database\
    \ class is ready, compile it into a shared library\n(e.g., <code>libmyawesomedb.so</code>).\
    \ Make sure the <code>LD_LIBRARY_PATH</code> environment\nvariable contains the\
    \ path to your dynamic library. You may then test\nyour backend with the CLI as\
    \ follows.</p>\n<pre><code>$ ycsb-cpp-cli -p ycsb.cpp.library=libmyawesomedb.so\
    \ -p ycsb.cpp.backend=myawesomedb\n</code></pre>\n<p>The <code>ycsb.cpp.library</code>\
    \ and <code>ycsb.cpp.backend</code> properties are the only properties\nneeded\
    \ by ycsb-cpp-interface. Any other properties provided will be propagated\nto\
    \ your database implementation in the form of an <code>std::unordered_map&lt;std::string,\
    \ std::string&gt;</code>.\nNote that <code>ycsb.cpp.library</code> may accept\
    \ a full path to your dynamic library,\nif you don't want to change the <code>LD_LIBRARY_PATH</code>\
    \ environment variable.</p>\n<h2><a id=\"user-content-running-ycsb-with-your-c-db-backend\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#running-ycsb-with-your-c-db-backend\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ YCSB with your C++ DB backend</h2>\n<p>ycsb-cpp-interface provides a convenience\
    \ script, <code>ycsb-cpp</code>, to run YCSB\nwith your own backend. It can be\
    \ used in a way similar to the original ycsb script,\nas follows.</p>\n<pre><code>$\
    \ ycsb-cpp load -p ycsb.cpp.library=libmyawesomedb.so -p ycsb.cpp.backend=myawesomedb\
    \ -P workloadfile\n$ ycsb-cpp run -p ycsb.cpp.library=libmyawesomedb.so -p ycsb.cpp.backend=myawesomedb\
    \ -P workloadfile\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1661162651.0
olcf/spack-environments:
  data_format: 2
  description: Spack Environments Templates for OLCF resources
  filenames:
  - linux-centos7-broadwell/or-slurm/spack.yaml
  full_name: olcf/spack-environments
  latest_release: null
  readme: '<p>OLCF Spack Environments Templates</p>

    <p>Companion files the for: <a href="https://docs.olcf.ornl.gov/software/spack_env/index.html"
    rel="nofollow">OLCF Documentaton for spack environments</a></p>

    <h2><a id="user-content-purpose" class="anchor" aria-hidden="true" href="#purpose"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Purpose</h2>

    <p>The provided Spack environment files are intended to assist OLCF users in setup
    their development environment at the

    OLCF.  The base environment file includes the compilers and packages that are
    installed at the system level.</p>

    <p>Spack documentation can be found <a href="https://spack.readthedocs.io/" rel="nofollow">here</a>.</p>

    '
  stargazers_count: 1
  subscribers_count: 19
  topics: []
  updated_at: 1662034991.0
range3/chfs-containers:
  data_format: 2
  description: null
  filenames:
  - spack/envs/chfs-master/spack.yaml
  - spack/envs/chfs/spack.yaml
  full_name: range3/chfs-containers
  latest_release: null
  readme: '<h1><a id="user-content-chfs-containers" class="anchor" aria-hidden="true"
    href="#chfs-containers"><span aria-hidden="true" class="octicon octicon-link"></span></a>chfs-containers</h1>

    <h2><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>example</h2>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    explicitly pull the latest chfs image </span>

    docker pull range3/chfs:master


    git clone https://github.com/range3/chfs-containers

    <span class="pl-c1">cd</span> chfs-containers


    <span class="pl-c"><span class="pl-c">#</span> start servers</span>

    docker-compose up -d


    <span class="pl-c"><span class="pl-c">#</span> start another container for client</span>

    docker run -it --rm --network chfs_net --privileged range3/chfs:master bash

    <span class="pl-c"><span class="pl-c">#</span> set CHFS_SERVER env</span>

    <span class="pl-k">export</span> CHFS_SERVER=<span class="pl-s"><span class="pl-pds">$(</span>chlist
    -c -s ofi+sockets://172.30.0.3:50000<span class="pl-pds">)</span></span>


    <span class="pl-c"><span class="pl-c">#</span> list chfs servers</span>

    chlist


    <span class="pl-c"><span class="pl-c">#</span> mount chfs via FUSE</span>

    mkdir /tmp/m

    chmkdir /tmp/m

    chfuse -o direct_io,modules=subdir,subdir=<span class="pl-s"><span class="pl-pds">"</span>/tmp/m<span
    class="pl-pds">"</span></span> /tmp/m


    <span class="pl-c"><span class="pl-c">#</span> &lt;ctrl-D&gt;</span>

    <span class="pl-c"><span class="pl-c">#</span> the client container is removed</span>


    <span class="pl-c"><span class="pl-c">#</span> stop and remove server containers</span>

    docker-compose down</pre></div>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1652094301.0
ravisiv/PhishingEmailDetection:
  data_format: 2
  description: null
  filenames:
  - src_simple/spack/capstone/spack.yaml
  - src/spack/capstone/spack.yaml
  full_name: ravisiv/PhishingEmailDetection
  latest_release: null
  readme: "<p>Phishing Detection Using Natural Language Processing and Machine\nLearning</p>\n\
    <p>Tai Chowdhury^1^, Ravi Sivaraman^1^, Apurv Mittal^1^, Daniel Engels^2^,\nHarsha\
    \ Kommanapalli^3^</p>\n<p>1 Master of Science in Data Science</p>\n<p>Southern\
    \ Methodist University<br>\nDallas, TX 75275 USA</p>\n<p>^2^ AT&amp;T Virtualization\
    \ Center, SMU, Dallas, TX</p>\n<p>^3^ Meta, Menlo Park, CA</p>\n<p>^1^{tchowdhury,\
    \ apurvm, rsivaraman}@smu.edu</p>\n<p>^2^ <a href=\"mailto:daniel.w.engels@gmail.com\"\
    >daniel.w.engels@gmail.com</a></p>\n<p>^3^ <a href=\"mailto:harshanaidu@yahoo.com\"\
    >harshanaidu@yahoo.com</a></p>\n<p><strong>Abstract.</strong> Phishing emails\
    \ are a primary mode of entry for attackers\ninto an organization. A successful\
    \ phishing attempt leads to\nunauthorized access to sensitive information and\
    \ systems. However,\nautomatically identifying phishing emails is often difficult\
    \ since many\nphishing emails have composite features such as body text and metadata\n\
    that are nearly indistinguishable from valid emails. This paper presents\na novel\
    \ machine learning-based framework, the DARTH framework, that\ncharacterizes and\
    \ combines multiple models, with one model for each\nindividual composite feature,\
    \ that enables the accurate identification\nof phishing emails. The framework\
    \ analyses each composite feature\nindependently utilizing a multi-faceted approach\
    \ using Natural Language\nProcessing (NLP) and neural network-based techniques\
    \ and combines the\nresults of these analysis to classify the emails as malicious\
    \ or\nlegitimate. Utilizing the framework on more than 150,000 emails and\ntraining\
    \ data from multiple sources including the authors' personal\nemails and phishtank.com\
    \ resulted in the precision (correct\nidentification of malicious observations\
    \ to the total prediction of\nmalicious observations) of 99.97% with an f-score\
    \ of 99.98% and\naccurately identifying phishing emails 99.98% of the time. Utilizing\n\
    multiple machine learning techniques combined in an ensemble approach\nacross\
    \ a range of composite features yields highly accurate\nidentification of phishing\
    \ emails.</p>\n<ol>\n<li>Introduction</li>\n</ol>\n<p>Phishing is a method of\
    \ stealing private and sensitive information using\ndeceptive emails, websites,\
    \ and text messages. The attackers utilize\nsocial engineering approaches to entice\
    \ people to perform actions, such\nas clicking on a hyperlink, that leads to the\
    \ installation of malware or\nthe stealing of personal information. To this end,\
    \ attackers often\npretend to be someone from a reputable organization and use\
    \ fraudulent\ntechniques to steal online users' data, such as passwords or credit\
    \ card\ninformation. The improvements in cybersecurity protections, to the\npoint,\
    \ that humans are the weakest link in the cybersecurity chain have\nbeen attributed\
    \ to the advancement of social engineering attacks such as\nphishing. According\
    \ to research from IRONSCALES (2021), 81% of the\norganization that participated\
    \ in the survey have dealt with phishing\nattacks [14]. According to Verizon's\
    \ 2021 Data Breach Investigations\nReport (DBIR), around 25% of all data breaches\
    \ involve phishing, and 85%\ninvolve a human element [27]. The rise of social\
    \ media has complicated\nthe issue even further as the attackers use sophisticated\
    \ tools to carry\nout these attacks. Hackers use LinkedIn to create faux messages,\
    \ making\nup 47% of social media phishing attempts [14]. The cost of the data\n\
    breach on LinkedIn alone was $4.42 Billion in 2021 [14]. This affects\nindividuals\
    \ to organizations; it has both privacy and financial\nimplications.</p>\n<p>Anti-Phishing\
    \ Working Group (APWG) is an organization that collects,\nanalyzes, and exchanges\
    \ a list of credential URLs. It publishes a\nquarterly report on phishing activities\
    \ across the globe. The number of\nphishing attacks has doubled from 2020 to 2021.\
    \ More than 260,000\nreported phishing attacks in July 2021 [41]. Webmail continues\
    \ to be\namong the top methods of phishing attempts. There has been an increase\n\
    in phishing attacks on named brands, from 400 per July to 700 in\nSeptember 2021.\
    \ There are state laws for penalizing criminals for\nphishing attacks. Anti-phishing\
    \ Act of 2005 imposes fines and\nimprisonment for up to five years or both for\
    \ a person involved with\nphishing attacks [26]. California leads the way in having\
    \ a strong\nstate law on phishing attacks.</p>\n<p>Currently, most phishing attack\
    \ detection methods are purely one-method\napproaches. This type of method may\
    \ not be effective in detecting\nsophisticated phishing attacks. Most experts\
    \ use two types of phishing\ndetection systems, list-based detection systems and\
    \ Machine\nLearning-Based Detection Systems [6]. A blocklist of URLs is created\n\
    in a list-based system to identify malicious links using URL metadata\ngathered\
    \ from spam detection systems, user notifications, third-party\norganizations,\
    \ and other cybersecurity platforms. Blacklist-based\nmethods have a low false-positive\
    \ rate compared to machine\nlearning-based approaches. The success rate is about\
    \ 20% [6]. This\nmethod requires constant URL updates to the blocklist database,\n\
    worsening the problem.</p>\n<p>Recent research on phishing detection focuses on\
    \ machine learning\ntechniques like Artificial Neural Networks (ANN) [9], Bayesian\n\
    Additive Regression Trees (BART) [6], Graph Convolutional Networks\n(GCN), and\
    \ Natural Language Processing (NLP) [8] for feature detection\nlike various attributes\
    \ in the observed dataset. These research papers\nprimarily focused on URL metadata,\
    \ with few analyzing the email texts\n[9]. Several previous research papers focused\
    \ mainly either on URL\nmetadata or email texts. This creates an opportunity to\
    \ research and\ncreate a holistic model which inherits multiple techniques on\
    \ various\naspects of malicious emails like URLs, attachments, images, senders,\n\
    body text, etc., to identify phishing attempts effectively.</p>\n<p>This research\
    \ employs various modeling techniques to detect\nsophisticated phishing attacks,\
    \ including bagging and boosting modeling\ntechniques. One of the significant\
    \ challenges of phishing detection is\nthe preprocessing of text in the URLs and\
    \ email body. Boosting\ntechniques like Extreme Gradient Boosting (XGBoost) handles\
    \ large\ndataset for text preprocessing, extract essential features, and handle\n\
    noise properly for spam classification. In another industry research,\nSupport\
    \ Vector Machine (SVM) has been used for spam classifications as\nit can combine\
    \ statistical framework and other combinations such as user\nbehavior features\
    \ to create a model that can yield accuracy scores of\n&gt;97% [22].</p>\n<p>This\
    \ research uses Natural Language Processing (NLP) techniques,\nclustering, and\
    \ neural network-based machine learning models to identify\nphishing attempts\
    \ by analyzing the email content before users access it.\nThis research recommends\
    \ a set of processes rather than relying on a\nsingle process to address sophisticated\
    \ attacks. It targets phishing\nattempts more holistically by using a multi-faceted\
    \ approach that\nanalyzes the embedded URLs, email body, sender's information,\
    \ email\nattachments, and other email metadata to classify malicious emails. This\n\
    research brings incremental improvements to the existing models.\nInstitutions\
    \ and researchers interested in the security of email\ncommunication can use the\
    \ output from this multi-faceted approach.</p>\n<p>This study further analyzes\
    \ English language body text and assigns\nscores based on the text characteristics\
    \ persuading users to access the\nmalicious content. The phishing email text is\
    \ primarily classified into\ntwo major categories, \"Masquerade-ness\" and \"\
    Urgent-ness.\"\n\"Masquerade-ness\" is a phishing email characteristic that urges\
    \ the\nreceiver to click the URLs with less analytical thought. To aid such\n\
    behavior, such emails masquerade themselves as a famous brand by phony\nadvertising\
    \ that is attractive to the receiver. This masquerading\nbehavior is measured\
    \ from NLP analytics using Sentence Vectors.</p>\n<p>Similarly, \"Urgent-ness\"\
    \ is a phishing email characteristic that urges\nreceivers to access the malicious\
    \ content by creating a false sense of\nurgency (like the receiver needs to click\
    \ now to get the deal, etc.).\nThis \"Urgent-ness\" from the email text is measured\
    \ using Sentence\nVectors. These sentence vectors are fed to neural networks as\
    \ features\nto detect phishing emails.</p>\n<p>In addition to NLP, this research\
    \ emphasizes using Neural Network\nmodeling techniques to detect accuracy improvements.\
    \ This technique\nperforms better for sophisticated attacks in which blacklisting,\n\
    heuristic detection, and visual similarity methods do not perform well\nin terms\
    \ of detection [18]. Current techniques require more manual\nprocesses and human\
    \ intervention, which becomes inefficient for faster\ndetection of sophisticated\
    \ attacks. Zhu et al*.* (2020) mention that\nthese methods allow attackers to\
    \ cut through the constricted filters and\nrules. Neural Network models can address\
    \ these problems by using robust\nhistorical datasets to create a model that reduces\
    \ manual inputs for\nphishing detection. There are several types of modeling techniques.</p>\n\
    <p>Feed Forward Neural Network (FFNN), Artificial Neural Network (ANN),\nConvolutional\
    \ Neural Network (CNN), Recurrent Neuron Network (RNN), and\nEnsemble Neural Network\
    \ (ENN) are some of the crucial neural networks\nfor models that have been used\
    \ for phishing websites and email detection\n[17]. ANN is a neural network model\
    \ which is a self-structured neural\nnetwork. It mimics the human brain's neural\
    \ network, where several\nneurons or hidden layers are connected for passing information\
    \ from the\ninput layer to the output layer. This model has been highly used for\n\
    URL-based phishing detection models as it provides high accuracy scores\n[9].\
    \ FFNN is another popular neural network model. Soon et al. have\nmentioned the\
    \ increased usage of FFNN since it has a history of\nproducing accuracy scores\
    \ of 95% or up [17]. It helps create an\neffective modeling relationship between\
    \ input layers and output layers\nthrough feedforward neural networks [18]. ENN\
    \ is another powerful\nmodeling technique that gathers multiple neural network\
    \ models to detect\nthe attack using covariance matrices. The matrices are calculated\
    \ by\ncollecting average, maximum, and minimum values of the output and\nproviding\
    \ the final score using majority votes [18]. CNN modeling\ntechniques can deal\
    \ with some of the complex issues with new and\nsophisticated phishing detection.\
    \ It is a fully connected artificial\nneural network that can read images and\
    \ handwritten data for image\ndetection. It consists of several coevolutionary\
    \ layers, max-pooling, or\nfully connected layers [20]. Coevolutionary layers\
    \ can detect\n\"chrematistic features\" in images [20]. These layers can be helpful\
    \ to\nsee phishing attacks by analyzing URLs.</p>\n<p>The model improves detection\
    \ by adding more embedded layers. The model\nalso performs well with NLP, where\
    \ it can classify the attacks with a\nhigher accuracy score by adjusting the representation\
    \ of words in URLs\n[20]. And lastly, there is the Recurrent Neural Network (RNN)\
    \ which\nuses sequential data to predict words or speech in language translation\n\
    and speech detection. RNN takes characters from URLs as input and\nsequentially\
    \ analyze them for each URL to study pattern for attack\ndetection. The classification\
    \ model is built using Least Square Time\nSeries.</p>\n<p>The data for this study\
    \ has been acquired from PhishTank.com, Mendeley\nData [15], and NapierOne [16].\
    \ Phishtank has datasets that break\ndown URLs into different features that detect\
    \ malicious emails. The data\nis confirmed phishing attempts, gathered collaboratively\
    \ by the\nregistered users, which are further reviewed by PhishTank operated by\n\
    <em>Cisco Talos Intelligence Group</em>. Mendeley Data is the dataset prepared\n\
    by Hannousse et al*.* (2021) with confirmed malicious and legitimate\nURLs with\
    \ their domain and sub-domain classifications [15]. NapierOne\nprovides a dataset\
    \ of documents often sent as attachments with malicious\ncontents. NapierOne is\
    \ managed by the School of Computing at Edinburgh\nNapier University [16].</p>\n\
    <p>This paper presents the DARTH framework, a novel multi-faceted solution\nto\
    \ the email phishing detection problem. DARTH deconstructs an email in\naccordance\
    \ with the email composite features such as body text and\nmetadata that are nearly\
    \ indistinguishable from valid emails. Each\ncomposite feature is analyzed by\
    \ its respective neural network model,\nand an Ensemble Neural Network (ENN) utilizes\
    \ the output of these models\nto determine phishing classification. The exemplary\
    \ multi-faceted DARTH\nmethod presented in this paper utilizes the following composite\n\
    features: email body text, the entropy of attached files, metadata of\nemail,\
    \ and embedded URLs contained anywhere within the email.</p>\n<ol start=\"2\"\
    >\n<li>Literature Review</li>\n</ol>\n<p>Traditionally, phishing detection research\
    \ has focused on methods for\nautomated phishing detection. This section presents\
    \ related work\ncovering different aspects of phishing detection. This section\
    \ begins\nwith a brief history of phishing, followed by an overview of the most\n\
    common phishing detection methods. Researchers have tackled this problem\ndifferently\
    \ over time. Some researchers have focused on machine learning\nmodels, while\
    \ others have focused on manual add-ins and natural language\nprocessing elements\
    \ on email text.</p>\n<p><strong>2.1 Origin and Types of Phishing</strong></p>\n\
    <p>Phishing, as defined in Merriam-Webster, is \"the practice of tricking\nInternet\
    \ users into revealing personal or confidential information which\ncan then be\
    \ used illicitly\" [33].</p>\n<hr>\n<p>Phishing Type        Description</p>\n\
    <hr>\n<p><em>Standard Phishing</em>  Stealing sensitive information by pretending\
    \ to be an\nauthorized person or an organization. It is not a<br>\ntargeted attack\
    \ and can be conducted for a large<br>\ngroup or for a mass attack.</p>\n<p><em>Malware\
    \ Phishing</em>   It introduces bugs/viruses into the victim's machine<br>\nand\
    \ network by convincing a user to click a link or<br>\ndownload an attachment\
    \ in order to install the<br>\nmalware. It is currently one of the most widely\
    \ used<br>\nform of a phishing attack.</p>\n<p><em>Spear Phishing</em>     In\
    \ contrast to the standard phishing where a large<br>\nnumber of users are attacked\
    \ at once, spear-phishing<br>\nis a targeted attack towards a big target like\
    \ CEOs,<br>\nCelebrities, etc. This requires intense research of<br>\nthe potential\
    \ victim to convince them into engaging<br>\nwith the scam.</p>\n<p><em>Smishing</em>\
    \           SMS + Phishing = SMISHING. In this type of attacks<br>\nthe SMS or\
    \ text messages are used to deliver the<br>\nmalicious links to the unsuspecting\
    \ user. The links<br>\nare often short of the actual URLs.</p>\n<p><em>Search\
    \ Engine       In this technique, the fraudulent sites are injected<br>\nPhishing</em>\
    \            into the search results often in the form of paid<br>\nads.</p>\n\
    <p><em>Vishing</em>            Vishing is a method where a hacker contacts the\
    \ user<br>\nover a phone call pretending to be from a known<br>\norganization\
    \ and tries to extract the sensitive<br>\nfinancial information from the user\
    \ like banking and<br>\ncredit card details.</p>\n<p><em>Pharming</em>       \
    \    It is a technically sophisticated form of phishing<br>\ninvolving the internet's\
    \ domain name system (DNS).<br>\nPharming reroutes legitimate web traffic to a\
    \ spoofed\npage without the user's knowledge, often to steal<br>\nvaluable information.</p>\n\
    <p><em>Clone Phishing</em>     In clone phishing attackers make changes to an<br>\n\
    existing email, resulting in a nearly identical<br>\n(cloned) email but with malicious\
    \ URLs and<br>\nattachments. This requires a compromise of an email<br>\naccount.</p>\n\
    <p><em>Man-In-The-Middle   Man-In-The-Middle (MITM) attack is when a hacker<br>\n\
    (MITM)</em>              eavesdrop into conversation among the two or more<br>\n\
    individuals. Hackers create a public Wi-Fi network<br>\nwhich unsuspecting users\
    \ join allowing the attackers<br>\nto capture information and transmit incorrect<br>\n\
    information including malware to the involved<br>\nparties.</p>\n<p><em>Business\
    \ Email      Business Email Compromise (BEC) involves a phony<br>\nCompromise</em>\
    \          email usually claiming to be an urgent request for<br>\npayment or\
    \ purchase from someone within or associated\nwith a target's company.</p>\n<h2><a\
    \ id=\"user-content-malvertising-------in-case-of-malvertising-the-attackers-post-amalicious-advertisement-on-the-legitimate-websitesthe-animation-or-video-or-links-within-theadvertisement-has-links-to-the-malicious-software-tosteal-information-from-the-users\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#malvertising-------in-case-of-malvertising-the-attackers-post-amalicious-advertisement-on-the-legitimate-websitesthe-animation-or-video-or-links-within-theadvertisement-has-links-to-the-malicious-software-tosteal-information-from-the-users\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>\n<em>Malvertising</em>\
    \       In case of Malvertising the attackers post a<br>\nmalicious advertisement\
    \ on the legitimate websites.<br>\nThe animation or video or links within the<br>\n\
    advertisement has links to the malicious software to<br>\nsteal information from\
    \ the users.</h2>\n<p>: Table 1: Types of Phishing and their brief description</p>\n\
    <p>The term \"phishing\" was coined by a then-teenager named Koceilah\nRekouche\
    \ [32]. Rekouche developed the first phishing attack. With a\nsmall group of teenagers,\
    \ Rekouche developed the AOHell software\ndesigned to steal the passwords of America\
    \ Online (AOL) users [32]. It\nwas arguably the first phishing software, and it\
    \ was used for stealing\npasswords and credit card information beginning in January\
    \ 1995.\nAOHell's phishing system was made publicly available, its release\nleading\
    \ to many other automated phishing systems over the years [32].</p>\n<p>Started\
    \ by teenagers and adopted by several other amateurs, phishing\nactivity spread\
    \ from AOL to other networks. Slowly, professional\ncriminals took notice of this\
    \ phishing activity and got involved in\nphishing schemes. Although phishing started\
    \ small, it grew to become one\nof the major cyber security threats worldwide\
    \ leading to large financial\nlosses to individuals, corporations, and even governments\
    \ [32].</p>\n<p>Phishing which started as very basic technology soon grew into\
    \ a\nsophisticated methodical attacks. As organizations started building\nalgorithms\
    \ to identify phishing attempts, hackers continued to invent\nnew ways to evade\
    \ the detection. Phishing attackers have constantly\ndeveloped new techniques\
    \ to hide their phishing attacks like Smishing,\nSpear phishing, Malvare phishing\
    \ and Malvertising.</p>\n<p>Humans are the weakest link in the phishing scheme\
    \ as they can be easily\nmanipulated for information or duped into clicking on\
    \ malicious links\nvia social engineering techniques.\_</p>\n<p><strong>2.2 URL-Based\
    \ Phishing Detection</strong></p>\n<p>Past studies have used methods of detecting\
    \ phishing attacks using URLs.\nDutta et al*.* (2021) mention that phishing techniques\
    \ are mainly\nclassified as technical subterfuge and Social Engineering. Technical\n\
    subterfuge such as Keylogging DNS poisoning uses a tool to attack, while\nsocial\
    \ engineering such as Spear phishing whaling tricks victims into\naccessing a\
    \ compromised URL [2]. Dutta et al*.* (2021) evaluate the\ndetection of social\
    \ engineering phishing attempts delivered via email.\_\nHaynes et al*.* (2021)\
    \ propose a lightweight phishing detection system\nto identify phishing URLs.\
    \ They have used NLP transformers and applied\nANN (Artificial Neural Networks)\
    \ [9]. Authors suggest that the models\nmay predict if the website is phishing\
    \ or not, just using the texts in\nthe URL by applying transformers on the texts.\
    \ Authors propose this idea\nto improve the speed of creating and validating models\
    \ as an edge\ncompared to other phishing techniques [9].\_While Haynes et al*.*\n\
    (2021) have provided groundbreaking works for the Neural Network\nModeling technique.\
    \ There is an opportunity to use the Neural Network\nmodeling technique for email\
    \ body text-based Natural Language\nProcessing.\_</p>\n<p>Existing phishing techniques\
    \ are based on source code that scrapes web\npages' content. Machine learning\
    \ techniques require essential manual\nfeature engineering and do not detect new\
    \ phishing offenses\neffectively.\u202FAljofey et al*.* (2020), in their research\
    \ on *Effective\nPhishing Detection Model Based on Character Level Convolutional\
    \ Neural\nNetwork from URL,\u202F*introduced a deep learning model that uses a\n\
    convolutional neural network (CNN) to evaluate the URLs of the websites\nto identify\
    \ malicious sites and potential for phishing. It captures the\nsequential pattern\
    \ of URL strings without prior knowledge about phishing\nand uses the sequential\
    \ features for faster classification. For\nperformance metrics, the model accuracy\
    \ is compared with traditional\nmodels using hand-crafted character embedding,\
    \ character level TF-IDF,\nand character level count vector features [13]. Using\
    \ the\nconvolutional neural network model and character level TF-IDF analysis\n\
    is crucial for this study as both techniques are important supplementary\nmethods\
    \ to build the entire process. Each of the previous studies has\ninvestigated\
    \ both ways individually. This study includes both approaches\nto create a set\
    \ of sophisticated phishing attack detection techniques.\_</p>\n<p>In traditional\
    \ machine learning techniques, website URLs are first\nanalyzed with different\
    \ hand-crafted features to improve detection\naccuracy. URLs are analyzed to perform\
    \ feature adaptation from phishing\nwebsites. Using these features, the engineers\
    \ constructed the training\nset using labeled features. On the other hand, the\
    \ convolutional neural\nnetwork (CNN) model requires less human effect. It identifies\
    \ individual\ncharacters from URLs based on prescribed character vocabulary and\
    \ then\nrepresents each character as a fixed-length vector using one-hot\nencoding\
    \ [13]. The model identifies similar characters that can be\nunnoticeable in website\
    \ URLs. One of the significant advantages of this\nmodel is that it does not have\
    \ to rely on third-party services for\ndetection. The study provides a unique\
    \ modeling technique that can aid\nthe build the new proposed set of models. However,\
    \ the study has not\ninvestigated multiple modeling techniques or included any\
    \ Natural\nLanguage Processing techniques.\_\_</p>\n<p>A Recurrent Neural Network\
    \ (RNN) is a neural network that uses\nsequential or time-series data, widely\
    \ used for language translation,\nnatural language processing, speech detection,\
    \ and image detection. All\nparameters are the same across each hidden layer,\
    \ meaning weights are\nthe same on each node. These features have been helpful\
    \ for the precise\ndetection of phishing attacks. Neeharika et al*.* have conducted\
    \ a study\nthat found that this neural network model has a high accuracy rate.\n\
    Also, they did not have to create extra features for model building\n[21]. The\
    \ model takes characters from sequentially listed URLs and\npredicts whether the\
    \ URL is part of a phishing attack or not using Least\nSquares Time Series units\
    \ [21]. This research can be convenient for\nthe proposed study because of its\
    \ performance and feature of eliminating\nmanual inputs for feature creation.\
    \ The proposed research also plans to\nexplore the email body text, and RNN has\
    \ the potential for detecting a\nparticular set of sequence data that may be linked\
    \ to phishing attacks.</p>\n<p>\_ Ensemble Neural Network (ENN) is a method where\
    \ many neural networks\nare used to solve a problem. Multiple neural networks,\
    \ regression, and\nclassification neural networks are analyzed. Findings reveal\
    \ that\nnumerous neural network ensemble are a better fit. The optimization\n\
    process uses covariance matrices calculated by the maximum likelihood\nalgorithm\
    \ under the Bayesian framework. The network does not calculate\nthe gradient,\
    \ which allows it to utilize complicated neural models and\nloss functions [29].\
    \ The appropriate networks are selected from the\navailable set of neural networks\
    \ to achieve an effective ensemble, using\nan approach called Genetic Algorithm-based\
    \ Selective Neural Network\nEnsemble (GASEN). GASEN term proposed by Zhi-Hua et\
    \ al*.* (2002), trains\nneural networks, assigns random weights to the networks,\
    \ evolves, and\nemploys a genetic algorithm to find the better fit among available\n\
    networks. The study by Zhi-Hua et al. (2002) shows that compared to\nbagging and\
    \ boosting, ENN can create a better neural network with\nsmaller sizes [28].</p>\n\
    <p>Soon et al*.* (2020) have used ENN, RNN, and FFNN on phishing datasets\nand\
    \ have produced several highly accurate models using different\nhyperparameters.\
    \ The researchers have run all three models using two\nscenarios to improve accuracy\
    \ detection. In the first scenario, all\nthree models have been executed using\
    \ a range of 1-18 input layers. All\nthe models have been performed for the second\
    \ scenario using a 0.001 --\n0.1 learning rate. The final experiment shows that\
    \ ENN has produced a\nbetter accuracy score than RNN and FFNN. The researchers\
    \ have concluded\nthat ENN requires fewer neurons than the other two models. A\
    \ lower\nlearning rate produces better results for phishing detection due to its\n\
    ability to reduce error [17].\_</p>\n<p>Various Machine Learning techniques have\
    \ been used to identify phishing\nattempts. Dharani et al*.* (2021) proposed using\
    \ Machine Learning\nMethods such as Random Forest Algorithm and Extreme Gradient\
    \ Boosting\n(XGBoost) Algorithm for efficient and accurate phishing website\n\
    detection on its Uniform Resource Locator [5]. Most research focused\non identifying\
    \ phishing attempts by evaluating the URLs rather than the\nemail content and\
    \ patterns. Another similar study,\_Akinyelu et al*.*\n(2014),*\_<em>in the paper</em>\_\
    Classification of Phishing Email Using Random\nForest Machine Learning Technique,*\_\
    mentions that most tools and\ntechniques are used to flag emails to identify spam\
    \ emails. In contrast,\nphishing detection tools are not standard [3]. Most phishing\
    \ detection\ntechniques involve scanning URLs through block-listed [4] sets of\
    \ URLs\npreviously flagged as malicious. While these studies present essential\n\
    aspects of the Random Forest classification modeling technique for the\nURL portion\
    \ of the emailing system, these studies are based on one\nmethod that can be ineffective\
    \ for detecting early phishing. The\npaper's authors compare multiple modeling\
    \ techniques and Random Forest\nto build the optimal machine learning model. This\
    \ research study also\ninvestigates other avenues of the emailing system, such\
    \ as the text body\nof the email processing using Natural Language Processing.\_\
    </p>\n<p>Abu-Nimeh et al*.* (2009*)*, in their work,\u202F <em>Distributed Phishing\n\
    Detection by Applying Variable Selection Using Bayesian Additive\nRegression Trees,</em>\u202F\
    focused on detecting phishing emails on mobile\ndevices. They have used distributed\
    \ detection techniques applying\nvariable selection using Bayesian additive regression\
    \ trees. The study\nnotes that BART improves accuracy when combined with other\
    \ machine\nlearning classifiers. The study concludes that future work is necessary.\n\
    However, BART can be tool to improve accuracy [6].\_</p>\n<p><strong>2.3 Natural\
    \ Language Processing on Text</strong></p>\n<p>While other studies have focused\
    \ on a single method*,* Ramanathan, &amp;\nWechsler, H*.* (2012) propose a multi-layered\
    \ methodology called\nphishGILLNET. The researchers applied the methods three\
    \ times: Fisher\nsimilarity, second Adaboost, and third used NLP techniques on\
    \ misspelled\nwords to identify phishing [10]. The authors used a large dataset\
    \ of\npublic corpus emails (about 400,000) to conduct the study and noted\noutstanding\
    \ results. \_ The paper mentioned that users of social media,\nsuch as Internet\
    \ Messages, chat, blog posts, etc., could apply the\nphishGILLNET methods [10].</p>\n\
    <p>Attackers continually evolve their methods to evade advances in\nprotection\
    \ and exploit newly discovered vulnerabilities or events. The\ncurrent anti-phishing\
    \ products use a combination of blocklist,\nheuristic, visual, and machine learning\
    \ to detect the attacks.\_Sahingoz\net al*.* (2019)\_promote using classification\
    \ algorithms and natural\nlanguage processing base features to see malicious links\
    \ and emails in\nreal-time rather than from a list of databases. Experiments used\
    \ a newly\nconstructed test dataset utilizing the Random Forrest model with\n\
    NLP-based features that created an accuracy rate of 99.98% [11]. The\nstudy can\
    \ be helpful as it combines machine learning techniques with\nNatural Language\
    \ Processing. However, it still lacks research on how an\nintegrated approach\
    \ can improve phishing detection using URLs and text\nbody text.\_\_\_</p>\n<p>Since\
    \ phishing detection is a classification case,\_Sahingoz et al*.*\ndeploy a model\
    \ which extracts keywords using the \"frequency-inverse\ndocument\" algorithm.\
    \ The drawback of this technique is that the model is\nhelpful with the English\
    \ language. Also, it tends to produce many false\npositives, although the model\
    \ has a high accuracy rate. The model\ndetermines website legitimacy, detects\
    \ possible target domains using a\nsearch engine, and determines whether the domains\
    \ in the query are\nlegitimate or not. It can also study offline websites using\
    \ a support\nvector machine. The Adaptive Regularization of Weights algorithm\
    \ is used\nfor fraud detection on online websites. These are all non-linear\n\
    approaches for detecting phishing attacks [11].\_</p>\n<p>Sanglerdsinlapachai\
    \ et al.\u202F(2010) have added a new dimension to the\nliterature by focusing\
    \ on domain top page similarity. Their\nresearch,\u202F<em>\"Using Domain Top-Page\
    \ Similarity Feature in Machine\nLearning-Based Web Phishing Detection,\"</em>\
    \ explored\u202Fdomain top page\nsimilarity to detect any new phishing websites.\
    \ Authors note the high\nsuccess rate of detecting phishing, though the samples\
    \ were tiny [7].\nOn the other hand, Abbasi et al*.* (2021)*\u202F*argue that\
    \ the root cause of\nthe problem is the internet users' lack of ability to identify\
    \ malicious\nemails or products. The study introduced the phishing funnel model\n\
    (PFM). It is a design artifact that predicts users' susceptibility to\nphishing\
    \ outlets such as websites, emails, etc. [12]. For the target\nvariable, the research\
    \ focused on user behavior regarding interaction\nwith phishing attacks instead\
    \ of predicting whether the links of\nwebsites or emails are related to phishing\
    \ attacks. Using over 1,200\nemployees and around 49,000 phishing interactions,\
    \ the model has\noutperformed other existing models, reducing the number of phishing\n\
    attacks as it was able to make users classify incoming emails and\nattachments\
    \ as malicious items [12].</p>\n<p>Despite the large sample size, the question\
    \ is, can we avoid human\nerrors by training machines to identify phishing. Alhogail\
    \ &amp; Alsabih et\nal*.* (2021) research seemed to look for the answer*.* Alhogail\
    \ &amp;\nAlsabih\u202Fet al*.* (2021).\u202Fhave emphasized the importance of\
    \ using machine\nlearning methods to detect phishing instead of relying on humans.\
    \ In\ntheir studies on <em>Applying machine learning and natural language\nprocessing\
    \ to detect phishing emails,</em> authors propose a deep learning\nmethod using\
    \ Graph convolutional network (GCN) and natural language\nprocessing on the email\
    \ body text. The method is more efficient at\ndetecting zero-day phishing emails\
    \ than other methods [8]. However,\nthe authors concluded that more study is necessary\
    \ to confirm the\nfindings.</p>\n<p>Regarding PFM, the proposed research will\
    \ address some of the gaps that\nneed to be addressed.\_ Alhogail &amp; Alsabih*\u202F\
    <em>et al</em>.* analysis brings up\nthree research gaps. First, prior works have\
    \ not investigated the\ndetails of user behavior as a target variable. Second,\
    \ previous studies\nhave focused on \"single decision,\" such as binary classification\
    \ of the\nmalicious or non-malicious status. Third, prior models did not emphasize\n\
    tools much when studying users' susceptibility to attacks [12].\_This\nresearch\
    \ paper addresses the second gap in binary classification's\n\"single decision.\"\
    </p>\n<p>**<br>\n**</p>\n<p><strong>2.4 Email Attachment Phishing Detection</strong></p>\n\
    <p>Cybercriminals target users by sending malicious attachments through\nemails.\
    \ The attachments are sent through different file formats such as\npdf, SVG, XML,\
    \ JSON, etc. Users often download the files by mistake,\ncontaining malware that\
    \ installs automatically on their devices. This\nallows the attackers to gain\
    \ personal information through fraudulent\nactivities such as transferring money\
    \ from victims' banks, stealing\ntrade secrets from organizations, and even threatening\
    \ individuals for\ndifferent motives. Machine learning models can be instrumental\
    \ in\ndetecting attacks through attachments. Akinyelu et al*.* (2014) have\nstudied\
    \ branches in emails using Random Forest models to improve the\naccuracy rate\
    \ for phishing detection. This technique has produced an\naccuracy rate of 99.7%\
    \ compared to other machine learning models that\nhave made 97% [3].</p>\n<p>Phishing\
    \ attacks lure individuals to access malicious email content,\nincluding attachments\
    \ and links. Attackers often use one or more of the\nphishing techniques listed\
    \ in Table 2 to persuade users to access the\ninformation by downloading an attachment\
    \ (malware) or simply clicking on\na link that installs malware into the victim's\
    \ system [35].</p>\n<hr>\n<p>Techniques      Description</p>\n<hr>\n<p><em>Authority</em>\
    \     Attackers claim to be someone from reputable\norganizations to ask for the\
    \ victim's information</p>\n<p><em>Urgency</em>       Attackers ask victims to\
    \ respond to a claim in an\nurgent manner</p>\n<p><em>Reciprocity</em>   Attackers\
    \ claim to favor victims using stated service</p>\n<p><em>Social Proof</em>  Attackers\
    \ try to gain information by saying others have\nresponded to the claim.</p>\n\
    <p><em>Reward</em>        Attackers offer a reward to the victims for a response.</p>\n\
    <p><em>Loss</em>          Attackers claim victims will deal with some form of\n\
    loss if they don't respond.</p>\n<h2><a id=\"user-content-scarcity------attackers-offer-a-limited-amount-of-opportunity-to-thevictims-such-as-a-claim-for-the-first-20-responders\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#scarcity------attackers-offer-a-limited-amount-of-opportunity-to-thevictims-such-as-a-claim-for-the-first-20-responders\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>\n<em>Scarcity</em>\
    \      Attackers offer a limited amount of opportunity to the\nvictims such as\
    \ a claim for the first 20 responders.</h2>\n<p>: Table 2. Attachment-based phishing\
    \ techniques [35]</p>\n<p>Using Table 2, Williams et al*.* (2018) mentioned two\
    \ theoretical\nframeworks that have been applied to detecting these attacks. The\n\
    Suspicion, Cognition, and Automaticity Model (SCAM) is a theory that\ntakes a\
    \ company's users' knowledge, beliefs, and habits to analyze\nusers' susceptibility\
    \ to phishing attacks [35]. Protection Motivation\nTheory (PMT) is a theory that\
    \ has been applied to generic security\nbehavior to understand users' perceptions\
    \ of these types of attacks\n[35]. For the research, Williams et al*.* (2018)\
    \ have created two\nhypotheses -- 1) users will respond to authority-based attacks,\
    \ and 2)\nwill respond to urgency-based attacks. Given the results from the\n\
    statistical modeling, both hypotheses have been validated with a z score\nof 72.68,\
    \ p &lt; 0.001 for hypothesis 1, and a z score of 39.12, p &lt;\n0.001 for hypothesis\
    \ 2.</p>\n<p><strong><br>\n2.5 Moving to a Multi-Faceted Approach</strong></p>\n\
    <p>Prior studies are highly based on analyzing URLs and body text\nseparately[11,\
    \ 5]. Abbasi et al*.* introduce a phishing detection\nstudy using user behavior\
    \ only [12]. These phishing detection methods\nhave been primarily uni-directional\
    \ since the authors have analyzed\nphishing detection of emails using unique composite\
    \ features along with\nNLP or machine learning model separately. For example,\
    \ Dharani et al*.*\n(2021) have used the non-neural network model XGBoost on text\
    \ data from\nURLs only [5]. Furthermore, others, such as Aljofey et al., have\
    \ used\nneural network models such as Convolutional Neural Network on URL data\n\
    [13]. These studies are primarily uni-directional as authors have used\none feature\
    \ and one technique to analyze phishing attacks. Few studies\nhave studied email\
    \ features using combination techniques such as NLP\ntransformation and Neural\
    \ Networks. Haynes et al. used NLP for URL text\npreprocessing and Artificial\
    \ Neural Network for phishing attack\ndetection [9]. Sahingoz et al*.* (2019)\
    \ have analyzed the emails'\nbody text using NLP preprocessing and random forest\
    \ modeling techniques\nfor phishing attack detection [11]. Although these simplistic\n\
    multi-faceted studies have added more tools to the algorithm for\nphishing attack\
    \ detection, this research still lacks a comprehensive\ndesign of the algorithm\
    \ that includes all the composite features of\nemail such as body text, URLs,\
    \ metadata, and attachments for analysis\nand detection. Comprehensive design\
    \ is crucial as attackers constantly\nupgrade and build more sophisticated techniques\
    \ to target people for\nstealing confidential information or other cyber attacks.\
    \ Relying on a\nsingle method is not a good reliable long-term solution as attackers\
    \ may\novercome that check. Using as much information available to design a\n\
    phishing detection model is a promising approach. This multi-faceted\nphishing\
    \ detection approach utilizes various composite features of the\nemails in the\
    \ algorithm for data processing and modeling for attack\ndetection.</p>\n<p>The\
    \ proposed multi-faceted study introduces a new method called DARTH\nFramework.\
    \ It helps us to address the gaps that currently exist in\nphishing detection.\
    \ It contains clear target variables, creates models\nto lower users' susceptibility\
    \ to attacks, and relies on multiple\nmethods to design models for phishing attack\
    \ detection. The framework\nfills the gap in the existing literature for early\
    \ phishing detection.\_\_</p>\n<p>This research combines multiple machine learning\
    \ modeling techniques and\nmachine learning on all the available avenues of emailing\
    \ systems.\_\_The\nresearch hypothesizes that the DARTH framework can combine\
    \ the natural\nlanguage processing of email text and machine learning algorithms\
    \ on the\nmetadata to identify phishing email attempts.</p>\n<p>3 Methods</p>\n\
    <p>As covered in Section 2, there is a gap in phishing detection techniques\n\
    and prior research. Most researches focus on one aspect of detecting\nphishing\
    \ emails, while few studies attempted a simplistic multi-feature\napproach. In\
    \ this research, the proposed algorithm addresses the problem\nby including multiple\
    \ composite email features, preprocessing them, and\nexecuting simulation to predict\
    \ whether emails being phishing or\nlegitimate. In this Section, the intrinsic\
    \ details of the DARTH\nframework are presented.</p>\n<p><strong>3.1 The DARTH\
    \ Framework</strong></p>\n<p>The novel DARTH framework breaks emails into several\
    \ parts such as the\nbody texts, the embedded URLs in the emails, the email headers,\
    \ and\nemail attachment metadata. The data extracted from the emails are first\n\
    processed to vectorize the data and add composite data to add more\nfeatures to\
    \ the data. The pre-processed data are analyzed through\nvarious individual machine\
    \ learning models. As a final step, the output\nfrom the individual models is\
    \ fed into an ensemble neural network. The\noutput of the ensemble model is to\
    \ predict if the is phishing or\nlegitimate. An important aspect of the DARTH\
    \ framework is that the\nindividual models like URLs and attachments are trained\
    \ on an external\ndataset published in prior studies and research, more details\
    \ are\ncovered in Section 3.5. The framework is flexible and allows the\naddition\
    \ of any new neural network models on individual composite\nfeatures of the emails\
    \ or a more complex model to improve the accuracy\nof the output. Figure 1 explains\
    \ the DARTH framework and its various\nfacets. Sections 3.2 to 3.5 covers each\
    \ aspect of the DARTH framework in\ndetail.</p>\n<p><a target=\"_blank\" rel=\"\
    noopener noreferrer\" href=\"media/image1.tiff\"><img src=\"media/image1.tiff\"\
    \ alt=\"Diagram Description automatically generated\" style=\"max-width: 100%;\"\
    ></a>{width=\"4.8125in\"\nheight=\"1.9833333333333334in\"}</p>\n<p><strong>Fig.\
    \ 1.</strong> The DARTH Framework Basic Architecture</p>\n<p><strong>3.2 The Ensemble\
    \ Model</strong></p>\n<p>The final layer of the DARTH Framework is the ensemble\
    \ model which takes\ninputs from various other models, primarily three different\
    \ models\npredicting the output of the email being malicious or legitimate based\n\
    on an individual composite feature of the email data. Darth Ensemble is\na two-layer\
    \ ensemble, where it takes the output of the email body text\nmodel, the embedded\
    \ URLs model predictions, and the prediction output on\nthe emails header metadata\
    \ to the final layer. The model is built to\ntake the inputs from the email attachments\
    \ as well as an additional\nmodel. This ensemble model is actually the ensemble\
    \ of other small\nensemble models and the results from each model are evaluated\
    \ in detail\nin Sections 4 and 5.</p>\n<p>\_<strong>3.3 Feature Models</strong></p>\n\
    <p>As discussed in Sections 3.1 and 3.2, there are several individual\ncomposite\
    \ features of the emails which are evaluated in detail to get\nthe individual\
    \ predictions prior to combining them into an ensemble\nmodel. Over 50,000 emails\
    \ were evaluated with a mix of legitimate and\nmalicious emails. Sections 3.3.1\
    \ to Section 3.3.4 covers each composite\nfeature in detail.</p>\n<p>The composite\
    \ feature models are neural network models as it was\nrecorded to give superior\
    \ results. However, additional modeling\ntechniques like Logistic Regression,\
    \ RandomForest, and XGBoost model are\nalso created to compare the results to\
    \ that of the neural network model.\nA neural network model is designed just like\
    \ the human brain where\ninformation is gathered, processed in neurons, and then\
    \ can provide\npredictions in terms of the categorical variable or continuous\
    \ variable.\nThere are three types of layers -- input, dense, and output. The\
    \ input\nlayer captures all the data, the dense layer processes the data and\n\
    learns from the dataset, and the output layer provides prediction. Deep\nlearning\
    \ is a subset of machine learning which uses dense layers in the\nneural network\
    \ to learn from the data at a granular level. Each layer\ncontains neurons that\
    \ learn from the data and assigns weights to all the\ncomposite features. The\
    \ output layer classifies whether an email is spam\nor not.</p>\n<p><strong>3.3.1\
    \ Email Body Text Model</strong></p>\n<p>The email body text was evaluated in\
    \ detail as it has the most profound\nimpact on the receiver of the email. Email\
    \ texts can trick an\nunsuspecting individual to access malicious content by clicking\
    \ on the\nembedded links, downloading attachments or sharing sensitive personal\n\
    details with the attacker. Phishing emails often have one of two\nimportant aspects,\
    \ they \"masquerade\" the actual identity of the sender\nto be someone trusted\
    \ or they create an \"urgency\" in the mind of the\nreceiver of the email to take\
    \ quick action without thinking a lot about\nthe authenticity of the email. For\
    \ example, an email from a well-known\ne-commerce website telling the receiver\
    \ that their order was canceled\ndue to a problem and they must click on the link\
    \ to confirm their\npayment details. The user may actually access the link and\
    \ provide\nsensitive information like password and credit card details to the\n\
    spurious webpage.</p>\n<p>The email text data is analyzed using non-parametric\
    \ methods like\nclustering to group them into similar groups using the KMeans\
    \ clustering\ntechnique to understand if there is a pattern to the phishing emails\
    \ and\ntext can be used to identify such attempts. The pattern of such\nmalicious\
    \ emails includes words like \"click now\", \"urgent\",\n\"immediately\", \"now\"\
    \ etc.</p>\n<p>Various NLP techniques were employed to analyze the texts like\n\
    \"word2vec\", \"topic modeling\" and \"BERT\". The final model was built using\n\
    the transfer learning from the BERT [31], which is a pretrained\nEnglish NLP model\
    \ and is used to classify the email text as a phishing\nattempt or legitimate.\
    \ A common email phishing technique is posing as\nAmazon for a deep discount on\
    \ a product or an official email from\nMicrosoft. Such phishing attempts then\
    \ hide their actual URLs under the\npopular domain names for Microsoft it can\
    \ be <em>Microsoft-sales-nk.com,</em>\nto lure users into accessing the link thinking\
    \ it's from Microsoft. This\nsentiment is captured from body text and sent as\
    \ a composite feature to\nneural networks. As discussed earlier, a common technique\
    \ is to create a\nsense of urgency by urging receivers to click on links with\
    \ little\nthought. The urgency of the texts can be measured quantitatively using\n\
    Natural Language Processing. This composite feature is passed on to\nneural networks\
    \ as additional data. Thus, two new features are\nintroduced, \"Masquerade-ness\"\
    \ and \"Urgent-ness\".</p>\n<p>BERT is short for \"Bidirectional Encoder Representations\
    \ from\nTransformers\", it's a deep learning model trained upon the Wikipedia\n\
    articles. It is a bidirectional model which helps in analyzing the text\nin both\
    \ directions of the target word such that it can predict the\nprevious text as\
    \ well as the next text based on the surrounding words.\nDue to its training on\
    \ a huge dataset of Wikipedia articles, it is\nextremely powerful and has been\
    \ used in the industry for various tasks\nlike sentiment analysis, text prediction,\
    \ chatbots, auto-completion of\nthe queries and email, etc. Using BERT as transfer\
    \ learning proves to be\na powerful tool in predicting the outcome of the email\
    \ texts malicious\nor legitimate.</p>\n<p><strong>3.3.2 Embedded URLs Model</strong></p>\n\
    <p>This featured model is built upon the embedded URLs in the emails. One\nof\
    \ the major patterns noticed as part of email text clustering and topic\nmodeling\
    \ of phishing emails is that the users are urged into accessing a\nlink embedded\
    \ in the email. It's pertinent that the URLs to be analyzed\nas a perfectly normal-looking\
    \ email from a trusted sender may contain a\nmalicious link. In the event of man-in-the-middle\
    \ attacks, where an\nattacker might access the conversations and relay an updated\
    \ message\nwith a malicious link, or in the event of an account being compromised\n\
    the emails received may appear to be trustworthy but may have the\nmalicious content.\
    \ It's essential to analyze each URLs even if the\nreceiver trusts the sender.\
    \ The URLs have various vital features like\nsubdomains, top domains, suffixes,\
    \ age of the URLs, etc. The features of\nthe URLs were trained on an independent\
    \ dataset with verified phishing\nfrom phishtank.com website data and Hannousse\
    \ et al*.* (2021) published\ndataset. This model allows models to be created using\
    \ external data and\nadded to the ensemble. Ensembling models trained with external\
    \ data\nprovided valuable information to the neural networks to better detect\n\
    phishing from the externally trained models.</p>\n<p>The embedded URLs data from\
    \ the email were analyzed through the\npre-trained models, including Logistic\
    \ Regression, XGBoost, and Neural\nNetwork models, to predict the legitimate or\
    \ malicious links. This model\nis to identify the malicious emails solely based\
    \ on the probability of\nthe embedded URLs being malicious or not. The output\
    \ from the neural\nnetwork model is sent further into the ensemble model.</p>\n\
    <p><strong>3.3.3 Email Headers Model</strong></p>\n<p>A crucial part of every\
    \ email is the header section which contains\nimportant information about the\
    \ email and can help determine if the\nemail can be malicious. Even though it's\
    \ an integral part of any email,\nthe content of the header is not immediately\
    \ visible to the user and is\neasy to ignore. The headers of the email consist\
    \ of a large amount of\ninformation such as the sender details, the email's route\
    \ to get to the\ninbox (computers' addresses that an email may have been transferred\n\
    through), MIME-version (Multipurpose Internet Mail Extension), and the\nattachment\
    \ counts, etc. This information was used as test data to\npredict whether the\
    \ email was suspicious to be malicious. The original\nmodel is built on the train\
    \ data from the confirmed malicious emails.</p>\n<p>The header data was analyzed\
    \ using Logistic Regression, a Random Forest\nmodel, and a Neural Network model.\
    \ The results were compared, and the\nneural network model output was sent further\
    \ to the ensemble model for\nfinal prediction. This model predicts whether the\
    \ email is malicious or\nlegitimate solely on the header information. The email\
    \ may contain\nseveral tens of headers, but for this analysis, only the first\
    \ 11\nheaders per email were used.</p>\n<p><strong>3.3.4 Additional Models</strong></p>\n\
    <p>Some additional models can also be added to the framework, for example\nemail\
    \ attachments. This model analyzes the entropy of the email\nattachments and compares\
    \ that to the typical entropy of such file types.\nIf there is a significant difference\
    \ in the entropy of the attachment\ncompared to the expected entropy then it can\
    \ be a malicious email.</p>\n<p>NapierOne has published a large dataset of malicious\
    \ files of different\ntypes. A small subset of the NapierOne dataset was used\
    \ to calculate the\nentropy of the different file types. The entropy measures\
    \ the randomness\nof the data in a file and if the entropy value is higher than\
    \ expected\nit could be due to any hidden executables in the simple file types\
    \ (like\ntext files). The entropy of different file types is calculated and\n\
    published in Figure 2. It should be noted that if the entropy values of\ncertain\
    \ files type are not within the threshold doesn't necessarily mean\nthat it's\
    \ malicious, however, this is important information that must\nbe accounted for\
    \ to identify phishing attempts.</p>\n<p>Though the documents were analyzed, the\
    \ email attachments are not part\nof the final ensemble model.</p>\n<p><a target=\"\
    _blank\" rel=\"noopener noreferrer\" href=\"media/image2.png\"><img src=\"media/image2.png\"\
    \ alt=\"Chart, waterfall chart Description automatically generated\" style=\"\
    max-width: 100%;\"></a>{width=\"4.8125in\"\nheight=\"2.5069444444444446in\"}</p>\n\
    <p>Fig. 2. Calculated Entropy for each file type shows how the entropy\nvaries\
    \ by the type of file with non-malicious content</p>\n<p><strong>3.4 Parsed and\
    \ Processed Data</strong></p>\n<p>One of the major aspects of any data analysis\
    \ is data processing. It is\nimportant that data is appropriately handled to extract\
    \ as much\ninformation as possible. The models, such as URL models and attachments,\n\
    were trained on externally published data; see Section 3.5 for the\ndetails of\
    \ the data sources. However, recent emails with phishing\nattempts are required\
    \ to test the models accurately. The authors'\npersonal emails were used for model\
    \ building and testing. To use the\ndata appropriately, it's required to parse\
    \ the information from the\nemails, for example: extract the email body text,\
    \ parse out the embedded\nURLs and separate the header information.</p>\n<p><strong>3.4.1\
    \ Email Data Extraction</strong></p>\n<p>Email data is comprised of a .msg filetype\
    \ that stores the entire\ncontent of an email in text format. This format of files\
    \ can be\ndownloaded from email providers as mbox files. As such, each .msg file\n\
    contains the entire data of an email. Data is parsed to read the email\nheaders,\
    \ body text, attachment counts, and, if there are any\nattachments, the file type.</p>\n\
    <p>For email body text, the email is scanned for the content type. If the\nbody\
    \ is plain text, then the entire body is used as text. However, if\nthe body text\
    \ is in HTML format, all visible texts are harvested and\nstored as plain text.\
    \ Similarly, if the text is base64 encoded, it's\nfirst decoded then the text\
    \ is stored. The stored text is pre-processed\nbefore running any further models.</p>\n\
    <p>The header information is extracted from the email headers and stored as\n\
    a dataset with multiple features as the header information. Similarly,\neach email\
    \ is scanned for any URLs. Once the URLs are identified, they\nare stored for\
    \ further processing covered in Section 3.4.2.</p>\n<p>Since data extraction is\
    \ a performance-intensive process, SMU's\nManeFrame II HPC (High-Performance Computing)\
    \ was extensively used to\ncomplete the data extraction.</p>\n<p><strong>3.4.2\
    \ Preprocessing Data</strong></p>\n<p>The email body text analysis requires Natural\
    \ Language Processing (NLP)\ntechniques. This requires the text data to be tokenized\
    \ for any further\nresearch. Tokenization breaks down text into words which are\
    \ called\ntokens. It establishes the meaning and context of the text by analyzing\n\
    the sequences of the words. A new feature is added to the data frame,\ncontaining\
    \ word token counts from the text. Using the new feature, we\ncan notice the frequency\
    \ of different token lengths for both legitimate\nand malicious emails. Malicious\
    \ emails generally have 100 or few tokens,\nas shown in Figure 3.</p>\n<p><a target=\"\
    _blank\" rel=\"noopener noreferrer\" href=\"media/image3.png\"><img src=\"media/image3.png\"\
    \ alt=\"\" style=\"max-width: 100%;\"></a>{width=\"4.8125in\" height=\"2.5833333333333335in\"\
    }</p>\n<p>Fig. 3. Email token length by the legitimate and malicious emails. The\n\
    malicious emails have lower token length compared to that legitimate\nemails.</p>\n\
    <p>Once the URLs are extracted from the emails, a master list of URLs is\ncreated.\
    \ These URLs have lots of useful metadata which is extracted\nduring pre-processing.\
    \ The extracted information has more than 50\nfeatures about the URLs, the features\
    \ include top-level domains,\nsubdomains, and suffixes. The pre-processed URL\
    \ data is sent to the\nvarious models to predict if the URL is suspected to be\
    \ malicious or\nnot. The predicted outcome is stored against each URL. Any new\
    \ URL\nextracted from the email is scanned through the master list to capture\n\
    the predicted outcome for the existing URLs and if a new URL is found,\nits pre-processed\
    \ similarly to other URLs and added to the master list.</p>\n<p><strong>3.5 Data\
    \ Sources</strong></p>\n<p>The data is sourced from various different reputed\
    \ places to design the\nDARTH framework. The training data is taken from different\
    \ data sources\nlike phishtank.com and Hannousse et al*.* (2021) published a URLs\n\
    dataset. NapierOne has published a dataset of malicious documents which\nis useful\
    \ in calculating the entropy of such files. For emails, the\npersonal emails of\
    \ the authors have been used to manually identify the\nphishing attempts to be\
    \ used as an test and train dataset for the model.\nThe details of various data\
    \ sources used are listed in Table 3.</p>\n<p>+-------------------+--------------------------------------------------+\n\
    | <strong>Data Sources</strong>  | <strong>Description</strong>              \
    \                    |\n+===================+==================================================+\n\
    | <em>phishtank.com</em>   | Phishtank.com is an internet community website  \
    \ |\n|                   | where phishing data is published for anyone to   |\n\
    |                   | download. The website is managed by Cisco Talos  |\n|  \
    \                 | Intelligence data. It is an open-source platform |\n|    \
    \               | for any of its registered users to submit URLs   |\n|      \
    \             | suspected of phishing. The Cisco team verifies   |\n|        \
    \           | the submitted request and any additional         |\n|          \
    \         | information provided along with the request. If  |\n|            \
    \       | Cisco teams deem it to be phishing, then the     |\n|              \
    \     | link is then added to a list of phishing         |\n|                \
    \   | websites. There are currently about 4900         |\n|                  \
    \ | confirmed phishing URLs available at             |\n|                   |\
    \ phishtank.com.                                   |\n+-------------------+--------------------------------------------------+\n\
    | <em>URLs dataset     | Hannousse et al</em>.* (2021) published a URLs      \
    \ |\n| from the research | dataset with the research paper <em>Web page      \
    \  |\n| paper</em> Web page   | phishing detection* [15]. This dataset has an\
    \  |\n| phishing          | equal number of phishing and non-phishing URLs   |\n\
    | detection         | and the URL metadata. This dataset includes      |\n| <em>[15].</em>\
    \         | various features of the URLs including domain,   |\n|            \
    \       | sub-domain, age of the domain, number of hits    |\n|              \
    \     | etc.                                             |\n+-------------------+--------------------------------------------------+\n\
    | <em>UCI Spam         | The UCI dataset is a list of emails that are     |\n\
    | dataset</em>          | classified as spam and non-spam email with email |\n\
    |                   | metadata [30]. The texts are analyzed for NLP. |\n|    \
    \               | The attachments and URLs available in the email  |\n|      \
    \             | is used against the respective models for URLs   |\n|        \
    \           | and documents.                                   |\n+-------------------+--------------------------------------------------+\n\
    | <em>NapierOne Mixed  | NapierOne Mixed File Dataset [16] published a  |\n| File\
    \ Dataset</em>     | list of file types and 5000 files of each file   |\n|   \
    \                | type. In addition, the list contains some common |\n|     \
    \              | ransomware affected/encrypted files of the same  |\n|       \
    \            | files in those 5000 examples. This study used    |\n|         \
    \          | only non-ransomware-affected files to calculate  |\n|           \
    \        | the entropy of a typical file type.              |\n+-------------------+--------------------------------------------------+\n\
    | <em>Author's         | Authors' emails with confirmed spam and phishing |\n\
    | personal emails   | attempts are downloaded and read by a Python     |\n| with\
    \ phishing     | script along with the metadata, email body text, |\n| attempts</em>\
    \         | attachments, and embedded URLs. The data is used |\n|            \
    \       | as model verification for NLP-based text         |\n|              \
    \     | classifier models as well as URL verification    |\n|                \
    \   | against the phishtank.com dataset.               |\n+-------------------+--------------------------------------------------+\n\
    | <em>Sample text      | To train the BERT model to identify the emails   |\n\
    | messages to train | urging users to acts swiftly with little thought |\n| models\
    \ in         | to the content of the email, it was required to  |\n| detecting\
    \         | capture sample email text to identify such       |\n| \"urgent-ness\"\
    \ in  | attempts. Sample email text taken from below     |\n| the messages</em>\
    \     | sources used to train the model to identify the  |\n|                \
    \   | \"urgentness\" in the emails.                      |\n|                \
    \   |                                                  |\n|                  \
    \ | 1.  Mobile Ecosystem Forum (Feb'2022):           |\n|                   |\
    \     <a href=\"https://mobileecosystemforum.com\" rel=\"nofollow\">https://mobileecosystemforum.com</a>\
    \             |\n|                   | /2022/02/18/top-five-text-message-scams-in-2021/\
    \ |\n|                   |                                                  |\n\
    |                   | 2.  Panda Security:                              |\n|  \
    \                 |     <a href=\"https://www.pandasecurity\" rel=\"nofollow\"\
    >https://www.pandasecurity</a>                    |\n|                   | .com/en/mediacenter/security/text-message-scams/\
    \ |\n+-------------------+--------------------------------------------------+</p>\n\
    <p>: Table 3: Dataset and sources with the description of each data source.</p>\n\
    <p>4 Results</p>\n<p>The DARTH framework for phishing email detection contains\
    \ an ensemble\nmodel, which is composed of four neural network models using email\
    \ body\ntext, embedded URLs, email metadata, and attachment datasets. In order\n\
    to evaluate the performance of this model, six other ensemble models are\ncreated\
    \ using all four datasets individually and a combination of those\ndatasets. Ensemble\
    \ model 1 uses the email body text dataset. The data\nhas been preprocessed using\
    \ NLP and trained with the BERT modeling\ntechnique for phishing detection. Ensemble\
    \ Model 2 uses an embedded URL\ndataset, and Ensemble Model 3 uses an email metadata\
    \ dataset. All three\nof those models are built using the neural network modeling\
    \ technique.</p>\n<p>All other models are different combinations of the above\
    \ three models\ninto an ensemble neural network model to predict whether the email\
    \ is\nmalicious or legitimate. The ensemble of Model 1 and Model 2 is called\n\
    Model 4 which consists of the predictions based on the BERT model for\nbody texts\
    \ and the predictions based on the embedded links in the email.\nModel 5 is a\
    \ combination of model 3 and model 2 which includes\npredictions based on the\
    \ email headers and the embedded links.\nSimilarly, model 6 is an ensemble of\
    \ Models 1 and 3. And the final model\nis the ensemble of all three models which\
    \ takes into account the\npredictions based on the email body texts, embedded\
    \ links in the emails,\nand the header information captured from the emails. This\
    \ is called\nensemble Model 7. The results from all seven models were compared\
    \ to\nidentify the best model with the highest accuracy and precision. The\nmodels\
    \ were tested on an test dataset to determine the accuracy and\nother metrics\
    \ of the model proficiency. Table 4 defines all seven models\nand the steps for\
    \ training those models.</p>\n<p>As discussed previously, several models were\
    \ created with different\ndatasets to predict whether the data were malicious\
    \ or legitimate for\nthe respective data. In the end, all the predictions and\
    \ composite\nfeatures from individual models were combined in an ensemble model\
    \ to\naccurately identify whether the emails were phishing or not. The results\n\
    from these models with their Accuracy, Precision, and F Score are listed\nin Table\
    \ 5. This also includes the results from prior studies by the\nrespective authors\
    \ in a similar field and is relevant to the DARTH\nframework presented in this\
    \ paper.</p>\n<p>In the listed results, the most essential metrics are precision\
    \ and\nF-Score as the target feature is imbalanced. <em>Precision</em> and <em>F-score</em>\n\
    are important metrics for performance evaluation for predicting\nimbalanced features\
    \ because it breaks down both of its scores for each\nclass -- 0 being legitimate\
    \ and 1 being malicious. Precision tells how\nwell the model has predicted over\
    \ correct and incorrect predictions for\neach class. Recall tells us the number\
    \ of true positives has been found\nover the number of true positives in the population.\
    \ F-Score is the\nweighted mean of both recall and precision metrics.</p>\n<hr>\n\
    <p><strong>Ensemble Model</strong>   <strong>Input Feature        <strong>Notes</strong>\n\
    Models</strong></p>\n<hr>\n<p><em>Ensemble Model 1    Body Text NN           Trained\
    \ with email body text\n(EM1): Email Body                           dataset. Model\
    \ outputs and\nText</em>                                       results are from\
    \ the trained\nmodel utilizing transfer\nlearning from BERT. Output\nalso includes\
    \ urgency\nprediction based on the\nmodel trained on external\ndata.</p>\n<p><em>Ensemble\
    \ Model 2    URLs NN                Trained with URLs dataset\n(EM2): Embedded\
    \                             and its metadata. The model\nURLs</em>         \
    \                              was trained on the external\ndataset and was used\
    \ to\npredict the embedded URLs\nfrom the emails.</p>\n<p><em>Ensemble Model 3\
    \    Metadata NN            Trained with email header\n(EM3): Metadata</em>  \
    \                          metadata obtained from the\nemails dataset. Predicted\n\
    outputs are used for further\nensemble models.</p>\n<p><em>Ensemble Model 4  \
    \  Email Text NN and      Pre-trained models from\n(EM4): Email Texts   Embedded\
    \ URLs NN       earlier steps were used in\nand Embedded URLs</em>           \
    \               the ensemble NN model\nincuding the predictions\nfrom the respective\
    \ models\nfor phishing detection.</p>\n<p><em>Ensemble Model 5    Embedded URLs\
    \ NN,      Pre-trained models with URL\n(EM5): Metadata and  Header Metadata NN\
    \     and Metadata datasets.\nURLs</em>                                      \
    \ Prediction outputs from\nthose models used in the\nensemble model for phishing\n\
    email prediction.</p>\n<p><em>Ensemble Model 6    Header Metadata NN,    Pre-trained\
    \ models with\n(EM6): Metadata and  Body Text NN           email body text (BERT)\
    \ and\nBody Text</em>                                  Metadata datasets.\nPrediction\
    \ outputs from\nthose models used in the\nensemble model for phishing\nemail prediction</p>\n\
    <h2><a id=\"user-content-ensemble-model-7----body-text-nn-embedded-final-model-utilizingem7-body-text----urls-nn-metadata-nn---inputes-from-pre-trainedurls-metadata-----------------------------models-and-their-predictedoutput-for-this-ensemblemodel-to-detect-phishingemails\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#ensemble-model-7----body-text-nn-embedded-final-model-utilizingem7-body-text----urls-nn-metadata-nn---inputes-from-pre-trainedurls-metadata-----------------------------models-and-their-predictedoutput-for-this-ensemblemodel-to-detect-phishingemails\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>\n<em>Ensemble\
    \ Model 7    Body Text NN, Embedded Final model utilizing\n(EM7): Body Text, \
    \   URLs NN, Metadata NN   inputes from pre-trained\nURLs, Metadata</em>     \
    \                        models and their predicted\noutput for this ensemble\n\
    model to detect phishing\nemails.</h2>\n<p>: Table 4: List of Ensemble Models\
    \ and the details about the inputs to\nthe model</p>\n<p>To evaluate the model's\
    \ effectiveness, Table 5 presents accuracy,\nprecision, and f-scores for all seven\
    \ models. Also, scores from other\nrelevant research projects are presented in\
    \ Table 5. The DARTH framework\nwith an ensemble of models utilizing all email\
    \ composite features\nprovides high accuracy and precision results. The framework\
    \ is Ensemble\nModel 7, producing an accuracy score of over 99%. The model consistently\n\
    performs better than the other models with individual email features and\nother\
    \ published studies. The performance scores of the models are listed\nin Table\
    \ 5. As previously mentioned, six other ensemble models have been\ncreated to\
    \ evaluate the framework.</p>\n<hr>\n<p><strong>Category</strong>    <strong>Models</strong>\
    \               <strong>Accuracy</strong>   <strong>Precision</strong>   <strong>F-Score</strong></p>\n\
    <hr>\n<p><em>Email Body     <strong>EM1: Email Body Text</strong> <strong>96.00</strong>\
    \      <strong>96.00</strong>       <strong>96.00</strong>\nText</em></p>\n<pre><code>\
    \              Ahogail et al. \\[8\\] NLP 98.20          98.20           98.20\n\
    \              and Graph Convolutional                                 \n    \
    \          Network on Email Body                                   \n        \
    \      Text                                                    \n\n          \
    \    Ramanathan et al. \\[10\\] 97.00          NA              100.00\n      \
    \        Topic Modeling plus                                     \n          \
    \    Adaboost on Email Body                                  \n              Text\
    \                                                    \n</code></pre>\n<p><em>Embedded\
    \ URLs</em> <strong>EM2: Embedded URLs</strong>   <strong>92.00</strong>     \
    \ <strong>92.00</strong>       <strong>92.00</strong></p>\n<pre><code>       \
    \       Haynes et al. \\[9\\] Bert 96.30          96.90           96.30\n    \
    \          on URL                                                  \n\n      \
    \        Aljofey et al. \\[13\\]    95.20          95.00           95.20\n   \
    \           CNN using URL                                           \n\n     \
    \         Dharani et al. \\[5\\]     93.70          93.80           92.80\n  \
    \            XGBoost and Random on                                   \n      \
    \        URL                                                     \n\n        \
    \      Sahingoz et al. \\[11\\]   98.00          97.00           98.00\n     \
    \         Random Forest and NLP on                                \n         \
    \     URL                                                     \n</code></pre>\n\
    <p><em>Ensemble       <strong>EM3: Metadata</strong>        <strong>98.00</strong>\
    \      <strong>98.00</strong>       <strong>98.00</strong>\nModel -<br>\nMetadata</em></p>\n\
    <p><em>Ensemble       <strong>EM4: Email Body Text   <strong>97.39</strong>  \
    \    <strong>97.66</strong>       <strong>97.38</strong>\nModel - Body    and\
    \ Embedded URLs</strong><br>\nText and URLs</em></p>\n<p><em>Ensemble       <strong>EM5:\
    \ Metadata and      <strong>99.95</strong>      <strong>99.93</strong>       <strong>99.96</strong>\n\
    Model -         URLs</strong><br>\nMetadata and<br>\nURLs</em></p>\n<pre><code>\
    \              Soon et al. \\[17\\] ENN - 94.20          NA              NA\n\
    \              URL and Metadata                                        \n</code></pre>\n\
    <p><em>Ensemble       <strong>EM6: Metadata and Body <strong>99.94</strong>  \
    \    <strong>99.93</strong>       <strong>99.94</strong>\nModel -         Text</strong><br>\n\
    Metadata and<br>\nBody Text</em></p>\n<h2><a id=\"user-content-ensemble-------em7-body-text-urls---9998------9997-------9998model---body----and-metadatatext-metadataand-url\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#ensemble-------em7-body-text-urls---9998------9997-------9998model---body----and-metadatatext-metadataand-url\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><em>Ensemble\
    \       <strong>EM7: Body Text, URLs   <strong>99.98</strong>      <strong>99.97</strong>\
    \       <strong>99.98</strong>\nModel - Body    and Metadata</strong><br>\nText,\
    \ Metadata<br>\nand URL</em></h2>\n<p>: Table 5: Results of various models computed\
    \ and in comparison to\npreviously published research studies by the respective\
    \ authors</p>\n<p>EM1: Email Body Text, EM2: Embedded URLs, and EM3: Metadata\
    \ models are\nbased on individual composite features of emails, such as email\
    \ body\ntext, URLs, and metadata. These models performed at 96%, 92%, and 98%\n\
    accuracy, precision, and f-score, respectively. Ensemble Model 4, which\nutilizes\
    \ email body text and URLs, has an accuracy score of 97.39%.\nEnsemble model 5,\
    \ which utilizes metadata and URLs, has an accuracy\nscore of 99.95%. Ensemble\
    \ model 6, which utilizes metadata and body\ntext, also has produced an accuracy\
    \ score of 99.94%. Ensemble Model 5\nand Ensemble Model 6 scored higher than Ensemble\
    \ model 4. Both Ensemble\nModel 5 and Ensemble Model 6 utilize metadata, unlike\
    \ Ensemble Model 4.\nAll multi-faceted ensemble models have higher accuracy and\
    \ precision\nscores compared to that of individual composite feature models. The\n\
    model performance accuracy scores are presented in Figure 4. The results\nshow\
    \ that not all composite feature combinations yield similar scores.\nAmong all\
    \ the composite feature models, the ensemble models with\nmetadata as a composite\
    \ feature yields higher accuracy and precision\nscores.</p>\n<p><a target=\"_blank\"\
    \ rel=\"noopener noreferrer\" href=\"media/image4.emf\"><img src=\"media/image4.emf\"\
    \ alt=\"\" style=\"max-width: 100%;\"></a>{width=\"4.8125in\" height=\"2.4680555555555554in\"\
    }</p>\n<p>Fig. 4. Accuracy percentage distribution of each ensemble model</p>\n\
    <p>5 Discussion</p>\n<p>The results, as covered in Section 4, show that the ensemble\
    \ models with\nmultiple composite features yield much higher accuracy. Individual\n\
    composite features analysis does give promising results with accuracy\nabove 90%,\
    \ however, the ensemble models with muti-faceted features are\nmore successful\
    \ in identifying the malicious emails. The attackers\ncontinue to change their\
    \ tactics to dupe unsuspecting individuals. A\nsingle feature-based model is likely\
    \ to fail in scenarios when attackers\nmake the emails look even more similar\
    \ to legitimate emails. Analyzing\nand building an ensemble model using several\
    \ aspects of the received\nemail does provide better results.</p>\n<p>One of the\
    \ crucial aspects of emails is the attachments. The document or\nfile attached\
    \ to the email can contain malware; otherwise, a\nlegitimate-looking email can\
    \ install malicious content in the users'\nsystem and network. As part of the\
    \ DARTH framework, it is recommended to\nadd optional models, including attachment\
    \ analysis. The documents\nentropy varies by different file types, which can help\
    \ identify\nsuspicious documents; however, the attachment is not part of the\n\
    analysis of this study due to the unavailability of malicious test\nemails with\
    \ attachments. Results from Figure 4 show the improvement in\naccuracy scores\
    \ when multiple composite featuire models are combined\ninto an ensemble model.\
    \ Body text models produced less effective\naccuracy scores by itself, but metadata-based\
    \ models produced results\nwith higer accuracy and precision. This points to the\
    \ fact that email\nmetadata is an important aspect in the identification of legitimate\
    \ or\nmalicious emails. The email headers are part of the metadata, and often\n\
    users ignore that information as it is not typically visible to the\ncommon users.\
    \ The results show that metadata may hold more clues to\nfinding the phishing\
    \ than other features and is an important feature\nwhich plays a significant role\
    \ in identifying the phishing email with\nbetter accuracyin the DARTH framework.</p>\n\
    <p>Other traditional research models from Soon et al., Ahogail et al. and\nHaynes\
    \ et al. have not scored as high as the EM:5, EM:6, and EM:7 as\ncovered in Table\
    \ 5. Among the traditional research discussed in Table 5,\ngraph convolutional\
    \ networks and NLP on email body text for a phishing\ndetection model have produced\
    \ high accuracy of 98.20% [8], however,\nthe multi-faceted ensemble models (EM:7)\
    \ yields higher accuracy of\n99.98%.</p>\n<p>The individual feature models for\
    \ DARTH framework are trained using\nexternal datasets like Hannousse et al*.*\
    \ (2021) dataset to train the\nURL model. It provides a good baseline for the\
    \ ensemble models to\nperform as it learns from previous researches and applies\
    \ to new\nstudies.</p>\n<p>The results demonstrate that phishing detection can\
    \ be improved through\na multi-faceted approach. The existing phishing detection\
    \ tools used in\nthe industry can employ the techniques that are covered as part\
    \ of the\nDARTH framework. It can help and thwart the phishing attacks on an\n\
    individual or an organization using such tools. This same idea can be\nused for\
    \ many problem domains where adding multiple models have a better\noutcome than\
    \ a few highly tuned models.</p>\n<p>Any research conducted has a responsibility\
    \ toward society, and the\nethics of said research and its possible implications\
    \ must be discussed\nin detail. Section 5.1 talks about the ethical considerations\
    \ as part of\nthe study of the DARTH framework.</p>\n<p><strong>5.1 Ethics</strong></p>\n\
    <p>Algorithmic bias is a field of study under <em>algorithmic ethics</em> that\n\
    analyzes the fairness of an algorithm based upon its probability of\nerrors and\
    \ compliance with its solution requirements. Ethics encompasses\na broad range\
    \ of topics, and primarily those are social ethics and\nalgorithmic ethics. Algorithm\
    \ ethics defines how the algorithm needs to\nbehave and act. It should also clearly\
    \ list the behavior it should avoid\nfor producing outcomes or providing recommendations.\
    \ It also deals with\nfairness which helps to understand and reduce bias in the\
    \ algorithm. The\nethics state that the design of the algorithm should be auditable\
    \ so the\nmodels can be analyzed for further development. One of the normative\n\
    concerns of ethics is fairness. It deals with the algorithm's trade-off\nbetween\
    \ accuracy and different notions of fairness [38]. Tramadol et\nal. describe the\
    \ fairness of algorithm in four ways -- protect\ncategories such as race are not\
    \ distinctly used for the function of the\nalgorithm, false-positive error and\
    \ false-negative error are equal for\nall classes of categorical variables, algorithms\
    \ are properly\n'calibrated' between different classes, and equal probability\n\
    estimates across all classes of categorical variables [39]. There are\nsome drawbacks\
    \ to these definitions. One cannot just remove sensitive\ncategorical features\
    \ such as race and ethnicity from the dataset. Veale\net al. suggest two ways\
    \ fairness can be preserved in the algorithm.</p>\n<p>One is that a third party\
    \ can audit the dataset and algorithm design to\nreduce discrimination [1]. The\
    \ other way is to have the algorithm\ndesigners collaborate with other relevant\
    \ stakeholders who are experts\nin the domain [1]. Bias tends to arise when there\
    \ is a lack of\nfairness. It occurs when algorithm developers deviate from requirements\n\
    that list out the data collection and algorithm design standards.\nRemoving skewed\
    \ data, using a biased estimator, or introducing\ncompensatory bias to the algorithm\
    \ are ways to reduce bias [39]. The\nalgorithm may behave unethically if biases\
    \ are not reduced.</p>\n<p>The institution for Electrical and Electronics Engineers\
    \ (IEEE) is an\norganization that has devoted itself to defining a code of ethics\
    \ and\nstandards for engineering professionals, most notably in emerging areas\n\
    such as AI, robotics, and data management. The code of ethics ensures\nthat professionals\
    \ comply with the company and government rules. One of\nthe organization's standards\
    \ is IEEE P7003 which deals with algorithm\nethics. The standard provides a framework\
    \ that makes algorithm\ndevelopers prioritize ethics and communicate with regulators\
    \ and other\nstakeholders for any clarity or feedback on the objective or\nfunctionality\
    \ of the application [40]. The proposed algorithm's\nobjective is to identify\
    \ phishing attacks accurately.</p>\n<p>Given this classification problem, the\
    \ research is subjected to Type I\nand Type II errors. False-positive and false-negative\
    \ rates dictate the\nbiasness and fairness of the phishing attack detection algorithm.\
    \ In the\nproposed framework, the model performance shows that the algorithm is\n\
    not biased towards predicting legitimate emails over phishing emails.\nThe precision\
    \ rate for spam and legitimate email detection is over 99%.\nThe recall rate for\
    \ spam and legitimate email detection is also over\n99%. The models in the proposed\
    \ framework sound ethical as the false\npositive and false negative error rates\
    \ are low and equal for both\nclasses. Biases often challenge the framework as\
    \ one can question the\nalgorithm's fairness. The authors have set strict rules\
    \ and procedures\nfor collecting both legitimate and spam emails to address this\
    \ concern.\nMost importantly, the algorithm should provide enough spam email data\
    \ of\nseveral types. Since there are more volumes of legitimate emails, the\n\
    algorithm will have a natural bias towards that class compared to the\nother class.</p>\n\
    <p>Unintended bias and unfairness in algorithm impacts societies\nnegatively.\
    \ Technology improves societies worldwide by bringing\nefficiency through technological\
    \ innovations, which benefit people in\nall aspects of their lives. These innovations\
    \ occur by scaling and\nspeeding technical advances using an astronomical amount\
    \ of data. As the\nvolume of sophisticated data grows, the threat of phishing\
    \ attacks from\ndifferent cybercriminal parties worldwide increases. The proposed\n\
    algorithm from this research can reduce this problem. It can protect a\nperson\
    \ or entity from revealing personal or sensitive information by\nmistake or aggressive\
    \ cyberattacks. The protection can astronomically\nbenefit anyone in this digital\
    \ age. It can address ethics from a\npractical standpoint as the researchers present\
    \ the steps to handle data\ncollection, processing, and model building. Researchers\
    \ also need to\nmake proper judgments in the interest of the public or stakeholders\n\
    since they deal with a group of people's sensitive and personal\ninformation.\
    \ It is essential due to the public's lack of understanding\nor misconception\
    \ of algorithms in detail. First, the proposed framework\nshows all the steps\
    \ of collecting private emails. Then it lists out the\nprocesses that have been\
    \ used to wrangle the data for building the model\nfor the algorithm. The algorithm\
    \ does not require human intervention as\none does not need to access these emails\
    \ for any data processing for\nphishing detection. There is no purpose for accessing\
    \ any individual\nemail for building this algorithm. The lack of human intervention\n\
    addresses a critical aspect of the cybersecurity code of ethics:\npersonal autonomy.\
    \ As described previously, some existing methods\nrequire human intervention to\
    \ preprocess emails for phishing attack\nprevention. In a manual process, the\
    \ scientist may have to access\nprivate emails for preprocessing or may end up\
    \ mistakenly taking a step\nthat may leak the confidential information of the\
    \ senders and\nrecipients. After detecting the attack, cybersecurity analysts\
    \ must take\nmanual steps where the individual must take the server down to act\
    \ on\nthe attack.</p>\n<p>Formosa et al. state that during a manual process like\
    \ that, the chance\nof preventing the attack is low [37]. The proposed algorithm\n\
    eliminates manual processes like this as there is no human intervention\nfor preprocessing,\
    \ and the suspicious emails never reach the\nrecipients' destination. The elimination\
    \ of the process benefits\neveryone as private information never gets leaked.\
    \ Also, the public and\nany institution never have to face any threats the attackers\
    \ pose. All\nthe steps meet the requirement of IEEE standards.</p>\n<p>The composite\
    \ features like URLs and body text don't give better\nresults, but metadata performs\
    \ better than other composite features of\nemails. Headers are essential metadata\
    \ that users typically don't see\nbut are an important composite feature in detecting\
    \ phishing.</p>\n<p>6 Conclusion</p>\n<p>The multi-faceted approach of using an\
    \ ensemble of multiple independent\ncomposite feature models yields highly accurate\
    \ ensemble phishing\ndetection models even when lower quality feature models are\
    \ used. The\nnovel DARTH framework decomposes email into composite features and\n\
    allows independent models on each composite feature to be developed and\nused.\
    \ The ensemble of the output of these models achieved 99.98%\naccuracy in detecting\
    \ phishing emails in our test data.</p>\n<p>Adding more composite features improves\
    \ the accuracy of the ensemble\nmodel. Experiments showed that the ensemble model\
    \ created from two\ncomposite features always yielded better results than the\
    \ models for\nindividual composite feature, and the ensemble model created from\
    \ three\ncomposite feature yielded superior results compared to the ensemble\n\
    models with two composite features. Ensemble models developed using the\nmultiple\
    \ models DARTH framework are more accurate compared to a single\nmodel with only\
    \ non-composite features as input.</p>\n<p>The DARTH framework is usable in any\
    \ problem domain with identifiable\nand separable composite features. The phishing\
    \ email detection problem\ndomain is perhaps the exemplary domain for the DARTH\
    \ framework.</p>\n<p>References</p>\n<ol>\n<li>\n<p>Veale M, Binns R (2017) Fairer\
    \ machine learning in the real world:\nmitigating discrimination without collecting\
    \ sensitive data. Big\nData Soc 4(2):205395171774353. <a href=\"https://doi.org/10.1177/20539\"\
    \ rel=\"nofollow\">https://doi.org/10.1177/20539</a>\n51717743530</p>\n</li>\n\
    <li>\n<p>Dutta AK (2021) Detecting phishing websites using machine learning\n\
    technique. PLoS ONE 16(10): e0258361.\n<a href=\"https://doi.org/10.1371/journal.pone.0258361\"\
    \ rel=\"nofollow\">https://doi.org/10.1371/journal.pone.0258361</a></p>\n</li>\n\
    <li>\n<p>Akinyelu, Ayo &amp; Adewumi, Aderemi. (2014). Classification of Phishing\n\
    Email Using Random Forest Machine Learning Technique. Journal of\nApplied Mathematics.\
    \ 2014. 10.1155/2014/425731.</p>\n</li>\n<li>\n<p>P. Prakash, M. Kumar, R. R.\
    \ Kompella and M. Gupta, \"PhishNet:\nPredictive Blocklisting to Detect Phishing\
    \ Attacks,\" 2010\nProceedings IEEE INFOCOM, 2010, pp. 1-5, doi:\n10.1109/INFCOM.2010.5462216.</p>\n\
    </li>\n<li>\n<p>Detection of Phishing Websites Using Ensemble Machine Learning\n\
    Approach Dharani M.,\_Soumya Badkul,\_Kimaya Gharat,\_Amarsinh Vidhate\nand\_\
    Dhanashri Bhosale ITM Web Conf., 40 (2021) 03012,</p>\n</li>\n</ol>\n<blockquote>\n\
    <p>DOI: <a href=\"https://doi.org/10.1051/itmconf/20214003012\" rel=\"nofollow\"\
    >https://doi.org/10.1051/itmconf/20214003012</a></p>\n</blockquote>\n<ol start=\"\
    6\">\n<li>\n<p>Abu-Nimeh, S., et al. \"Distributed Phishing Detection by Applying\n\
    Variable Selection Using Bayesian Additive Regression Trees.\" 2009\nIEEE International\
    \ Conference on Communications, IEEE, 2009, pp.\n1--5, <a href=\"https://doi.org/10.1109/ICC.2009.5198931\"\
    \ rel=\"nofollow\">https://doi.org/10.1109/ICC.2009.5198931</a>.</p>\n</li>\n\
    <li>\n<p>Sanglerdsinlapachai, N, and A Rungsawang. \"Using Domain Top-Page\nSimilarity\
    \ Feature in Machine Learning-Based Web Phishing\nDetection.\" IEEE, 2010, pp.\
    \ 187--190,\n<a href=\"https://doi.org/10.1109/WKDD.2010.108\" rel=\"nofollow\"\
    >https://doi.org/10.1109/WKDD.2010.108</a>.</p>\n</li>\n<li>\n<p>Alhogail, &amp;\
    \ Alsabih, A. (2021). Applying machine learning and\nnatural language processing\
    \ to detect phishing email. Computers &amp;\nSecurity, 110, 102414--.\n<a href=\"\
    https://doi.org/10.1016/j.cose.2021.102414\" rel=\"nofollow\">https://doi.org/10.1016/j.cose.2021.102414</a></p>\n\
    </li>\n<li>\n<p>Haynes, Shirazi, H., &amp; Ray, I. (2021). Lightweight URL-based\n\
    phishing detection using natural language processing transformers\nfor mobile\
    \ devices. Procedia Computer Science, 191, 127--134.\n<a href=\"https://doi.org/10.1016/j.procs.2021.07.040\"\
    \ rel=\"nofollow\">https://doi.org/10.1016/j.procs.2021.07.040</a></p>\n</li>\n\
    <li>\n<p>Ramanathan, &amp; Wechsler, H. (2012). phishGILLNET---phishing detection\n\
    methodology using probabilistic latent semantic analysis, AdaBoost,\nand co-training.\
    \ EURASIP Journal on Multimedia and Information\nSecurity, 2012(1), 1--1. <a href=\"\
    https://doi.org/10.1186/1687-417X-2012-1\" rel=\"nofollow\">https://doi.org/10.1186/1687-417X-2012-1</a></p>\n\
    </li>\n<li>\n<p>Sahingoz, Buber, E., Demir, O., &amp; Diri, B. (2019). Machine\
    \ learning\nbased phishing detection from URLs. Expert Systems with\nApplications,\
    \ 117, 345--357.\n<a href=\"https://doi.org/10.1016/j.eswa.2018.09.029\" rel=\"\
    nofollow\">https://doi.org/10.1016/j.eswa.2018.09.029</a></p>\n</li>\n<li>\n<p>Abbasi,\
    \ Dobolyi, D., Vance, A., &amp; Zahedi, F. M. (2021). The Phishing\nFunnel Model:\
    \ A Design Artifact to Predict User Susceptibility to\nPhishing Websites. Information\
    \ Systems Research, 32(2), 410--436.\n<a href=\"https://doi.org/10.1287/isre.2020.0973\"\
    \ rel=\"nofollow\">https://doi.org/10.1287/isre.2020.0973</a></p>\n</li>\n<li>\n\
    <p>Aljofey, Jiang, Q., Qu, Q., Huang, M., &amp; Niyigena, J.-P. (2020). An\nEffective\
    \ Phishing Detection Model Based on Character Level\nConvolutional Neural Network\
    \ from URL. Electronics (Basel), 9(9),\n1514--. <a href=\"https://doi.org/10.3390/electronics9091514\"\
    \ rel=\"nofollow\">https://doi.org/10.3390/electronics9091514</a></p>\n</li>\n\
    <li>\n<p>Jones, C. (2022, January 18). 50 phishing stats you should know\nin 2022.\
    \ 50 Phishing Stats You Should Know In 2022. Retrieved\nJanuary 27, 2022, from\n\
    <a href=\"https://expertinsights.com/insights/50-phishing-stats-you-should-know/\"\
    \ rel=\"nofollow\">https://expertinsights.com/insights/50-phishing-stats-you-should-know/</a></p>\n\
    </li>\n<li>\n<p>Hannousse, Abdelhakim; Yahiouche, Salima (2021), \"Web page phishing\n\
    detection\", Mendeley Data, V3, doi: 10.17632/c2gw7fy2j4.3</p>\n</li>\n<li>\n\
    <p>NapierOne Mixed File Dataset was accessed on\_February 19, 2022, from\n<a href=\"\
    https://registry.opendata.aws/napierone\" rel=\"nofollow\">https://registry.opendata.aws/napierone</a>.</p>\n\
    </li>\n<li>\n<p>Kim Soon, Kim On, C., Mohd Rusli, N., Soo Fun, T., Alfred, R.,\
    \ &amp; Tse\nGuan, T. (2020). Comparison of simple feedforward neural network,\n\
    recurrent neural network and ensemble neural networks in phishing\ndetection.\_\
    Journal of Physics. Conference Series,\_1502(1), 12033--.\n<a href=\"https://doi.org/10.1088/1742-6596/1502/1/012033\"\
    \ rel=\"nofollow\">https://doi.org/10.1088/1742-6596/1502/1/012033</a></p>\n</li>\n\
    <li>\n<p>Zhu, Ju, Y., Chen, Z., Liu, F., &amp; Fang, X. (2020). DTOF-ANN: An\n\
    Artificial Neural Network phishing detection model based on Decision\nTree and\
    \ Optimal Features. Applied Soft Computing, 95, 106505--.\n<a href=\"https://doi.org/10.1016/j.asoc.2020.106505\"\
    \ rel=\"nofollow\">https://doi.org/10.1016/j.asoc.2020.106505</a></p>\n</li>\n\
    <li>\n<p>Mohammad, Thabtah, F., &amp; McCluskey, L. (2013). Predicting phishing\n\
    websites based on self-structuring neural network. Neural Computing\n&amp; Applications,\
    \ 25(2), 443--458.\n<a href=\"https://doi.org/10.1007/s00521-013-1490-z\" rel=\"\
    nofollow\">https://doi.org/10.1007/s00521-013-1490-z</a></p>\n</li>\n<li>\n<p>Wei,\
    \ Ke, Q., Nowak, J., Korytkowski, M., Scherer, R., &amp; Wo\u017Aniak, M.\n(2020).\
    \ Accurate and fast URL phishing detector: A convolutional\nneural network approach.\
    \ Computer Networks (Amsterdam, Netherlands:\n1999), 178, 107275--. <a href=\"\
    https://doi.org/10.1016/j.comnet.2020.107275\" rel=\"nofollow\">https://doi.org/10.1016/j.comnet.2020.107275</a></p>\n\
    </li>\n<li>\n<p>Kamireddy Neeharika, K P Ruphaa Sri, B Vishruthi, &amp; M Suresh\
    \ Anand.\n(2021). Precise Detection of Phishing URLS Using Recurrent Neural\n\
    Networks. i-Manager's Journal on Computer Science, 9(1), 21--.\n<a href=\"https://doi.org/10.26634/jcom.9.1.18154\"\
    \ rel=\"nofollow\">https://doi.org/10.26634/jcom.9.1.18154</a></p>\n</li>\n<li>\n\
    <p>Ahmad, Rafie, M., &amp; Ghorabie, S. M. (2021). Spam detection on\nTwitter\
    \ using a support vector machine and users' features by\nidentifying their interactions.\
    \ Multimedia Tools and Applications,\n80(8), 11583--11605. <a href=\"https://doi.org/10.1007/s11042-020-10405-7\"\
    \ rel=\"nofollow\">https://doi.org/10.1007/s11042-020-10405-7</a></p>\n</li>\n\
    <li>\n<p>Grimes. (2017). Hacking the Hacker: Learn from the Experts Who Take\n\
    down Hackers. John Wiley &amp; Sons, Incorporated.</p>\n</li>\n<li>\n<p>California\
    \ Enacts Tough Anti-phishing Law; California Gov. Arnold\nSchwarzenegger has signed\
    \ anti-phishing legislation into law.\n(2005).\_InternetWeek (Manhasset, N.Y.).</p>\n\
    </li>\n<li>\n<p>Bowman, S. R., Angeli, G., Potts, C., &amp; Manning, C. D. (2015).\
    \ A\nlarge, annotated corpus for learning natural language inference.\narXiv preprint\
    \ arXiv:1508.05326. Chicago.</p>\n</li>\n<li>\n<p>California Enacts Tough Anti-phishing\
    \ Law; California Gov. Arnold\nSchwarzenegger has signed anti-phishing legislation\
    \ into law.\n(2005). InternetWeek (Manhasset, N.Y.).</p>\n</li>\n<li>\n<p>Verizon\
    \ 2021 Data Breach Investigations Report [Online].\nAvailable: verizon.com/dbir/</p>\n\
    </li>\n<li>\n<p>Zhou, Z. H., Wu, J., &amp; Tang, W. (2002). Ensembling neural\
    \ networks:\nmany could be better than all.\_Artificial intelligence,\_137(1-2),\n\
    239-263.</p>\n</li>\n<li>\n<p>Chen, Y., Chang, H., Meng, J., &amp; Zhang, D. (2019).\
    \ Ensemble Neural\nNetworks (ENN): A gradient-free stochastic method.\_Neural\n\
    Networks,\_110, 170-185.</p>\n</li>\n<li>\n<p>Dua, D. and Graff, C. (2019). UCI\
    \ Machine Learning Repository\n[<a href=\"http://archive.ics.uci.edu/ml%5C%5D\"\
    \ rel=\"nofollow\">http://archive.ics.uci.edu/ml\\]</a>. Irvine, CA: the University\
    \ of\nCalifornia, School of Information and Computer Science.</p>\n</li>\n<li>\n\
    <p>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert:\nPre-training\
    \ of deep bidirectional transformers for language\nunderstanding.\_arXiv preprint\
    \ arXiv:1810.04805</p>\n</li>\n<li>\n<p>Rekouche, K. (2011). Early phishing.\_\
    <em>arXiv preprint\narXiv:1106.4692</em>.</p>\n</li>\n<li>\n<p>Merriam-Webster.\
    \ (n.d.). Phishing. In\_Merriam-Webster.com\ndictionary. Retrieved April 9, 2022,\
    \ from\n<a href=\"https://www.merriam-webster.com/dictionary/phishing\" rel=\"\
    nofollow\">https://www.merriam-webster.com/dictionary/phishing</a></p>\n</li>\n\
    <li>\n<p>Webroot, Types of Phishing Attacks You Need to Know to Stay Safe\n[Online].\
    \ Available:\n<a href=\"https://mypage.webroot.com/rs/557-FSI-195/images/Webroot_11%20Types%20of%20Phishing_eBook.pdf\"\
    \ rel=\"nofollow\">https://mypage.webroot.com/rs/557-FSI-195/images/Webroot_11%20Types%20of%20Phishing_eBook.pdf</a></p>\n\
    </li>\n<li>\n<p>Williams, Hinds, J., &amp; Joinson, A. N. (2018). Exploring\n\
    susceptibility to phishing in the workplace. International Journal\nof Human-Computer\
    \ Studies, 120, 1--13.\n<a href=\"https://doi.org/10.1016/j.ijhcs.2018.06.004\"\
    \ rel=\"nofollow\">https://doi.org/10.1016/j.ijhcs.2018.06.004</a></p>\n</li>\n\
    <li>\n<p>Vallor, S. (n.d.). Intro to cybersecurity ethics - an introduction\n\
    to cybersecurity ethics module author: Shannon. StuDocu. Retrieved\nMay 30, 2022,\
    \ from\n<a href=\"https://www.studocu.com/en-us/document/boston-university/information-security/intro-to-cybersecurity-ethics/17140429\"\
    \ rel=\"nofollow\">https://www.studocu.com/en-us/document/boston-university/information-security/intro-to-cybersecurity-ethics/17140429</a></p>\n\
    </li>\n<li>\n<p>Formosa, Paul, et al. \"A Principlist Framework for Cybersecurity\n\
    Ethics.\" Computers &amp; Security, vol. 109, Elsevier Ltd, 2021, p.\n102382--,\
    \ <a href=\"https://doi.org/10.1016/j.cose.2021.102382\" rel=\"nofollow\">https://doi.org/10.1016/j.cose.2021.102382</a>.</p>\n\
    </li>\n<li>\n<p>Kearns, M., &amp; Roth, A. (2022, March 9). Ethical Algorithm\
    \ Design\nShould Guide Technology Regulation. Brookings. Retrieved June 9,\n2022,\
    \ from\n<a href=\"https://www.brookings.edu/research/ethical-algorithm-design-should-guide-technology-regulation/#footnote-3\"\
    \ rel=\"nofollow\">https://www.brookings.edu/research/ethical-algorithm-design-should-guide-technology-regulation/#footnote-3</a></p>\n\
    </li>\n<li>\n<p>Tsamados, Andreas, et al. \"The Ethics of Algorithms: Key Problems\n\
    and Solutions.\" AI &amp; Society, vol. 37, no. 1, Springer London, 2021,\npp.\
    \ 215--30, <a href=\"https://doi.org/10.1007/s00146-021-01154-8\" rel=\"nofollow\"\
    >https://doi.org/10.1007/s00146-021-01154-8</a>.</p>\n</li>\n<li>\n<p>IEEE Announces\
    \ Standards Project Addressing Algorithmic Bias\nConsiderations. (2017, Mar 09).\
    \ Business Wire\n<a href=\"http://proxy.libraries.smu.edu/login?url=https://www.proquest.com/wire-feeds/ieee-announces-standards-project-addressing/docview/1875371352/se-2?accountid=6667\"\
    \ rel=\"nofollow\">https://www.proquest.com/wire-feeds/ieee-announces-standards-project-addressing/docview/1875371352/se-2?accountid=6667</a></p>\n\
    </li>\n<li>\n<p>Anti-Phishing Working Group (APWG) (2014) Phishing Activity Trends\n\
    Report, 3rd Quarter 2021 [Online]. Available:\n<a href=\"https://docs.apwg.org/reports/apwg_trends_report_q3_2021.pdf\"\
    \ rel=\"nofollow\">https://docs.apwg.org/reports/apwg_trends_report_q3_2021.pdf</a></p>\n\
    </li>\n</ol>\n"
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1658945123.0
renjithravindrankannath/spack_repository_test:
  data_format: 2
  description: New spack repository to reproduce the issue observed in PR 31591
  filenames:
  - share/spack/gitlab/cloud_pipelines/stacks/aws-isc/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/build_systems/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/aws-ahug-aarch64/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/e4s-mac/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/tutorial/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/radiuss-aws/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/aws-ahug/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/e4s-oneapi/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/e4s-on-power/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/radiuss/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/radiuss-aws-aarch64/spack.yaml
  - share/spack/gitlab/cloud_pipelines/stacks/aws-isc-aarch64/spack.yaml
  full_name: renjithravindrankannath/spack_repository_test
  latest_release: null
  readme: "<h1><a id=\"user-content--spack\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#-spack\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>\n\
    <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/a01512f4480c4615a82f2b929789547a60d78e1f68d26be1a56e33d9258735d4/68747470733a2f2f63646e2e7261776769742e636f6d2f737061636b2f737061636b2f646576656c6f702f73686172652f737061636b2f6c6f676f2f737061636b2d6c6f676f2e737667\"\
    ><img src=\"https://camo.githubusercontent.com/a01512f4480c4615a82f2b929789547a60d78e1f68d26be1a56e33d9258735d4/68747470733a2f2f63646e2e7261776769742e636f6d2f737061636b2f737061636b2f646576656c6f702f73686172652f737061636b2f6c6f676f2f737061636b2d6c6f676f2e737667\"\
    \ width=\"64\" valign=\"middle\" alt=\"Spack\" data-canonical-src=\"https://cdn.rawgit.com/spack/spack/develop/share/spack/logo/spack-logo.svg\"\
    \ style=\"max-width: 100%;\"></a> Spack</h1>\n<p><a href=\"https://github.com/spack/spack/actions\"\
    ><img src=\"https://github.com/spack/spack/workflows/linux%20tests/badge.svg\"\
    \ alt=\"Unit Tests\" style=\"max-width: 100%;\"></a>\n<a href=\"https://github.com/spack/spack/actions/workflows/bootstrap.yml\"\
    ><img src=\"https://github.com/spack/spack/actions/workflows/bootstrap.yml/badge.svg\"\
    \ alt=\"Bootstrapping\" style=\"max-width: 100%;\"></a>\n<a href=\"https://github.com/spack/spack/actions?query=workflow%3A%22macOS+builds+nightly%22\"\
    ><img src=\"https://github.com/spack/spack/workflows/macOS%20builds%20nightly/badge.svg?branch=develop\"\
    \ alt=\"macOS Builds (nightly)\" style=\"max-width: 100%;\"></a>\n<a href=\"https://codecov.io/gh/spack/spack\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4e223725486ecdae463d02d6ebd2b814945366210a908fc6f04b044ada8a7820/68747470733a2f2f636f6465636f762e696f2f67682f737061636b2f737061636b2f6272616e63682f646576656c6f702f67726170682f62616467652e737667\"\
    \ alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/spack/spack/branch/develop/graph/badge.svg\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://github.com/spack/spack/actions/workflows/build-containers.yml\"\
    ><img src=\"https://github.com/spack/spack/actions/workflows/build-containers.yml/badge.svg\"\
    \ alt=\"Containers\" style=\"max-width: 100%;\"></a>\n<a href=\"https://spack.readthedocs.io\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/523aba38ec3f8d2294b874493fe63feed5805b98460c385611397b02be14a51c/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f737061636b2f62616467652f3f76657273696f6e3d6c6174657374\"\
    \ alt=\"Read the Docs\" data-canonical-src=\"https://readthedocs.org/projects/spack/badge/?version=latest\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://github.com/psf/black\"><img\
    \ src=\"https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667\"\
    \ alt=\"Code style: black\" data-canonical-src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://slack.spack.io\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/4bbdc2b44561be6dfffe64e15730e1c5a2bed9c4efe6f9942638091a4ce3ede2/68747470733a2f2f736c61636b2e737061636b2e696f2f62616467652e737667\"\
    \ alt=\"Slack\" data-canonical-src=\"https://slack.spack.io/badge.svg\" style=\"\
    max-width: 100%;\"></a></p>\n<p>Spack is a multi-platform package manager that\
    \ builds and installs\nmultiple versions and configurations of software. It works\
    \ on Linux,\nmacOS, and many supercomputers. Spack is non-destructive: installing\
    \ a\nnew version of a package does not break existing installations, so many\n\
    configurations of the same package can coexist.</p>\n<p>Spack offers a simple\
    \ \"spec\" syntax that allows users to specify versions\nand configuration options.\
    \ Package files are written in pure Python, and\nspecs allow package authors to\
    \ write a single script for many different\nbuilds of the same package.  With\
    \ Spack, you can build your software\n<em>all</em> the ways you want to.</p>\n\
    <p>See the\n<a href=\"https://spack.readthedocs.io/en/latest/features.html\" rel=\"\
    nofollow\">Feature Overview</a>\nfor examples and highlights.</p>\n<p>To install\
    \ spack and your first package, make sure you have Python.\nThen:</p>\n<pre><code>$\
    \ git clone -c feature.manyFiles=true https://github.com/spack/spack.git\n$ cd\
    \ spack/bin\n$ ./spack install zlib\n</code></pre>\n<h2><a id=\"user-content-documentation\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#documentation\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h2>\n<p><a href=\"\
    https://spack.readthedocs.io/\" rel=\"nofollow\"><strong>Full documentation</strong></a>\
    \ is available, or\nrun <code>spack help</code> or <code>spack help --all</code>.</p>\n\
    <p>For a cheat sheet on Spack syntax, run <code>spack help --spec</code>.</p>\n\
    <h2><a id=\"user-content-tutorial\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #tutorial\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Tutorial</h2>\n\
    <p>We maintain a\n<a href=\"https://spack.readthedocs.io/en/latest/tutorial.html\"\
    \ rel=\"nofollow\"><strong>hands-on tutorial</strong></a>.\nIt covers basic to\
    \ advanced usage, packaging, developer features, and large HPC\ndeployments. \
    \ You can do all of the exercises on your own laptop using a\nDocker container.</p>\n\
    <p>Feel free to use these materials to teach users at your organization\nabout\
    \ Spack.</p>\n<h2><a id=\"user-content-community\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#community\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Community</h2>\n<p>Spack is an open source project.  Questions, discussion,\
    \ and\ncontributions are welcome. Contributions can be anything from new\npackages\
    \ to bugfixes, documentation, or even new core features.</p>\n<p>Resources:</p>\n\
    <ul>\n<li>\n<strong>Slack workspace</strong>: <a href=\"https://spackpm.slack.com\"\
    \ rel=\"nofollow\">spackpm.slack.com</a>.\nTo get an invitation, visit <a href=\"\
    https://slack.spack.io\" rel=\"nofollow\">slack.spack.io</a>.</li>\n<li>\n<strong>Mailing\
    \ list</strong>: <a href=\"https://groups.google.com/d/forum/spack\" rel=\"nofollow\"\
    >groups.google.com/d/forum/spack</a>\n</li>\n<li>\n<strong>Twitter</strong>: <a\
    \ href=\"https://twitter.com/spackpm\" rel=\"nofollow\">@spackpm</a>. Be sure\
    \ to\n<code>@mention</code> us!</li>\n</ul>\n<h2><a id=\"user-content-contributing\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#contributing\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contributing</h2>\n<p>Contributing\
    \ to Spack is relatively easy.  Just send us a\n<a href=\"https://help.github.com/articles/using-pull-requests/\"\
    >pull request</a>.\nWhen you send your request, make <code>develop</code> the\
    \ destination branch on the\n<a href=\"https://github.com/spack/spack\">Spack\
    \ repository</a>.</p>\n<p>Your PR must pass Spack's unit tests and documentation\
    \ tests, and must be\n<a href=\"https://www.python.org/dev/peps/pep-0008/\" rel=\"\
    nofollow\">PEP 8</a> compliant.  We enforce\nthese guidelines with our CI process.\
    \ To run these tests locally, and for\nhelpful tips on git, see our\n<a href=\"\
    https://spack.readthedocs.io/en/latest/contribution_guide.html\" rel=\"nofollow\"\
    >Contribution Guide</a>.</p>\n<p>Spack's <code>develop</code> branch has the latest\
    \ contributions. Pull requests\nshould target <code>develop</code>, and users\
    \ who want the latest package versions,\nfeatures, etc. can use <code>develop</code>.</p>\n\
    <h2><a id=\"user-content-releases\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #releases\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Releases</h2>\n\
    <p>For multi-user site deployments or other use cases that need very stable\n\
    software installations, we recommend using Spack's\n<a href=\"https://github.com/spack/spack/releases\"\
    >stable releases</a>.</p>\n<p>Each Spack release series also has a corresponding\
    \ branch, e.g.\n<code>releases/v0.14</code> has <code>0.14.x</code> versions of\
    \ Spack, and <code>releases/v0.13</code> has\n<code>0.13.x</code> versions. We\
    \ backport important bug fixes to these branches but\nwe do not advance the package\
    \ versions or make other changes that would\nchange the way Spack concretizes\
    \ dependencies within a release branch.\nSo, you can base your Spack deployment\
    \ on a release branch and <code>git pull</code>\nto get fixes, without the package\
    \ churn that comes with <code>develop</code>.</p>\n<p>The latest release is always\
    \ available with the <code>releases/latest</code> tag.</p>\n<p>See the <a href=\"\
    https://spack.readthedocs.io/en/latest/developer_guide.html#releases\" rel=\"\
    nofollow\">docs on releases</a>\nfor more details.</p>\n<h2><a id=\"user-content-code-of-conduct\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#code-of-conduct\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Code of Conduct</h2>\n<p>Please\
    \ note that Spack has a\n<a href=\".github/CODE_OF_CONDUCT.md\"><strong>Code of\
    \ Conduct</strong></a>. By participating in\nthe Spack community, you agree to\
    \ abide by its rules.</p>\n<h2><a id=\"user-content-authors\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#authors\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Authors</h2>\n<p>Many thanks go to Spack's <a href=\"\
    https://github.com/spack/spack/graphs/contributors\">contributors</a>.</p>\n<p>Spack\
    \ was created by Todd Gamblin, <a href=\"mailto:tgamblin@llnl.gov\">tgamblin@llnl.gov</a>.</p>\n\
    <h3><a id=\"user-content-citing-spack\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#citing-spack\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Citing Spack</h3>\n<p>If you are referencing Spack in a publication,\
    \ please cite the following paper:</p>\n<ul>\n<li>Todd Gamblin, Matthew P. LeGendre,\
    \ Michael R. Collette, Gregory L. Lee,\nAdam Moody, Bronis R. de Supinski, and\
    \ W. Scott Futral.\n<a href=\"https://www.computer.org/csdl/proceedings/sc/2015/3723/00/2807623.pdf\"\
    \ rel=\"nofollow\"><strong>The Spack Package Manager: Bringing Order to HPC Software\
    \ Chaos</strong></a>.\nIn <em>Supercomputing 2015 (SC\u201915)</em>, Austin, Texas,\
    \ November 15-20 2015. LLNL-CONF-669890.</li>\n</ul>\n<p>On GitHub, you can copy\
    \ this citation in APA or BibTeX format via the \"Cite this repository\"\nbutton.\
    \ Or, see the comments in <code>CITATION.cff</code> for the raw BibTeX.</p>\n\
    <h2><a id=\"user-content-license\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #license\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>License</h2>\n\
    <p>Spack is distributed under the terms of both the MIT license and the\nApache\
    \ License (Version 2.0). Users may choose either license, at their\noption.</p>\n\
    <p>All new contributions must be made under both the MIT and Apache-2.0\nlicenses.</p>\n\
    <p>See <a href=\"https://github.com/spack/spack/blob/develop/LICENSE-MIT\">LICENSE-MIT</a>,\n\
    <a href=\"https://github.com/spack/spack/blob/develop/LICENSE-APACHE\">LICENSE-APACHE</a>,\n\
    <a href=\"https://github.com/spack/spack/blob/develop/COPYRIGHT\">COPYRIGHT</a>,\
    \ and\n<a href=\"https://github.com/spack/spack/blob/develop/NOTICE\">NOTICE</a>\
    \ for details.</p>\n<p>SPDX-License-Identifier: (Apache-2.0 OR MIT)</p>\n<p>LLNL-CODE-811652</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1662060167.0
robertu94/libpressio-sperr:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: robertu94/libpressio-sperr
  latest_release: null
  readme: '<h1><a id="user-content-libpressio-sperr" class="anchor" aria-hidden="true"
    href="#libpressio-sperr"><span aria-hidden="true" class="octicon octicon-link"></span></a>LibPressio-SPERR</h1>

    <p>A LibPressio compressor plugin for SPERR. Packaged seperately because of GPL
    Licensing</p>

    <h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>Via Spack</p>

    <pre><code>git clone https://github.com/robertu94/spack_packages robertu94_packages

    spack repo add ./robertu94_packages


    spack install libpressio-sperr

    </code></pre>

    <p>Manually Via CMake</p>

    <pre><code># install cmake, sperr, libpressio and dependencies first


    cmake -S . -B build -DCMAKE_INSTALL_PREFIX

    cmake --build build

    cmake --install

    </code></pre>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1658183703.0
robertu94/sz-zfp-zchecker:
  data_format: 2
  description: container for the ISC/SC compression tutorial
  filenames:
  - spack.yaml
  full_name: robertu94/sz-zfp-zchecker
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1652997619.0
salotz/scoot:
  data_format: 2
  description: Boost/STL for Scopes
  filenames:
  - spack.yaml
  full_name: salotz/scoot
  latest_release: null
  readme: "<h1><a id=\"user-content-scoot\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#scoot\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>scoot</h1>\n\
    <h2><a id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\"\
    \ href=\"#installation\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<p>The module is under <code>src/scoot</code>.\
    \ You can copy this subtree into your\nproject and then add it to the <code>package.path</code>\
    \ in your Scopes\n<code>_project.sc</code> file.</p>\n<h3><a id=\"user-content-with-spack\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#with-spack\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>With Spack</h3>\n<p>This module\
    \ is available as the <code>scoot</code> package in the\n<a href=\"https://github.com/salotz/snailpacks\"\
    >snailpacks</a> repository. This will pull in the necessary dependencies\nincluding\
    \ Scopes.</p>\n<div class=\"highlight highlight-source-shell\"><pre>  spack install\
    \ scoot</pre></div>\n<p>See the <a href=\"https://github.com/salotz/snailpacks\"\
    >snailpacks</a> documentation for more best practices of installing.</p>\n<h2><a\
    \ id=\"user-content-development-environment\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#development-environment\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Development Environment</h2>\n<p>We use <a href=\"\
    https://spack.io/\" rel=\"nofollow\">Spack</a> to install dependencies. First\
    \ install Spack.</p>\n<p>Then you'll need our custom repo of build recipes:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>  mkdir -p <span class=\"\
    pl-s\"><span class=\"pl-pds\">`</span>/.spack/repos</span>\n<span class=\"pl-s\"\
    >  git clone git@github.com:salotz/snailpacks.git <span class=\"pl-pds\">`</span></span>/.spack/repos/snailpacks\n\
    \  spack repo add <span class=\"pl-s\"><span class=\"pl-pds\">`</span>/resources/spack-repos/snailpacks</span></pre></div>\n\
    <p>Then you need to create an environment in this folder that will\ncontain the\
    \ headers and libraries etc., this will create this and\ninstall the packages:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>  make init</pre></div>\n\
    <p>Then you can activate the environment to get started:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>  spacktivate <span class=\"pl-c1\">.</span></pre></div>\n\
    <p>Run some commands:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> run the sanity entrypoint</span>\n\
    make sanity\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> run the tests</span>\n\
    make <span class=\"pl-c1\">test</span></pre></div>\n<p>To exit the environment\
    \ (i.e. unset the env variables):</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>  despacktivate</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1661112922.0
salotz/scopes-demos:
  data_format: 2
  description: null
  filenames:
  - 003_python-embed/spack.yaml
  - template/{{name}}/spack.yaml
  - 002_pong/spack.yaml
  - 001_chipmunk2d-hello-world/spack.yaml
  full_name: salotz/scopes-demos
  latest_release: null
  readme: "<h2><a id=\"user-content-running-the-demos\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#running-the-demos\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Running the Demos</h2>\n<p>You will need Spack installed\
    \ as well as the <a href=\"\">snailpacks</a> repo. The\nquick bootstrap script\
    \ should be enough to get going if you don't have\nthis installed already:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>curl --proto <span class=\"\
    pl-s\"><span class=\"pl-pds\">'</span>=https<span class=\"pl-pds\">'</span></span>\
    \ --tlsv1.2 -sSf https://raw.githubusercontent.com/salotz/snailpacks/master/bootstrap.sh\
    \ <span class=\"pl-k\">|</span> sh</pre></div>\n<p>Then for each demo you can\
    \ build the environment, activate it, and run\nthem.</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>  <span class=\"pl-c1\">cd</span> XXX_demo-name\n\
    \  make env\n  spacktivate <span class=\"pl-c1\">.</span>\n  make run</pre></div>\n\
    <h2><a id=\"user-content-creating-a-new-demo\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#creating-a-new-demo\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Creating a New Demo</h2>\n<p>You can use the template\
    \ for a quick start (requires <code>copier</code> &gt; 6):</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>copier template</pre></div>\n<p>To update\
    \ the</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1657842137.0
salotz/scopes-lib_copier-template:
  data_format: 2
  description: Copier template for a Scopes library
  filenames:
  - template/spack.yaml
  full_name: salotz/scopes-lib_copier-template
  latest_release: null
  readme: "<h1><a id=\"user-content-project-template-for-a-scopes-lang-library\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#project-template-for-a-scopes-lang-library\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Project\
    \ Template for a Scopes Lang Library</h1>\n<p>This is a project template generator\
    \ and updater using the\n<a href=\"https://github.com/copier-org/copier/\">copier</a>\
    \ tool for creating libraries for the <a href=\"http://scopes.rocks\" rel=\"nofollow\"\
    >Scopes</a> programming language.</p>\n<p>Please install from the latest copier\
    \ for this to work, not the latest\nstable release. Currently I am using\n<a href=\"\
    https://github.com/pypa/pipx\">pipx</a>:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>pipx install copier</pre></div>\n<h2><a id=\"user-content-generating-and-updating-a-project\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#generating-and-updating-a-project\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Generating\
    \ and Updating a Project</h2>\n<p>Then you can generate your project:</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>copier <span class=\"pl-s\"\
    ><span class=\"pl-pds\">'</span>gh:salotz/scopes-lib_copier-template<span class=\"\
    pl-pds\">'</span></span> name-of-folder</pre></div>\n<p>This should generate something\
    \ like the following (<code>repo_name = my-lib</code>):</p>\n<pre><code>name-of-folder\n\
    \u251C\u2500\u2500 __env.sc\n\u251C\u2500\u2500 Makefile\n\u251C\u2500\u2500 README.md\n\
    \u251C\u2500\u2500 spack.yaml\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500\
    \ my-lib\n        \u251C\u2500\u2500 init.sc\n        \u2514\u2500\u2500 ...\n\
    </code></pre>\n<p>You can update the project with:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre><span class=\"pl-c1\">cd</span> name-of-folder\n\
    copier update</pre></div>\n<p>See documentation of copier for more details.</p>\n\
    <h2><a id=\"user-content-development-environment\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#development-environment\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Development Environment</h2>\n<p>See the docs in <code>template/README.md.jinja</code>\
    \ that will be generated for\neach project.</p>\n<h2><a id=\"user-content-libraries-using-this-template\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#libraries-using-this-template\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Libraries\
    \ Using this Template</h2>\n<ul>\n<li><a href=\"https://github.com/salotz/raylib-scopes\"\
    >scopes-raylib</a></li>\n<li><a href=\"https://github.com/salotz/scopes-chipmunk2d\"\
    >scopes-chipmunk2d</a></li>\n</ul>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - copier-template
  - scopes-lang
  updated_at: 1648781021.0
salotz/snailpacks:
  data_format: 2
  description: Spack repo for multimedia development
  filenames:
  - examples/c-scons/spack.yaml
  - examples/c-cmake/spack.yaml
  - examples/c-wgpu/spack.yaml
  - examples/c-embed-python/spack.yaml
  - examples/c-embed-chibi/spack.yaml
  full_name: salotz/snailpacks
  latest_release: null
  stargazers_count: 1
  subscribers_count: 1
  topics:
  - spack
  - spack-repo
  - scopes-lang
  - multimedia
  - game-development
  - package-manager
  - development-environment
  updated_at: 1648089720.0
sayefsakin/auto_profiler:
  data_format: 2
  description: null
  filenames:
  - py_src/spack.yaml
  full_name: sayefsakin/auto_profiler
  latest_release: null
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1659512207.0
spack/spack-configs:
  data_format: 2
  description: Share Spack configuration files with other HPC sites
  filenames:
  - NERSC/perlmutter/e4s-22.05/spack.yaml
  full_name: spack/spack-configs
  latest_release: null
  readme: '<h1><a id="user-content-spack-configs" class="anchor" aria-hidden="true"
    href="#spack-configs"><span aria-hidden="true" class="octicon octicon-link"></span></a>Spack
    Configs</h1>

    <p>This is a repository that sites can use to share their configuration

    files for Spack.  You can contribute your own configuration files, or

    browse around and look at what others have done.</p>

    <h2><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>Spack is distributed under the terms of both the MIT license and the

    Apache License (Version 2.0). Users may choose either license, at their

    option.</p>

    <p>All new contributions must be made under both the MIT and Apache-2.0

    licenses.</p>

    <p>See <a href="https://github.com/spack/spack-configs/blob/master/LICENSE-MIT">LICENSE-MIT</a>,

    <a href="https://github.com/spack/spack-configs/blob/master/LICENSE-APACHE">LICENSE-APACHE</a>,

    <a href="https://github.com/spack/spack-configs/blob/master/COPYRIGHT">COPYRIGHT</a>,
    and

    <a href="https://github.com/spack/spack-configs/blob/master/NOTICE">NOTICE</a>
    for details.</p>

    <p>SPDX-License-Identifier: (Apache-2.0 OR MIT)</p>

    <p>LLNL-CODE-811652</p>

    '
  stargazers_count: 42
  subscribers_count: 22
  topics: []
  updated_at: 1660476953.0
srini009/serviz:
  data_format: 2
  description: Ascent visualization microservice built using the Mochi software stack
  filenames:
  - spack_laptop.yaml
  - spack.yaml
  full_name: srini009/serviz
  latest_release: v0.1.0
  readme: '<h1><a id="user-content-serviz-a-shared-in-situ-visualization-service"
    class="anchor" aria-hidden="true" href="#serviz-a-shared-in-situ-visualization-service"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>SERVIZ: A Shared In
    Situ Visualization Service</h1>

    <p>This is an experimental repo implementing a distributed Ascent visualization
    microservice.</p>

    <p>Inline and in transit visualization have arisen as popular models of in situ
    visualization for high performance computing (HPC) applications. Inline visualization
    is invoked through a library call on the HPC application (simulation code), while
    in transit methods involve the invocation of the visualization module on in transit
    resources. Compared to inline methods, in transit methods have the flexibility
    to run at a lower level of concurrency than the simulation code, allowing them
    to offer better efficiency for the visualization operation. The state-of-the-art
    in transit schemes are limited to employing a dedicated in transit resource for
    every HPC application.

    This results in significant idle time on the in transit resource and severely
    limits the cost savings that can be achieved over the inline model.

    This research proposes that a single, in transit visualization service be shared
    amongst multiple HPC applications to make efficient use of the in transit resources
    by reducing the idle time. We realize this idea through SERVIZ, a shared in transit
    visualization service. SERVIZ achieves cost savings of up to 40% over inline (at
    scale) and up to 4x reduction in idle time compared to a dedicated in transit
    implementation.

    In all, the results from this work identify that a shared in transit resource
    is an attractive approach for cost efficiency.</p>

    <p>SERVIZ is a hybrid MPI + RPC visualization program that can be partitioned
    into multiple "instances" that are each capable

    of serving multiple clients simultaneously. RPC is used to transfer simulation
    data to the SERVIZ instance, and MPI is subsequently used to

    parallelize the visualization operation.

    <a target="_blank" rel="noopener noreferrer" href="SERVIZ.svg"><img src="SERVIZ.svg"
    alt="SERVIZ" style="max-width: 100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1649894816.0
supercontainers/sc-tutorials:
  data_format: 2
  description: SC Tutorials
  filenames:
  - exercises/spack_containerize/spack.yaml
  full_name: supercontainers/sc-tutorials
  latest_release: null
  readme: '<h1><a id="user-content-getting-started-with-containers-on-hpc" class="anchor"
    aria-hidden="true" href="#getting-started-with-containers-on-hpc"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Getting Started with Containers on HPC</h1>

    <p>View this on the <a href="https://supercontainers.github.io/sc-tutorials/"
    rel="nofollow">Tutorial Homepage</a>.</p>

    <h2><a id="user-content-hpc-containers-tutorial-session" class="anchor" aria-hidden="true"
    href="#hpc-containers-tutorial-session"><span aria-hidden="true" class="octicon
    octicon-link"></span></a>HPC Containers Tutorial Session</h2>

    <p><a target="_blank" rel="noopener noreferrer" href="fig/ecp.jpg"><img src="fig/ecp.jpg"
    width="200" style="max-width: 100%;"></a><a target="_blank" rel="noopener noreferrer"
    href="fig/pawsey.png"><img src="fig/pawsey.png" width="200" style="max-width:
    100%;"></a><a target="_blank" rel="noopener noreferrer" href="fig/redhat.png"><img
    src="fig/redhat.png" width="200" style="max-width: 100%;"></a></p>

    <h2><a id="user-content-details" class="anchor" aria-hidden="true" href="#details"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Details</h2>

    <p>Full-day Tutorial Session</p>

    <p>Venue: Supercomputing Conference (SC 22)</p>

    <p>Date: Sunday November 13, 2022 8:30am - 5pm Central Standard Time (GMT -6)</p>

    <p>Location: Dallas TX, USA</p>

    <p>Link: <a href="https://sc22.supercomputing.org/presentation/?id=tut114&amp;sess=sess185"
    rel="nofollow">SC 2022 Tutorial Details</a></p>

    <p>Keywords: Containerized HPC, System Software and Runtime Systems, Scientific
    Software Development, DevOps</p>

    <h2><a id="user-content-abstract" class="anchor" aria-hidden="true" href="#abstract"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Abstract</h2>

    <p>Within just the past few years, the use of containers has revolutionized the
    way in which industries and enterprises have developed and deployed computational
    software and distributed systems. The containerization model has gained traction
    within the HPC community as well with the promise of improved reliability, reproducibility,
    portability, and levels of customization that were previously not possible on
    supercomputers. This adoption has been enabled by a number of HPC Container runtimes
    that have emerged including Singularity, Shifter, Enroot, Charliecloud and others.</p>

    <p>This hands-on tutorial looks to train users on the usability of containers
    on HPC resources. We will provide a detailed background on Linux containers, along
    with introductory hands-on experience building a container image, sharing the
    container and running it on a HPC cluster. Furthermore, the tutorial will provide
    more advanced information on how to run MPI-based and GPU-enabled HPC applications,
    how to optimize I/O intensive workflows, and how to setup GUI enabled interactive
    sessions. Cutting-edge examples will include machine learning and bioinformatics.
    Users will leave the tutorial with a solid foundational understanding of how to
    utilize containers with HPC resources through Shifter and Singularity, as well
    as an in-depth knowledge to deploy custom containers on their own resources.</p>

    <h2><a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>Please consult the website for prerequisites and recommended setup steps.</p>

    <h2><a id="user-content-questions" class="anchor" aria-hidden="true" href="#questions"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Questions</h2>

    <p>You can ask questions verbally or with this <a href="https://docs.google.com/document/d/11gMZ-T7iA5XiRWPLYIqX7Gqv7RMb-NF9kzGYHrnOi04/edit?usp=sharing"
    rel="nofollow">Google Doc</a>.

    Please append your question below the others in the document.</p>

    <p>We have also created a Slack Team for this.  The invitation link is <a href="https://join.slack.com/t/hpc-containers/shared_invite/enQtODI3NzY1NDU4OTk5LTUxOTgyOWJmYjIwOWI5YWU2MzBhZDI3Zjc1YmZmMjAxZjgzYzk4ZWEwNmFlNzlkOWI0MGNlZDNlMTBhYTBlOWY"
    rel="nofollow">here</a>.</p>

    <h2><a id="user-content-schedule---autogenerated-from-the-metadata" class="anchor"
    aria-hidden="true" href="#schedule---autogenerated-from-the-metadata"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Schedule - Autogenerated from the metadata</h2>

    '
  stargazers_count: 1
  subscribers_count: 7
  topics: []
  updated_at: 1649669614.0
sxs-collaboration/spectre:
  data_format: 2
  description: SpECTRE is a code for multi-scale, multi-physics problems in astrophysics
    and gravitational physics.
  filenames:
  - support/DevEnvironments/spack.yaml
  full_name: sxs-collaboration/spectre
  latest_release: v2022.09.02
  readme: "<p><a href=\"https://github.com/sxs-collaboration/spectre/blob/develop/LICENSE.txt\"\
    ><img src=\"https://camo.githubusercontent.com/83d3746e5881c1867665223424263d8e604df233d0a11aae0813e0414d433943/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667\"\
    \ alt=\"license\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-blue.svg\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://en.wikipedia.org/wiki/C%2B%2B#Standardization\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a3dbfd7a9a0364af5f02772460bf69fce89f741e10fb7c8e9aa3f26a0d96cfe7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f632532422532422d31372d626c75652e737667\"\
    \ alt=\"Standard\" data-canonical-src=\"https://img.shields.io/badge/c%2B%2B-17-blue.svg\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://github.com/sxs-collaboration/spectre/actions\"\
    ><img src=\"https://github.com/sxs-collaboration/spectre/workflows/Tests/badge.svg?branch=develop\"\
    \ alt=\"Build Status\" style=\"max-width: 100%;\"></a>\n<a href=\"https://coveralls.io/github/sxs-collaboration/spectre?branch=develop\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/e909837c9640462fc3f2587028319e5f5dc6198453d97af6b12696ddeb34930b/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f7378732d636f6c6c61626f726174696f6e2f737065637472652f62616467652e7376673f6272616e63683d646576656c6f70\"\
    \ alt=\"Coverage Status\" data-canonical-src=\"https://coveralls.io/repos/github/sxs-collaboration/spectre/badge.svg?branch=develop\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://codecov.io/gh/sxs-collaboration/spectre\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2b0d1a9c279878e18a98627f608e86ad2f22a730eebd7713b9ba9b173a16cdbb/68747470733a2f2f636f6465636f762e696f2f67682f7378732d636f6c6c61626f726174696f6e2f737065637472652f6272616e63682f646576656c6f702f67726170682f62616467652e737667\"\
    \ alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/sxs-collaboration/spectre/branch/develop/graph/badge.svg\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://github.com/sxs-collaboration/spectre/releases/tag/v2022.09.02\"\
    ><img src=\"https://camo.githubusercontent.com/fc16c77b4bdaa976a3569aec79029bfdecccfb10e4482096e985263ec8717c40/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72656c656173652d76323032322e30392e30322d696e666f726d6174696f6e616c\"\
    \ alt=\"release\" data-canonical-src=\"https://img.shields.io/badge/release-v2022.09.02-informational\"\
    \ style=\"max-width: 100%;\"></a>\n<a href=\"https://doi.org/10.5281/zenodo.7045030\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2d3865497ff5ed4773e26e9272b9060c5f2afc42e8e04788627c2d3dc8e25adb/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f646f692f31302e353238312f7a656e6f646f2e373034353033302e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/doi/10.5281/zenodo.7045030.svg\"\
    \ style=\"max-width: 100%;\"></a></p>\n<h2><a id=\"user-content-what-is-spectre\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#what-is-spectre\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>What is SpECTRE?</h2>\n<p>SpECTRE\
    \ is an open-source code for multi-scale, multi-physics problems\nin astrophysics\
    \ and gravitational physics. In the future, we hope that\nit can be applied to\
    \ problems across discipline boundaries in fluid\ndynamics, geoscience, plasma\
    \ physics, nuclear physics, and\nengineering. It runs at petascale and is designed\
    \ for future exascale\ncomputers.</p>\n<p>SpECTRE is being developed in support\
    \ of our collaborative Simulating\neXtreme Spacetimes (SXS) research program into\
    \ the multi-messenger\nastrophysics of neutron star mergers, core-collapse supernovae,\
    \ and\ngamma-ray bursts.</p>\n<h2><a id=\"user-content-citing-spectre\" class=\"\
    anchor\" aria-hidden=\"true\" href=\"#citing-spectre\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Citing SpECTRE</h2>\n<p>Please cite\
    \ SpECTRE in any publications that make use of its code or data. Cite\nthe latest\
    \ version that you use in your publication. The DOI for this version\nis:</p>\n\
    <ul>\n<li>DOI: <a href=\"https://doi.org/10.5281/zenodo.7045030\" rel=\"nofollow\"\
    >10.5281/zenodo.7045030</a>\n</li>\n</ul>\n<p>You can cite this BibTeX entry in\
    \ your publication:</p>\n\n\n<div class=\"highlight highlight-text-bibtex\"><pre><span\
    \ class=\"pl-k\">@software</span>{<span class=\"pl-en\">spectrecode</span>,\n\
    \    <span class=\"pl-s\">author</span> = <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>Deppe, Nils and Throwe, William and Kidder, Lawrence E. and\
    \ Vu,</span>\n<span class=\"pl-s\">Nils L. and H\\'ebert, Fran\\c{c}ois and Moxon,\
    \ Jordan and Armaza, Crist\\'obal and</span>\n<span class=\"pl-s\">Bonilla, Gabriel\
    \ S. and Kim, Yoonsoo and Kumar, Prayush and Lovelace, Geoffrey</span>\n<span\
    \ class=\"pl-s\">and Macedo, Alexandra and Nelli, Kyle C. and O'Shea, Eamonn and\
    \ Pfeiffer, Harald</span>\n<span class=\"pl-s\">P. and Scheel, Mark A. and Teukolsky,\
    \ Saul A. and Wittek, Nikolas A. and</span>\n<span class=\"pl-s\">others<span\
    \ class=\"pl-pds\">\"</span></span>,\n    <span class=\"pl-s\">title</span> =\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>\\texttt{SpECTRE v2022.09.02}<span\
    \ class=\"pl-pds\">\"</span></span>,\n    <span class=\"pl-s\">version</span>\
    \ = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2022.09.02<span class=\"\
    pl-pds\">\"</span></span>,\n    <span class=\"pl-s\">publisher</span> = <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>Zenodo<span class=\"pl-pds\"\
    >\"</span></span>,\n    <span class=\"pl-s\">doi</span> = <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>10.5281/zenodo.7045030<span class=\"pl-pds\"\
    >\"</span></span>,\n    <span class=\"pl-s\">url</span> = <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>https://spectre-code.org<span class=\"pl-pds\"\
    >\"</span></span>,\n    <span class=\"pl-s\">howpublished</span> =\n<span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>\\href{https://doi.org/10.5281/zenodo.7045030}{10.5281/zenodo.7045030}<span\
    \ class=\"pl-pds\">\"</span></span>,\n    <span class=\"pl-s\">license</span>\
    \ = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>MIT<span class=\"pl-pds\"\
    >\"</span></span>,\n    <span class=\"pl-s\">year</span> = <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>2022<span class=\"pl-pds\">\"</span></span>,\n\
    \    <span class=\"pl-s\">month</span> = <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>9<span class=\"pl-pds\">\"</span></span>\n}</pre></div>\n\n<p>To aid\
    \ reproducibility of your scientific results with SpECTRE, we recommend you\n\
    keep track of the version(s) you used and report this information in your\npublication.\
    \ We also recommend you supply the YAML input files and, if\nappropriate, any\
    \ additional C++ code you wrote to compile SpECTRE executables as\nsupplemental\
    \ material to the publication.</p>\n<p>See our <a href=\"https://spectre-code.org/publication_policies.html\"\
    \ rel=\"nofollow\">publication policy</a>\nfor more information.</p>\n<h2><a id=\"\
    user-content-viewing-documentation\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #viewing-documentation\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Viewing Documentation</h2>\n<p>The documentation can be viewed at\
    \ <a href=\"https://spectre-code.org/\" rel=\"nofollow\">https://spectre-code.org/</a>.</p>\n"
  stargazers_count: 115
  subscribers_count: 14
  topics: []
  updated_at: 1661483140.0
trilinos/ForTrilinos:
  data_format: 2
  description: ForTrilinos provides portable object-oriented Fortran interfaces to
    Trilinos C++ packages.
  filenames:
  - scripts/spack.yaml
  full_name: trilinos/ForTrilinos
  latest_release: v2.1.0
  readme: '<h1><a id="user-content-fortrilinos" class="anchor" aria-hidden="true"
    href="#fortrilinos"><span aria-hidden="true" class="octicon octicon-link"></span></a>ForTrilinos</h1>

    <p><a href="https://cloud.cees.ornl.gov/jenkins-ci/job/ForTrilinos-master-continuous"
    rel="nofollow"><img src="https://camo.githubusercontent.com/857fffb6b672ed62abe998b01a81c3932111fcba10541918cb2f938f414440e6/68747470733a2f2f636c6f75642e636565732e6f726e6c2e676f762f6a656e6b696e732d63692f6275696c645374617475732f69636f6e3f6a6f623d466f725472696c696e6f732d6d61737465722d636f6e74696e756f7573"
    alt="Build Status" data-canonical-src="https://cloud.cees.ornl.gov/jenkins-ci/buildStatus/icon?job=ForTrilinos-master-continuous"
    style="max-width: 100%;"></a>

    <a href="http://fortrilinos.readthedocs.io/en/latest/?badge=latest" rel="nofollow"><img
    src="https://camo.githubusercontent.com/e261f09cffcfcbf7e647f541614bf7912e3018ccd3a085f035a1219a854f5867/687474703a2f2f72656164746865646f63732e6f72672f70726f6a656374732f666f727472696c696e6f732f62616467652f3f76657273696f6e3d6c6174657374"
    alt="Documentation Status" data-canonical-src="http://readthedocs.org/projects/fortrilinos/badge/?version=latest"
    style="max-width: 100%;"></a>

    <a href="https://codecov.io/gh/trilinos/ForTrilinos/branch/develop" rel="nofollow"><img
    src="https://camo.githubusercontent.com/fbeea009914f87218441791dba76a1a512b7c287749f94ff47d7b76f49902d23/68747470733a2f2f636f6465636f762e696f2f67682f7472696c696e6f732f466f725472696c696e6f732f6272616e63682f646576656c6f702f67726170682f62616467652e737667"
    alt="codecov" data-canonical-src="https://codecov.io/gh/trilinos/ForTrilinos/branch/develop/graph/badge.svg"
    style="max-width: 100%;"></a></p>

    <p><a href="http://trilinos.org/packages/fortrilinos" rel="nofollow">ForTrilinos</a>
    is a part of the <a href="http://trilinos.org" rel="nofollow">Trilinos</a> project
    and provides object-oriented Fortran interfaces to Trilinos C++ packages.</p>

    <p>This is the new effort to provide Fortran interfaces to Trilinos through

    automatic code generation using SWIG. The previous effort (ca. 2008-2012) can

    be obtained by downloading Trilinos releases prior to 12.12. See <a href="https://fortrilinos.readthedocs.io/en/latest/install.html#version-compatibility"
    rel="nofollow">the

    documentation</a> for details on version compatibility.</p>

    <h2><a id="user-content-provided-functionality" class="anchor" aria-hidden="true"
    href="#provided-functionality"><span aria-hidden="true" class="octicon octicon-link"></span></a>Provided
    functionality</h2>

    <p>ForTrilinos provides Fortran interfaces for the following capabilities:</p>

    <ul>

    <li>Parameter lists and XML parsers (through Teuchos);</li>

    <li>Distributed linear algebra object including sparse graphs, sparse matrices,
    and dense vectors (through Tpetra);</li>

    <li>Linear solvers and preconditioners (through Stratimikos, Ifpack2, Belos, MueLu);</li>

    <li>Eigen solvers (through Anasazi).</li>

    </ul>

    <h2><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h2>

    <ul>

    <li>

    <p><a href="https://fortrilinos.readthedocs.org" rel="nofollow">Documentation</a></p>

    </li>

    <li>

    <p><a href="https://trilinos.github.io/ForTrilinos/" rel="nofollow">Summary</a></p>

    </li>

    </ul>

    <h2><a id="user-content-installing-fortrilinos" class="anchor" aria-hidden="true"
    href="#installing-fortrilinos"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installing
    ForTrilinos</h2>

    <p>Please consult the documentation available <a href="https://fortrilinos.readthedocs.io/en/latest/install.html"
    rel="nofollow">here</a>.</p>

    <h2><a id="user-content-questions-bug-reporting-and-issue-tracking" class="anchor"
    aria-hidden="true" href="#questions-bug-reporting-and-issue-tracking"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Questions, Bug Reporting, and Issue Tracking</h2>

    <p>Questions, bug reporting and issue tracking are provided by GitHub. Please

    report all bugs by creating a new issue with the bug tag. You can ask

    questions by creating a new issue with the question tag.</p>

    <h2><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <p>We encourage you to contribute to ForTrilinos! Please check out the

    <a href="CONTRIBUTING.md">guidelines</a> about how to proceed.</p>

    <h2><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>ForTrilinos is licensed under a BSD license.</p>

    '
  stargazers_count: 24
  subscribers_count: 10
  topics:
  - trilinos
  - fortran
  - swig
  - scientific-computing
  updated_at: 1654781824.0
ukri-excalibur/excalibur-tests:
  data_format: 2
  description: Performance benchmarks and regression tests for the ExCALIBUR project
  filenames:
  - spack-environments/tursa/cpu/spack.yaml
  - spack-environments/csd3-icelake/compute-node/spack.yaml
  - spack-environments/csd3-skylake/compute-node/spack.yaml
  - spack-environments/tesseract/compute-node/spack.yaml
  - spack-environments/myriad/compute-node/spack.yaml
  - spack-environments/dial3/compute-node/spack.yaml
  full_name: ukri-excalibur/excalibur-tests
  latest_release: null
  readme: "<h1><a id=\"user-content-excalibur-tests\" class=\"anchor\" aria-hidden=\"\
    true\" href=\"#excalibur-tests\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>ExCALIBUR tests</h1>\n<p>Performance benchmarks and regression tests\
    \ for the ExCALIBUR project.</p>\n<p>These benchmarks are based on a similar project\
    \ by\n<a href=\"https://github.com/stackhpc/hpc-tests\">StackHPC</a>.</p>\n<p><em><strong>Note</strong>:\
    \ at the moment the ExCALIBUR benchmarks are a work-in-progress.</em></p>\n<h2><a\
    \ id=\"user-content-requirements\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #requirements\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Requirements</h2>\n\
    <h3><a id=\"user-content-spack\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #spack\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Spack</h3>\n\
    <p><em><strong>Note</strong>: in some HPC facilities there may be already a central\
    \ Spack\ninstallation available.  In principle you should be able to use that\
    \ one (you\nonly need to have <code>spack</code> in the <code>PATH</code>), but\
    \ you may need an up-to-date version\nof Spack in order to install some packages.\
    \  Instructions below show you how to\ninstall Spack locally.</em></p>\n<p><a\
    \ href=\"https://spack.io/\" rel=\"nofollow\">Spack</a> is a package manager specifically\
    \ designed for HPC\nfacilities.  Follow the <a href=\"https://spack.readthedocs.io/en/latest/getting_started.html\"\
    \ rel=\"nofollow\">official\ninstructions</a> to\ninstall the latest version of\
    \ Spack.</p>\n<p>In order to use Spack in ReFrame, the framework we use to run\
    \ the benchmarks\n(see below), the directory where the <code>spack</code> program\
    \ is installed needs to be in\nthe <code>PATH</code> environment variable.  This\
    \ can be achieved for instance by running\nthe commands to get shell support described\
    \ in Spack documentation, which you\ncan also add to your shell init script to\
    \ do it automatically in every session.\nFor example, if you use a shell of the\
    \ family bash/zsh/sh you can add to your\ninit script:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre><span class=\"pl-k\">export</span> SPACK_ROOT=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>/path/to/spack<span class=\"\
    pl-pds\">\"</span></span>\n<span class=\"pl-k\">if</span> [ <span class=\"pl-k\"\
    >-f</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"\
    pl-smi\">${SPACK_ROOT}</span>/share/spack/setup-env.sh<span class=\"pl-pds\">\"\
    </span></span> ]<span class=\"pl-k\">;</span> <span class=\"pl-k\">then</span>\n\
    \    <span class=\"pl-c1\">source</span> <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span><span class=\"pl-smi\">${SPACK_ROOT}</span>/share/spack/setup-env.sh<span\
    \ class=\"pl-pds\">\"</span></span>\n<span class=\"pl-k\">fi</span></pre></div>\n\
    <p>replacing <code>/path/to/spack</code> with the actual path to your Spack installation.</p>\n\
    <p>ReFrame requires a <a href=\"https://spack.readthedocs.io/en/latest/environments.html\"\
    \ rel=\"nofollow\">Spack\nEnvironment</a>.  We\nprovide Spack environments for\
    \ some of the systems that are part of the\nExCALIBUR and DiRac projects.  If\
    \ you want to use a different Spack environment,\nset the environment variable\
    \ <code>EXCALIBUR_SPACK_ENV</code> to the path of the directory\nwhere the environment\
    \ is.  If this is not set, ReFrame will try to use the\nenvironment for the current\
    \ system, if known, otherwise it will automatically\ncreate a very basic environment.</p>\n\
    <h3><a id=\"user-content-reframe\" class=\"anchor\" aria-hidden=\"true\" href=\"\
    #reframe\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>ReFrame</h3>\n\
    <p><a href=\"https://reframe-hpc.readthedocs.io/en/stable/\" rel=\"nofollow\"\
    >ReFrame</a> is a high-level\nframework for writing regression tests for HPC systems.\
    \  For our tests we\nrequire ReFrame 3.11.0.  Follow the <a href=\"https://reframe-hpc.readthedocs.io/en/stable/started.html\"\
    \ rel=\"nofollow\">official\ninstructions</a> to\ninstall this package.  Note\
    \ that ReFrame requires Python 3.6: in your HPC system\nyou may need to load a\
    \ specific module to have this version of Python available.</p>\n<p>We provide\
    \ a ReFrame configuration file with the settings of some systems that\nare part\
    \ of the ExCALIBUR project.  You can point ReFrame to this file by\nsetting the\n\
    <a href=\"https://reframe-hpc.readthedocs.io/en/stable/manpage.html#envvar-RFM_CONFIG_FILE\"\
    \ rel=\"nofollow\"><code>RFM_CONFIG_FILE</code></a>\nenvironment variable:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span>\
    \ RFM_CONFIG_FILE=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"\
    pl-smi\">${PWD}</span>/reframe_config.py<span class=\"pl-pds\">\"</span></span></pre></div>\n\
    <p>If you want to use a different ReFrame configuration file, for example because\n\
    you use a different system, you can set this environment variable to the path\
    \ of\nthat file.</p>\n<p><strong>Note</strong>: in order to use the Spack build\
    \ system in ReFrame, the <code>spack</code>\nexecutable must be in the <code>PATH</code>,\
    \ also on the computing nodes of a cluster, if\nyou want to run your benchmarks\
    \ on them.  Note that by default ReFrame uses</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-k\">!</span><span class=\"pl-c\"><span class=\"pl-c\">#</span>/bin/bash</span></pre></div>\n\
    <p>as <a href=\"https://en.wikipedia.org/wiki/Shebang_(Unix)\" rel=\"nofollow\"\
    >shebang</a>, which would not load\nthe user's init script.  If you have added\
    \ Spack to your <code>PATH</code> within your init\nscript, you may want to set\
    \ the\n<a href=\"https://reframe-hpc.readthedocs.io/en/stable/manpage.html#envvar-RFM_USE_LOGIN_SHELL\"\
    \ rel=\"nofollow\"><code>RFM_USE_LOGIN_SHELL</code></a>\nenvironment variable\
    \ in order to make ReFrame use</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-k\">!</span><span class=\"pl-c\"><span class=\"pl-c\">#</span>/bin/bash\
    \ -l</span></pre></div>\n<p>as shebang line, instead.</p>\n<h2><a id=\"user-content-usage\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#usage\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Usage</h2>\n<p>Once you have set up\
    \ Spack and ReFrame, you can execute a benchmark with</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>reframe -c apps/BENCH_NAME -r --performance-report</pre></div>\n\
    <p>where <code>apps/BENCH_NAME</code> is the directory where the benchmark is.\
    \  The command\nabove supposes you have the program <code>reframe</code> in your\
    \ PATH, if it is not the\ncase you can also call <code>reframe</code> with its\
    \ relative or absolute path.  For\nexample, to run the Sombrero benchmark in the\
    \ <code>apps/sombrero</code> directory you can\nuse</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>reframe -c apps/sombrero -r --performance-report</pre></div>\n\
    <p>For benchmark using the Spack build system, the tests define a default Spack\
    \ specification\nto be installed in the environment, but users can change it when\
    \ invoking ReFrame on the\ncommand line with the\n<a href=\"https://reframe-hpc.readthedocs.io/en/stable/manpage.html#cmdoption-S\"\
    \ rel=\"nofollow\"><code>-S</code></a> option to set\nthe <code>spack_spec</code>\
    \ variable:</p>\n<pre><code>reframe -c apps/sombrero -r --performance-report -S\
    \ spack_spec='sombrero@2021-08-16%intel'\n</code></pre>\n<h3><a id=\"user-content-selecting-system-and-queue-access-options\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#selecting-system-and-queue-access-options\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Selecting\
    \ system and queue access options</h3>\n<p>The provided ReFrame configuration\
    \ file contains the settings for multiple systems.  If you\nuse it, the automatic\
    \ detection of the system may fail, as some systems may use clashing\nhostnames.\
    \  You can always use the flag <a href=\"https://reframe-hpc.readthedocs.io/en/stable/manpage.html#cmdoption-system\"\
    \ rel=\"nofollow\"><code>--system NAME:PARTITION</code></a>\nto specify the system\
    \ (and optionally the partition) to use.</p>\n<p>Additionally, if submitting jobs\
    \ to the compute nodes requires additional options, like for\nexample the resource\
    \ group you belong to (for example <code>--account=...</code> for Slurm), you\
    \ have\nto pass the command line flag\n<a href=\"https://reframe-hpc.readthedocs.io/en/stable/manpage.html#cmdoption-J\"\
    \ rel=\"nofollow\"><code>--job-option=...</code></a>\nto <code>reframe</code>\
    \ (e.g., <code>--job-option='--account=...'</code>).</p>\n<h3><a id=\"user-content-unsupported-systems\"\
    \ class=\"anchor\" aria-hidden=\"true\" href=\"#unsupported-systems\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Unsupported systems</h3>\n<p>The\
    \ configuration provided in <a href=\"./reframe_config.py\"><code>reframe_config.py</code></a>\
    \ lets you run the\nbenchmarks on systems for which the configuration has been\
    \ already contributed.  However you\ncan still use this framework on any system\
    \ by choosing the \"generic\" system with <code>--system generic</code>, or using\
    \ your own ReFrame configuration.  Note, however, that if you use the\n\"generic\"\
    \ system, ReFrame will not know anything about the queue manager of your system,\
    \ if\nany, or the MPI launcher.  For the benchmarks using the Spack build system,\
    \ if you choose\nthe \"generic\" system, a new empty Spack environment will be\
    \ automatically created in\n<code>spack-environments/generic</code>.  In any case,\
    \ you can always make the benchmarks use a\ndifferent Spack environment by setting\
    \ the environment variable <code>EXCALIBUR_SPACK_ENV</code>\ndescribed above.</p>\n\
    <h2><a id=\"user-content-contributing-new-systems-or-benchmarks\" class=\"anchor\"\
    \ aria-hidden=\"true\" href=\"#contributing-new-systems-or-benchmarks\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contributing\
    \ new systems or benchmarks</h2>\n<p>Feel free to add new benchmark apps or support\
    \ new systems that are part of the\nExCALIBUR benchmarking collaboration.  Read\n\
    <a href=\"./CONTRIBUTING.md\"><code>CONTRIBUTING.md</code></a> for more details.</p>\n"
  stargazers_count: 2
  subscribers_count: 7
  topics: []
  updated_at: 1657545914.0
uturuncoglu/testing:
  data_format: 2
  description: It is used for component testing and GitHub Action implementation
  filenames:
  - spack.yaml
  full_name: uturuncoglu/testing
  latest_release: null
  readme: '<h1><a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>testing</h1>

    <p>It is used for component testing and GitHub Action implementation</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1657212117.0
vanderwb/spack-crayenv:
  data_format: 2
  description: null
  filenames:
  - spack.yaml
  full_name: vanderwb/spack-crayenv
  latest_release: null
  readme: '<h1><a id="user-content-ncar-spack-deployment" class="anchor" aria-hidden="true"
    href="#ncar-spack-deployment"><span aria-hidden="true" class="octicon octicon-link"></span></a>NCAR
    Spack Deployment</h1>

    <p>This branch tracks the <strong>production</strong> deployment of Spack for
    the following configuration:</p>

    <table>

    <thead>

    <tr>

    <th></th>

    <th>crayenv</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Creation date</td>

    <td>Fri Jun 17 18:21:49 MDT 2022</td>

    </tr>

    <tr>

    <td>ncar-spack commit</td>

    <td>24319d896f9bd0d58f5327fd89f251d80b844198</td>

    </tr>

    <tr>

    <td>Host version</td>

    <td>22.02</td>

    </tr>

    <tr>

    <td>Spack version</td>

    <td>v0.18.0</td>

    </tr>

    <tr>

    <td>Deployment path</td>

    <td>/glade/scratch/vanderwb/spack-tests/crayenv/22.02</td>

    </tr>

    <tr>

    <td>Environments path</td>

    <td>/glade/scratch/vanderwb/spack-tests/crayenv/22.02/envs</td>

    </tr>

    </tbody>

    </table>

    <p>This repository should <em>only</em> be updated via the <code>publish</code>
    script contained in the build environment. Any manual changes to this branch will
    cause headaches when you or another consultant attempt to publish new packages!</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1655513096.0
